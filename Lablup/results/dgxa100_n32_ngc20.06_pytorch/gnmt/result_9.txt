+ echo 'Beginning trial 2 of 5'
Beginning trial 2 of 5
+ srun --ntasks=32 --container-name=rnn_translator python -c '
import mlperf_log_utils
from mlperf_logging.mllog import constants
mlperf_log_utils.mlperf_submission_log(constants.GNMT)'
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
:::MLLOG {"namespace": "", "time_ms": 1592471602183, "event_type": "POINT_IN_TIME", "key": "submission_benchmark", "value": "gnmt", "metadata": {"file": "/workspace/rnn_translator/mlperf_log_utils.py", "lineno": 19}}
:::MLLOG {"namespace": "", "time_ms": 1592471602224, "event_type": "POINT_IN_TIME", "key": "submission_org", "value": "NVIDIA", "metadata": {"file": "/workspace/rnn_translator/mlperf_log_utils.py", "lineno": 24}}
:::MLLOG {"namespace": "", "time_ms": 1592471602224, "event_type": "POINT_IN_TIME", "key": "submission_division", "value": "closed", "metadata": {"file": "/workspace/rnn_translator/mlperf_log_utils.py", "lineno": 28}}
:::MLLOG {"namespace": "", "time_ms": 1592471602224, "event_type": "POINT_IN_TIME", "key": "submission_status", "value": "onprem", "metadata": {"file": "/workspace/rnn_translator/mlperf_log_utils.py", "lineno": 32}}
:::MLLOG {"namespace": "", "time_ms": 1592471602224, "event_type": "POINT_IN_TIME", "key": "submission_platform", "value": "32xNVIDIA DGX A100", "metadata": {"file": "/workspace/rnn_translator/mlperf_log_utils.py", "lineno": 36}}
+ '[' 1 -eq 1 ']'
+ srun --ntasks=32 bash -c 'echo -n '\''Clearing cache on '\'' && hostname && sync && sudo /sbin/sysctl vm.drop_caches=3'
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
Clearing cache on luna-0211
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
Clearing cache on luna-0184
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
Clearing cache on luna-0206
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
Clearing cache on luna-0188
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
Clearing cache on luna-0185
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
Clearing cache on luna-0190
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
Clearing cache on luna-0210
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
Clearing cache on luna-0183
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
Clearing cache on luna-0207
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
Clearing cache on luna-0240
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
Clearing cache on luna-0181
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
Clearing cache on luna-0186
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
Clearing cache on luna-0189
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
Clearing cache on luna-0191
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
Clearing cache on luna-0177
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
Clearing cache on luna-0182
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
Clearing cache on luna-0239
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
Clearing cache on luna-0208
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
Clearing cache on luna-0235
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
Clearing cache on luna-0174
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
Clearing cache on luna-0178
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
Clearing cache on luna-0175
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
Clearing cache on luna-0187
Clearing cache on luna-0209
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
Clearing cache on luna-0205
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
Clearing cache on luna-0176
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
Clearing cache on luna-0238
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
Clearing cache on luna-0237
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
Clearing cache on luna-0233
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
Clearing cache on luna-0234
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
Clearing cache on luna-0236
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
Clearing cache on luna-0173
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
+ srun --ntasks=32 --container-name=rnn_translator python -c '
from mlperf_logging.mllog import constants
from seq2seq.utils import log_event
log_event(key=constants.CACHE_CLEAR, value=True)'
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
:::MLLOG {"namespace": "", "time_ms": 1592471608213, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
:::MLLOG {"namespace": "", "time_ms": 1592471608238, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
:::MLLOG {"namespace": "", "time_ms": 1592471608253, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
:::MLLOG {"namespace": "", "time_ms": 1592471608256, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
:::MLLOG {"namespace": "", "time_ms": 1592471608255, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
:::MLLOG {"namespace": "", "time_ms": 1592471608263, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
:::MLLOG {"namespace": "", "time_ms": 1592471608267, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
:::MLLOG {"namespace": "", "time_ms": 1592471608266, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
:::MLLOG {"namespace": "", "time_ms": 1592471608281, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
:::MLLOG {"namespace": "", "time_ms": 1592471608283, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
:::MLLOG {"namespace": "", "time_ms": 1592471608284, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
:::MLLOG {"namespace": "", "time_ms": 1592471608284, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
:::MLLOG {"namespace": "", "time_ms": 1592471608292, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
:::MLLOG {"namespace": "", "time_ms": 1592471608294, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
:::MLLOG {"namespace": "", "time_ms": 1592471608296, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
:::MLLOG {"namespace": "", "time_ms": 1592471608299, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
:::MLLOG {"namespace": "", "time_ms": 1592471608304, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
:::MLLOG {"namespace": "", "time_ms": 1592471608306, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
:::MLLOG {"namespace": "", "time_ms": 1592471608305, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
:::MLLOG {"namespace": "", "time_ms": 1592471608308, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
:::MLLOG {"namespace": "", "time_ms": 1592471608310, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
:::MLLOG {"namespace": "", "time_ms": 1592471608318, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
:::MLLOG {"namespace": "", "time_ms": 1592471608321, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
:::MLLOG {"namespace": "", "time_ms": 1592471608326, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
:::MLLOG {"namespace": "", "time_ms": 1592471608333, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
:::MLLOG {"namespace": "", "time_ms": 1592471608348, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
:::MLLOG {"namespace": "", "time_ms": 1592471608354, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
:::MLLOG {"namespace": "", "time_ms": 1592471608355, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
:::MLLOG {"namespace": "", "time_ms": 1592471608356, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
:::MLLOG {"namespace": "", "time_ms": 1592471608359, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
:::MLLOG {"namespace": "", "time_ms": 1592471608365, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
:::MLLOG {"namespace": "", "time_ms": 1592471608385, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
+ srun --mpi=none --ntasks=256 --ntasks-per-node=8 --container-name=rnn_translator --container-mounts=/lustre/fsr/datasets/wmt16_de_en:/data,/raid/scratch/svcnvdlfw/gnmt:/preproc_data,/lustre/fsw/mlperf-ci/13842451/results:/results ./run_and_time.sh
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
STARTING TIMING RUN AT 2020-06-18 02:13:30 AM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
STARTING TIMING RUN AT 2020-06-18 02:13:30 AM
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 3    --dwu-num-chunks 2    --dwu-num-rs-pg 1    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-overlap-reductions    --dwu-grad-norm    '
+ RESULTS_DIR=gnmt_wmt16
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    '
+ LR=4.0e-3
+ MATH=fp16
+ declare -a CMD
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ '[' -n 7 ']'
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 3    --dwu-num-chunks 2    --dwu-num-rs-pg 1    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-overlap-reductions    --dwu-grad-norm    '
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    '
+ '[' 256 -gt 32 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ MATH=fp16
+ declare -a CMD
+ '[' -n 2 ']'
+ '[' -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ']'
Using TCMalloc
+ echo 'Using TCMalloc'
+ '[' 256 -gt 32 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ export LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
running benchmark
+ echo 'running benchmark'
+ '[' -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ']'
Using TCMalloc
running benchmark
+ echo 'Using TCMalloc'
+ export LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ echo 'running benchmark'
STARTING TIMING RUN AT 2020-06-18 02:13:30 AM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
STARTING TIMING RUN AT 2020-06-18 02:13:30 AM
+ LR=4.0e-3
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ RESULTS_DIR=gnmt_wmt16
+ MAX_SEQ_LEN=75
+ LR=4.0e-3
+ NUMEPOCHS=12
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 3    --dwu-num-chunks 2    --dwu-num-rs-pg 1    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-overlap-reductions    --dwu-grad-norm    '
+ WARMUP_STEPS=200
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    '
+ MATH=fp16
+ REMAIN_STEPS=2259
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
+ declare -a CMD
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 3    --dwu-num-chunks 2    --dwu-num-rs-pg 1    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-overlap-reductions    --dwu-grad-norm    '
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 4 ']'
+ '[' 256 -gt 32 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ']'
+ '[' -n 1 ']'
+ '[' 256 -gt 32 ']'
Using TCMalloc
+ echo 'Using TCMalloc'
+ export LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
running benchmark
+ echo 'running benchmark'
+ '[' -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ']'
+ echo 'Using TCMalloc'
+ export LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
Using TCMalloc
running benchmark
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
STARTING TIMING RUN AT 2020-06-18 02:13:30 AM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 3    --dwu-num-chunks 2    --dwu-num-rs-pg 1    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-overlap-reductions    --dwu-grad-norm    '
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 6 ']'
+ '[' 256 -gt 32 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ']'
Using TCMalloc
+ echo 'Using TCMalloc'
+ export LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
running benchmark
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
STARTING TIMING RUN AT 2020-06-18 02:13:30 AM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 3    --dwu-num-chunks 2    --dwu-num-rs-pg 1    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-overlap-reductions    --dwu-grad-norm    '
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 0 ']'
+ '[' 256 -gt 32 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
STARTING TIMING RUN AT 2020-06-18 02:13:30 AM
+ '[' -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ']'
+ echo 'Using TCMalloc'
Using TCMalloc
+ export LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
running benchmark
+ echo 'running benchmark'
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 3    --dwu-num-chunks 2    --dwu-num-rs-pg 1    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-overlap-reductions    --dwu-grad-norm    '
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    '
+ MATH=fp16
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
+ declare -a CMD
+ '[' -n 3 ']'
+ '[' 256 -gt 32 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ']'
Using TCMalloc
+ echo 'Using TCMalloc'
+ export LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
running benchmark
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
STARTING TIMING RUN AT 2020-06-18 02:13:30 AM
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 3    --dwu-num-chunks 2    --dwu-num-rs-pg 1    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-overlap-reductions    --dwu-grad-norm    '
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 5 ']'
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ '[' 256 -gt 32 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ']'
Using TCMalloc
+ echo 'Using TCMalloc'
+ export LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ echo 'running benchmark'
running benchmark
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
STARTING TIMING RUN AT 2020-06-18 02:13:30 AM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 3    --dwu-num-chunks 2    --dwu-num-rs-pg 1    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-overlap-reductions    --dwu-grad-norm    '
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 7 ']'
Using TCMalloc
STARTING TIMING RUN AT 2020-06-18 02:13:30 AM
+ '[' 256 -gt 32 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ']'
+ echo 'Using TCMalloc'
+ export LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
running benchmark
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 3    --dwu-num-chunks 2    --dwu-num-rs-pg 1    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-overlap-reductions    --dwu-grad-norm    '
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 7 ']'
+ '[' 256 -gt 32 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ']'
Using TCMalloc
+ echo 'Using TCMalloc'
+ export LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
running benchmark
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
STARTING TIMING RUN AT 2020-06-18 02:13:30 AM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 3    --dwu-num-chunks 2    --dwu-num-rs-pg 1    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-overlap-reductions    --dwu-grad-norm    '
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 1 ']'
STARTING TIMING RUN AT 2020-06-18 02:13:30 AM
STARTING TIMING RUN AT 2020-06-18 02:13:30 AM
+ '[' 256 -gt 32 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ']'
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
Using TCMalloc
+ echo 'Using TCMalloc'
+ export LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ RESULTS_DIR=gnmt_wmt16
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ echo 'running benchmark'
+ LR=4.0e-3
+ TRAIN_BATCH_SIZE=32
+ TRAIN_BATCH_SIZE=32
running benchmark
+ TEST_BATCH_SIZE=16
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ NUMEPOCHS=12
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 3    --dwu-num-chunks 2    --dwu-num-rs-pg 1    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-overlap-reductions    --dwu-grad-norm    '
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 3    --dwu-num-chunks 2    --dwu-num-rs-pg 1    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-overlap-reductions    --dwu-grad-norm    '
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    '
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    '
+ MATH=fp16
+ MATH=fp16
+ declare -a CMD
+ declare -a CMD
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
+ '[' -n 3 ']'
+ '[' 256 -gt 32 ']'
+ '[' -n 5 ']'
+ '[' 256 -gt 32 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ '[' -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ']'
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ '[' -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ']'
Using TCMalloc
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
Using TCMalloc
+ echo 'Using TCMalloc'
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ echo 'Using TCMalloc'
+ export LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ export LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
running benchmark
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
running benchmark
+ echo 'running benchmark'
+ echo 'running benchmark'
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
STARTING TIMING RUN AT 2020-06-18 02:13:30 AM
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
+ LR=4.0e-3
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 3    --dwu-num-chunks 2    --dwu-num-rs-pg 1    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-overlap-reductions    --dwu-grad-norm    '
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 0 ']'
+ '[' 256 -gt 32 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ']'
+ echo 'Using TCMalloc'
Using TCMalloc
+ export LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
running benchmark
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
STARTING TIMING RUN AT 2020-06-18 02:13:30 AM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 3    --dwu-num-chunks 2    --dwu-num-rs-pg 1    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-overlap-reductions    --dwu-grad-norm    '
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 4 ']'
+ '[' 256 -gt 32 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ']'
Using TCMalloc
+ echo 'Using TCMalloc'
+ export LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ echo 'running benchmark'
running benchmark
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
STARTING TIMING RUN AT 2020-06-18 02:13:30 AM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
STARTING TIMING RUN AT 2020-06-18 02:13:30 AM
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 3    --dwu-num-chunks 2    --dwu-num-rs-pg 1    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-overlap-reductions    --dwu-grad-norm    '
+ DATASET_DIR=/data
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    '
+ MATH=fp16
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ declare -a CMD
+ LR=4.0e-3
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ '[' -n 2 ']'
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ '[' 256 -gt 32 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 3    --dwu-num-chunks 2    --dwu-num-rs-pg 1    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-overlap-reductions    --dwu-grad-norm    '
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    '
+ MATH=fp16
+ declare -a CMD
+ '[' -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ']'
+ echo 'Using TCMalloc'
Using TCMalloc
+ export LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ echo 'running benchmark'
running benchmark
+ '[' -n 6 ']'
+ '[' 256 -gt 32 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ']'
Using TCMalloc
+ echo 'Using TCMalloc'
+ export LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
running benchmark
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
STARTING TIMING RUN AT 2020-06-18 02:13:30 AM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
STARTING TIMING RUN AT 2020-06-18 02:13:30 AM
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 3    --dwu-num-chunks 2    --dwu-num-rs-pg 1    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-overlap-reductions    --dwu-grad-norm    '
+ DATASET_DIR=/data
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    '
+ MATH=fp16
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ declare -a CMD
+ LR=4.0e-3
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ '[' -n 0 ']'
+ DECAY_INTERVAL=283
+ '[' 256 -gt 32 ']'
+ TARGET=24.0
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 3    --dwu-num-chunks 2    --dwu-num-rs-pg 1    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-overlap-reductions    --dwu-grad-norm    '
+ '[' -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ']'
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    '
Using TCMalloc
+ echo 'Using TCMalloc'
+ export LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ MATH=fp16
+ declare -a CMD
running benchmark
+ LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ echo 'running benchmark'
+ '[' -n 4 ']'
+ '[' 256 -gt 32 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
+ '[' -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ']'
+ echo 'Using TCMalloc'
Using TCMalloc
running benchmark
+ export LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
STARTING TIMING RUN AT 2020-06-18 02:13:30 AM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 3    --dwu-num-chunks 2    --dwu-num-rs-pg 1    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-overlap-reductions    --dwu-grad-norm    '
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 7 ']'
+ '[' 256 -gt 32 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ']'
+ echo 'Using TCMalloc'
Using TCMalloc
+ export LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ echo 'running benchmark'
running benchmark
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
STARTING TIMING RUN AT 2020-06-18 02:13:30 AM
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ DATASET_DIR=/data
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ PREPROC_DATADIR=/preproc_data
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ RESULTS_DIR=gnmt_wmt16
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ LR=4.0e-3
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 3    --dwu-num-chunks 2    --dwu-num-rs-pg 1    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-overlap-reductions    --dwu-grad-norm    '
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 3 ']'
+ '[' 256 -gt 32 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ']'
Using TCMalloc
+ echo 'Using TCMalloc'
+ export LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
running benchmark
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
STARTING TIMING RUN AT 2020-06-18 02:13:30 AM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
+ TARGET=24.0
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 3    --dwu-num-chunks 2    --dwu-num-rs-pg 1    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-overlap-reductions    --dwu-grad-norm    '
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    '
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ MATH=fp16
+ declare -a CMD
+ '[' -n 1 ']'
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ '[' 256 -gt 32 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ']'
Using TCMalloc
+ echo 'Using TCMalloc'
+ export LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
running benchmark
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ echo 'running benchmark'
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
STARTING TIMING RUN AT 2020-06-18 02:13:30 AM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 3    --dwu-num-chunks 2    --dwu-num-rs-pg 1    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-overlap-reductions    --dwu-grad-norm    '
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    '
STARTING TIMING RUN AT 2020-06-18 02:13:30 AM
+ MATH=fp16
+ declare -a CMD
+ '[' -n 5 ']'
+ '[' 256 -gt 32 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ '[' -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ']'
+ echo 'Using TCMalloc'
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
+ TARGET=24.0
Using TCMalloc
+ export LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ echo 'running benchmark'
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
running benchmark
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 3    --dwu-num-chunks 2    --dwu-num-rs-pg 1    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-overlap-reductions    --dwu-grad-norm    '
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 6 ']'
+ '[' 256 -gt 32 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
Using TCMalloc
+ '[' -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ']'
+ echo 'Using TCMalloc'
+ export LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
running benchmark
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
STARTING TIMING RUN AT 2020-06-18 02:13:30 AM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 3    --dwu-num-chunks 2    --dwu-num-rs-pg 1    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-overlap-reductions    --dwu-grad-norm    '
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 5 ']'
+ '[' 256 -gt 32 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ']'
Using TCMalloc
+ echo 'Using TCMalloc'
+ export LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
running benchmark
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
STARTING TIMING RUN AT 2020-06-18 02:13:30 AM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
STARTING TIMING RUN AT 2020-06-18 02:13:30 AM
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 3    --dwu-num-chunks 2    --dwu-num-rs-pg 1    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-overlap-reductions    --dwu-grad-norm    '
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    '
+ MATH=fp16
+ declare -a CMD
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ TRAIN_BATCH_SIZE=32
+ '[' -n 6 ']'
+ TEST_BATCH_SIZE=16
+ '[' 256 -gt 32 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ '[' -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ']'
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
Using TCMalloc
+ echo 'Using TCMalloc'
+ export LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 3    --dwu-num-chunks 2    --dwu-num-rs-pg 1    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-overlap-reductions    --dwu-grad-norm    '
running benchmark
+ echo 'running benchmark'
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    '
+ MATH=fp16
+ declare -a CMD
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
STARTING TIMING RUN AT 2020-06-18 02:13:30 AM
+ '[' -n 7 ']'
+ '[' 256 -gt 32 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ']'
Using TCMalloc
+ echo 'Using TCMalloc'
+ export LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
running benchmark
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
STARTING TIMING RUN AT 2020-06-18 02:13:30 AM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ LR=4.0e-3
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ TRAIN_BATCH_SIZE=32
+ DECAY_INTERVAL=283
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ NUMEPOCHS=12
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 3    --dwu-num-chunks 2    --dwu-num-rs-pg 1    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-overlap-reductions    --dwu-grad-norm    '
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    '
+ MATH=fp16
+ declare -a CMD
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 3    --dwu-num-chunks 2    --dwu-num-rs-pg 1    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-overlap-reductions    --dwu-grad-norm    '
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 2 ']'
+ '[' 256 -gt 32 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ']'
+ echo 'Using TCMalloc'
+ '[' -n 3 ']'
Using TCMalloc
+ '[' 256 -gt 32 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ export LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
running benchmark
+ echo 'running benchmark'
STARTING TIMING RUN AT 2020-06-18 02:13:30 AM
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
+ '[' -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ']'
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
Using TCMalloc
+ echo 'Using TCMalloc'
+ export LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
running benchmark
STARTING TIMING RUN AT 2020-06-18 02:13:30 AM
+ echo 'running benchmark'
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ DECAY_INTERVAL=283
+ LR=4.0e-3
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 3    --dwu-num-chunks 2    --dwu-num-rs-pg 1    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-overlap-reductions    --dwu-grad-norm    '
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    '
+ MATH=fp16
+ TRAIN_BATCH_SIZE=32
+ declare -a CMD
+ TEST_BATCH_SIZE=16
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
+ WARMUP_STEPS=200
+ '[' -n 4 ']'
+ '[' 256 -gt 32 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 3    --dwu-num-chunks 2    --dwu-num-rs-pg 1    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-overlap-reductions    --dwu-grad-norm    '
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 0 ']'
+ '[' 256 -gt 32 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ']'
Using TCMalloc
+ echo 'Using TCMalloc'
+ export LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
running benchmark
+ echo 'running benchmark'
STARTING TIMING RUN AT 2020-06-18 02:13:30 AM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 3    --dwu-num-chunks 2    --dwu-num-rs-pg 1    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-overlap-reductions    --dwu-grad-norm    '
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 5 ']'
+ '[' 256 -gt 32 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ']'
+ echo 'Using TCMalloc'
Using TCMalloc
+ export LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
running benchmark
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
+ '[' -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ']'
+ echo 'Using TCMalloc'
Using TCMalloc
+ export LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
running benchmark
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
STARTING TIMING RUN AT 2020-06-18 02:13:30 AM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 3    --dwu-num-chunks 2    --dwu-num-rs-pg 1    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-overlap-reductions    --dwu-grad-norm    '
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 2 ']'
+ '[' 256 -gt 32 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ']'
Using TCMalloc
+ echo 'Using TCMalloc'
+ export LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
running benchmark
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
STARTING TIMING RUN AT 2020-06-18 02:13:30 AM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
STARTING TIMING RUN AT 2020-06-18 02:13:30 AM
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ TRAIN_BATCH_SIZE=32
+ MAX_SEQ_LEN=75
+ TEST_BATCH_SIZE=16
+ NUMEPOCHS=12
+ WARMUP_STEPS=200
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 3    --dwu-num-chunks 2    --dwu-num-rs-pg 1    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-overlap-reductions    --dwu-grad-norm    '
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    '
+ TARGET=24.0
+ MATH=fp16
+ MAX_SEQ_LEN=75
+ declare -a CMD
+ NUMEPOCHS=12
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 3    --dwu-num-chunks 2    --dwu-num-rs-pg 1    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-overlap-reductions    --dwu-grad-norm    '
+ '[' -n 7 ']'
STARTING TIMING RUN AT 2020-06-18 02:13:30 AM
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    '
Using TCMalloc
+ '[' 256 -gt 32 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ MATH=fp16
STARTING TIMING RUN AT 2020-06-18 02:13:30 AM
+ declare -a CMD
running benchmark
+ '[' -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ']'
+ '[' -n 3 ']'
+ '[' 256 -gt 32 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ echo 'Using TCMalloc'
Using TCMalloc
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ export LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ LR=4.0e-3
+ TRAIN_BATCH_SIZE=32
+ echo 'running benchmark'
running benchmark
+ '[' -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ']'
+ echo 'Using TCMalloc'
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ export LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ echo 'running benchmark'
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ TARGET=24.0
STARTING TIMING RUN AT 2020-06-18 02:13:30 AM
+ RESULTS_DIR=gnmt_wmt16
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ LR=4.0e-3
+ TRAIN_BATCH_SIZE=32
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 3    --dwu-num-chunks 2    --dwu-num-rs-pg 1    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-overlap-reductions    --dwu-grad-norm    '
+ DATASET_DIR=/data
+ TEST_BATCH_SIZE=16
+ PREPROC_DATADIR=/preproc_data
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    '
+ MATH=fp16
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ declare -a CMD
+ WARMUP_STEPS=200
+ DECAY_INTERVAL=283
+ REMAIN_STEPS=2259
+ TARGET=24.0
+ DECAY_INTERVAL=283
+ MAX_SEQ_LEN=75
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ TARGET=24.0
+ NUMEPOCHS=12
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
Using TCMalloc
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 3    --dwu-num-chunks 2    --dwu-num-rs-pg 1    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-overlap-reductions    --dwu-grad-norm    '
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    '
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ MAX_SEQ_LEN=75
running benchmark
+ MATH=fp16
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
Using TCMalloc
+ NUMEPOCHS=12
+ declare -a CMD
running benchmark
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 3    --dwu-num-chunks 2    --dwu-num-rs-pg 1    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-overlap-reductions    --dwu-grad-norm    '
STARTING TIMING RUN AT 2020-06-18 02:13:30 AM
+ '[' -n 4 ']'
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    '
Using TCMalloc
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
+ MATH=fp16
running benchmark
+ '[' 256 -gt 32 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ declare -a CMD
+ '[' -n 2 ']'
+ '[' -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ']'
+ '[' 256 -gt 32 ']'
+ '[' -n 1 ']'
+ echo 'Using TCMalloc'
+ export LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ '[' 256 -gt 32 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ echo 'running benchmark'
+ '[' -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ']'
+ echo 'Using TCMalloc'
+ export LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ echo 'running benchmark'
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ '[' -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ']'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ echo 'Using TCMalloc'
+ export LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 3    --dwu-num-chunks 2    --dwu-num-rs-pg 1    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-overlap-reductions    --dwu-grad-norm    '
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    '
+ MATH=fp16
+ declare -a CMD
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
Using TCMalloc
+ '[' -n 1 ']'
+ '[' 256 -gt 32 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
running benchmark
+ '[' -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ']'
+ echo 'Using TCMalloc'
+ export LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
STARTING TIMING RUN AT 2020-06-18 02:13:30 AM
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 3    --dwu-num-chunks 2    --dwu-num-rs-pg 1    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-overlap-reductions    --dwu-grad-norm    '
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 0 ']'
+ '[' 256 -gt 32 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ']'
Using TCMalloc
+ echo 'Using TCMalloc'
+ export LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
running benchmark
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
STARTING TIMING RUN AT 2020-06-18 02:13:30 AM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 3    --dwu-num-chunks 2    --dwu-num-rs-pg 1    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-overlap-reductions    --dwu-grad-norm    '
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 7 ']'
+ '[' 256 -gt 32 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ']'
Using TCMalloc
+ echo 'Using TCMalloc'
+ export LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ echo 'running benchmark'
running benchmark
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
STARTING TIMING RUN AT 2020-06-18 02:13:30 AM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 3    --dwu-num-chunks 2    --dwu-num-rs-pg 1    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-overlap-reductions    --dwu-grad-norm    '
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 6 ']'
+ '[' 256 -gt 32 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ']'
Using TCMalloc
+ echo 'Using TCMalloc'
+ export LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
running benchmark
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
STARTING TIMING RUN AT 2020-06-18 02:13:30 AM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 3    --dwu-num-chunks 2    --dwu-num-rs-pg 1    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-overlap-reductions    --dwu-grad-norm    '
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 4 ']'
+ '[' 256 -gt 32 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ']'
+ echo 'Using TCMalloc'
Using TCMalloc
+ export LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
running benchmark
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
STARTING TIMING RUN AT 2020-06-18 02:13:30 AM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 3    --dwu-num-chunks 2    --dwu-num-rs-pg 1    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-overlap-reductions    --dwu-grad-norm    '
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 6 ']'
+ '[' 256 -gt 32 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ']'
+ echo 'Using TCMalloc'
Using TCMalloc
+ export LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
running benchmark
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
STARTING TIMING RUN AT 2020-06-18 02:13:30 AM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
STARTING TIMING RUN AT 2020-06-18 02:13:30 AM
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 3    --dwu-num-chunks 2    --dwu-num-rs-pg 1    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-overlap-reductions    --dwu-grad-norm    '
+ RESULTS_DIR=gnmt_wmt16
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    '
+ LR=4.0e-3
+ MATH=fp16
+ TRAIN_BATCH_SIZE=32
+ declare -a CMD
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 3    --dwu-num-chunks 2    --dwu-num-rs-pg 1    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-overlap-reductions    --dwu-grad-norm    '
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    '
+ MATH=fp16
+ '[' -n 2 ']'
+ declare -a CMD
+ '[' 256 -gt 32 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ']'
+ echo 'Using TCMalloc'
+ '[' -n 5 ']'
+ '[' 256 -gt 32 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
Using TCMalloc
running benchmark
+ export LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ echo 'running benchmark'
+ '[' -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ']'
+ echo 'Using TCMalloc'
Using TCMalloc
+ export LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
running benchmark
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
STARTING TIMING RUN AT 2020-06-18 02:13:30 AM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 3    --dwu-num-chunks 2    --dwu-num-rs-pg 1    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-overlap-reductions    --dwu-grad-norm    '
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    '
+ MATH=fp16
+ declare -a CMD
STARTING TIMING RUN AT 2020-06-18 02:13:30 AM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
STARTING TIMING RUN AT 2020-06-18 02:13:30 AM
+ DATASET_DIR=/data
STARTING TIMING RUN AT 2020-06-18 02:13:30 AM
+ PREPROC_DATADIR=/preproc_data
+ '[' -n 5 ']'
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ LR=4.0e-3
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ TRAIN_BATCH_SIZE=32
+ '[' 256 -gt 32 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
+ WARMUP_STEPS=200
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
Using TCMalloc
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 3    --dwu-num-chunks 2    --dwu-num-rs-pg 1    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-overlap-reductions    --dwu-grad-norm    '
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    '
running benchmark
+ '[' -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ']'
Using TCMalloc
+ MATH=fp16
running benchmark
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ declare -a CMD
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
+ '[' -n 4 ']'
+ '[' 256 -gt 32 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ echo 'Using TCMalloc'
+ export LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ echo 'running benchmark'
+ '[' -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ']'
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ echo 'Using TCMalloc'
+ export LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
+ NUMEPOCHS=12
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 3    --dwu-num-chunks 2    --dwu-num-rs-pg 1    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-overlap-reductions    --dwu-grad-norm    '
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 3    --dwu-num-chunks 2    --dwu-num-rs-pg 1    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-overlap-reductions    --dwu-grad-norm    '
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    '
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    '
+ MATH=fp16
+ declare -a CMD
+ MATH=fp16
+ declare -a CMD
+ '[' -n 3 ']'
+ '[' 256 -gt 32 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
+ '[' -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ']'
Using TCMalloc
+ echo 'Using TCMalloc'
+ export LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ '[' -n 6 ']'
running benchmark
+ echo 'running benchmark'
+ '[' 256 -gt 32 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
STARTING TIMING RUN AT 2020-06-18 02:13:30 AM
+ '[' -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ']'
Using TCMalloc
+ echo 'Using TCMalloc'
+ export LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
+ echo 'running benchmark'
STARTING TIMING RUN AT 2020-06-18 02:13:30 AM
+ LR=4.0e-3
+ TRAIN_BATCH_SIZE=32
running benchmark
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
STARTING TIMING RUN AT 2020-06-18 02:13:30 AM
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 3    --dwu-num-chunks 2    --dwu-num-rs-pg 1    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-overlap-reductions    --dwu-grad-norm    '
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 0 ']'
+ '[' 256 -gt 32 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ '[' -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ']'
+ WARMUP_STEPS=200
Using TCMalloc
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
+ TARGET=24.0
running benchmark
+ echo 'Using TCMalloc'
+ export LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
STARTING TIMING RUN AT 2020-06-18 02:13:30 AM
+ LR=4.0e-3
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
STARTING TIMING RUN AT 2020-06-18 02:13:30 AM
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 3    --dwu-num-chunks 2    --dwu-num-rs-pg 1    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-overlap-reductions    --dwu-grad-norm    '
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    '
+ REMAIN_STEPS=2259
+ MATH=fp16
+ declare -a CMD
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ DATASET_DIR=/data
STARTING TIMING RUN AT 2020-06-18 02:13:30 AM
+ NUMEPOCHS=12
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 3    --dwu-num-chunks 2    --dwu-num-rs-pg 1    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-overlap-reductions    --dwu-grad-norm    '
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    '
+ LR=4.0e-3
+ TRAIN_BATCH_SIZE=32
+ MATH=fp16
+ declare -a CMD
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
Using TCMalloc
+ '[' -n 7 ']'
+ '[' 256 -gt 32 ']'
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
running benchmark
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ '[' -n 1 ']'
+ '[' 256 -gt 32 ']'
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ '[' -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ']'
+ WARMUP_STEPS=200
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ REMAIN_STEPS=2259
+ echo 'Using TCMalloc'
+ export LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
Using TCMalloc
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
running benchmark
+ MAX_SEQ_LEN=75
+ LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ echo 'running benchmark'
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ NUMEPOCHS=12
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 3    --dwu-num-chunks 2    --dwu-num-rs-pg 1    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-overlap-reductions    --dwu-grad-norm    '
Using TCMalloc
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    '
+ MATH=fp16
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 3    --dwu-num-chunks 2    --dwu-num-rs-pg 1    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-overlap-reductions    --dwu-grad-norm    '
Using TCMalloc
running benchmark
+ declare -a CMD
+ '[' -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ']'
running benchmark
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 3    --dwu-num-chunks 2    --dwu-num-rs-pg 1    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-overlap-reductions    --dwu-grad-norm    '
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    '
+ MATH=fp16
+ declare -a CMD
+ echo 'Using TCMalloc'
+ export LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ echo 'running benchmark'
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 0 ']'
+ '[' -n 3 ']'
+ '[' 256 -gt 32 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' 256 -gt 32 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
STARTING TIMING RUN AT 2020-06-18 02:13:30 AM
+ '[' -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ']'
+ echo 'Using TCMalloc'
+ '[' -n 4 ']'
+ '[' -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ']'
+ echo 'Using TCMalloc'
+ export LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ echo 'running benchmark'
+ '[' 256 -gt 32 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ export LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
Using TCMalloc
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ '[' -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ']'
+ echo 'Using TCMalloc'
+ export LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
running benchmark
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
STARTING TIMING RUN AT 2020-06-18 02:13:30 AM
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ echo 'running benchmark'
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ DATASET_DIR=/data
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 3    --dwu-num-chunks 2    --dwu-num-rs-pg 1    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-overlap-reductions    --dwu-grad-norm    '
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    '
+ MATH=fp16
+ LR=4.0e-3
+ declare -a CMD
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ '[' -n 2 ']'
+ DECAY_INTERVAL=283
+ '[' 256 -gt 32 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ '[' -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ']'
+ echo 'Using TCMalloc'
+ export LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 3    --dwu-num-chunks 2    --dwu-num-rs-pg 1    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-overlap-reductions    --dwu-grad-norm    '
Using TCMalloc
running benchmark
+ LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ echo 'running benchmark'
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    '
+ MATH=fp16
+ declare -a CMD
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
+ '[' -n 1 ']'
+ '[' 256 -gt 32 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ']'
Using TCMalloc
+ echo 'Using TCMalloc'
+ export LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
running benchmark
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
STARTING TIMING RUN AT 2020-06-18 02:13:30 AM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 3    --dwu-num-chunks 2    --dwu-num-rs-pg 1    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-overlap-reductions    --dwu-grad-norm    '
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 0 ']'
+ '[' 256 -gt 32 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ']'
+ echo 'Using TCMalloc'
Using TCMalloc
+ export LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ echo 'running benchmark'
running benchmark
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
STARTING TIMING RUN AT 2020-06-18 02:13:30 AM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 3    --dwu-num-chunks 2    --dwu-num-rs-pg 1    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-overlap-reductions    --dwu-grad-norm    '
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 6 ']'
+ '[' 256 -gt 32 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ']'
Using TCMalloc
+ echo 'Using TCMalloc'
+ export LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
running benchmark
+ echo 'running benchmark'
STARTING TIMING RUN AT 2020-06-18 02:13:30 AM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 3    --dwu-num-chunks 2    --dwu-num-rs-pg 1    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-overlap-reductions    --dwu-grad-norm    '
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 7 ']'
+ '[' 256 -gt 32 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ']'
Using TCMalloc
+ echo 'Using TCMalloc'
+ export LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
running benchmark
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
STARTING TIMING RUN AT 2020-06-18 02:13:30 AM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 3    --dwu-num-chunks 2    --dwu-num-rs-pg 1    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-overlap-reductions    --dwu-grad-norm    '
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 5 ']'
+ '[' 256 -gt 32 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ']'
Using TCMalloc
+ echo 'Using TCMalloc'
+ export LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
running benchmark
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
STARTING TIMING RUN AT 2020-06-18 02:13:30 AM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 3    --dwu-num-chunks 2    --dwu-num-rs-pg 1    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-overlap-reductions    --dwu-grad-norm    '
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 3 ']'
+ '[' 256 -gt 32 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ']'
+ echo 'Using TCMalloc'
Using TCMalloc
+ export LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ echo 'running benchmark'
running benchmark
STARTING TIMING RUN AT 2020-06-18 02:13:30 AM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 3    --dwu-num-chunks 2    --dwu-num-rs-pg 1    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-overlap-reductions    --dwu-grad-norm    '
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 1 ']'
+ '[' 256 -gt 32 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ']'
+ echo 'Using TCMalloc'
Using TCMalloc
+ export LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ echo 'running benchmark'
running benchmark
STARTING TIMING RUN AT 2020-06-18 02:13:30 AM
+ DATASET_DIR=/data
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
STARTING TIMING RUN AT 2020-06-18 02:13:30 AM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ LR=4.0e-3
+ REMAIN_STEPS=2259
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 3    --dwu-num-chunks 2    --dwu-num-rs-pg 1    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-overlap-reductions    --dwu-grad-norm    '
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    '
+ REMAIN_STEPS=2259
+ MATH=fp16
+ declare -a CMD
+ '[' -n 2 ']'
+ DECAY_INTERVAL=283
+ '[' 256 -gt 32 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ']'
Using TCMalloc
+ TARGET=24.0
+ echo 'Using TCMalloc'
+ export LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ MAX_SEQ_LEN=75
running benchmark
+ NUMEPOCHS=12
+ echo 'running benchmark'
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 3    --dwu-num-chunks 2    --dwu-num-rs-pg 1    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-overlap-reductions    --dwu-grad-norm    '
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 2 ']'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
STARTING TIMING RUN AT 2020-06-18 02:13:30 AM
+ '[' 256 -gt 32 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
STARTING TIMING RUN AT 2020-06-18 02:13:30 AM
+ DATASET_DIR=/data
+ LR=4.0e-3
+ PREPROC_DATADIR=/preproc_data
+ TRAIN_BATCH_SIZE=32
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ RESULTS_DIR=gnmt_wmt16
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
STARTING TIMING RUN AT 2020-06-18 02:13:30 AM
+ LR=4.0e-3
Using TCMalloc
+ TARGET=24.0
+ '[' -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ']'
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ echo 'Using TCMalloc'
+ MAX_SEQ_LEN=75
+ export LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ NUMEPOCHS=12
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 3    --dwu-num-chunks 2    --dwu-num-rs-pg 1    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-overlap-reductions    --dwu-grad-norm    '
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    '
running benchmark
+ TARGET=24.0
+ MATH=fp16
+ declare -a CMD
+ echo 'running benchmark'
+ MAX_SEQ_LEN=75
+ '[' -n 7 ']'
Using TCMalloc
+ '[' 256 -gt 32 ']'
+ NUMEPOCHS=12
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
running benchmark
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 3    --dwu-num-chunks 2    --dwu-num-rs-pg 1    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-overlap-reductions    --dwu-grad-norm    '
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    '
+ '[' -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ']'
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ MATH=fp16
+ declare -a CMD
+ LR=4.0e-3
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ echo 'Using TCMalloc'
+ WARMUP_STEPS=200
+ export LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ REMAIN_STEPS=2259
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ DECAY_INTERVAL=283
+ echo 'running benchmark'
+ TARGET=24.0
+ '[' -n 4 ']'
+ MAX_SEQ_LEN=75
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
+ NUMEPOCHS=12
+ '[' 256 -gt 32 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 3    --dwu-num-chunks 2    --dwu-num-rs-pg 1    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-overlap-reductions    --dwu-grad-norm    '
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    '
Using TCMalloc
+ MATH=fp16
+ declare -a CMD
+ '[' -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ']'
running benchmark
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ echo 'Using TCMalloc'
+ export LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ echo 'running benchmark'
+ '[' -n 5 ']'
+ '[' 256 -gt 32 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ '[' -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ']'
+ echo 'Using TCMalloc'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
Using TCMalloc
running benchmark
+ export LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ echo 'running benchmark'
STARTING TIMING RUN AT 2020-06-18 02:13:30 AM
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ LR=4.0e-3
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 3    --dwu-num-chunks 2    --dwu-num-rs-pg 1    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-overlap-reductions    --dwu-grad-norm    '
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    '
+ MATH=fp16
+ declare -a CMD
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
+ '[' -n 6 ']'
+ '[' 256 -gt 32 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ']'
Using TCMalloc
+ echo 'Using TCMalloc'
+ export LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
running benchmark
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
STARTING TIMING RUN AT 2020-06-18 02:13:30 AM
+ DATASET_DIR=/data
STARTING TIMING RUN AT 2020-06-18 02:13:30 AM
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
+ LR=4.0e-3
+ TARGET=24.0
+ TRAIN_BATCH_SIZE=32
+ MAX_SEQ_LEN=75
+ TEST_BATCH_SIZE=16
+ NUMEPOCHS=12
+ WARMUP_STEPS=200
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 3    --dwu-num-chunks 2    --dwu-num-rs-pg 1    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-overlap-reductions    --dwu-grad-norm    '
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    '
+ MATH=fp16
+ REMAIN_STEPS=2259
STARTING TIMING RUN AT 2020-06-18 02:13:30 AM
+ declare -a CMD
+ DECAY_INTERVAL=283
STARTING TIMING RUN AT 2020-06-18 02:13:30 AM
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 3    --dwu-num-chunks 2    --dwu-num-rs-pg 1    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-overlap-reductions    --dwu-grad-norm    '
+ '[' -n 0 ']'
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    '
+ '[' 256 -gt 32 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ MATH=fp16
Using TCMalloc
+ declare -a CMD
+ '[' -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ']'
+ '[' -n 2 ']'
+ DATASET_DIR=/data
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ '[' 256 -gt 32 ']'
running benchmark
+ echo 'Using TCMalloc'
+ export LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
Using TCMalloc
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
STARTING TIMING RUN AT 2020-06-18 02:13:30 AM
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ '[' -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ']'
running benchmark
+ echo 'running benchmark'
+ LR=4.0e-3
+ TRAIN_BATCH_SIZE=32
+ echo 'Using TCMalloc'
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ export LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ echo 'running benchmark'
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ DATASET_DIR=/data
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ PREPROC_DATADIR=/preproc_data
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ RESULTS_DIR=gnmt_wmt16
+ NUMEPOCHS=12
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 3    --dwu-num-chunks 2    --dwu-num-rs-pg 1    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-overlap-reductions    --dwu-grad-norm    '
+ LR=4.0e-3
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 3    --dwu-num-chunks 2    --dwu-num-rs-pg 1    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-overlap-reductions    --dwu-grad-norm    '
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    '
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
Using TCMalloc
running benchmark
+ MATH=fp16
+ WARMUP_STEPS=200
Using TCMalloc
running benchmark
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    '
+ MATH=fp16
+ REMAIN_STEPS=2259
+ declare -a CMD
+ declare -a CMD
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
+ DECAY_INTERVAL=283
+ '[' -n 3 ']'
+ TARGET=24.0
+ '[' -n 7 ']'
+ MAX_SEQ_LEN=75
+ '[' 256 -gt 32 ']'
+ NUMEPOCHS=12
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 3    --dwu-num-chunks 2    --dwu-num-rs-pg 1    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-overlap-reductions    --dwu-grad-norm    '
+ '[' 256 -gt 32 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    '
+ '[' -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ']'
+ echo 'Using TCMalloc'
+ MATH=fp16
+ '[' -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ']'
+ echo 'Using TCMalloc'
+ export LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ declare -a CMD
+ export LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
+ '[' -n 0 ']'
+ '[' 256 -gt 32 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
+ '[' -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ']'
+ echo 'Using TCMalloc'
Using TCMalloc
+ export LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
running benchmark
+ echo 'running benchmark'
STARTING TIMING RUN AT 2020-06-18 02:13:30 AM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 3    --dwu-num-chunks 2    --dwu-num-rs-pg 1    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-overlap-reductions    --dwu-grad-norm    '
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 1 ']'
+ '[' 256 -gt 32 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ']'
+ echo 'Using TCMalloc'
Using TCMalloc
+ export LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
running benchmark
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
STARTING TIMING RUN AT 2020-06-18 02:13:30 AM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
STARTING TIMING RUN AT 2020-06-18 02:13:30 AM
+ DATASET_DIR=/data
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ DECAY_INTERVAL=283
+ REMAIN_STEPS=2259
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ DECAY_INTERVAL=283
+ NUMEPOCHS=12
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 3    --dwu-num-chunks 2    --dwu-num-rs-pg 1    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-overlap-reductions    --dwu-grad-norm    '
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    '
+ MATH=fp16
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 3    --dwu-num-chunks 2    --dwu-num-rs-pg 1    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-overlap-reductions    --dwu-grad-norm    '
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    '
+ MATH=fp16
+ declare -a CMD
+ declare -a CMD
STARTING TIMING RUN AT 2020-06-18 02:13:30 AM
+ '[' -n 1 ']'
+ '[' -n 6 ']'
+ '[' 256 -gt 32 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' 256 -gt 32 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DATASET_DIR=/data
+ '[' -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ']'
+ echo 'Using TCMalloc'
Using TCMalloc
+ '[' -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ']'
Using TCMalloc
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ export LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ echo 'running benchmark'
+ echo 'Using TCMalloc'
+ export LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ LR=4.0e-3
running benchmark
+ LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ echo 'running benchmark'
running benchmark
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 3    --dwu-num-chunks 2    --dwu-num-rs-pg 1    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-overlap-reductions    --dwu-grad-norm    '
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    '
+ MATH=fp16
+ declare -a CMD
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
+ '[' -n 4 ']'
+ '[' 256 -gt 32 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
STARTING TIMING RUN AT 2020-06-18 02:13:30 AM
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
+ '[' -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ']'
+ echo 'Using TCMalloc'
Using TCMalloc
+ export LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
running benchmark
+ echo 'running benchmark'
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 3    --dwu-num-chunks 2    --dwu-num-rs-pg 1    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-overlap-reductions    --dwu-grad-norm    '
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    '
STARTING TIMING RUN AT 2020-06-18 02:13:30 AM
+ MATH=fp16
+ declare -a CMD
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ '[' -n 5 ']'
+ LR=4.0e-3
+ '[' 256 -gt 32 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ '[' -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ']'
+ DECAY_INTERVAL=283
+ TARGET=24.0
Using TCMalloc
+ echo 'Using TCMalloc'
+ export LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
running benchmark
+ echo 'running benchmark'
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 3    --dwu-num-chunks 2    --dwu-num-rs-pg 1    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-overlap-reductions    --dwu-grad-norm    '
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 3 ']'
+ '[' 256 -gt 32 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
+ '[' -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ']'
Using TCMalloc
+ echo 'Using TCMalloc'
+ export LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ echo 'running benchmark'
running benchmark
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
STARTING TIMING RUN AT 2020-06-18 02:13:30 AM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 3    --dwu-num-chunks 2    --dwu-num-rs-pg 1    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-overlap-reductions    --dwu-grad-norm    '
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 0 ']'
+ '[' 256 -gt 32 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ']'
Using TCMalloc
+ echo 'Using TCMalloc'
+ export LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
running benchmark
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
STARTING TIMING RUN AT 2020-06-18 02:13:30 AM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 3    --dwu-num-chunks 2    --dwu-num-rs-pg 1    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-overlap-reductions    --dwu-grad-norm    '
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 4 ']'
+ '[' 256 -gt 32 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
STARTING TIMING RUN AT 2020-06-18 02:13:30 AM
+ '[' -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ']'
+ echo 'Using TCMalloc'
Using TCMalloc
+ DATASET_DIR=/data
+ export LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
running benchmark
+ echo 'running benchmark'
+ LR=4.0e-3
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 3    --dwu-num-chunks 2    --dwu-num-rs-pg 1    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-overlap-reductions    --dwu-grad-norm    '
STARTING TIMING RUN AT 2020-06-18 02:13:30 AM
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    '
+ MATH=fp16
STARTING TIMING RUN AT 2020-06-18 02:13:30 AM
+ declare -a CMD
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
+ DATASET_DIR=/data
+ DATASET_DIR=/data
STARTING TIMING RUN AT 2020-06-18 02:13:30 AM
+ PREPROC_DATADIR=/preproc_data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ RESULTS_DIR=gnmt_wmt16
+ '[' -n 6 ']'
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ LR=4.0e-3
+ LR=4.0e-3
STARTING TIMING RUN AT 2020-06-18 02:13:30 AM
+ '[' 256 -gt 32 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ TRAIN_BATCH_SIZE=32
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ TEST_BATCH_SIZE=16
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
STARTING TIMING RUN AT 2020-06-18 02:13:30 AM
+ WARMUP_STEPS=200
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ REMAIN_STEPS=2259
Using TCMalloc
+ '[' -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ']'
+ LR=4.0e-3
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
running benchmark
+ MAX_SEQ_LEN=75
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ echo 'Using TCMalloc'
+ export LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ NUMEPOCHS=12
+ TARGET=24.0
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ MAX_SEQ_LEN=75
+ WARMUP_STEPS=200
+ echo 'running benchmark'
+ REMAIN_STEPS=2259
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 3    --dwu-num-chunks 2    --dwu-num-rs-pg 1    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-overlap-reductions    --dwu-grad-norm    '
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 3    --dwu-num-chunks 2    --dwu-num-rs-pg 1    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-overlap-reductions    --dwu-grad-norm    '
+ LR=4.0e-3
+ TRAIN_BATCH_SIZE=32
+ NUMEPOCHS=12
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    '
Using TCMalloc
running benchmark
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    '
STARTING TIMING RUN AT 2020-06-18 02:13:30 AM
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 3    --dwu-num-chunks 2    --dwu-num-rs-pg 1    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-overlap-reductions    --dwu-grad-norm    '
+ MATH=fp16
Using TCMalloc
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    '
+ declare -a CMD
+ MAX_SEQ_LEN=75
running benchmark
+ MATH=fp16
+ MATH=fp16
+ declare -a CMD
STARTING TIMING RUN AT 2020-06-18 02:13:30 AM
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ '[' -n 3 ']'
+ NUMEPOCHS=12
+ '[' 256 -gt 32 ']'
+ declare -a CMD
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ REMAIN_STEPS=2259
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 3    --dwu-num-chunks 2    --dwu-num-rs-pg 1    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-overlap-reductions    --dwu-grad-norm    '
+ '[' -n 1 ']'
+ '[' 256 -gt 32 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DECAY_INTERVAL=283
+ TARGET=24.0
STARTING TIMING RUN AT 2020-06-18 02:13:30 AM
+ '[' -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ']'
+ echo 'Using TCMalloc'
+ export LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ echo 'running benchmark'
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    '
+ MATH=fp16
+ '[' -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ']'
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ echo 'Using TCMalloc'
+ export LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ TRAIN_BATCH_SIZE=32
Using TCMalloc
+ declare -a CMD
+ TEST_BATCH_SIZE=16
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 3    --dwu-num-chunks 2    --dwu-num-rs-pg 1    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-overlap-reductions    --dwu-grad-norm    '
+ WARMUP_STEPS=200
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    '
+ MATH=fp16
running benchmark
+ REMAIN_STEPS=2259
+ declare -a CMD
+ DECAY_INTERVAL=283
+ DATASET_DIR=/data
Using TCMalloc
+ TARGET=24.0
Using TCMalloc
+ '[' -n 2 ']'
+ MAX_SEQ_LEN=75
+ PREPROC_DATADIR=/preproc_data
running benchmark
+ '[' 256 -gt 32 ']'
running benchmark
+ NUMEPOCHS=12
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 3    --dwu-num-chunks 2    --dwu-num-rs-pg 1    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-overlap-reductions    --dwu-grad-norm    '
+ RESULTS_DIR=gnmt_wmt16
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    '
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
Using TCMalloc
+ MATH=fp16
Using TCMalloc
+ '[' -n 4 ']'
+ declare -a CMD
running benchmark
+ LR=4.0e-3
+ DATASET_DIR=/data
running benchmark
+ '[' -n 5 ']'
+ PREPROC_DATADIR=/preproc_data
+ TRAIN_BATCH_SIZE=32
+ RESULTS_DIR=gnmt_wmt16
+ '[' 256 -gt 32 ']'
+ LR=4.0e-3
+ '[' 256 -gt 32 ']'
+ TRAIN_BATCH_SIZE=32
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ TEST_BATCH_SIZE=16
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ WARMUP_STEPS=200
+ '[' -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ']'
+ '[' -n 5 ']'
+ TEST_BATCH_SIZE=16
Using TCMalloc
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
+ echo 'Using TCMalloc'
+ '[' 256 -gt 32 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
running benchmark
+ WARMUP_STEPS=200
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ export LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ NUMEPOCHS=12
+ REMAIN_STEPS=2259
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 3    --dwu-num-chunks 2    --dwu-num-rs-pg 1    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-overlap-reductions    --dwu-grad-norm    '
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    '
+ echo 'running benchmark'
+ MATH=fp16
+ '[' -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ']'
+ '[' -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ']'
+ DECAY_INTERVAL=283
+ declare -a CMD
+ echo 'Using TCMalloc'
+ echo 'Using TCMalloc'
+ export LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ TARGET=24.0
+ LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ export LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ echo 'running benchmark'
+ echo 'running benchmark'
+ '[' -n 2 ']'
+ MAX_SEQ_LEN=75
+ '[' 256 -gt 32 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ']'
+ '[' -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ']'
+ NUMEPOCHS=12
+ echo 'Using TCMalloc'
+ echo 'Using TCMalloc'
+ export LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 3    --dwu-num-chunks 2    --dwu-num-rs-pg 1    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-overlap-reductions    --dwu-grad-norm    '
+ export LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ echo 'running benchmark'
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    '
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
+ echo 'running benchmark'
+ MATH=fp16
+ declare -a CMD
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
+ '[' -n 3 ']'
+ '[' 256 -gt 32 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ']'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
+ echo 'Using TCMalloc'
+ export LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
STARTING TIMING RUN AT 2020-06-18 02:13:30 AM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
STARTING TIMING RUN AT 2020-06-18 02:13:30 AM
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ DATASET_DIR=/data
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 3    --dwu-num-chunks 2    --dwu-num-rs-pg 1    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-overlap-reductions    --dwu-grad-norm    '
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    '
+ MATH=fp16
+ LR=4.0e-3
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ declare -a CMD
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ '[' -n 0 ']'
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 3    --dwu-num-chunks 2    --dwu-num-rs-pg 1    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-overlap-reductions    --dwu-grad-norm    '
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    '
+ '[' 256 -gt 32 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ MATH=fp16
+ declare -a CMD
+ '[' -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ']'
+ '[' -n 6 ']'
Using TCMalloc
+ echo 'Using TCMalloc'
+ export LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ '[' 256 -gt 32 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
running benchmark
+ echo 'running benchmark'
+ '[' -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ']'
Using TCMalloc
+ echo 'Using TCMalloc'
+ export LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
running benchmark
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
STARTING TIMING RUN AT 2020-06-18 02:13:30 AM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
STARTING TIMING RUN AT 2020-06-18 02:13:30 AM
+ RESULTS_DIR=gnmt_wmt16
+ DATASET_DIR=/data
+ LR=4.0e-3
+ PREPROC_DATADIR=/preproc_data
STARTING TIMING RUN AT 2020-06-18 02:13:30 AM
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ RESULTS_DIR=gnmt_wmt16
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ LR=4.0e-3
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 3    --dwu-num-chunks 2    --dwu-num-rs-pg 1    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-overlap-reductions    --dwu-grad-norm    '
+ TRAIN_BATCH_SIZE=32
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    '
+ TEST_BATCH_SIZE=16
+ MATH=fp16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ declare -a CMD
+ DATASET_DIR=/data
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
STARTING TIMING RUN AT 2020-06-18 02:13:30 AM
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ LR=4.0e-3
+ TRAIN_BATCH_SIZE=32
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 3    --dwu-num-chunks 2    --dwu-num-rs-pg 1    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-overlap-reductions    --dwu-grad-norm    '
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    '
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ MATH=fp16
+ declare -a CMD
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 3    --dwu-num-chunks 2    --dwu-num-rs-pg 1    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-overlap-reductions    --dwu-grad-norm    '
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    '
+ MATH=fp16
Using TCMalloc
+ '[' -n 1 ']'
+ '[' -n 2 ']'
+ declare -a CMD
+ '[' 256 -gt 32 ']'
running benchmark
+ '[' 256 -gt 32 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ']'
+ echo 'Using TCMalloc'
+ export LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
Using TCMalloc
+ '[' -n 7 ']'
+ DATASET_DIR=/data
+ LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
running benchmark
+ echo 'running benchmark'
+ '[' 256 -gt 32 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ']'
+ echo 'Using TCMalloc'
Using TCMalloc
+ export LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ echo 'running benchmark'
running benchmark
+ '[' -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ']'
+ echo 'Using TCMalloc'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
+ export LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ echo 'running benchmark'
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 3    --dwu-num-chunks 2    --dwu-num-rs-pg 1    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-overlap-reductions    --dwu-grad-norm    '
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    '
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
+ MATH=fp16
+ declare -a CMD
STARTING TIMING RUN AT 2020-06-18 02:13:30 AM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ '[' -n 1 ']'
+ '[' 256 -gt 32 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ']'
Using TCMalloc
+ echo 'Using TCMalloc'
+ export LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
running benchmark
+ echo 'running benchmark'
STARTING TIMING RUN AT 2020-06-18 02:13:30 AM
+ DATASET_DIR=/data
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 3    --dwu-num-chunks 2    --dwu-num-rs-pg 1    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-overlap-reductions    --dwu-grad-norm    '
+ PREPROC_DATADIR=/preproc_data
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    '
+ RESULTS_DIR=gnmt_wmt16
+ MATH=fp16
+ LR=4.0e-3
+ declare -a CMD
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 3    --dwu-num-chunks 2    --dwu-num-rs-pg 1    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-overlap-reductions    --dwu-grad-norm    '
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    '
+ MATH=fp16
+ '[' -n 4 ']'
STARTING TIMING RUN AT 2020-06-18 02:13:30 AM
+ declare -a CMD
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
+ '[' 256 -gt 32 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' -n 4 ']'
+ '[' 256 -gt 32 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
Using TCMalloc
+ '[' -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ']'
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ echo 'Using TCMalloc'
+ export LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ WARMUP_STEPS=200
running benchmark
+ LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ REMAIN_STEPS=2259
+ '[' -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ']'
Using TCMalloc
+ echo 'running benchmark'
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ echo 'Using TCMalloc'
+ export LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
running benchmark
+ NUMEPOCHS=12
+ LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 3    --dwu-num-chunks 2    --dwu-num-rs-pg 1    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-overlap-reductions    --dwu-grad-norm    '
+ echo 'running benchmark'
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 3 ']'
+ '[' 256 -gt 32 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
+ '[' -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ']'
Using TCMalloc
+ echo 'Using TCMalloc'
+ export LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
running benchmark
STARTING TIMING RUN AT 2020-06-18 02:13:30 AM
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
STARTING TIMING RUN AT 2020-06-18 02:13:30 AM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 3    --dwu-num-chunks 2    --dwu-num-rs-pg 1    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-overlap-reductions    --dwu-grad-norm    '
+ DATASET_DIR=/data
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 1 ']'
+ '[' 256 -gt 32 ']'
+ PREPROC_DATADIR=/preproc_data
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
Using TCMalloc
+ '[' -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ']'
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ echo 'Using TCMalloc'
running benchmark
+ WARMUP_STEPS=200
+ export LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ REMAIN_STEPS=2259
+ LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ echo 'running benchmark'
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 3    --dwu-num-chunks 2    --dwu-num-rs-pg 1    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-overlap-reductions    --dwu-grad-norm    '
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 7 ']'
+ '[' 256 -gt 32 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ']'
Using TCMalloc
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
+ echo 'Using TCMalloc'
+ export LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
running benchmark
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
STARTING TIMING RUN AT 2020-06-18 02:13:30 AM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
STARTING TIMING RUN AT 2020-06-18 02:13:30 AM
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 3    --dwu-num-chunks 2    --dwu-num-rs-pg 1    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-overlap-reductions    --dwu-grad-norm    '
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    '
+ MATH=fp16
+ declare -a CMD
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ TRAIN_BATCH_SIZE=32
+ '[' -n 4 ']'
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ '[' 256 -gt 32 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 3    --dwu-num-chunks 2    --dwu-num-rs-pg 1    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-overlap-reductions    --dwu-grad-norm    '
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    '
+ MATH=fp16
+ declare -a CMD
+ '[' -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ']'
+ '[' -n 0 ']'
Using TCMalloc
+ '[' 256 -gt 32 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ echo 'Using TCMalloc'
+ export LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ echo 'running benchmark'
running benchmark
+ '[' -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ']'
Using TCMalloc
+ echo 'Using TCMalloc'
+ export LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
running benchmark
+ echo 'running benchmark'
STARTING TIMING RUN AT 2020-06-18 02:13:30 AM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
+ LR=4.0e-3
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 3    --dwu-num-chunks 2    --dwu-num-rs-pg 1    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-overlap-reductions    --dwu-grad-norm    '
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 5 ']'
+ '[' 256 -gt 32 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ']'
+ echo 'Using TCMalloc'
Using TCMalloc
+ export LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
running benchmark
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
STARTING TIMING RUN AT 2020-06-18 02:13:30 AM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 3    --dwu-num-chunks 2    --dwu-num-rs-pg 1    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-overlap-reductions    --dwu-grad-norm    '
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 6 ']'
+ '[' 256 -gt 32 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ']'
Using TCMalloc
+ echo 'Using TCMalloc'
+ export LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
running benchmark
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
STARTING TIMING RUN AT 2020-06-18 02:13:30 AM
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ RESULTS_DIR=gnmt_wmt16
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ LR=4.0e-3
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ DECAY_INTERVAL=283
+ TARGET=24.0
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 3    --dwu-num-chunks 2    --dwu-num-rs-pg 1    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-overlap-reductions    --dwu-grad-norm    '
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 0 ']'
+ '[' 256 -gt 32 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ']'
Using TCMalloc
+ echo 'Using TCMalloc'
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ export LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
running benchmark
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
STARTING TIMING RUN AT 2020-06-18 02:13:30 AM
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
STARTING TIMING RUN AT 2020-06-18 02:13:30 AM
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ DATASET_DIR=/data
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
+ PREPROC_DATADIR=/preproc_data
+ TARGET=24.0
+ RESULTS_DIR=gnmt_wmt16
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ LR=4.0e-3
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 3    --dwu-num-chunks 2    --dwu-num-rs-pg 1    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-overlap-reductions    --dwu-grad-norm    '
+ TRAIN_BATCH_SIZE=32
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    '
+ MATH=fp16
+ TEST_BATCH_SIZE=16
+ declare -a CMD
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ '[' -n 2 ']'
+ NUMEPOCHS=12
+ '[' 256 -gt 32 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 3    --dwu-num-chunks 2    --dwu-num-rs-pg 1    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-overlap-reductions    --dwu-grad-norm    '
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    '
+ '[' -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ']'
+ MATH=fp16
+ declare -a CMD
Using TCMalloc
+ echo 'Using TCMalloc'
+ export LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
running benchmark
+ echo 'running benchmark'
+ '[' -n 3 ']'
+ '[' 256 -gt 32 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ']'
+ echo 'Using TCMalloc'
Using TCMalloc
+ export LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ echo 'running benchmark'
running benchmark
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
STARTING TIMING RUN AT 2020-06-18 02:13:30 AM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 3    --dwu-num-chunks 2    --dwu-num-rs-pg 1    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-overlap-reductions    --dwu-grad-norm    '
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 7 ']'
+ '[' 256 -gt 32 ']'
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ']'
Using TCMalloc
+ echo 'Using TCMalloc'
+ export LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ echo 'running benchmark'
running benchmark
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
STARTING TIMING RUN AT 2020-06-18 02:13:30 AM
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ WARMUP_STEPS=200
STARTING TIMING RUN AT 2020-06-18 02:13:30 AM
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ TARGET=24.0
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 3    --dwu-num-chunks 2    --dwu-num-rs-pg 1    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-overlap-reductions    --dwu-grad-norm    '
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    '
+ MATH=fp16
+ declare -a CMD
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ '[' -n 5 ']'
+ LR=4.0e-3
+ '[' 256 -gt 32 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ TRAIN_BATCH_SIZE=32
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ TEST_BATCH_SIZE=16
Using TCMalloc
+ WARMUP_STEPS=200
+ '[' -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ']'
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
STARTING TIMING RUN AT 2020-06-18 02:13:30 AM
+ TARGET=24.0
running benchmark
+ echo 'Using TCMalloc'
+ export LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ MAX_SEQ_LEN=75
+ LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ NUMEPOCHS=12
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 3    --dwu-num-chunks 2    --dwu-num-rs-pg 1    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-overlap-reductions    --dwu-grad-norm    '
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    '
+ echo 'running benchmark'
+ MATH=fp16
+ declare -a CMD
+ '[' -n 5 ']'
+ '[' 256 -gt 32 ']'
Using TCMalloc
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ']'
running benchmark
+ echo 'Using TCMalloc'
+ export LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ echo 'running benchmark'
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
STARTING TIMING RUN AT 2020-06-18 02:13:30 AM
+ LR=4.0e-3
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ DATASET_DIR=/data
+ NUMEPOCHS=12
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 3    --dwu-num-chunks 2    --dwu-num-rs-pg 1    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-overlap-reductions    --dwu-grad-norm    '
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    '
+ LR=4.0e-3
+ TRAIN_BATCH_SIZE=32
+ MATH=fp16
+ TEST_BATCH_SIZE=16
+ declare -a CMD
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ '[' -n 2 ']'
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 3    --dwu-num-chunks 2    --dwu-num-rs-pg 1    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-overlap-reductions    --dwu-grad-norm    '
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    '
+ '[' 256 -gt 32 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ MATH=fp16
+ declare -a CMD
+ '[' -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ']'
Using TCMalloc
+ echo 'Using TCMalloc'
+ export LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ echo 'running benchmark'
+ '[' -n 1 ']'
running benchmark
+ '[' 256 -gt 32 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ']'
+ echo 'Using TCMalloc'
+ export LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
Using TCMalloc
running benchmark
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
STARTING TIMING RUN AT 2020-06-18 02:13:30 AM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
STARTING TIMING RUN AT 2020-06-18 02:13:30 AM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ TRAIN_BATCH_SIZE=32
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 3    --dwu-num-chunks 2    --dwu-num-rs-pg 1    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-overlap-reductions    --dwu-grad-norm    '
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 6 ']'
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ '[' 256 -gt 32 ']'
Using TCMalloc
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ']'
+ REMAIN_STEPS=2259
+ echo 'Using TCMalloc'
running benchmark
+ DECAY_INTERVAL=283
+ export LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ TARGET=24.0
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
STARTING TIMING RUN AT 2020-06-18 02:13:30 AM
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 3    --dwu-num-chunks 2    --dwu-num-rs-pg 1    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-overlap-reductions    --dwu-grad-norm    '
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 3 ']'
+ DATASET_DIR=/data
+ '[' 256 -gt 32 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ TRAIN_BATCH_SIZE=32
STARTING TIMING RUN AT 2020-06-18 02:13:30 AM
+ '[' -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ']'
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ DATASET_DIR=/data
Using TCMalloc
+ REMAIN_STEPS=2259
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
running benchmark
+ echo 'Using TCMalloc'
+ export LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ DECAY_INTERVAL=283
+ TRAIN_BATCH_SIZE=32
+ echo 'running benchmark'
+ TARGET=24.0
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ MAX_SEQ_LEN=75
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
+ NUMEPOCHS=12
+ TARGET=24.0
STARTING TIMING RUN AT 2020-06-18 02:13:30 AM
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 3    --dwu-num-chunks 2    --dwu-num-rs-pg 1    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-overlap-reductions    --dwu-grad-norm    '
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 3    --dwu-num-chunks 2    --dwu-num-rs-pg 1    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-overlap-reductions    --dwu-grad-norm    '
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    '
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    '
+ MATH=fp16
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ TRAIN_BATCH_SIZE=32
+ MATH=fp16
+ TEST_BATCH_SIZE=16
+ declare -a CMD
+ WARMUP_STEPS=200
+ declare -a CMD
+ REMAIN_STEPS=2259
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
+ '[' -n 7 ']'
STARTING TIMING RUN AT 2020-06-18 02:13:30 AM
+ '[' 256 -gt 32 ']'
+ DECAY_INTERVAL=283
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ '[' -n 6 ']'
+ NUMEPOCHS=12
+ '[' -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ']'
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 3    --dwu-num-chunks 2    --dwu-num-rs-pg 1    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-overlap-reductions    --dwu-grad-norm    '
+ DATASET_DIR=/data
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    '
+ MATH=fp16
+ declare -a CMD
Using TCMalloc
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
Using TCMalloc
+ '[' -n 1 ']'
running benchmark
+ '[' 256 -gt 32 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
running benchmark
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ echo 'Using TCMalloc'
+ export LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
Using TCMalloc
+ MAX_SEQ_LEN=75
+ '[' -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ']'
+ '[' 256 -gt 32 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ NUMEPOCHS=12
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 3    --dwu-num-chunks 2    --dwu-num-rs-pg 1    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-overlap-reductions    --dwu-grad-norm    '
running benchmark
+ echo 'Using TCMalloc'
+ LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ export LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
Using TCMalloc
running benchmark
+ echo 'running benchmark'
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    '
+ echo 'running benchmark'
+ MATH=fp16
+ '[' -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ']'
+ declare -a CMD
+ echo 'Using TCMalloc'
+ export LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ '[' -n 5 ']'
+ echo 'running benchmark'
+ '[' 256 -gt 32 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
+ '[' -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ']'
+ echo 'Using TCMalloc'
+ export LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
STARTING TIMING RUN AT 2020-06-18 02:13:30 AM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
+ TARGET=24.0
STARTING TIMING RUN AT 2020-06-18 02:13:30 AM
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 3    --dwu-num-chunks 2    --dwu-num-rs-pg 1    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-overlap-reductions    --dwu-grad-norm    '
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    '
+ MATH=fp16
+ declare -a CMD
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ '[' -n 2 ']'
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ '[' 256 -gt 32 ']'
+ TRAIN_BATCH_SIZE=32
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
STARTING TIMING RUN AT 2020-06-18 02:13:30 AM
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
Using TCMalloc
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ '[' -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ']'
+ REMAIN_STEPS=2259
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ DECAY_INTERVAL=283
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ TARGET=24.0
STARTING TIMING RUN AT 2020-06-18 02:13:30 AM
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ echo 'Using TCMalloc'
running benchmark
+ export LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ DATASET_DIR=/data
+ echo 'running benchmark'
+ MAX_SEQ_LEN=75
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ NUMEPOCHS=12
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
STARTING TIMING RUN AT 2020-06-18 02:13:30 AM
+ LR=4.0e-3
+ TRAIN_BATCH_SIZE=32
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 3    --dwu-num-chunks 2    --dwu-num-rs-pg 1    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-overlap-reductions    --dwu-grad-norm    '
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    '
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
STARTING TIMING RUN AT 2020-06-18 02:13:30 AM
+ REMAIN_STEPS=2259
+ RESULTS_DIR=gnmt_wmt16
+ DECAY_INTERVAL=283
+ MATH=fp16
+ TARGET=24.0
+ LR=4.0e-3
+ MAX_SEQ_LEN=75
+ TRAIN_BATCH_SIZE=32
+ NUMEPOCHS=12
+ declare -a CMD
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 3    --dwu-num-chunks 2    --dwu-num-rs-pg 1    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-overlap-reductions    --dwu-grad-norm    '
+ TEST_BATCH_SIZE=16
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    '
+ WARMUP_STEPS=200
+ MATH=fp16
Using TCMalloc
+ REMAIN_STEPS=2259
Using TCMalloc
+ declare -a CMD
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ MAX_SEQ_LEN=75
running benchmark
+ NUMEPOCHS=12
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
+ '[' -n 6 ']'
running benchmark
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ DATASET_DIR=/data
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 3    --dwu-num-chunks 2    --dwu-num-rs-pg 1    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-overlap-reductions    --dwu-grad-norm    '
+ LR=4.0e-3
+ '[' -n 3 ']'
+ '[' 256 -gt 32 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
Using TCMalloc
+ PREPROC_DATADIR=/preproc_data
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    '
+ MATH=fp16
running benchmark
+ TRAIN_BATCH_SIZE=32
+ '[' 256 -gt 32 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ declare -a CMD
STARTING TIMING RUN AT 2020-06-18 02:13:30 AM
+ TEST_BATCH_SIZE=16
+ RESULTS_DIR=gnmt_wmt16
+ '[' -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ']'
+ LR=4.0e-3
+ TRAIN_BATCH_SIZE=32
+ '[' -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ']'
+ echo 'Using TCMalloc'
+ export LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ echo 'Using TCMalloc'
+ export LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ echo 'running benchmark'
+ TEST_BATCH_SIZE=16
+ '[' -n 0 ']'
+ WARMUP_STEPS=200
Using TCMalloc
+ '[' 256 -gt 32 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ WARMUP_STEPS=200
+ echo 'running benchmark'
running benchmark
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
+ '[' -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ']'
+ TARGET=24.0
+ echo 'Using TCMalloc'
+ REMAIN_STEPS=2259
+ MAX_SEQ_LEN=75
+ DECAY_INTERVAL=283
STARTING TIMING RUN AT 2020-06-18 02:13:30 AM
+ NUMEPOCHS=12
+ export LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ TARGET=24.0
STARTING TIMING RUN AT 2020-06-18 02:13:30 AM
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 3    --dwu-num-chunks 2    --dwu-num-rs-pg 1    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-overlap-reductions    --dwu-grad-norm    '
+ MAX_SEQ_LEN=75
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    '
+ echo 'running benchmark'
+ NUMEPOCHS=12
+ MATH=fp16
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 3    --dwu-num-chunks 2    --dwu-num-rs-pg 1    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-overlap-reductions    --dwu-grad-norm    '
+ declare -a CMD
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    '
+ MATH=fp16
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
+ '[' -n 0 ']'
+ '[' 256 -gt 32 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
STARTING TIMING RUN AT 2020-06-18 02:13:30 AM
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
+ DATASET_DIR=/data
+ declare -a CMD
+ PREPROC_DATADIR=/preproc_data
Using TCMalloc
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ '[' -n 1 ']'
+ '[' -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ']'
+ '[' 256 -gt 32 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
running benchmark
+ echo 'Using TCMalloc'
+ TRAIN_BATCH_SIZE=32
+ export LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ echo 'running benchmark'
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 3    --dwu-num-chunks 2    --dwu-num-rs-pg 1    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-overlap-reductions    --dwu-grad-norm    '
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    '
+ MATH=fp16
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
Using TCMalloc
+ declare -a CMD
+ '[' -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ']'
+ DATASET_DIR=/data
+ echo 'Using TCMalloc'
+ export LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
Using TCMalloc
+ '[' -n 4 ']'
running benchmark
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
running benchmark
+ PREPROC_DATADIR=/preproc_data
+ DATASET_DIR=/data
+ LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ TRAIN_BATCH_SIZE=32
+ echo 'running benchmark'
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ DECAY_INTERVAL=283
+ '[' 256 -gt 32 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ TARGET=24.0
+ LR=4.0e-3
+ MAX_SEQ_LEN=75
+ TRAIN_BATCH_SIZE=32
+ NUMEPOCHS=12
+ TEST_BATCH_SIZE=16
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 3    --dwu-num-chunks 2    --dwu-num-rs-pg 1    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-overlap-reductions    --dwu-grad-norm    '
Using TCMalloc
+ WARMUP_STEPS=200
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    '
+ REMAIN_STEPS=2259
running benchmark
+ MATH=fp16
+ DECAY_INTERVAL=283
+ declare -a CMD
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ '[' -n 7 ']'
+ TARGET=24.0
Using TCMalloc
+ '[' -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ']'
+ echo 'Using TCMalloc'
+ MAX_SEQ_LEN=75
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
running benchmark
+ NUMEPOCHS=12
+ '[' 256 -gt 32 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 3    --dwu-num-chunks 2    --dwu-num-rs-pg 1    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-overlap-reductions    --dwu-grad-norm    '
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 3    --dwu-num-chunks 2    --dwu-num-rs-pg 1    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-overlap-reductions    --dwu-grad-norm    '
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    '
+ export LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ '[' -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ']'
+ echo 'Using TCMalloc'
+ MATH=fp16
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    '
+ MATH=fp16
+ declare -a CMD
+ export LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ echo 'running benchmark'
+ declare -a CMD
+ echo 'running benchmark'
+ '[' -n 4 ']'
+ '[' 256 -gt 32 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' -n 7 ']'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
+ '[' 256 -gt 32 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ']'
+ '[' -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ']'
+ echo 'Using TCMalloc'
+ export LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
STARTING TIMING RUN AT 2020-06-18 02:13:30 AM
+ echo 'running benchmark'
+ echo 'Using TCMalloc'
+ export LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
STARTING TIMING RUN AT 2020-06-18 02:13:30 AM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 3    --dwu-num-chunks 2    --dwu-num-rs-pg 1    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-overlap-reductions    --dwu-grad-norm    '
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    '
+ MATH=fp16
+ declare -a CMD
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 3    --dwu-num-chunks 2    --dwu-num-rs-pg 1    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-overlap-reductions    --dwu-grad-norm    '
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 6 ']'
+ '[' 256 -gt 32 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ']'
Using TCMalloc
+ echo 'Using TCMalloc'
+ export LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
running benchmark
+ echo 'running benchmark'
STARTING TIMING RUN AT 2020-06-18 02:13:30 AM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 3    --dwu-num-chunks 2    --dwu-num-rs-pg 1    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-overlap-reductions    --dwu-grad-norm    '
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    '
+ MATH=fp16
+ declare -a CMD
STARTING TIMING RUN AT 2020-06-18 02:13:30 AM
+ '[' -n 6 ']'
+ '[' 256 -gt 32 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DATASET_DIR=/data
Using TCMalloc
+ '[' -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ']'
+ echo 'Using TCMalloc'
running benchmark
+ export LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
STARTING TIMING RUN AT 2020-06-18 02:13:30 AM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ '[' -n 0 ']'
+ DECAY_INTERVAL=283
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ TARGET=24.0
+ '[' 256 -gt 32 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ MAX_SEQ_LEN=75
+ LR=4.0e-3
+ TRAIN_BATCH_SIZE=32
+ NUMEPOCHS=12
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 3    --dwu-num-chunks 2    --dwu-num-rs-pg 1    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-overlap-reductions    --dwu-grad-norm    '
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
Using TCMalloc
+ REMAIN_STEPS=2259
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    '
+ MATH=fp16
Using TCMalloc
+ '[' -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ']'
running benchmark
+ declare -a CMD
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ '[' -n 7 ']'
+ '[' 256 -gt 32 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ echo 'Using TCMalloc'
+ export LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ '[' -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ']'
+ NUMEPOCHS=12
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 3    --dwu-num-chunks 2    --dwu-num-rs-pg 1    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-overlap-reductions    --dwu-grad-norm    '
+ echo 'running benchmark'
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 2 ']'
Using TCMalloc
+ '[' 256 -gt 32 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
running benchmark
+ '[' -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ']'
+ echo 'Using TCMalloc'
+ export LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ echo 'running benchmark'
+ echo 'Using TCMalloc'
+ export LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ echo 'running benchmark'
running benchmark
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
STARTING TIMING RUN AT 2020-06-18 02:13:30 AM
STARTING TIMING RUN AT 2020-06-18 02:13:30 AM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ LR=4.0e-3
+ TRAIN_BATCH_SIZE=32
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 3    --dwu-num-chunks 2    --dwu-num-rs-pg 1    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-overlap-reductions    --dwu-grad-norm    '
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    '
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 3    --dwu-num-chunks 2    --dwu-num-rs-pg 1    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-overlap-reductions    --dwu-grad-norm    '
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    '
+ MATH=fp16
+ declare -a CMD
+ MATH=fp16
+ declare -a CMD
+ '[' -n 4 ']'
+ '[' -n 5 ']'
+ '[' 256 -gt 32 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' 256 -gt 32 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ']'
+ '[' -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ']'
Using TCMalloc
+ echo 'Using TCMalloc'
+ export LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
Using TCMalloc
+ echo 'Using TCMalloc'
+ export LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
running benchmark
+ LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ echo 'running benchmark'
running benchmark
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
STARTING TIMING RUN AT 2020-06-18 02:13:30 AM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 3    --dwu-num-chunks 2    --dwu-num-rs-pg 1    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-overlap-reductions    --dwu-grad-norm    '
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 3 ']'
+ '[' 256 -gt 32 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
STARTING TIMING RUN AT 2020-06-18 02:13:30 AM
+ '[' -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ']'
Using TCMalloc
+ echo 'Using TCMalloc'
+ export LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
running benchmark
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ echo 'running benchmark'
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 3    --dwu-num-chunks 2    --dwu-num-rs-pg 1    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-overlap-reductions    --dwu-grad-norm    '
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 2 ']'
+ '[' 256 -gt 32 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
STARTING TIMING RUN AT 2020-06-18 02:13:30 AM
+ '[' -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ']'
+ echo 'Using TCMalloc'
+ export LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ echo 'running benchmark'
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
Using TCMalloc
running benchmark
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 3    --dwu-num-chunks 2    --dwu-num-rs-pg 1    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-overlap-reductions    --dwu-grad-norm    '
STARTING TIMING RUN AT 2020-06-18 02:13:30 AM
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    '
+ MATH=fp16
+ declare -a CMD
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ '[' -n 0 ']'
+ RESULTS_DIR=gnmt_wmt16
+ '[' 256 -gt 32 ']'
+ LR=4.0e-3
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ '[' -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ']'
+ REMAIN_STEPS=2259
Using TCMalloc
+ echo 'Using TCMalloc'
+ export LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ DECAY_INTERVAL=283
running benchmark
+ echo 'running benchmark'
+ TARGET=24.0
+ MAX_SEQ_LEN=75
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ NUMEPOCHS=12
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 3    --dwu-num-chunks 2    --dwu-num-rs-pg 1    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-overlap-reductions    --dwu-grad-norm    '
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    '
+ MATH=fp16
+ declare -a CMD
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
+ '[' -n 1 ']'
+ '[' 256 -gt 32 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ']'
Using TCMalloc
+ echo 'Using TCMalloc'
+ export LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
running benchmark
+ echo 'running benchmark'
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
STARTING TIMING RUN AT 2020-06-18 02:13:30 AM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 3    --dwu-num-chunks 2    --dwu-num-rs-pg 1    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-overlap-reductions    --dwu-grad-norm    '
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 5 ']'
+ '[' 256 -gt 32 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ']'
+ echo 'Using TCMalloc'
Using TCMalloc
+ export LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
running benchmark
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
STARTING TIMING RUN AT 2020-06-18 02:13:30 AM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 3    --dwu-num-chunks 2    --dwu-num-rs-pg 1    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-overlap-reductions    --dwu-grad-norm    '
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 3 ']'
+ '[' 256 -gt 32 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ']'
Using TCMalloc
+ echo 'Using TCMalloc'
+ export LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ echo 'running benchmark'
running benchmark
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
STARTING TIMING RUN AT 2020-06-18 02:13:30 AM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 3    --dwu-num-chunks 2    --dwu-num-rs-pg 1    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-overlap-reductions    --dwu-grad-norm    '
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 4 ']'
+ '[' 256 -gt 32 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ']'
Using TCMalloc
+ echo 'Using TCMalloc'
+ export LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
STARTING TIMING RUN AT 2020-06-18 02:13:30 AM
+ echo 'running benchmark'
running benchmark
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 3    --dwu-num-chunks 2    --dwu-num-rs-pg 1    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-overlap-reductions    --dwu-grad-norm    '
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 1 ']'
+ '[' 256 -gt 32 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
Using TCMalloc
running benchmark
STARTING TIMING RUN AT 2020-06-18 02:13:30 AM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 3    --dwu-num-chunks 2    --dwu-num-rs-pg 1    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-overlap-reductions    --dwu-grad-norm    '
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 6 ']'
+ '[' 256 -gt 32 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ']'
Using TCMalloc
+ echo 'Using TCMalloc'
running benchmark
+ export LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
STARTING TIMING RUN AT 2020-06-18 02:13:30 AM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ TRAIN_BATCH_SIZE=32
STARTING TIMING RUN AT 2020-06-18 02:13:30 AM
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ TARGET=24.0
+ RESULTS_DIR=gnmt_wmt16
+ MAX_SEQ_LEN=75
+ LR=4.0e-3
+ NUMEPOCHS=12
+ TRAIN_BATCH_SIZE=32
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 3    --dwu-num-chunks 2    --dwu-num-rs-pg 1    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-overlap-reductions    --dwu-grad-norm    '
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    '
+ MATH=fp16
+ REMAIN_STEPS=2259
+ declare -a CMD
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 3    --dwu-num-chunks 2    --dwu-num-rs-pg 1    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-overlap-reductions    --dwu-grad-norm    '
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    '
+ MATH=fp16
+ '[' -n 2 ']'
+ declare -a CMD
+ '[' 256 -gt 32 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ']'
+ '[' -n 3 ']'
STARTING TIMING RUN AT 2020-06-18 02:13:30 AM
Using TCMalloc
+ echo 'Using TCMalloc'
+ export LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ '[' 256 -gt 32 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
running benchmark
+ echo 'running benchmark'
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ '[' -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ']'
+ LR=4.0e-3
Using TCMalloc
+ echo 'Using TCMalloc'
+ TRAIN_BATCH_SIZE=32
+ export LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ TEST_BATCH_SIZE=16
+ echo 'running benchmark'
+ WARMUP_STEPS=200
running benchmark
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 3    --dwu-num-chunks 2    --dwu-num-rs-pg 1    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-overlap-reductions    --dwu-grad-norm    '
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    '
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
+ MATH=fp16
+ declare -a CMD
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
+ '[' -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ']'
+ echo 'Using TCMalloc'
+ export LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ echo 'running benchmark'
+ '[' -n 4 ']'
+ '[' 256 -gt 32 ']'
STARTING TIMING RUN AT 2020-06-18 02:13:30 AM
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
STARTING TIMING RUN AT 2020-06-18 02:13:30 AM
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
Using TCMalloc
+ '[' -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ']'
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ TRAIN_BATCH_SIZE=32
running benchmark
+ echo 'Using TCMalloc'
STARTING TIMING RUN AT 2020-06-18 02:13:30 AM
+ TEST_BATCH_SIZE=16
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ export LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ WARMUP_STEPS=200
+ LR=4.0e-3
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
+ echo 'running benchmark'
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ NUMEPOCHS=12
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 3    --dwu-num-chunks 2    --dwu-num-rs-pg 1    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-overlap-reductions    --dwu-grad-norm    '
+ DATASET_DIR=/data
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 3    --dwu-num-chunks 2    --dwu-num-rs-pg 1    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-overlap-reductions    --dwu-grad-norm    '
Using TCMalloc
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    '
STARTING TIMING RUN AT 2020-06-18 02:13:30 AM
+ PREPROC_DATADIR=/preproc_data
STARTING TIMING RUN AT 2020-06-18 02:13:30 AM
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    '
running benchmark
+ MATH=fp16
+ RESULTS_DIR=gnmt_wmt16
+ MATH=fp16
+ declare -a CMD
+ declare -a CMD
+ LR=4.0e-3
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ '[' -n 3 ']'
+ DECAY_INTERVAL=283
+ '[' 256 -gt 32 ']'
+ '[' -n 5 ']'
+ TARGET=24.0
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
Using TCMalloc
+ '[' 256 -gt 32 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ']'
STARTING TIMING RUN AT 2020-06-18 02:13:30 AM
+ echo 'Using TCMalloc'
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 3    --dwu-num-chunks 2    --dwu-num-rs-pg 1    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-overlap-reductions    --dwu-grad-norm    '
+ export LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
Using TCMalloc
+ echo 'running benchmark'
running benchmark
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    '
+ MATH=fp16
+ declare -a CMD
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
running benchmark
+ '[' -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ']'
+ RESULTS_DIR=gnmt_wmt16
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ LR=4.0e-3
+ '[' -n 0 ']'
+ TRAIN_BATCH_SIZE=32
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ '[' 256 -gt 32 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
+ DECAY_INTERVAL=283
+ TARGET=24.0
Using TCMalloc
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ echo 'Using TCMalloc'
Using TCMalloc
running benchmark
+ '[' -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ']'
+ export LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
running benchmark
+ MAX_SEQ_LEN=75
+ LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ echo 'Using TCMalloc'
+ export LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ echo 'running benchmark'
+ NUMEPOCHS=12
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 3    --dwu-num-chunks 2    --dwu-num-rs-pg 1    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-overlap-reductions    --dwu-grad-norm    '
+ NUMEPOCHS=12
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 3    --dwu-num-chunks 2    --dwu-num-rs-pg 1    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-overlap-reductions    --dwu-grad-norm    '
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    '
+ LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ echo 'running benchmark'
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    '
+ MATH=fp16
+ MATH=fp16
+ declare -a CMD
+ declare -a CMD
STARTING TIMING RUN AT 2020-06-18 02:13:30 AM
+ '[' -n 7 ']'
+ '[' -n 1 ']'
+ '[' 256 -gt 32 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' 256 -gt 32 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ '[' -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ']'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
+ '[' -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ']'
+ echo 'Using TCMalloc'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 3    --dwu-num-chunks 2    --dwu-num-rs-pg 1    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-overlap-reductions    --dwu-grad-norm    '
+ echo 'Using TCMalloc'
+ export LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ echo 'running benchmark'
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    '
+ MATH=fp16
+ export LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ echo 'running benchmark'
+ declare -a CMD
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
+ '[' -n 5 ']'
+ DATASET_DIR=/data
+ '[' 256 -gt 32 ']'
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
Using TCMalloc
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
+ TARGET=24.0
running benchmark
+ MAX_SEQ_LEN=75
+ '[' -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ']'
+ NUMEPOCHS=12
+ echo 'Using TCMalloc'
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 3    --dwu-num-chunks 2    --dwu-num-rs-pg 1    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-overlap-reductions    --dwu-grad-norm    '
+ export LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ echo 'running benchmark'
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 5 ']'
+ '[' 256 -gt 32 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
+ '[' -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ']'
Using TCMalloc
+ echo 'Using TCMalloc'
+ export LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
running benchmark
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
STARTING TIMING RUN AT 2020-06-18 02:13:30 AM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 3    --dwu-num-chunks 2    --dwu-num-rs-pg 1    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-overlap-reductions    --dwu-grad-norm    '
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 7 ']'
+ '[' 256 -gt 32 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ']'
Using TCMalloc
+ echo 'Using TCMalloc'
+ export LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
running benchmark
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
STARTING TIMING RUN AT 2020-06-18 02:13:30 AM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 3    --dwu-num-chunks 2    --dwu-num-rs-pg 1    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-overlap-reductions    --dwu-grad-norm    '
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 6 ']'
+ '[' 256 -gt 32 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
STARTING TIMING RUN AT 2020-06-18 02:13:30 AM
+ '[' -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ']'
Using TCMalloc
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
STARTING TIMING RUN AT 2020-06-18 02:13:30 AM
+ RESULTS_DIR=gnmt_wmt16
STARTING TIMING RUN AT 2020-06-18 02:13:30 AM
+ echo 'Using TCMalloc'
+ export LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
running benchmark
+ LR=4.0e-3
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ echo 'running benchmark'
+ WARMUP_STEPS=200
+ LR=4.0e-3
+ REMAIN_STEPS=2259
+ TRAIN_BATCH_SIZE=32
+ DECAY_INTERVAL=283
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ DATASET_DIR=/data
+ TARGET=24.0
Using TCMalloc
+ REMAIN_STEPS=2259
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ MAX_SEQ_LEN=75
running benchmark
+ DECAY_INTERVAL=283
+ LR=4.0e-3
+ NUMEPOCHS=12
STARTING TIMING RUN AT 2020-06-18 02:13:30 AM
+ TARGET=24.0
+ TRAIN_BATCH_SIZE=32
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 3    --dwu-num-chunks 2    --dwu-num-rs-pg 1    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-overlap-reductions    --dwu-grad-norm    '
+ MAX_SEQ_LEN=75
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    '
+ MATH=fp16
+ NUMEPOCHS=12
+ declare -a CMD
+ TEST_BATCH_SIZE=16
+ '[' -n 7 ']'
+ WARMUP_STEPS=200
+ '[' 256 -gt 32 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ REMAIN_STEPS=2259
+ '[' -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ']'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
+ echo 'Using TCMalloc'
+ export LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ echo 'running benchmark'
+ MAX_SEQ_LEN=75
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
Using TCMalloc
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 3    --dwu-num-chunks 2    --dwu-num-rs-pg 1    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-overlap-reductions    --dwu-grad-norm    '
Using TCMalloc
+ NUMEPOCHS=12
running benchmark
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    '
+ MATH=fp16
running benchmark
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 3    --dwu-num-chunks 2    --dwu-num-rs-pg 1    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-overlap-reductions    --dwu-grad-norm    '
+ DATASET_DIR=/data
+ declare -a CMD
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    '
+ MATH=fp16
Using TCMalloc
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ declare -a CMD
+ LR=4.0e-3
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ '[' -n 4 ']'
+ WARMUP_STEPS=200
+ '[' 256 -gt 32 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
running benchmark
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ '[' -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ']'
+ '[' -n 2 ']'
+ NUMEPOCHS=12
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 3    --dwu-num-chunks 2    --dwu-num-rs-pg 1    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-overlap-reductions    --dwu-grad-norm    '
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    '
+ MATH=fp16
+ declare -a CMD
+ '[' 256 -gt 32 ']'
+ '[' -n 2 ']'
+ '[' 256 -gt 32 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ']'
+ echo 'Using TCMalloc'
+ echo 'Using TCMalloc'
+ export LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
STARTING TIMING RUN AT 2020-06-18 02:13:30 AM
+ export LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ echo 'running benchmark'
+ '[' -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ']'
+ echo 'running benchmark'
+ echo 'Using TCMalloc'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
STARTING TIMING RUN AT 2020-06-18 02:13:30 AM
+ export LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
+ echo 'running benchmark'
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ DATASET_DIR=/data
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
+ LR=4.0e-3
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ LR=4.0e-3
+ TRAIN_BATCH_SIZE=32
+ REMAIN_STEPS=2259
+ TEST_BATCH_SIZE=16
+ DECAY_INTERVAL=283
+ WARMUP_STEPS=200
+ TARGET=24.0
+ REMAIN_STEPS=2259
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 3    --dwu-num-chunks 2    --dwu-num-rs-pg 1    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-overlap-reductions    --dwu-grad-norm    '
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    '
+ MATH=fp16
+ declare -a CMD
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 3    --dwu-num-chunks 2    --dwu-num-rs-pg 1    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-overlap-reductions    --dwu-grad-norm    '
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 1 ']'
+ '[' 256 -gt 32 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' -n 4 ']'
+ '[' 256 -gt 32 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
Using TCMalloc
running benchmark
+ '[' -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ']'
+ echo 'Using TCMalloc'
+ export LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ echo 'running benchmark'
Using TCMalloc
+ '[' -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ']'
+ echo 'Using TCMalloc'
+ export LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
STARTING TIMING RUN AT 2020-06-18 02:13:30 AM
STARTING TIMING RUN AT 2020-06-18 02:13:30 AM
running benchmark
+ echo 'running benchmark'
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ TRAIN_BATCH_SIZE=32
+ DATASET_DIR=/data
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
+ PREPROC_DATADIR=/preproc_data
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ MAX_SEQ_LEN=75
+ TRAIN_BATCH_SIZE=32
+ NUMEPOCHS=12
+ TEST_BATCH_SIZE=16
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 3    --dwu-num-chunks 2    --dwu-num-rs-pg 1    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-overlap-reductions    --dwu-grad-norm    '
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    '
+ MATH=fp16
+ declare -a CMD
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
Using TCMalloc
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ '[' -n 0 ']'
+ '[' 256 -gt 32 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ']'
running benchmark
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 3    --dwu-num-chunks 2    --dwu-num-rs-pg 1    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-overlap-reductions    --dwu-grad-norm    '
+ echo 'Using TCMalloc'
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    '
+ MATH=fp16
+ declare -a CMD
+ export LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ echo 'running benchmark'
+ '[' -n 0 ']'
+ '[' 256 -gt 32 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ']'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
+ echo 'Using TCMalloc'
Using TCMalloc
+ export LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
running benchmark
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
STARTING TIMING RUN AT 2020-06-18 02:13:30 AM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
STARTING TIMING RUN AT 2020-06-18 02:13:30 AM
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 3    --dwu-num-chunks 2    --dwu-num-rs-pg 1    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-overlap-reductions    --dwu-grad-norm    '
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 3 ']'
+ '[' 256 -gt 32 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
Using TCMalloc
+ '[' -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ']'
+ echo 'Using TCMalloc'
+ export LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
running benchmark
+ echo 'running benchmark'
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 3    --dwu-num-chunks 2    --dwu-num-rs-pg 1    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-overlap-reductions    --dwu-grad-norm    '
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 6 ']'
+ '[' 256 -gt 32 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
Using TCMalloc
+ '[' -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ']'
+ echo 'Using TCMalloc'
+ export LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
running benchmark
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
STARTING TIMING RUN AT 2020-06-18 02:13:30 AM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 3    --dwu-num-chunks 2    --dwu-num-rs-pg 1    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-overlap-reductions    --dwu-grad-norm    '
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 5 ']'
+ '[' 256 -gt 32 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ']'
+ echo 'Using TCMalloc'
Using TCMalloc
+ export LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
running benchmark
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
STARTING TIMING RUN AT 2020-06-18 02:13:30 AM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 3    --dwu-num-chunks 2    --dwu-num-rs-pg 1    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-overlap-reductions    --dwu-grad-norm    '
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 1 ']'
+ '[' 256 -gt 32 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ']'
+ echo 'Using TCMalloc'
Using TCMalloc
+ export LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
running benchmark
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
STARTING TIMING RUN AT 2020-06-18 02:13:30 AM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 3    --dwu-num-chunks 2    --dwu-num-rs-pg 1    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-overlap-reductions    --dwu-grad-norm    '
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 7 ']'
+ '[' 256 -gt 32 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ']'
+ echo 'Using TCMalloc'
Using TCMalloc
+ export LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
running benchmark
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
STARTING TIMING RUN AT 2020-06-18 02:13:30 AM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 3    --dwu-num-chunks 2    --dwu-num-rs-pg 1    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-overlap-reductions    --dwu-grad-norm    '
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 0 ']'
+ '[' 256 -gt 32 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ']'
+ echo 'Using TCMalloc'
Using TCMalloc
+ export LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
running benchmark
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
STARTING TIMING RUN AT 2020-06-18 02:13:30 AM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 3    --dwu-num-chunks 2    --dwu-num-rs-pg 1    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-overlap-reductions    --dwu-grad-norm    '
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 2 ']'
+ '[' 256 -gt 32 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ']'
+ echo 'Using TCMalloc'
Using TCMalloc
+ export LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ echo 'running benchmark'
running benchmark
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
STARTING TIMING RUN AT 2020-06-18 02:13:30 AM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
STARTING TIMING RUN AT 2020-06-18 02:13:30 AM
+ LR=4.0e-3
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ WARMUP_STEPS=200
+ RESULTS_DIR=gnmt_wmt16
+ REMAIN_STEPS=2259
+ LR=4.0e-3
+ TRAIN_BATCH_SIZE=32
+ DECAY_INTERVAL=283
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ TARGET=24.0
+ REMAIN_STEPS=2259
+ MAX_SEQ_LEN=75
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ NUMEPOCHS=12
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 3    --dwu-num-chunks 2    --dwu-num-rs-pg 1    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-overlap-reductions    --dwu-grad-norm    '
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 3    --dwu-num-chunks 2    --dwu-num-rs-pg 1    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-overlap-reductions    --dwu-grad-norm    '
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    '
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    '
+ MATH=fp16
+ MATH=fp16
+ declare -a CMD
+ declare -a CMD
+ '[' -n 6 ']'
+ '[' 256 -gt 32 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' -n 3 ']'
+ '[' -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ']'
+ echo 'Using TCMalloc'
Using TCMalloc
+ '[' 256 -gt 32 ']'
+ export LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
running benchmark
+ echo 'running benchmark'
+ '[' -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ']'
Using TCMalloc
+ echo 'Using TCMalloc'
+ export LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
running benchmark
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
STARTING TIMING RUN AT 2020-06-18 02:13:30 AM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 3    --dwu-num-chunks 2    --dwu-num-rs-pg 1    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-overlap-reductions    --dwu-grad-norm    '
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 4 ']'
+ '[' 256 -gt 32 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ']'
+ echo 'Using TCMalloc'
Using TCMalloc
+ export LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ echo 'running benchmark'
running benchmark
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
STARTING TIMING RUN AT 2020-06-18 02:13:30 AM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 3    --dwu-num-chunks 2    --dwu-num-rs-pg 1    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-overlap-reductions    --dwu-grad-norm    '
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    '
+ MATH=fp16
+ declare -a CMD
STARTING TIMING RUN AT 2020-06-18 02:13:30 AM
+ '[' -n 7 ']'
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ '[' 256 -gt 32 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ LR=4.0e-3
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ '[' -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ']'
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
Using TCMalloc
+ echo 'Using TCMalloc'
+ export LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ TARGET=24.0
+ LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
running benchmark
+ echo 'running benchmark'
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 3    --dwu-num-chunks 2    --dwu-num-rs-pg 1    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-overlap-reductions    --dwu-grad-norm    '
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 0 ']'
+ '[' 256 -gt 32 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
Using TCMalloc
+ '[' -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ']'
+ echo 'Using TCMalloc'
+ export LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
running benchmark
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
STARTING TIMING RUN AT 2020-06-18 02:13:30 AM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 3    --dwu-num-chunks 2    --dwu-num-rs-pg 1    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-overlap-reductions    --dwu-grad-norm    '
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 0 ']'
+ '[' 256 -gt 32 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ']'
+ echo 'Using TCMalloc'
Using TCMalloc
+ export LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
running benchmark
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
STARTING TIMING RUN AT 2020-06-18 02:13:30 AM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 3    --dwu-num-chunks 2    --dwu-num-rs-pg 1    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-overlap-reductions    --dwu-grad-norm    '
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 7 ']'
+ '[' 256 -gt 32 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ']'
Using TCMalloc
+ echo 'Using TCMalloc'
+ export LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
running benchmark
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
STARTING TIMING RUN AT 2020-06-18 02:13:30 AM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 3    --dwu-num-chunks 2    --dwu-num-rs-pg 1    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-overlap-reductions    --dwu-grad-norm    '
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 1 ']'
+ '[' 256 -gt 32 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ']'
Using TCMalloc
+ echo 'Using TCMalloc'
+ export LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ echo 'running benchmark'
running benchmark
STARTING TIMING RUN AT 2020-06-18 02:13:30 AM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 3    --dwu-num-chunks 2    --dwu-num-rs-pg 1    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-overlap-reductions    --dwu-grad-norm    '
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 3 ']'
+ '[' 256 -gt 32 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ']'
+ echo 'Using TCMalloc'
Using TCMalloc
running benchmark
+ export LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
STARTING TIMING RUN AT 2020-06-18 02:13:30 AM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 3    --dwu-num-chunks 2    --dwu-num-rs-pg 1    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-overlap-reductions    --dwu-grad-norm    '
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 2 ']'
+ '[' 256 -gt 32 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ']'
Using TCMalloc
+ echo 'Using TCMalloc'
+ export LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
running benchmark
+ LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
STARTING TIMING RUN AT 2020-06-18 02:13:30 AM
STARTING TIMING RUN AT 2020-06-18 02:13:30 AM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 3    --dwu-num-chunks 2    --dwu-num-rs-pg 1    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-overlap-reductions    --dwu-grad-norm    '
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    '
+ MATH=fp16
+ declare -a CMD
+ DATASET_DIR=/data
+ '[' -n 1 ']'
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ '[' 256 -gt 32 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ LR=4.0e-3
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ MAX_SEQ_LEN=75
STARTING TIMING RUN AT 2020-06-18 02:13:30 AM
+ NUMEPOCHS=12
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 3    --dwu-num-chunks 2    --dwu-num-rs-pg 1    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-overlap-reductions    --dwu-grad-norm    '
+ '[' -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ']'
Using TCMalloc
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    '
+ MATH=fp16
+ echo 'Using TCMalloc'
+ export LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
running benchmark
+ declare -a CMD
+ DATASET_DIR=/data
+ echo 'running benchmark'
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ TRAIN_BATCH_SIZE=32
+ '[' -n 5 ']'
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ '[' 256 -gt 32 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 3    --dwu-num-chunks 2    --dwu-num-rs-pg 1    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-overlap-reductions    --dwu-grad-norm    '
+ '[' -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ']'
+ echo 'Using TCMalloc'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
Using TCMalloc
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    '
+ MATH=fp16
+ export LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ echo 'running benchmark'
+ declare -a CMD
running benchmark
+ '[' -n 6 ']'
+ '[' 256 -gt 32 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ']'
+ echo 'Using TCMalloc'
Using TCMalloc
+ export LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
running benchmark
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
STARTING TIMING RUN AT 2020-06-18 02:13:30 AM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 3    --dwu-num-chunks 2    --dwu-num-rs-pg 1    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-overlap-reductions    --dwu-grad-norm    '
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 6 ']'
+ '[' 256 -gt 32 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ']'
Using TCMalloc
+ echo 'Using TCMalloc'
+ export LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
running benchmark
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
STARTING TIMING RUN AT 2020-06-18 02:13:30 AM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 3    --dwu-num-chunks 2    --dwu-num-rs-pg 1    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-overlap-reductions    --dwu-grad-norm    '
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 5 ']'
+ '[' 256 -gt 32 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ']'
Using TCMalloc
+ echo 'Using TCMalloc'
+ export LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
STARTING TIMING RUN AT 2020-06-18 02:13:30 AM
+ LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ echo 'running benchmark'
running benchmark
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 3    --dwu-num-chunks 2    --dwu-num-rs-pg 1    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-overlap-reductions    --dwu-grad-norm    '
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    '
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
+ MATH=fp16
+ declare -a CMD
STARTING TIMING RUN AT 2020-06-18 02:13:30 AM
+ DATASET_DIR=/data
+ '[' -n 4 ']'
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ '[' 256 -gt 32 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ TRAIN_BATCH_SIZE=32
STARTING TIMING RUN AT 2020-06-18 02:13:30 AM
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
Using TCMalloc
+ '[' -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ']'
+ echo 'Using TCMalloc'
+ export LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ echo 'running benchmark'
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ NUMEPOCHS=12
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 3    --dwu-num-chunks 2    --dwu-num-rs-pg 1    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-overlap-reductions    --dwu-grad-norm    '
running benchmark
+ LR=4.0e-3
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    '
+ MATH=fp16
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ declare -a CMD
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 3    --dwu-num-chunks 2    --dwu-num-rs-pg 1    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-overlap-reductions    --dwu-grad-norm    '
+ '[' -n 3 ']'
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    '
+ MATH=fp16
+ declare -a CMD
+ '[' 256 -gt 32 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
+ '[' -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ']'
Using TCMalloc
+ echo 'Using TCMalloc'
+ export LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ '[' -n 2 ']'
+ LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
running benchmark
+ echo 'running benchmark'
+ '[' 256 -gt 32 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ']'
Using TCMalloc
+ echo 'Using TCMalloc'
+ export LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
running benchmark
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
STARTING TIMING RUN AT 2020-06-18 02:13:30 AM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 3    --dwu-num-chunks 2    --dwu-num-rs-pg 1    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-overlap-reductions    --dwu-grad-norm    '
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 5 ']'
+ '[' 256 -gt 32 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ']'
Using TCMalloc
+ echo 'Using TCMalloc'
+ export LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
running benchmark
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
STARTING TIMING RUN AT 2020-06-18 02:13:30 AM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 3    --dwu-num-chunks 2    --dwu-num-rs-pg 1    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-overlap-reductions    --dwu-grad-norm    '
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 3 ']'
+ '[' 256 -gt 32 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ']'
Using TCMalloc
+ echo 'Using TCMalloc'
+ export LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
running benchmark
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
STARTING TIMING RUN AT 2020-06-18 02:13:30 AM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 3    --dwu-num-chunks 2    --dwu-num-rs-pg 1    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-overlap-reductions    --dwu-grad-norm    '
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 2 ']'
+ '[' 256 -gt 32 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ']'
Using TCMalloc
+ echo 'Using TCMalloc'
+ export LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ echo 'running benchmark'
running benchmark
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
STARTING TIMING RUN AT 2020-06-18 02:13:30 AM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
+ TARGET=24.0
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 3    --dwu-num-chunks 2    --dwu-num-rs-pg 1    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-overlap-reductions    --dwu-grad-norm    '
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    '
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ MATH=fp16
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ declare -a CMD
+ '[' -n 6 ']'
+ '[' 256 -gt 32 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ']'
Using TCMalloc
+ echo 'Using TCMalloc'
+ export LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
running benchmark
+ echo 'running benchmark'
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
STARTING TIMING RUN AT 2020-06-18 02:13:30 AM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 3    --dwu-num-chunks 2    --dwu-num-rs-pg 1    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-overlap-reductions    --dwu-grad-norm    '
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 7 ']'
+ '[' 256 -gt 32 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ']'
Using TCMalloc
+ echo 'Using TCMalloc'
+ export LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ echo 'running benchmark'
running benchmark
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
STARTING TIMING RUN AT 2020-06-18 02:13:30 AM
STARTING TIMING RUN AT 2020-06-18 02:13:30 AM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ TRAIN_BATCH_SIZE=32
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ NUMEPOCHS=12
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 3    --dwu-num-chunks 2    --dwu-num-rs-pg 1    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-overlap-reductions    --dwu-grad-norm    '
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 3    --dwu-num-chunks 2    --dwu-num-rs-pg 1    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-overlap-reductions    --dwu-grad-norm    '
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    '
+ MATH=fp16
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    '
+ MATH=fp16
+ declare -a CMD
+ declare -a CMD
+ '[' -n 0 ']'
+ '[' -n 1 ']'
+ '[' 256 -gt 32 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' 256 -gt 32 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ']'
+ '[' -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ']'
+ echo 'Using TCMalloc'
+ echo 'Using TCMalloc'
Using TCMalloc
+ export LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
Using TCMalloc
+ export LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
running benchmark
+ echo 'running benchmark'
running benchmark
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
STARTING TIMING RUN AT 2020-06-18 02:13:30 AM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 3    --dwu-num-chunks 2    --dwu-num-rs-pg 1    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-overlap-reductions    --dwu-grad-norm    '
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 4 ']'
+ '[' 256 -gt 32 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ']'
Using TCMalloc
+ echo 'Using TCMalloc'
+ export LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ echo 'running benchmark'
running benchmark
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
STARTING TIMING RUN AT 2020-06-18 02:13:30 AM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 3    --dwu-num-chunks 2    --dwu-num-rs-pg 1    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-overlap-reductions    --dwu-grad-norm    '
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 5 ']'
+ '[' 256 -gt 32 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ']'
Using TCMalloc
+ echo 'Using TCMalloc'
+ export LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
running benchmark
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
STARTING TIMING RUN AT 2020-06-18 02:13:30 AM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 3    --dwu-num-chunks 2    --dwu-num-rs-pg 1    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-overlap-reductions    --dwu-grad-norm    '
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 6 ']'
+ '[' 256 -gt 32 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ']'
Using TCMalloc
+ echo 'Using TCMalloc'
+ export LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
running benchmark
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
STARTING TIMING RUN AT 2020-06-18 02:13:30 AM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 3    --dwu-num-chunks 2    --dwu-num-rs-pg 1    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-overlap-reductions    --dwu-grad-norm    '
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 3 ']'
+ '[' 256 -gt 32 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ']'
Using TCMalloc
+ echo 'Using TCMalloc'
+ export LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ echo 'running benchmark'
running benchmark
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
STARTING TIMING RUN AT 2020-06-18 02:13:30 AM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 3    --dwu-num-chunks 2    --dwu-num-rs-pg 1    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-overlap-reductions    --dwu-grad-norm    '
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 2 ']'
+ '[' 256 -gt 32 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ']'
Using TCMalloc
+ echo 'Using TCMalloc'
+ export LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ echo 'running benchmark'
running benchmark
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
STARTING TIMING RUN AT 2020-06-18 02:13:30 AM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ TRAIN_BATCH_SIZE=32
STARTING TIMING RUN AT 2020-06-18 02:13:30 AM
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ DATASET_DIR=/data
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 3    --dwu-num-chunks 2    --dwu-num-rs-pg 1    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-overlap-reductions    --dwu-grad-norm    '
+ LR=4.0e-3
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    '
+ MATH=fp16
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ declare -a CMD
STARTING TIMING RUN AT 2020-06-18 02:13:30 AM
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 3    --dwu-num-chunks 2    --dwu-num-rs-pg 1    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-overlap-reductions    --dwu-grad-norm    '
+ '[' -n 1 ']'
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    '
+ MATH=fp16
+ '[' 256 -gt 32 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ TRAIN_BATCH_SIZE=32
+ declare -a CMD
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
Using TCMalloc
+ '[' -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ']'
+ echo 'Using TCMalloc'
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ export LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
running benchmark
+ echo 'running benchmark'
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 3    --dwu-num-chunks 2    --dwu-num-rs-pg 1    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-overlap-reductions    --dwu-grad-norm    '
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    '
+ '[' -n 7 ']'
+ MATH=fp16
+ declare -a CMD
+ '[' 256 -gt 32 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ']'
Using TCMalloc
+ echo 'Using TCMalloc'
+ export LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ '[' -n 6 ']'
running benchmark
+ echo 'running benchmark'
+ '[' 256 -gt 32 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ']'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
Using TCMalloc
+ echo 'Using TCMalloc'
+ export LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ echo 'running benchmark'
running benchmark
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
STARTING TIMING RUN AT 2020-06-18 02:13:30 AM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ MAX_SEQ_LEN=75
STARTING TIMING RUN AT 2020-06-18 02:13:30 AM
+ NUMEPOCHS=12
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 3    --dwu-num-chunks 2    --dwu-num-rs-pg 1    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-overlap-reductions    --dwu-grad-norm    '
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    '
+ MATH=fp16
+ declare -a CMD
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ '[' -n 0 ']'
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ '[' 256 -gt 32 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 3    --dwu-num-chunks 2    --dwu-num-rs-pg 1    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-overlap-reductions    --dwu-grad-norm    '
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    '
+ MATH=fp16
STARTING TIMING RUN AT 2020-06-18 02:13:30 AM
+ '[' -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ']'
+ declare -a CMD
Using TCMalloc
+ echo 'Using TCMalloc'
+ export LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
running benchmark
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ TRAIN_BATCH_SIZE=32
+ echo 'running benchmark'
+ TEST_BATCH_SIZE=16
+ '[' -n 2 ']'
+ '[' 256 -gt 32 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
+ TARGET=24.0
Using TCMalloc
+ MAX_SEQ_LEN=75
+ '[' -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ']'
+ NUMEPOCHS=12
running benchmark
+ echo 'Using TCMalloc'
+ export LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 3    --dwu-num-chunks 2    --dwu-num-rs-pg 1    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-overlap-reductions    --dwu-grad-norm    '
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 4 ']'
+ '[' 256 -gt 32 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
STARTING TIMING RUN AT 2020-06-18 02:13:30 AM
+ '[' -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ']'
+ echo 'Using TCMalloc'
Using TCMalloc
+ export LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
running benchmark
+ DATASET_DIR=/data
+ echo 'running benchmark'
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 3    --dwu-num-chunks 2    --dwu-num-rs-pg 1    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-overlap-reductions    --dwu-grad-norm    '
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 4 ']'
+ '[' 256 -gt 32 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ']'
Using TCMalloc
+ echo 'Using TCMalloc'
+ export LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
running benchmark
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
STARTING TIMING RUN AT 2020-06-18 02:13:30 AM
STARTING TIMING RUN AT 2020-06-18 02:13:30 AM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 3    --dwu-num-chunks 2    --dwu-num-rs-pg 1    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-overlap-reductions    --dwu-grad-norm    '
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    '
+ MATH=fp16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ declare -a CMD
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 3    --dwu-num-chunks 2    --dwu-num-rs-pg 1    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-overlap-reductions    --dwu-grad-norm    '
+ '[' -n 0 ']'
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    '
+ '[' 256 -gt 32 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ MATH=fp16
+ declare -a CMD
+ '[' -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ']'
Using TCMalloc
+ echo 'Using TCMalloc'
+ export LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
running benchmark
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
+ '[' -n 3 ']'
+ '[' 256 -gt 32 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
Using TCMalloc
+ '[' -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ']'
+ echo 'Using TCMalloc'
+ export LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
running benchmark
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
STARTING TIMING RUN AT 2020-06-18 02:13:30 AM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 3    --dwu-num-chunks 2    --dwu-num-rs-pg 1    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-overlap-reductions    --dwu-grad-norm    '
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 1 ']'
+ '[' 256 -gt 32 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ']'
Using TCMalloc
+ echo 'Using TCMalloc'
+ export LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
running benchmark
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
STARTING TIMING RUN AT 2020-06-18 02:13:30 AM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
STARTING TIMING RUN AT 2020-06-18 02:13:30 AM
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 3    --dwu-num-chunks 2    --dwu-num-rs-pg 1    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-overlap-reductions    --dwu-grad-norm    '
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    '
+ DATASET_DIR=/data
+ MATH=fp16
+ PREPROC_DATADIR=/preproc_data
+ declare -a CMD
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ '[' -n 5 ']'
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ '[' 256 -gt 32 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ WARMUP_STEPS=200
+ '[' -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ']'
+ REMAIN_STEPS=2259
Using TCMalloc
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ echo 'Using TCMalloc'
+ DECAY_INTERVAL=283
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ export LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ TARGET=24.0
running benchmark
+ MAX_SEQ_LEN=75
+ echo 'running benchmark'
+ NUMEPOCHS=12
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 3    --dwu-num-chunks 2    --dwu-num-rs-pg 1    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-overlap-reductions    --dwu-grad-norm    '
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 7 ']'
+ '[' 256 -gt 32 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
+ '[' -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ']'
Using TCMalloc
+ echo 'Using TCMalloc'
+ export LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
running benchmark
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
STARTING TIMING RUN AT 2020-06-18 02:13:30 AM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 3    --dwu-num-chunks 2    --dwu-num-rs-pg 1    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-overlap-reductions    --dwu-grad-norm    '
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 7 ']'
+ '[' 256 -gt 32 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ']'
Using TCMalloc
+ echo 'Using TCMalloc'
+ export LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ echo 'running benchmark'
running benchmark
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
STARTING TIMING RUN AT 2020-06-18 02:13:30 AM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
STARTING TIMING RUN AT 2020-06-18 02:13:30 AM
+ LR=4.0e-3
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ DATASET_DIR=/data
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ LR=4.0e-3
+ NUMEPOCHS=12
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 3    --dwu-num-chunks 2    --dwu-num-rs-pg 1    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-overlap-reductions    --dwu-grad-norm    '
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    '
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ MATH=fp16
+ DECAY_INTERVAL=283
+ declare -a CMD
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 3    --dwu-num-chunks 2    --dwu-num-rs-pg 1    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-overlap-reductions    --dwu-grad-norm    '
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 0 ']'
+ '[' 256 -gt 32 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' -n 4 ']'
+ '[' -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ']'
+ '[' 256 -gt 32 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
Using TCMalloc
+ echo 'Using TCMalloc'
+ export LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
running benchmark
+ echo 'running benchmark'
+ '[' -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ']'
Using TCMalloc
+ echo 'Using TCMalloc'
+ export LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ echo 'running benchmark'
running benchmark
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
STARTING TIMING RUN AT 2020-06-18 02:13:30 AM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 3    --dwu-num-chunks 2    --dwu-num-rs-pg 1    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-overlap-reductions    --dwu-grad-norm    '
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 2 ']'
+ '[' 256 -gt 32 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ']'
Using TCMalloc
+ echo 'Using TCMalloc'
+ export LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
running benchmark
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
STARTING TIMING RUN AT 2020-06-18 02:13:30 AM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 3    --dwu-num-chunks 2    --dwu-num-rs-pg 1    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-overlap-reductions    --dwu-grad-norm    '
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 5 ']'
+ '[' 256 -gt 32 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ']'
Using TCMalloc
+ echo 'Using TCMalloc'
+ export LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
running benchmark
+ LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
STARTING TIMING RUN AT 2020-06-18 02:13:30 AM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 3    --dwu-num-chunks 2    --dwu-num-rs-pg 1    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-overlap-reductions    --dwu-grad-norm    '
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 1 ']'
+ '[' 256 -gt 32 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ']'
Using TCMalloc
+ echo 'Using TCMalloc'
+ export LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
STARTING TIMING RUN AT 2020-06-18 02:13:30 AM
running benchmark
+ echo 'running benchmark'
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 3    --dwu-num-chunks 2    --dwu-num-rs-pg 1    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-overlap-reductions    --dwu-grad-norm    '
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    '
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
+ MATH=fp16
+ declare -a CMD
+ '[' -n 3 ']'
+ '[' 256 -gt 32 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ']'
Using TCMalloc
+ echo 'Using TCMalloc'
+ export LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
running benchmark
+ echo 'running benchmark'
STARTING TIMING RUN AT 2020-06-18 02:13:30 AM
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 3    --dwu-num-chunks 2    --dwu-num-rs-pg 1    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-overlap-reductions    --dwu-grad-norm    '
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 6 ']'
+ '[' 256 -gt 32 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ']'
Using TCMalloc
+ echo 'Using TCMalloc'
+ export LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ echo 'running benchmark'
running benchmark
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
STARTING TIMING RUN AT 2020-06-18 02:13:30 AM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 3    --dwu-num-chunks 2    --dwu-num-rs-pg 1    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-overlap-reductions    --dwu-grad-norm    '
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 1 ']'
+ '[' 256 -gt 32 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ']'
Using TCMalloc
+ echo 'Using TCMalloc'
+ export LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ echo 'running benchmark'
running benchmark
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
STARTING TIMING RUN AT 2020-06-18 02:13:30 AM
STARTING TIMING RUN AT 2020-06-18 02:13:30 AM
STARTING TIMING RUN AT 2020-06-18 02:13:30 AM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ TRAIN_BATCH_SIZE=32
+ TRAIN_BATCH_SIZE=32
+ DATASET_DIR=/data
+ TEST_BATCH_SIZE=16
+ TEST_BATCH_SIZE=16
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ WARMUP_STEPS=200
+ WARMUP_STEPS=200
+ LR=4.0e-3
+ REMAIN_STEPS=2259
+ REMAIN_STEPS=2259
+ TRAIN_BATCH_SIZE=32
+ DECAY_INTERVAL=283
+ DECAY_INTERVAL=283
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ TARGET=24.0
+ TARGET=24.0
+ REMAIN_STEPS=2259
+ MAX_SEQ_LEN=75
+ MAX_SEQ_LEN=75
+ DECAY_INTERVAL=283
+ NUMEPOCHS=12
+ NUMEPOCHS=12
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 3    --dwu-num-chunks 2    --dwu-num-rs-pg 1    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-overlap-reductions    --dwu-grad-norm    '
+ NUMEPOCHS=12
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 3    --dwu-num-chunks 2    --dwu-num-rs-pg 1    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-overlap-reductions    --dwu-grad-norm    '
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 3    --dwu-num-chunks 2    --dwu-num-rs-pg 1    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-overlap-reductions    --dwu-grad-norm    '
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    '
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    '
+ MATH=fp16
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    '
+ MATH=fp16
+ MATH=fp16
+ declare -a CMD
+ declare -a CMD
+ declare -a CMD
+ '[' -n 3 ']'
+ '[' 256 -gt 32 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' -n 4 ']'
+ '[' -n 5 ']'
+ '[' 256 -gt 32 ']'
+ '[' 256 -gt 32 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ']'
Using TCMalloc
+ echo 'Using TCMalloc'
+ export LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
running benchmark
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
+ '[' -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ']'
+ '[' -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ']'
Using TCMalloc
+ echo 'Using TCMalloc'
Using TCMalloc
+ echo 'Using TCMalloc'
+ export LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ export LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
running benchmark
+ echo 'running benchmark'
running benchmark
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
STARTING TIMING RUN AT 2020-06-18 02:13:30 AM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 3    --dwu-num-chunks 2    --dwu-num-rs-pg 1    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-overlap-reductions    --dwu-grad-norm    '
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 2 ']'
+ '[' 256 -gt 32 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ']'
Using TCMalloc
+ echo 'Using TCMalloc'
+ export LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ echo 'running benchmark'
running benchmark
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
STARTING TIMING RUN AT 2020-06-18 02:13:30 AM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 3    --dwu-num-chunks 2    --dwu-num-rs-pg 1    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-overlap-reductions    --dwu-grad-norm    '
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 0 ']'
+ '[' 256 -gt 32 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ']'
Using TCMalloc
+ echo 'Using TCMalloc'
+ export LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
running benchmark
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
STARTING TIMING RUN AT 2020-06-18 02:13:30 AM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 3    --dwu-num-chunks 2    --dwu-num-rs-pg 1    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-overlap-reductions    --dwu-grad-norm    '
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 6 ']'
+ '[' 256 -gt 32 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ']'
Using TCMalloc
+ echo 'Using TCMalloc'
+ export LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
running benchmark
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
STARTING TIMING RUN AT 2020-06-18 02:13:30 AM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 3    --dwu-num-chunks 2    --dwu-num-rs-pg 1    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-overlap-reductions    --dwu-grad-norm    '
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 7 ']'
+ '[' 256 -gt 32 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ']'
Using TCMalloc
+ echo 'Using TCMalloc'
+ export LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
running benchmark
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
STARTING TIMING RUN AT 2020-06-18 02:13:30 AM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 3    --dwu-num-chunks 2    --dwu-num-rs-pg 1    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-overlap-reductions    --dwu-grad-norm    '
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 7 ']'
+ '[' 256 -gt 32 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ']'
Using TCMalloc
+ echo 'Using TCMalloc'
+ export LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
running benchmark
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
STARTING TIMING RUN AT 2020-06-18 02:13:30 AM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 3    --dwu-num-chunks 2    --dwu-num-rs-pg 1    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-overlap-reductions    --dwu-grad-norm    '
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 5 ']'
+ '[' 256 -gt 32 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ']'
Using TCMalloc
+ echo 'Using TCMalloc'
+ export LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
running benchmark
+ echo 'running benchmark'
STARTING TIMING RUN AT 2020-06-18 02:13:30 AM
+ DATASET_DIR=/data
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 3    --dwu-num-chunks 2    --dwu-num-rs-pg 1    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-overlap-reductions    --dwu-grad-norm    '
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 4 ']'
+ '[' 256 -gt 32 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ']'
+ echo 'Using TCMalloc'
Using TCMalloc
+ export LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
running benchmark
+ echo 'running benchmark'
STARTING TIMING RUN AT 2020-06-18 02:13:30 AM
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 3    --dwu-num-chunks 2    --dwu-num-rs-pg 1    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-overlap-reductions    --dwu-grad-norm    '
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 1 ']'
+ '[' 256 -gt 32 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ']'
Using TCMalloc
+ echo 'Using TCMalloc'
+ export LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ echo 'running benchmark'
running benchmark
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
STARTING TIMING RUN AT 2020-06-18 02:13:30 AM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 3    --dwu-num-chunks 2    --dwu-num-rs-pg 1    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-overlap-reductions    --dwu-grad-norm    '
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 6 ']'
+ '[' 256 -gt 32 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ']'
Using TCMalloc
+ echo 'Using TCMalloc'
+ export LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
running benchmark
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
STARTING TIMING RUN AT 2020-06-18 02:13:30 AM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
+ TARGET=24.0
STARTING TIMING RUN AT 2020-06-18 02:13:30 AM
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 3    --dwu-num-chunks 2    --dwu-num-rs-pg 1    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-overlap-reductions    --dwu-grad-norm    '
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    '
+ DATASET_DIR=/data
+ MATH=fp16
+ PREPROC_DATADIR=/preproc_data
+ declare -a CMD
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 3    --dwu-num-chunks 2    --dwu-num-rs-pg 1    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-overlap-reductions    --dwu-grad-norm    '
+ '[' -n 3 ']'
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    '
+ MATH=fp16
+ '[' 256 -gt 32 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ declare -a CMD
+ '[' -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ']'
+ '[' -n 2 ']'
Using TCMalloc
+ echo 'Using TCMalloc'
+ '[' 256 -gt 32 ']'
+ export LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ echo 'running benchmark'
running benchmark
+ '[' -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ']'
+ echo 'Using TCMalloc'
Using TCMalloc
+ export LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ echo 'running benchmark'
running benchmark
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
STARTING TIMING RUN AT 2020-06-18 02:13:30 AM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 3    --dwu-num-chunks 2    --dwu-num-rs-pg 1    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-overlap-reductions    --dwu-grad-norm    '
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 0 ']'
+ '[' 256 -gt 32 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ']'
Using TCMalloc
+ echo 'Using TCMalloc'
+ export LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ echo 'running benchmark'
running benchmark
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
STARTING TIMING RUN AT 2020-06-18 02:13:30 AM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 3    --dwu-num-chunks 2    --dwu-num-rs-pg 1    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-overlap-reductions    --dwu-grad-norm    '
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 4 ']'
+ '[' 256 -gt 32 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ']'
Using TCMalloc
+ echo 'Using TCMalloc'
+ export LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ echo 'running benchmark'
running benchmark
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
STARTING TIMING RUN AT 2020-06-18 02:13:30 AM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 3    --dwu-num-chunks 2    --dwu-num-rs-pg 1    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-overlap-reductions    --dwu-grad-norm    '
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 6 ']'
STARTING TIMING RUN AT 2020-06-18 02:13:30 AM
+ '[' 256 -gt 32 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ']'
+ echo 'Using TCMalloc'
Using TCMalloc
+ DATASET_DIR=/data
+ export LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
running benchmark
+ echo 'running benchmark'
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 3    --dwu-num-chunks 2    --dwu-num-rs-pg 1    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-overlap-reductions    --dwu-grad-norm    '
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    '
+ MATH=fp16
+ declare -a CMD
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
+ '[' -n 5 ']'
+ '[' 256 -gt 32 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ']'
+ echo 'Using TCMalloc'
Using TCMalloc
+ export LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
running benchmark
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
STARTING TIMING RUN AT 2020-06-18 02:13:30 AM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 3    --dwu-num-chunks 2    --dwu-num-rs-pg 1    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-overlap-reductions    --dwu-grad-norm    '
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 1 ']'
+ '[' 256 -gt 32 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ']'
+ echo 'Using TCMalloc'
Using TCMalloc
+ export LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
running benchmark
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
STARTING TIMING RUN AT 2020-06-18 02:13:30 AM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 3    --dwu-num-chunks 2    --dwu-num-rs-pg 1    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-overlap-reductions    --dwu-grad-norm    '
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 3 ']'
+ '[' 256 -gt 32 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ']'
Using TCMalloc
+ echo 'Using TCMalloc'
+ export LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
running benchmark
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
STARTING TIMING RUN AT 2020-06-18 02:13:30 AM
STARTING TIMING RUN AT 2020-06-18 02:13:30 AM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ WARMUP_STEPS=200
+ RESULTS_DIR=gnmt_wmt16
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
+ LR=4.0e-3
+ TRAIN_BATCH_SIZE=32
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ NUMEPOCHS=12
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 3    --dwu-num-chunks 2    --dwu-num-rs-pg 1    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-overlap-reductions    --dwu-grad-norm    '
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    '
+ MATH=fp16
+ TARGET=24.0
+ declare -a CMD
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 3    --dwu-num-chunks 2    --dwu-num-rs-pg 1    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-overlap-reductions    --dwu-grad-norm    '
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 0 ']'
+ '[' 256 -gt 32 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' -n 7 ']'
+ '[' -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ']'
+ echo 'Using TCMalloc'
+ '[' 256 -gt 32 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
Using TCMalloc
+ export LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
running benchmark
+ echo 'running benchmark'
+ '[' -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ']'
Using TCMalloc
+ echo 'Using TCMalloc'
+ export LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
running benchmark
+ echo 'running benchmark'
STARTING TIMING RUN AT 2020-06-18 02:13:30 AM
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 3    --dwu-num-chunks 2    --dwu-num-rs-pg 1    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-overlap-reductions    --dwu-grad-norm    '
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 2 ']'
+ '[' 256 -gt 32 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ']'
Using TCMalloc
+ echo 'Using TCMalloc'
+ export LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
running benchmark
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
num_sockets = 2 num_nodes=8 cores_per_socket=64
+ exec numactl --physcpubind=96-111,224-239 --membind=6 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
num_sockets = 2 num_nodes=8 cores_per_socket=64
+ exec numactl --physcpubind=16-31,144-159 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
num_sockets = 2 num_nodes=8 cores_per_socket=64
+ exec numactl --physcpubind=112-127,240-255 --membind=7 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
num_sockets = 2 num_nodes=8 cores_per_socket=64
+ exec numactl --physcpubind=64-79,192-207 --membind=4 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
num_sockets = 2 num_nodes=8 cores_per_socket=64
+ exec numactl --physcpubind=112-127,240-255 --membind=7 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
num_sockets = 2 num_nodes=8 cores_per_socket=64
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ exec numactl --physcpubind=16-31,144-159 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
num_sockets = 2 num_nodes=8 cores_per_socket=64
num_sockets = 2 num_nodes=8 cores_per_socket=64
+ exec numactl --physcpubind=96-111,224-239 --membind=6 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
+ exec numactl --physcpubind=80-95,208-223 --membind=5 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
num_sockets = 2 num_nodes=8 cores_per_socket=64
+ exec numactl --physcpubind=112-127,240-255 --membind=7 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
num_sockets = 2 num_nodes=8 cores_per_socket=64
+ exec numactl --physcpubind=64-79,192-207 --membind=4 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
num_sockets = 2 num_nodes=8 cores_per_socket=64
num_sockets = 2 num_nodes=8 cores_per_socket=64
+ exec numactl --physcpubind=32-47,160-175 --membind=2 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
+ exec numactl --physcpubind=80-95,208-223 --membind=5 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
num_sockets = 2 num_nodes=8 cores_per_socket=64
num_sockets = 2 num_nodes=8 cores_per_socket=64
num_sockets = 2 num_nodes=8 cores_per_socket=64
+ exec numactl --physcpubind=16-31,144-159 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
num_sockets = 2 num_nodes=8 cores_per_socket=64
+ exec numactl --physcpubind=64-79,192-207 --membind=4 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
num_sockets = 2 num_nodes=8 cores_per_socket=64
+ exec numactl --physcpubind=112-127,240-255 --membind=7 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
+ exec numactl --physcpubind=32-47,160-175 --membind=2 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
num_sockets = 2 num_nodes=8 cores_per_socket=64
STARTING TIMING RUN AT 2020-06-18 02:13:31 AM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ TRAIN_BATCH_SIZE=32
+ exec numactl --physcpubind=112-127,240-255 --membind=7 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 3    --dwu-num-chunks 2    --dwu-num-rs-pg 1    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-overlap-reductions    --dwu-grad-norm    '
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 4 ']'
+ '[' 256 -gt 32 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ']'
Using TCMalloc
+ echo 'Using TCMalloc'
+ export LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ echo 'running benchmark'
running benchmark
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
STARTING TIMING RUN AT 2020-06-18 02:13:31 AM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 3    --dwu-num-chunks 2    --dwu-num-rs-pg 1    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-overlap-reductions    --dwu-grad-norm    '
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    '
+ MATH=fp16
+ declare -a CMD
STARTING TIMING RUN AT 2020-06-18 02:13:31 AM
+ '[' -n 7 ']'
+ '[' 256 -gt 32 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
STARTING TIMING RUN AT 2020-06-18 02:13:31 AM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ TRAIN_BATCH_SIZE=32
Using TCMalloc
+ '[' -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ']'
+ echo 'Using TCMalloc'
+ export LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ DATASET_DIR=/data
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
running benchmark
num_sockets = 2 num_nodes=8 cores_per_socket=64
+ echo 'running benchmark'
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ REMAIN_STEPS=2259
+ LR=4.0e-3
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ TRAIN_BATCH_SIZE=32
+ MAX_SEQ_LEN=75
+ TEST_BATCH_SIZE=16
+ NUMEPOCHS=12
+ WARMUP_STEPS=200
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 3    --dwu-num-chunks 2    --dwu-num-rs-pg 1    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-overlap-reductions    --dwu-grad-norm    '
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    '
+ MATH=fp16
+ declare -a CMD
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 3    --dwu-num-chunks 2    --dwu-num-rs-pg 1    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-overlap-reductions    --dwu-grad-norm    '
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    '
STARTING TIMING RUN AT 2020-06-18 02:13:31 AM
+ MATH=fp16
+ declare -a CMD
+ '[' -n 3 ']'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ '[' 256 -gt 32 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ '[' -n 2 ']'
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ '[' 256 -gt 32 ']'
+ '[' -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ']'
+ echo 'Using TCMalloc'
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
Using TCMalloc
+ export LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ DECAY_INTERVAL=283
+ TARGET=24.0
running benchmark
+ echo 'running benchmark'
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ '[' -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ']'
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 3    --dwu-num-chunks 2    --dwu-num-rs-pg 1    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-overlap-reductions    --dwu-grad-norm    '
Using TCMalloc
+ echo 'Using TCMalloc'
+ export LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    '
+ MATH=fp16
+ echo 'running benchmark'
+ declare -a CMD
running benchmark
+ '[' -n 0 ']'
+ '[' 256 -gt 32 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
+ '[' -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ']'
Using TCMalloc
+ echo 'Using TCMalloc'
+ export LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
running benchmark
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
STARTING TIMING RUN AT 2020-06-18 02:13:31 AM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 3    --dwu-num-chunks 2    --dwu-num-rs-pg 1    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-overlap-reductions    --dwu-grad-norm    '
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 1 ']'
+ '[' 256 -gt 32 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ']'
Using TCMalloc
+ echo 'Using TCMalloc'
+ export LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
running benchmark
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
STARTING TIMING RUN AT 2020-06-18 02:13:31 AM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ DECAY_INTERVAL=283
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 3    --dwu-num-chunks 2    --dwu-num-rs-pg 1    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-overlap-reductions    --dwu-grad-norm    '
STARTING TIMING RUN AT 2020-06-18 02:13:31 AM
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    '
+ MATH=fp16
+ declare -a CMD
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=4.0e-3
+ TRAIN_BATCH_SIZE=32
+ TEST_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=2259
+ '[' -n 6 ']'
+ DECAY_INTERVAL=283
+ '[' 256 -gt 32 ']'
+ TARGET=24.0
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=12
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 3    --dwu-num-chunks 2    --dwu-num-rs-pg 1    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-overlap-reductions    --dwu-grad-norm    '
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    '
+ MATH=fp16
+ '[' -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ']'
+ declare -a CMD
Using TCMalloc
+ echo 'Using TCMalloc'
+ export LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ echo 'running benchmark'
running benchmark
+ '[' -n 5 ']'
+ '[' 256 -gt 32 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ']'
+ echo 'Using TCMalloc'
Using TCMalloc
+ export LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ echo 'running benchmark'
running benchmark
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
+ exec numactl --physcpubind=48-63,176-191 --membind=3 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
num_sockets = 2 num_nodes=8 cores_per_socket=64
+ exec numactl --physcpubind=48-63,176-191 --membind=3 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
num_sockets = 2 num_nodes=8 cores_per_socket=64
+ exec numactl --physcpubind=112-127,240-255 --membind=7 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
num_sockets = 2 num_nodes=8 cores_per_socket=64
+ exec numactl --physcpubind=96-111,224-239 --membind=6 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
num_sockets = 2 num_nodes=8 cores_per_socket=64
+ exec numactl --physcpubind=0-15,128-143 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
num_sockets = 2 num_nodes=8 cores_per_socket=64
+ exec numactl --physcpubind=16-31,144-159 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
num_sockets = 2 num_nodes=8 cores_per_socket=64
num_sockets = 2 num_nodes=8 cores_per_socket=64
num_sockets = 2 num_nodes=8 cores_per_socket=64
+ exec numactl --physcpubind=80-95,208-223 --membind=5 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
+ exec numactl --physcpubind=48-63,176-191 --membind=3 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
+ exec numactl --physcpubind=80-95,208-223 --membind=5 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
num_sockets = 2 num_nodes=8 cores_per_socket=64
num_sockets = 2 num_nodes=8 cores_per_socket=64
+ exec numactl --physcpubind=32-47,160-175 --membind=2 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
+ exec numactl --physcpubind=64-79,192-207 --membind=4 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
num_sockets = 2 num_nodes=8 cores_per_socket=64
num_sockets = 2 num_nodes=8 cores_per_socket=64
num_sockets = 2 num_nodes=8 cores_per_socket=64
+ exec numactl --physcpubind=64-79,192-207 --membind=4 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
num_sockets = 2 num_nodes=8 cores_per_socket=64
num_sockets = 2 num_nodes=8 cores_per_socket=64
+ exec numactl --physcpubind=96-111,224-239 --membind=6 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
+ exec numactl --physcpubind=32-47,160-175 --membind=2 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
+ exec numactl --physcpubind=16-31,144-159 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
num_sockets = 2 num_nodes=8 cores_per_socket=64
num_sockets = 2 num_nodes=8 cores_per_socket=64
+ exec numactl --physcpubind=96-111,224-239 --membind=6 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
+ exec numactl --physcpubind=64-79,192-207 --membind=4 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
+ exec numactl --physcpubind=48-63,176-191 --membind=3 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
num_sockets = 2 num_nodes=8 cores_per_socket=64
+ exec numactl --physcpubind=96-111,224-239 --membind=6 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
num_sockets = 2 num_nodes=8 cores_per_socket=64
num_sockets = 2 num_nodes=8 cores_per_socket=64
+ exec numactl --physcpubind=80-95,208-223 --membind=5 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
+ exec numactl --physcpubind=64-79,192-207 --membind=4 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
+ exec numactl --physcpubind=16-31,144-159 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
num_sockets = 2 num_nodes=8 cores_per_socket=64
num_sockets = 2 num_nodes=8 cores_per_socket=64
num_sockets = 2 num_nodes=8 cores_per_socket=64
num_sockets = 2 num_nodes=8 cores_per_socket=64
+ exec numactl --physcpubind=48-63,176-191 --membind=3 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
+ exec numactl --physcpubind=64-79,192-207 --membind=4 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
+ exec numactl --physcpubind=0-15,128-143 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
+ exec numactl --physcpubind=16-31,144-159 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
num_sockets = 2 num_nodes=8 cores_per_socket=64
num_sockets = 2 num_nodes=8 cores_per_socket=64
num_sockets = 2 num_nodes=8 cores_per_socket=64
num_sockets = 2 num_nodes=8 cores_per_socket=64
num_sockets = 2 num_nodes=8 cores_per_socket=64
num_sockets = 2 num_nodes=8 cores_per_socket=64
num_sockets = 2 num_nodes=8 cores_per_socket=64
+ exec numactl --physcpubind=64-79,192-207 --membind=4 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
num_sockets = 2 num_nodes=8 cores_per_socket=64
+ exec numactl --physcpubind=0-15,128-143 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
num_sockets = 2 num_nodes=8 cores_per_socket=64
num_sockets = 2 num_nodes=8 cores_per_socket=64
+ exec numactl --physcpubind=112-127,240-255 --membind=7 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
+ exec numactl --physcpubind=64-79,192-207 --membind=4 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
+ exec numactl --physcpubind=32-47,160-175 --membind=2 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
+ exec numactl --physcpubind=48-63,176-191 --membind=3 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
+ exec numactl --physcpubind=80-95,208-223 --membind=5 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
+ exec numactl --physcpubind=80-95,208-223 --membind=5 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
num_sockets = 2 num_nodes=8 cores_per_socket=64
num_sockets = 2 num_nodes=8 cores_per_socket=64
num_sockets = 2 num_nodes=8 cores_per_socket=64
+ exec numactl --physcpubind=0-15,128-143 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
+ exec numactl --physcpubind=96-111,224-239 --membind=6 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
num_sockets = 2 num_nodes=8 cores_per_socket=64
num_sockets = 2 num_nodes=8 cores_per_socket=64
+ exec numactl --physcpubind=80-95,208-223 --membind=5 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
+ exec numactl --physcpubind=48-63,176-191 --membind=3 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
+ exec numactl --physcpubind=0-15,128-143 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
num_sockets = 2 num_nodes=8 cores_per_socket=64
+ exec numactl --physcpubind=64-79,192-207 --membind=4 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
num_sockets = 2 num_nodes=8 cores_per_socket=64
num_sockets = 2 num_nodes=8 cores_per_socket=64
+ exec numactl --physcpubind=32-47,160-175 --membind=2 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
+ exec numactl --physcpubind=96-111,224-239 --membind=6 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
num_sockets = 2 num_nodes=8 cores_per_socket=64
num_sockets = 2 num_nodes=8 cores_per_socket=64
num_sockets = 2 num_nodes=8 cores_per_socket=64
num_sockets = 2 num_nodes=8 cores_per_socket=64
num_sockets = 2 num_nodes=8 cores_per_socket=64
num_sockets = 2 num_nodes=8 cores_per_socket=64
num_sockets = 2 num_nodes=8 cores_per_socket=64
+ exec numactl --physcpubind=48-63,176-191 --membind=3 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
+ exec numactl --physcpubind=96-111,224-239 --membind=6 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
+ exec numactl --physcpubind=16-31,144-159 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
+ exec numactl --physcpubind=16-31,144-159 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
+ exec numactl --physcpubind=0-15,128-143 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
+ exec numactl --physcpubind=112-127,240-255 --membind=7 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
+ exec numactl --physcpubind=48-63,176-191 --membind=3 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
num_sockets = 2 num_nodes=8 cores_per_socket=64
+ exec numactl --physcpubind=64-79,192-207 --membind=4 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
num_sockets = 2 num_nodes=8 cores_per_socket=64
num_sockets = 2 num_nodes=8 cores_per_socket=64
num_sockets = 2 num_nodes=8 cores_per_socket=64
+ exec numactl --physcpubind=48-63,176-191 --membind=3 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
num_sockets = 2 num_nodes=8 cores_per_socket=64
+ exec numactl --physcpubind=96-111,224-239 --membind=6 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
+ exec numactl --physcpubind=64-79,192-207 --membind=4 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
+ exec numactl --physcpubind=32-47,160-175 --membind=2 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
+ exec numactl --physcpubind=80-95,208-223 --membind=5 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
num_sockets = 2 num_nodes=8 cores_per_socket=64
num_sockets = 2 num_nodes=8 cores_per_socket=64
num_sockets = 2 num_nodes=8 cores_per_socket=64
+ exec numactl --physcpubind=80-95,208-223 --membind=5 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
num_sockets = 2 num_nodes=8 cores_per_socket=64
num_sockets = 2 num_nodes=8 cores_per_socket=64
num_sockets = 2 num_nodes=8 cores_per_socket=64
num_sockets = 2 num_nodes=8 cores_per_socket=64
+ exec numactl --physcpubind=112-127,240-255 --membind=7 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
+ exec numactl --physcpubind=48-63,176-191 --membind=3 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
+ exec numactl --physcpubind=0-15,128-143 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
num_sockets = 2 num_nodes=8 cores_per_socket=64
+ exec numactl --physcpubind=16-31,144-159 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
+ exec numactl --physcpubind=0-15,128-143 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
+ exec numactl --physcpubind=96-111,224-239 --membind=6 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
num_sockets = 2 num_nodes=8 cores_per_socket=64
+ exec numactl --physcpubind=48-63,176-191 --membind=3 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
num_sockets = 2 num_nodes=8 cores_per_socket=64
num_sockets = 2 num_nodes=8 cores_per_socket=64
num_sockets = 2 num_nodes=8 cores_per_socket=64
num_sockets = 2 num_nodes=8 cores_per_socket=64
num_sockets = 2 num_nodes=8 cores_per_socket=64
+ exec numactl --physcpubind=80-95,208-223 --membind=5 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
+ exec numactl --physcpubind=112-127,240-255 --membind=7 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
num_sockets = 2 num_nodes=8 cores_per_socket=64
+ exec numactl --physcpubind=112-127,240-255 --membind=7 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
+ exec numactl --physcpubind=112-127,240-255 --membind=7 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
+ exec numactl --physcpubind=32-47,160-175 --membind=2 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
+ exec numactl --physcpubind=0-15,128-143 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
num_sockets = 2 num_nodes=8 cores_per_socket=64
+ exec numactl --physcpubind=96-111,224-239 --membind=6 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
num_sockets = 2 num_nodes=8 cores_per_socket=64
+ exec numactl --physcpubind=96-111,224-239 --membind=6 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
+ exec numactl --physcpubind=16-31,144-159 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
num_sockets = 2 num_nodes=8 cores_per_socket=64
num_sockets = 2 num_nodes=8 cores_per_socket=64
+ exec numactl --physcpubind=48-63,176-191 --membind=3 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
num_sockets = 2 num_nodes=8 cores_per_socket=64
+ exec numactl --physcpubind=0-15,128-143 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
num_sockets = 2 num_nodes=8 cores_per_socket=64
+ exec numactl --physcpubind=16-31,144-159 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
num_sockets = 2 num_nodes=8 cores_per_socket=64
+ exec numactl --physcpubind=48-63,176-191 --membind=3 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
+ exec numactl --physcpubind=112-127,240-255 --membind=7 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
num_sockets = 2 num_nodes=8 cores_per_socket=64
+ exec numactl --physcpubind=16-31,144-159 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
num_sockets = 2 num_nodes=8 cores_per_socket=64
+ exec numactl --physcpubind=80-95,208-223 --membind=5 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
+ exec numactl --physcpubind=32-47,160-175 --membind=2 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
num_sockets = 2 num_nodes=8 cores_per_socket=64
+ exec numactl --physcpubind=32-47,160-175 --membind=2 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
num_sockets = 2 num_nodes=8 cores_per_socket=64
num_sockets = 2 num_nodes=8 cores_per_socket=64
+ exec numactl --physcpubind=32-47,160-175 --membind=2 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
+ exec numactl --physcpubind=64-79,192-207 --membind=4 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
num_sockets = 2 num_nodes=8 cores_per_socket=64
num_sockets = 2 num_nodes=8 cores_per_socket=64
num_sockets = 2 num_nodes=8 cores_per_socket=64
num_sockets = 2 num_nodes=8 cores_per_socket=64
num_sockets = 2 num_nodes=8 cores_per_socket=64
num_sockets = 2 num_nodes=8 cores_per_socket=64
+ exec numactl --physcpubind=64-79,192-207 --membind=4 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
+ exec numactl --physcpubind=16-31,144-159 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
+ exec numactl --physcpubind=0-15,128-143 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
+ exec numactl --physcpubind=32-47,160-175 --membind=2 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
num_sockets = 2 num_nodes=8 cores_per_socket=64
num_sockets = 2 num_nodes=8 cores_per_socket=64
num_sockets = 2 num_nodes=8 cores_per_socket=64
num_sockets = 2 num_nodes=8 cores_per_socket=64
+ exec numactl --physcpubind=112-127,240-255 --membind=7 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
num_sockets = 2 num_nodes=8 cores_per_socket=64
+ exec numactl --physcpubind=112-127,240-255 --membind=7 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
num_sockets = 2 num_nodes=8 cores_per_socket=64
num_sockets = 2 num_nodes=8 cores_per_socket=64
num_sockets = 2 num_nodes=8 cores_per_socket=64
+ exec numactl --physcpubind=112-127,240-255 --membind=7 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
+ exec numactl --physcpubind=48-63,176-191 --membind=3 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
+ exec numactl --physcpubind=48-63,176-191 --membind=3 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
+ exec numactl --physcpubind=0-15,128-143 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
+ exec numactl --physcpubind=112-127,240-255 --membind=7 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
+ exec numactl --physcpubind=80-95,208-223 --membind=5 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
+ exec numactl --physcpubind=80-95,208-223 --membind=5 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
num_sockets = 2 num_nodes=8 cores_per_socket=64
+ exec numactl --physcpubind=96-111,224-239 --membind=6 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
num_sockets = 2 num_nodes=8 cores_per_socket=64
num_sockets = 2 num_nodes=8 cores_per_socket=64
num_sockets = 2 num_nodes=8 cores_per_socket=64
+ exec numactl --physcpubind=16-31,144-159 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
num_sockets = 2 num_nodes=8 cores_per_socket=64
num_sockets = 2 num_nodes=8 cores_per_socket=64
num_sockets = 2 num_nodes=8 cores_per_socket=64
num_sockets = 2 num_nodes=8 cores_per_socket=64
num_sockets = 2 num_nodes=8 cores_per_socket=64
num_sockets = 2 num_nodes=8 cores_per_socket=64
num_sockets = 2 num_nodes=8 cores_per_socket=64
num_sockets = 2 num_nodes=8 cores_per_socket=64
num_sockets = 2 num_nodes=8 cores_per_socket=64
num_sockets = 2 num_nodes=8 cores_per_socket=64
num_sockets = 2 num_nodes=8 cores_per_socket=64
num_sockets = 2 num_nodes=8 cores_per_socket=64
num_sockets = 2 num_nodes=8 cores_per_socket=64
num_sockets = 2 num_nodes=8 cores_per_socket=64
+ exec numactl --physcpubind=48-63,176-191 --membind=3 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
+ exec numactl --physcpubind=64-79,192-207 --membind=4 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
+ exec numactl --physcpubind=64-79,192-207 --membind=4 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
+ exec numactl --physcpubind=64-79,192-207 --membind=4 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
num_sockets = 2 num_nodes=8 cores_per_socket=64
num_sockets = 2 num_nodes=8 cores_per_socket=64
+ exec numactl --physcpubind=32-47,160-175 --membind=2 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
+ exec numactl --physcpubind=112-127,240-255 --membind=7 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
num_sockets = 2 num_nodes=8 cores_per_socket=64
num_sockets = 2 num_nodes=8 cores_per_socket=64
num_sockets = 2 num_nodes=8 cores_per_socket=64
+ exec numactl --physcpubind=0-15,128-143 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
+ exec numactl --physcpubind=96-111,224-239 --membind=6 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
num_sockets = 2 num_nodes=8 cores_per_socket=64
+ exec numactl --physcpubind=16-31,144-159 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
num_sockets = 2 num_nodes=8 cores_per_socket=64
+ exec numactl --physcpubind=32-47,160-175 --membind=2 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
+ exec numactl --physcpubind=0-15,128-143 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
+ exec numactl --physcpubind=32-47,160-175 --membind=2 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
+ exec numactl --physcpubind=80-95,208-223 --membind=5 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
+ exec numactl --physcpubind=48-63,176-191 --membind=3 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
+ exec numactl --physcpubind=80-95,208-223 --membind=5 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
num_sockets = 2 num_nodes=8 cores_per_socket=64
+ exec numactl --physcpubind=16-31,144-159 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
+ exec numactl --physcpubind=64-79,192-207 --membind=4 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
+ exec numactl --physcpubind=0-15,128-143 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
+ exec numactl --physcpubind=64-79,192-207 --membind=4 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
+ exec numactl --physcpubind=112-127,240-255 --membind=7 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
+ exec numactl --physcpubind=112-127,240-255 --membind=7 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
+ exec numactl --physcpubind=48-63,176-191 --membind=3 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
+ exec numactl --physcpubind=64-79,192-207 --membind=4 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
+ exec numactl --physcpubind=96-111,224-239 --membind=6 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
+ exec numactl --physcpubind=80-95,208-223 --membind=5 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
num_sockets = 2 num_nodes=8 cores_per_socket=64
num_sockets = 2 num_nodes=8 cores_per_socket=64
num_sockets = 2 num_nodes=8 cores_per_socket=64
num_sockets = 2 num_nodes=8 cores_per_socket=64
+ exec numactl --physcpubind=64-79,192-207 --membind=4 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
num_sockets = 2 num_nodes=8 cores_per_socket=64
num_sockets = 2 num_nodes=8 cores_per_socket=64
+ exec numactl --physcpubind=96-111,224-239 --membind=6 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
+ exec numactl --physcpubind=32-47,160-175 --membind=2 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
num_sockets = 2 num_nodes=8 cores_per_socket=64
num_sockets = 2 num_nodes=8 cores_per_socket=64
num_sockets = 2 num_nodes=8 cores_per_socket=64
+ exec numactl --physcpubind=0-15,128-143 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
num_sockets = 2 num_nodes=8 cores_per_socket=64
num_sockets = 2 num_nodes=8 cores_per_socket=64
num_sockets = 2 num_nodes=8 cores_per_socket=64
+ exec numactl --physcpubind=96-111,224-239 --membind=6 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
+ exec numactl --physcpubind=48-63,176-191 --membind=3 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
+ exec numactl --physcpubind=80-95,208-223 --membind=5 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
+ exec numactl --physcpubind=16-31,144-159 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
num_sockets = 2 num_nodes=8 cores_per_socket=64
+ exec numactl --physcpubind=48-63,176-191 --membind=3 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
+ exec numactl --physcpubind=64-79,192-207 --membind=4 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
num_sockets = 2 num_nodes=8 cores_per_socket=64
+ exec numactl --physcpubind=0-15,128-143 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
+ exec numactl --physcpubind=32-47,160-175 --membind=2 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
+ exec numactl --physcpubind=80-95,208-223 --membind=5 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
num_sockets = 2 num_nodes=8 cores_per_socket=64
+ exec numactl --physcpubind=80-95,208-223 --membind=5 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
num_sockets = 2 num_nodes=8 cores_per_socket=64
+ exec numactl --physcpubind=48-63,176-191 --membind=3 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
num_sockets = 2 num_nodes=8 cores_per_socket=64
num_sockets = 2 num_nodes=8 cores_per_socket=64
num_sockets = 2 num_nodes=8 cores_per_socket=64
+ exec numactl --physcpubind=32-47,160-175 --membind=2 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
+ exec numactl --physcpubind=48-63,176-191 --membind=3 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
+ exec numactl --physcpubind=112-127,240-255 --membind=7 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
+ exec numactl --physcpubind=16-31,144-159 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
num_sockets = 2 num_nodes=8 cores_per_socket=64
num_sockets = 2 num_nodes=8 cores_per_socket=64
+ exec numactl --physcpubind=96-111,224-239 --membind=6 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
+ exec numactl --physcpubind=80-95,208-223 --membind=5 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
num_sockets = 2 num_nodes=8 cores_per_socket=64
num_sockets = 2 num_nodes=8 cores_per_socket=64
num_sockets = 2 num_nodes=8 cores_per_socket=64
num_sockets = 2 num_nodes=8 cores_per_socket=64
num_sockets = 2 num_nodes=8 cores_per_socket=64
+ exec numactl --physcpubind=112-127,240-255 --membind=7 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
+ exec numactl --physcpubind=16-31,144-159 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
num_sockets = 2 num_nodes=8 cores_per_socket=64
num_sockets = 2 num_nodes=8 cores_per_socket=64
num_sockets = 2 num_nodes=8 cores_per_socket=64
+ exec numactl --physcpubind=0-15,128-143 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
+ exec numactl --physcpubind=0-15,128-143 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
num_sockets = 2 num_nodes=8 cores_per_socket=64
num_sockets = 2 num_nodes=8 cores_per_socket=64
num_sockets = 2 num_nodes=8 cores_per_socket=64
+ exec numactl --physcpubind=32-47,160-175 --membind=2 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
+ exec numactl --physcpubind=16-31,144-159 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
+ exec numactl --physcpubind=48-63,176-191 --membind=3 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
num_sockets = 2 num_nodes=8 cores_per_socket=64
+ exec numactl --physcpubind=0-15,128-143 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
+ exec numactl --physcpubind=80-95,208-223 --membind=5 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
+ exec numactl --physcpubind=16-31,144-159 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
num_sockets = 2 num_nodes=8 cores_per_socket=64
+ exec numactl --physcpubind=96-111,224-239 --membind=6 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
num_sockets = 2 num_nodes=8 cores_per_socket=64
+ exec numactl --physcpubind=64-79,192-207 --membind=4 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
+ exec numactl --physcpubind=96-111,224-239 --membind=6 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
num_sockets = 2 num_nodes=8 cores_per_socket=64
num_sockets = 2 num_nodes=8 cores_per_socket=64
num_sockets = 2 num_nodes=8 cores_per_socket=64
num_sockets = 2 num_nodes=8 cores_per_socket=64
num_sockets = 2 num_nodes=8 cores_per_socket=64
num_sockets = 2 num_nodes=8 cores_per_socket=64
+ exec numactl --physcpubind=32-47,160-175 --membind=2 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
+ exec numactl --physcpubind=32-47,160-175 --membind=2 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
+ exec numactl --physcpubind=32-47,160-175 --membind=2 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
+ exec numactl --physcpubind=0-15,128-143 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
+ exec numactl --physcpubind=64-79,192-207 --membind=4 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
+ exec numactl --physcpubind=80-95,208-223 --membind=5 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
+ exec numactl --physcpubind=96-111,224-239 --membind=6 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
num_sockets = 2 num_nodes=8 cores_per_socket=64
num_sockets = 2 num_nodes=8 cores_per_socket=64
num_sockets = 2 num_nodes=8 cores_per_socket=64
num_sockets = 2 num_nodes=8 cores_per_socket=64
num_sockets = 2 num_nodes=8 cores_per_socket=64
num_sockets = 2 num_nodes=8 cores_per_socket=64
+ exec numactl --physcpubind=96-111,224-239 --membind=6 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
num_sockets = 2 num_nodes=8 cores_per_socket=64
num_sockets = 2 num_nodes=8 cores_per_socket=64
+ exec numactl --physcpubind=0-15,128-143 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
num_sockets = 2 num_nodes=8 cores_per_socket=64
num_sockets = 2 num_nodes=8 cores_per_socket=64
num_sockets = 2 num_nodes=8 cores_per_socket=64
+ exec numactl --physcpubind=80-95,208-223 --membind=5 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
num_sockets = 2 num_nodes=8 cores_per_socket=64
num_sockets = 2 num_nodes=8 cores_per_socket=64
num_sockets = 2 num_nodes=8 cores_per_socket=64
num_sockets = 2 num_nodes=8 cores_per_socket=64
num_sockets = 2 num_nodes=8 cores_per_socket=64
+ exec numactl --physcpubind=112-127,240-255 --membind=7 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
+ exec numactl --physcpubind=112-127,240-255 --membind=7 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
+ exec numactl --physcpubind=16-31,144-159 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
num_sockets = 2 num_nodes=8 cores_per_socket=64
+ exec numactl --physcpubind=32-47,160-175 --membind=2 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
num_sockets = 2 num_nodes=8 cores_per_socket=64
num_sockets = 2 num_nodes=8 cores_per_socket=64
+ exec numactl --physcpubind=96-111,224-239 --membind=6 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
+ exec numactl --physcpubind=16-31,144-159 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
+ exec numactl --physcpubind=0-15,128-143 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
+ exec numactl --physcpubind=80-95,208-223 --membind=5 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
+ exec numactl --physcpubind=16-31,144-159 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
+ exec numactl --physcpubind=32-47,160-175 --membind=2 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
+ exec numactl --physcpubind=48-63,176-191 --membind=3 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
+ exec numactl --physcpubind=48-63,176-191 --membind=3 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
+ exec numactl --physcpubind=0-15,128-143 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
+ exec numactl --physcpubind=112-127,240-255 --membind=7 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
+ exec numactl --physcpubind=0-15,128-143 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
+ exec numactl --physcpubind=64-79,192-207 --membind=4 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
num_sockets = 2 num_nodes=8 cores_per_socket=64
num_sockets = 2 num_nodes=8 cores_per_socket=64
num_sockets = 2 num_nodes=8 cores_per_socket=64
+ exec numactl --physcpubind=16-31,144-159 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
num_sockets = 2 num_nodes=8 cores_per_socket=64
num_sockets = 2 num_nodes=8 cores_per_socket=64
+ exec numactl --physcpubind=112-127,240-255 --membind=7 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
num_sockets = 2 num_nodes=8 cores_per_socket=64
+ exec numactl --physcpubind=80-95,208-223 --membind=5 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
num_sockets = 2 num_nodes=8 cores_per_socket=64
+ exec numactl --physcpubind=96-111,224-239 --membind=6 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
+ exec numactl --physcpubind=0-15,128-143 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
+ exec numactl --physcpubind=32-47,160-175 --membind=2 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
+ exec numactl --physcpubind=96-111,224-239 --membind=6 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
num_sockets = 2 num_nodes=8 cores_per_socket=64
num_sockets = 2 num_nodes=8 cores_per_socket=64
num_sockets = 2 num_nodes=8 cores_per_socket=64
num_sockets = 2 num_nodes=8 cores_per_socket=64
+ exec numactl --physcpubind=16-31,144-159 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
+ exec numactl --physcpubind=48-63,176-191 --membind=3 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
+ exec numactl --physcpubind=64-79,192-207 --membind=4 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
+ exec numactl --physcpubind=112-127,240-255 --membind=7 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
num_sockets = 2 num_nodes=8 cores_per_socket=64
num_sockets = 2 num_nodes=8 cores_per_socket=64
+ exec numactl --physcpubind=32-47,160-175 --membind=2 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
num_sockets = 2 num_nodes=8 cores_per_socket=64
+ exec numactl --physcpubind=112-127,240-255 --membind=7 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
num_sockets = 2 num_nodes=8 cores_per_socket=64
+ exec numactl --physcpubind=0-15,128-143 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
num_sockets = 2 num_nodes=8 cores_per_socket=64
num_sockets = 2 num_nodes=8 cores_per_socket=64
num_sockets = 2 num_nodes=8 cores_per_socket=64
num_sockets = 2 num_nodes=8 cores_per_socket=64
+ exec numactl --physcpubind=80-95,208-223 --membind=5 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
num_sockets = 2 num_nodes=8 cores_per_socket=64
num_sockets = 2 num_nodes=8 cores_per_socket=64
num_sockets = 2 num_nodes=8 cores_per_socket=64
num_sockets = 2 num_nodes=8 cores_per_socket=64
+ exec numactl --physcpubind=0-15,128-143 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
+ exec numactl --physcpubind=16-31,144-159 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
+ exec numactl --physcpubind=96-111,224-239 --membind=6 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
+ exec numactl --physcpubind=64-79,192-207 --membind=4 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
+ exec numactl --physcpubind=32-47,160-175 --membind=2 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
+ exec numactl --physcpubind=80-95,208-223 --membind=5 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
+ exec numactl --physcpubind=112-127,240-255 --membind=7 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
+ exec numactl --physcpubind=96-111,224-239 --membind=6 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
num_sockets = 2 num_nodes=8 cores_per_socket=64
num_sockets = 2 num_nodes=8 cores_per_socket=64
num_sockets = 2 num_nodes=8 cores_per_socket=64
+ exec numactl --physcpubind=96-111,224-239 --membind=6 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
+ exec numactl --physcpubind=48-63,176-191 --membind=3 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
+ exec numactl --physcpubind=16-31,144-159 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
num_sockets = 2 num_nodes=8 cores_per_socket=64
num_sockets = 2 num_nodes=8 cores_per_socket=64
num_sockets = 2 num_nodes=8 cores_per_socket=64
num_sockets = 2 num_nodes=8 cores_per_socket=64
num_sockets = 2 num_nodes=8 cores_per_socket=64
num_sockets = 2 num_nodes=8 cores_per_socket=64
+ exec numactl --physcpubind=64-79,192-207 --membind=4 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
+ exec numactl --physcpubind=96-111,224-239 --membind=6 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
+ exec numactl --physcpubind=0-15,128-143 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
num_sockets = 2 num_nodes=8 cores_per_socket=64
+ exec numactl --physcpubind=32-47,160-175 --membind=2 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
num_sockets = 2 num_nodes=8 cores_per_socket=64
+ exec numactl --physcpubind=48-63,176-191 --membind=3 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
+ exec numactl --physcpubind=64-79,192-207 --membind=4 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
+ exec numactl --physcpubind=32-47,160-175 --membind=2 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
+ exec numactl --physcpubind=80-95,208-223 --membind=5 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
num_sockets = 2 num_nodes=8 cores_per_socket=64
+ exec numactl --physcpubind=96-111,224-239 --membind=6 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
num_sockets = 2 num_nodes=8 cores_per_socket=64
+ exec numactl --physcpubind=16-31,144-159 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
num_sockets = 2 num_nodes=8 cores_per_socket=64
num_sockets = 2 num_nodes=8 cores_per_socket=64
+ exec numactl --physcpubind=48-63,176-191 --membind=3 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
num_sockets = 2 num_nodes=8 cores_per_socket=64
+ exec numactl --physcpubind=80-95,208-223 --membind=5 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
num_sockets = 2 num_nodes=8 cores_per_socket=64
num_sockets = 2 num_nodes=8 cores_per_socket=64
+ exec numactl --physcpubind=32-47,160-175 --membind=2 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
num_sockets = 2 num_nodes=8 cores_per_socket=64
+ exec numactl --physcpubind=16-31,144-159 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
+ exec numactl --physcpubind=0-15,128-143 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
+ exec numactl --physcpubind=112-127,240-255 --membind=7 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
num_sockets = 2 num_nodes=8 cores_per_socket=64
+ exec numactl --physcpubind=16-31,144-159 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
num_sockets = 2 num_nodes=8 cores_per_socket=64
num_sockets = 2 num_nodes=8 cores_per_socket=64
num_sockets = 2 num_nodes=8 cores_per_socket=64
num_sockets = 2 num_nodes=8 cores_per_socket=64
num_sockets = 2 num_nodes=8 cores_per_socket=64
num_sockets = 2 num_nodes=8 cores_per_socket=64
+ exec numactl --physcpubind=64-79,192-207 --membind=4 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
+ exec numactl --physcpubind=80-95,208-223 --membind=5 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
+ exec numactl --physcpubind=112-127,240-255 --membind=7 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
+ exec numactl --physcpubind=48-63,176-191 --membind=3 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
+ exec numactl --physcpubind=32-47,160-175 --membind=2 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
+ exec numactl --physcpubind=0-15,128-143 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
num_sockets = 2 num_nodes=8 cores_per_socket=64
+ exec numactl --physcpubind=96-111,224-239 --membind=6 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
num_sockets = 2 num_nodes=8 cores_per_socket=64
+ exec numactl --physcpubind=32-47,160-175 --membind=2 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
num_sockets = 2 num_nodes=8 cores_per_socket=64
num_sockets = 2 num_nodes=8 cores_per_socket=64
num_sockets = 2 num_nodes=8 cores_per_socket=64
+ exec numactl --physcpubind=80-95,208-223 --membind=5 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
num_sockets = 2 num_nodes=8 cores_per_socket=64
num_sockets = 2 num_nodes=8 cores_per_socket=64
+ exec numactl --physcpubind=112-127,240-255 --membind=7 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
+ exec numactl --physcpubind=64-79,192-207 --membind=4 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
+ exec numactl --physcpubind=0-15,128-143 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
+ exec numactl --physcpubind=48-63,176-191 --membind=3 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 12 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 32 --test-batch-size 16 --optimizer FusedAdam --lr 4.0e-3 --warmup-steps 200 --remain-steps 2259 --decay-interval 283 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
:::MLLOG {"namespace": "", "time_ms": 1592471612227, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592471612234, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592471612258, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592471612321, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592471612325, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592471612329, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592471612358, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592471612372, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592471612378, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592471612384, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592471612383, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592471612387, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592471612392, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592471612393, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592471612406, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592471612411, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592471612417, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592471612419, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592471612422, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592471612426, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592471612426, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592471612425, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592471612435, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592471612436, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592471612438, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592471612441, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592471612444, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592471612450, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592471612454, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592471612455, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592471612454, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592471612455, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592471612459, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592471612461, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592471612463, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592471612464, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592471612464, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592471612473, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592471612476, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592471612480, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592471612481, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592471612483, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592471612484, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592471612486, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592471612485, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592471612487, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592471612487, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592471612490, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592471612494, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592471612494, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592471612494, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592471612495, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592471612494, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592471612498, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592471612496, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592471612500, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592471612508, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592471612508, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592471612511, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592471612511, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592471612521, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592471612529, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592471612536, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592471612536, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592471612538, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592471612537, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592471612538, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592471612540, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592471612540, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592471612543, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592471612543, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592471612550, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592471612548, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592471612550, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592471612552, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592471612550, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592471612551, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592471612552, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592471612555, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592471612554, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592471612555, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592471612555, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592471612557, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592471612561, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592471612561, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592471612565, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592471612568, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592471612567, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592471612570, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592471612580, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592471612584, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592471612584, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592471612595, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592471612595, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592471612600, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592471612599, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592471612599, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592471612601, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592471612602, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592471612604, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592471612609, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592471612610, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592471612617, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592471612617, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592471612619, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592471612624, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592471612624, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592471612625, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592471612627, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592471612627, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592471612630, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592471612632, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592471612635, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592471612636, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592471612638, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592471612639, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592471612641, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592471612642, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592471612647, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592471612644, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592471612647, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592471612648, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592471612649, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592471612650, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592471612652, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592471612654, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592471612654, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592471612655, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592471612654, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592471612655, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592471612656, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592471612658, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592471612659, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592471612662, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592471612664, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592471612664, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592471612664, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592471612673, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592471612673, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592471612675, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592471612674, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592471612680, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592471612683, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592471612682, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592471612684, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592471612691, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592471612691, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592471612692, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592471612694, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592471612697, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592471612699, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592471612700, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592471612701, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592471612701, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592471612704, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592471612704, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592471612705, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592471612707, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592471612709, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592471612708, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592471612709, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592471612709, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592471612709, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592471612711, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592471612717, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592471612714, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592471612717, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592471612717, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592471612722, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592471612723, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592471612722, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592471612724, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592471612725, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592471612728, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592471612732, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592471612732, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592471612731, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592471612731, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592471612734, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592471612732, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592471612735, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592471612742, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592471612749, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592471612749, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592471612753, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592471612751, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592471612753, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592471612754, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592471612757, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592471612763, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592471612767, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592471612771, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592471612771, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592471612777, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592471612776, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592471612777, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592471612776, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592471612779, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592471612778, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592471612778, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592471612781, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592471612782, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592471612785, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592471612787, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592471612788, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592471612786, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592471612793, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592471612800, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592471612802, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592471612805, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592471612804, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592471612805, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592471612808, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592471612812, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592471612814, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592471612821, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592471612822, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592471612822, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592471612823, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592471612829, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592471612830, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592471612832, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592471612838, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592471612837, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592471612840, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592471612839, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592471612843, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592471612849, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592471612852, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592471612850, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592471612855, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592471612862, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592471612868, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592471612876, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592471612878, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592471612880, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592471612903, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592471612917, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592471612918, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592471612933, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592471612943, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592471612942, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592471612953, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592471612957, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592471612979, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592471612988, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592471613000, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592471613001, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592471613002, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592471613014, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592471613015, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592471613080, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592471613084, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592471613111, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592471613114, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592471613132, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
NCCL version 2.7.5+cuda11.0
0: Saving results to: results/gnmt_wmt16
0: Run arguments: Namespace(apex_message_size=10000000.0, apex_num_allreduce_streams=1, batching='bucketing', beam_size=5, cov_penalty_factor=0.1, cuda=True, cudnn=True, dataset_dir='/data', decay_factor=0.5, decay_interval=283, decay_steps=4, distributed_weight_update=2, dropout=0.2, dwu_e5m2_allgather=False, dwu_full_pipeline=False, dwu_grad_norm=True, dwu_group_size=0, dwu_num_ag_pg=0, dwu_num_ar_pg=2, dwu_num_blocks=3, dwu_num_chunks=2, dwu_num_rs_pg=1, dwu_overlap_reductions=True, enable_apex_allreduce_overlap=False, env=False, epochs=12, eval=True, fused_attention=True, fused_xentropy=True, grad_clip=5.0, hidden_size=1024, init_scale=1024, intra_epoch_eval=0, keep_checkpoints=0, len_norm_const=5.0, len_norm_factor=0.6, local_rank=0, log_all_ranks=False, lr=0.004, math='fp16', max_length_test=150, max_length_train=75, max_size=None, min_length_test=0, min_length_train=0, num_buckets=5, num_layers=4, optimizer='FusedAdam', optimizer_extra='{}', prealloc_mode='once', preproc_data_dir='/preproc_data/75', print_freq=10, rank=0, remain_steps=2259, results_dir='results', resume=None, save='gnmt_wmt16', save_all=False, save_freq=5000, save_path='results/gnmt_wmt16', seed=None, shard_size=80, share_embedding=True, smoothing=0.1, start_epoch=0, target_bleu=24.0, test_batch_size=16, test_loader_workers=0, train_batch_size=32, train_global_batch_size=None, train_iter_size=1, train_loader_workers=1, upscale_interval=128, use_preproc_data=True, warmup_steps=200)
0: L2 promotion: 128B
0: Using random master seed: 383984864
:::MLLOG {"namespace": "", "time_ms": 1592471623431, "event_type": "POINT_IN_TIME", "key": "seed", "value": 383984864, "metadata": {"file": "/workspace/rnn_translator/seq2seq/utils.py", "lineno": 144}}
0: Worker 0 is using worker seed: 3488343056
0: Building vocabulary from /data/vocab.bpe.32000
0: Size of vocabulary: 32320
0: Fused attention flag set to True
0: GNMT(
  (embedder): Embedding(32320, 1024, padding_idx=0)
  (encoder): ResidualRecurrentEncoder(
    (rnn_layers): ModuleList(
      (0): EmuBidirLSTM(
        (bidir): LSTM(1024, 1024, bidirectional=True)
        (layer1): LSTM(1024, 1024)
        (layer2): LSTM(1024, 1024)
      )
      (1): LSTM(2048, 1024)
      (2): LSTM(1024, 1024)
      (3): LSTM(1024, 1024)
    )
    (dropout): Dropout(p=0.2, inplace=False)
    (embedder): Embedding(32320, 1024, padding_idx=0)
  )
  (decoder): ResidualRecurrentDecoder(
    (att_rnn): RecurrentAttention(
      (rnn): LSTM(1024, 1024)
      (attn): BahdanauAttention(
        (linear_q): Linear(in_features=1024, out_features=1024, bias=False)
        (linear_k): Linear(in_features=1024, out_features=1024, bias=False)
      )
      (dropout): Dropout(p=0.2, inplace=False)
    )
    (rnn_layers): ModuleList(
      (0): LSTM(2048, 1024)
      (1): LSTM(2048, 1024)
      (2): LSTM(2048, 1024)
    )
    (embedder): Embedding(32320, 1024, padding_idx=0)
    (classifier): Classifier(
      (classifier): Linear(in_features=1024, out_features=32320, bias=True)
    )
    (dropout): Dropout(p=0.2, inplace=False)
  )
)
0: Building LabelSmoothingLoss (smoothing: 0.1)
0: Fused xentropy flag set to True
0: Training optimizer config: {'optimizer': 'FusedAdam', 'lr': 0.004}
0: Number of parameters: 160671297
0: Saving state of the tokenizer
self._net_total_param_size=160671360, self._total_param_size=160677888, dwu_min_page_size=12288, self._block_size=53559296, self._chunk_size=26779648, self._shard_size=3347456
[0, 9, 34]
self._net_total_param_size=160671360, self._total_param_size=160677888, dwu_min_page_size=12288, self._block_size=53559296, self._chunk_size=26779648, self._shard_size=3347456
[0, 9, 34]
self._net_total_param_size=160671360, self._total_param_size=160677888, dwu_min_page_size=12288, self._block_size=53559296, self._chunk_size=26779648, self._shard_size=3347456
[0, 9, 34]
self._net_total_param_size=160671360, self._total_param_size=160677888, dwu_min_page_size=12288, self._block_size=53559296, self._chunk_size=26779648, self._shard_size=3347456
[0, 9, 34]
self._net_total_param_size=160671360, self._total_param_size=160677888, dwu_min_page_size=12288, self._block_size=53559296, self._chunk_size=26779648, self._shard_size=3347456
[0, 9, 34]
self._net_total_param_size=160671360, self._total_param_size=160677888, dwu_min_page_size=12288, self._block_size=53559296, self._chunk_size=26779648, self._shard_size=3347456
[0, 9, 34]
self._net_total_param_size=160671360, self._total_param_size=160677888, dwu_min_page_size=12288, self._block_size=53559296, self._chunk_size=26779648, self._shard_size=3347456
[0, 9, 34]
self._net_total_param_size=160671360, self._total_param_size=160677888, dwu_min_page_size=12288, self._block_size=53559296, self._chunk_size=26779648, self._shard_size=3347456
[0, 9, 34]
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([467968]), new_param_packed_fragment.size()=torch.Size([467968]), master_param_fragment.size()=torch.Size([467968])
model_param_fragment.size()=torch.Size([2879488]), new_param_packed_fragment.size()=torch.Size([2879488]), master_param_fragment.size()=torch.Size([2879488])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3073024]), new_param_packed_fragment.size()=torch.Size([3073024]), master_param_fragment.size()=torch.Size([3073024])
model_param_fragment.size()=torch.Size([3129344]), new_param_packed_fragment.size()=torch.Size([3129344]), master_param_fragment.size()=torch.Size([3129344])
model_param_fragment.size()=torch.Size([274432]), new_param_packed_fragment.size()=torch.Size([274432]), master_param_fragment.size()=torch.Size([274432])
model_param_fragment.size()=torch.Size([218112]), new_param_packed_fragment.size()=torch.Size([218112]), master_param_fragment.size()=torch.Size([218112])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
self._net_total_param_size=160671360, self._total_param_size=160677888, dwu_min_page_size=12288, self._block_size=53559296, self._chunk_size=26779648, self._shard_size=3347456
[0, 9, 34]
self._net_total_param_size=160671360, self._total_param_size=160677888, dwu_min_page_size=12288, self._block_size=53559296, self._chunk_size=26779648, self._shard_size=3347456
[0, 9, 34]
self._net_total_param_size=160671360, self._total_param_size=160677888, dwu_min_page_size=12288, self._block_size=53559296, self._chunk_size=26779648, self._shard_size=3347456
[0, 9, 34]
self._net_total_param_size=160671360, self._total_param_size=160677888, dwu_min_page_size=12288, self._block_size=53559296, self._chunk_size=26779648, self._shard_size=3347456
[0, 9, 34]
self._net_total_param_size=160671360, self._total_param_size=160677888, dwu_min_page_size=12288, self._block_size=53559296, self._chunk_size=26779648, self._shard_size=3347456
[0, 9, 34]
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([2968576]), new_param_packed_fragment.size()=torch.Size([2968576]), master_param_fragment.size()=torch.Size([2968576])
model_param_fragment.size()=torch.Size([378880]), new_param_packed_fragment.size()=torch.Size([378880]), master_param_fragment.size()=torch.Size([378880])
model_param_fragment.size()=torch.Size([1371136]), new_param_packed_fragment.size()=torch.Size([1371136]), master_param_fragment.size()=torch.Size([1371136])
model_param_fragment.size()=torch.Size([1976320]), new_param_packed_fragment.size()=torch.Size([1976320]), master_param_fragment.size()=torch.Size([1976320])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([283712]), new_param_packed_fragment.size()=torch.Size([283712]), master_param_fragment.size()=torch.Size([283712])
model_param_fragment.size()=torch.Size([3063744]), new_param_packed_fragment.size()=torch.Size([3063744]), master_param_fragment.size()=torch.Size([3063744])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
self._net_total_param_size=160671360, self._total_param_size=160677888, dwu_min_page_size=12288, self._block_size=53559296, self._chunk_size=26779648, self._shard_size=3347456
self._net_total_param_size=160671360, self._total_param_size=160677888, dwu_min_page_size=12288, self._block_size=53559296, self._chunk_size=26779648, self._shard_size=3347456
[0, 9, 34]
[0, 9, 34]
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3129344]), new_param_packed_fragment.size()=torch.Size([3129344]), master_param_fragment.size()=torch.Size([3129344])
model_param_fragment.size()=torch.Size([218112]), new_param_packed_fragment.size()=torch.Size([218112]), master_param_fragment.size()=torch.Size([218112])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
self._net_total_param_size=160671360, self._total_param_size=160677888, dwu_min_page_size=12288, self._block_size=53559296, self._chunk_size=26779648, self._shard_size=3347456
[0, 9, 34]
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([467968]), new_param_packed_fragment.size()=torch.Size([467968]), master_param_fragment.size()=torch.Size([467968])
model_param_fragment.size()=torch.Size([2879488]), new_param_packed_fragment.size()=torch.Size([2879488]), master_param_fragment.size()=torch.Size([2879488])
model_param_fragment.size()=torch.Size([3073024]), new_param_packed_fragment.size()=torch.Size([3073024]), master_param_fragment.size()=torch.Size([3073024])
model_param_fragment.size()=torch.Size([274432]), new_param_packed_fragment.size()=torch.Size([274432]), master_param_fragment.size()=torch.Size([274432])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([2217984]), new_param_packed_fragment.size()=torch.Size([2217984]), master_param_fragment.size()=torch.Size([2217984])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([1121280]), new_param_packed_fragment.size()=torch.Size([1121280]), master_param_fragment.size()=torch.Size([1121280])
model_param_fragment.size()=torch.Size([628736]), new_param_packed_fragment.size()=torch.Size([628736]), master_param_fragment.size()=torch.Size([628736])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([1024]), new_param_packed_fragment.size()=torch.Size([1024]), master_param_fragment.size()=torch.Size([1024])
model_param_fragment.size()=torch.Size([1]), new_param_packed_fragment.size()=torch.Size([1]), master_param_fragment.size()=torch.Size([1])
model_param_fragment.size()=torch.Size([1024]), new_param_packed_fragment.size()=torch.Size([1024]), master_param_fragment.size()=torch.Size([1024])
model_param_fragment.size()=torch.Size([1048576]), new_param_packed_fragment.size()=torch.Size([1048576]), master_param_fragment.size()=torch.Size([1048576])
model_param_fragment.size()=torch.Size([1048576]), new_param_packed_fragment.size()=torch.Size([1048576]), master_param_fragment.size()=torch.Size([1048576])
model_param_fragment.size()=torch.Size([611264]), new_param_packed_fragment.size()=torch.Size([611264]), master_param_fragment.size()=torch.Size([611264])
model_param_fragment.size()=torch.Size([1130560]), new_param_packed_fragment.size()=torch.Size([1130560]), master_param_fragment.size()=torch.Size([1130560])
self._net_total_param_size=160671360, self._total_param_size=160677888, dwu_min_page_size=12288, self._block_size=53559296, self._chunk_size=26779648, self._shard_size=3347456
[0, 9, 34]
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([2208704]), new_param_packed_fragment.size()=torch.Size([2208704]), master_param_fragment.size()=torch.Size([2208704])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([2217984]), new_param_packed_fragment.size()=torch.Size([2217984]), master_param_fragment.size()=torch.Size([2217984])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([1121280]), new_param_packed_fragment.size()=torch.Size([1121280]), master_param_fragment.size()=torch.Size([1121280])
model_param_fragment.size()=torch.Size([628736]), new_param_packed_fragment.size()=torch.Size([628736]), master_param_fragment.size()=torch.Size([628736])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([1024]), new_param_packed_fragment.size()=torch.Size([1024]), master_param_fragment.size()=torch.Size([1024])
model_param_fragment.size()=torch.Size([1]), new_param_packed_fragment.size()=torch.Size([1]), master_param_fragment.size()=torch.Size([1])
model_param_fragment.size()=torch.Size([1024]), new_param_packed_fragment.size()=torch.Size([1024]), master_param_fragment.size()=torch.Size([1024])
model_param_fragment.size()=torch.Size([1048576]), new_param_packed_fragment.size()=torch.Size([1048576]), master_param_fragment.size()=torch.Size([1048576])
model_param_fragment.size()=torch.Size([1048576]), new_param_packed_fragment.size()=torch.Size([1048576]), master_param_fragment.size()=torch.Size([1048576])
model_param_fragment.size()=torch.Size([611264]), new_param_packed_fragment.size()=torch.Size([611264]), master_param_fragment.size()=torch.Size([611264])
model_param_fragment.size()=torch.Size([1130560]), new_param_packed_fragment.size()=torch.Size([1130560]), master_param_fragment.size()=torch.Size([1130560])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([2208704]), new_param_packed_fragment.size()=torch.Size([2208704]), master_param_fragment.size()=torch.Size([2208704])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
self._net_total_param_size=160671360, self._total_param_size=160677888, dwu_min_page_size=12288, self._block_size=53559296, self._chunk_size=26779648, self._shard_size=3347456
[0, 9, 34]
self._net_total_param_size=160671360, self._total_param_size=160677888, dwu_min_page_size=12288, self._block_size=53559296, self._chunk_size=26779648, self._shard_size=3347456
[0, 9, 34]
self._net_total_param_size=160671360, self._total_param_size=160677888, dwu_min_page_size=12288, self._block_size=53559296, self._chunk_size=26779648, self._shard_size=3347456
[0, 9, 34]
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([2217984]), new_param_packed_fragment.size()=torch.Size([2217984]), master_param_fragment.size()=torch.Size([2217984])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([1121280]), new_param_packed_fragment.size()=torch.Size([1121280]), master_param_fragment.size()=torch.Size([1121280])
model_param_fragment.size()=torch.Size([628736]), new_param_packed_fragment.size()=torch.Size([628736]), master_param_fragment.size()=torch.Size([628736])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([1024]), new_param_packed_fragment.size()=torch.Size([1024]), master_param_fragment.size()=torch.Size([1024])
model_param_fragment.size()=torch.Size([1]), new_param_packed_fragment.size()=torch.Size([1]), master_param_fragment.size()=torch.Size([1])
model_param_fragment.size()=torch.Size([1024]), new_param_packed_fragment.size()=torch.Size([1024]), master_param_fragment.size()=torch.Size([1024])
model_param_fragment.size()=torch.Size([1048576]), new_param_packed_fragment.size()=torch.Size([1048576]), master_param_fragment.size()=torch.Size([1048576])
model_param_fragment.size()=torch.Size([1048576]), new_param_packed_fragment.size()=torch.Size([1048576]), master_param_fragment.size()=torch.Size([1048576])
model_param_fragment.size()=torch.Size([611264]), new_param_packed_fragment.size()=torch.Size([611264]), master_param_fragment.size()=torch.Size([611264])
model_param_fragment.size()=torch.Size([1130560]), new_param_packed_fragment.size()=torch.Size([1130560]), master_param_fragment.size()=torch.Size([1130560])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([2208704]), new_param_packed_fragment.size()=torch.Size([2208704]), master_param_fragment.size()=torch.Size([2208704])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([2968576]), new_param_packed_fragment.size()=torch.Size([2968576]), master_param_fragment.size()=torch.Size([2968576])
model_param_fragment.size()=torch.Size([378880]), new_param_packed_fragment.size()=torch.Size([378880]), master_param_fragment.size()=torch.Size([378880])
model_param_fragment.size()=torch.Size([1371136]), new_param_packed_fragment.size()=torch.Size([1371136]), master_param_fragment.size()=torch.Size([1371136])
model_param_fragment.size()=torch.Size([1976320]), new_param_packed_fragment.size()=torch.Size([1976320]), master_param_fragment.size()=torch.Size([1976320])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([283712]), new_param_packed_fragment.size()=torch.Size([283712]), master_param_fragment.size()=torch.Size([283712])
model_param_fragment.size()=torch.Size([3063744]), new_param_packed_fragment.size()=torch.Size([3063744]), master_param_fragment.size()=torch.Size([3063744])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([1314816]), new_param_packed_fragment.size()=torch.Size([1314816]), master_param_fragment.size()=torch.Size([1314816])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([2024448]), new_param_packed_fragment.size()=torch.Size([2024448]), master_param_fragment.size()=torch.Size([2024448])
model_param_fragment.size()=torch.Size([2169856]), new_param_packed_fragment.size()=torch.Size([2169856]), master_param_fragment.size()=torch.Size([2169856])
model_param_fragment.size()=torch.Size([1177600]), new_param_packed_fragment.size()=torch.Size([1177600]), master_param_fragment.size()=torch.Size([1177600])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([572416]), new_param_packed_fragment.size()=torch.Size([572416]), master_param_fragment.size()=torch.Size([572416])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([2832448]), new_param_packed_fragment.size()=torch.Size([2832448]), master_param_fragment.size()=torch.Size([2832448])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([515008]), new_param_packed_fragment.size()=torch.Size([515008]), master_param_fragment.size()=torch.Size([515008])
model_param_fragment.size()=torch.Size([2766848]), new_param_packed_fragment.size()=torch.Size([2766848]), master_param_fragment.size()=torch.Size([2766848])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([1082432]), new_param_packed_fragment.size()=torch.Size([1082432]), master_param_fragment.size()=torch.Size([1082432])
model_param_fragment.size()=torch.Size([2265024]), new_param_packed_fragment.size()=torch.Size([2265024]), master_param_fragment.size()=torch.Size([2265024])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
self._net_total_param_size=160671360, self._total_param_size=160677888, dwu_min_page_size=12288, self._block_size=53559296, self._chunk_size=26779648, self._shard_size=3347456
[0, 9, 34]
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([2274304]), new_param_packed_fragment.size()=torch.Size([2274304]), master_param_fragment.size()=torch.Size([2274304])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([1064960]), new_param_packed_fragment.size()=torch.Size([1064960]), master_param_fragment.size()=torch.Size([1064960])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3308608]), new_param_packed_fragment.size()=torch.Size([3308608]), master_param_fragment.size()=torch.Size([3308608])
model_param_fragment.size()=torch.Size([32320]), new_param_packed_fragment.size()=torch.Size([32320]), master_param_fragment.size()=torch.Size([32320])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([1314816]), new_param_packed_fragment.size()=torch.Size([1314816]), master_param_fragment.size()=torch.Size([1314816])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([2024448]), new_param_packed_fragment.size()=torch.Size([2024448]), master_param_fragment.size()=torch.Size([2024448])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
self._net_total_param_size=160671360, self._total_param_size=160677888, dwu_min_page_size=12288, self._block_size=53559296, self._chunk_size=26779648, self._shard_size=3347456
model_param_fragment.size()=torch.Size([2832448]), new_param_packed_fragment.size()=torch.Size([2832448]), master_param_fragment.size()=torch.Size([2832448])
model_param_fragment.size()=torch.Size([515008]), new_param_packed_fragment.size()=torch.Size([515008]), master_param_fragment.size()=torch.Size([515008])
[0, 9, 34]
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
self._net_total_param_size=160671360, self._total_param_size=160677888, dwu_min_page_size=12288, self._block_size=53559296, self._chunk_size=26779648, self._shard_size=3347456
[0, 9, 34]
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3016704]), new_param_packed_fragment.size()=torch.Size([3016704]), master_param_fragment.size()=torch.Size([3016704])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([322560]), new_param_packed_fragment.size()=torch.Size([322560]), master_param_fragment.size()=torch.Size([322560])
model_param_fragment.size()=torch.Size([1427456]), new_param_packed_fragment.size()=torch.Size([1427456]), master_param_fragment.size()=torch.Size([1427456])
model_param_fragment.size()=torch.Size([1920000]), new_param_packed_fragment.size()=torch.Size([1920000]), master_param_fragment.size()=torch.Size([1920000])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([1929280]), new_param_packed_fragment.size()=torch.Size([1929280]), master_param_fragment.size()=torch.Size([1929280])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([2274304]), new_param_packed_fragment.size()=torch.Size([2274304]), master_param_fragment.size()=torch.Size([2274304])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([1409984]), new_param_packed_fragment.size()=torch.Size([1409984]), master_param_fragment.size()=torch.Size([1409984])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([1064960]), new_param_packed_fragment.size()=torch.Size([1064960]), master_param_fragment.size()=torch.Size([1064960])
model_param_fragment.size()=torch.Size([331840]), new_param_packed_fragment.size()=torch.Size([331840]), master_param_fragment.size()=torch.Size([331840])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([3007424]), new_param_packed_fragment.size()=torch.Size([3007424]), master_param_fragment.size()=torch.Size([3007424])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3308608]), new_param_packed_fragment.size()=torch.Size([3308608]), master_param_fragment.size()=torch.Size([3308608])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([32320]), new_param_packed_fragment.size()=torch.Size([32320]), master_param_fragment.size()=torch.Size([32320])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([2169856]), new_param_packed_fragment.size()=torch.Size([2169856]), master_param_fragment.size()=torch.Size([2169856])
model_param_fragment.size()=torch.Size([1177600]), new_param_packed_fragment.size()=torch.Size([1177600]), master_param_fragment.size()=torch.Size([1177600])
model_param_fragment.size()=torch.Size([572416]), new_param_packed_fragment.size()=torch.Size([572416]), master_param_fragment.size()=torch.Size([572416])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([2766848]), new_param_packed_fragment.size()=torch.Size([2766848]), master_param_fragment.size()=torch.Size([2766848])
model_param_fragment.size()=torch.Size([1082432]), new_param_packed_fragment.size()=torch.Size([1082432]), master_param_fragment.size()=torch.Size([1082432])
model_param_fragment.size()=torch.Size([2265024]), new_param_packed_fragment.size()=torch.Size([2265024]), master_param_fragment.size()=torch.Size([2265024])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
self._net_total_param_size=160671360, self._total_param_size=160677888, dwu_min_page_size=12288, self._block_size=53559296, self._chunk_size=26779648, self._shard_size=3347456
[0, 9, 34]
model_param_fragment.size()=torch.Size([3016704]), new_param_packed_fragment.size()=torch.Size([3016704]), master_param_fragment.size()=torch.Size([3016704])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([322560]), new_param_packed_fragment.size()=torch.Size([322560]), master_param_fragment.size()=torch.Size([322560])
model_param_fragment.size()=torch.Size([1427456]), new_param_packed_fragment.size()=torch.Size([1427456]), master_param_fragment.size()=torch.Size([1427456])
model_param_fragment.size()=torch.Size([1920000]), new_param_packed_fragment.size()=torch.Size([1920000]), master_param_fragment.size()=torch.Size([1920000])
self._net_total_param_size=160671360, self._total_param_size=160677888, dwu_min_page_size=12288, self._block_size=53559296, self._chunk_size=26779648, self._shard_size=3347456
model_param_fragment.size()=torch.Size([1929280]), new_param_packed_fragment.size()=torch.Size([1929280]), master_param_fragment.size()=torch.Size([1929280])
[0, 9, 34]
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([1409984]), new_param_packed_fragment.size()=torch.Size([1409984]), master_param_fragment.size()=torch.Size([1409984])
model_param_fragment.size()=torch.Size([331840]), new_param_packed_fragment.size()=torch.Size([331840]), master_param_fragment.size()=torch.Size([331840])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([3007424]), new_param_packed_fragment.size()=torch.Size([3007424]), master_param_fragment.size()=torch.Size([3007424])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
self._net_total_param_size=160671360, self._total_param_size=160677888, dwu_min_page_size=12288, self._block_size=53559296, self._chunk_size=26779648, self._shard_size=3347456
[0, 9, 34]
model_param_fragment.size()=torch.Size([2169856]), new_param_packed_fragment.size()=torch.Size([2169856]), master_param_fragment.size()=torch.Size([2169856])
model_param_fragment.size()=torch.Size([1177600]), new_param_packed_fragment.size()=torch.Size([1177600]), master_param_fragment.size()=torch.Size([1177600])
self._net_total_param_size=160671360, self._total_param_size=160677888, dwu_min_page_size=12288, self._block_size=53559296, self._chunk_size=26779648, self._shard_size=3347456
model_param_fragment.size()=torch.Size([572416]), new_param_packed_fragment.size()=torch.Size([572416]), master_param_fragment.size()=torch.Size([572416])
self._net_total_param_size=160671360, self._total_param_size=160677888, dwu_min_page_size=12288, self._block_size=53559296, self._chunk_size=26779648, self._shard_size=3347456
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
[0, 9, 34]
[0, 9, 34]
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([2766848]), new_param_packed_fragment.size()=torch.Size([2766848]), master_param_fragment.size()=torch.Size([2766848])
model_param_fragment.size()=torch.Size([1082432]), new_param_packed_fragment.size()=torch.Size([1082432]), master_param_fragment.size()=torch.Size([1082432])
model_param_fragment.size()=torch.Size([2265024]), new_param_packed_fragment.size()=torch.Size([2265024]), master_param_fragment.size()=torch.Size([2265024])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3129344]), new_param_packed_fragment.size()=torch.Size([3129344]), master_param_fragment.size()=torch.Size([3129344])
model_param_fragment.size()=torch.Size([218112]), new_param_packed_fragment.size()=torch.Size([218112]), master_param_fragment.size()=torch.Size([218112])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
self._net_total_param_size=160671360, self._total_param_size=160677888, dwu_min_page_size=12288, self._block_size=53559296, self._chunk_size=26779648, self._shard_size=3347456
[0, 9, 34]
model_param_fragment.size()=torch.Size([467968]), new_param_packed_fragment.size()=torch.Size([467968]), master_param_fragment.size()=torch.Size([467968])
model_param_fragment.size()=torch.Size([2879488]), new_param_packed_fragment.size()=torch.Size([2879488]), master_param_fragment.size()=torch.Size([2879488])
model_param_fragment.size()=torch.Size([3073024]), new_param_packed_fragment.size()=torch.Size([3073024]), master_param_fragment.size()=torch.Size([3073024])
model_param_fragment.size()=torch.Size([274432]), new_param_packed_fragment.size()=torch.Size([274432]), master_param_fragment.size()=torch.Size([274432])
self._net_total_param_size=160671360, self._total_param_size=160677888, dwu_min_page_size=12288, self._block_size=53559296, self._chunk_size=26779648, self._shard_size=3347456
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
[0, 9, 34]
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
self._net_total_param_size=160671360, self._total_param_size=160677888, dwu_min_page_size=12288, self._block_size=53559296, self._chunk_size=26779648, self._shard_size=3347456
[0, 9, 34]
self._net_total_param_size=160671360, self._total_param_size=160677888, dwu_min_page_size=12288, self._block_size=53559296, self._chunk_size=26779648, self._shard_size=3347456
[0, 9, 34]
self._net_total_param_size=160671360, self._total_param_size=160677888, dwu_min_page_size=12288, self._block_size=53559296, self._chunk_size=26779648, self._shard_size=3347456
[0, 9, 34]
self._net_total_param_size=160671360, self._total_param_size=160677888, dwu_min_page_size=12288, self._block_size=53559296, self._chunk_size=26779648, self._shard_size=3347456
[0, 9, 34]
self._net_total_param_size=160671360, self._total_param_size=160677888, dwu_min_page_size=12288, self._block_size=53559296, self._chunk_size=26779648, self._shard_size=3347456
[0, 9, 34]
self._net_total_param_size=160671360, self._total_param_size=160677888, dwu_min_page_size=12288, self._block_size=53559296, self._chunk_size=26779648, self._shard_size=3347456
[0, 9, 34]
self._net_total_param_size=160671360, self._total_param_size=160677888, dwu_min_page_size=12288, self._block_size=53559296, self._chunk_size=26779648, self._shard_size=3347456
[0, 9, 34]
self._net_total_param_size=160671360, self._total_param_size=160677888, dwu_min_page_size=12288, self._block_size=53559296, self._chunk_size=26779648, self._shard_size=3347456
[0, 9, 34]
self._net_total_param_size=160671360, self._total_param_size=160677888, dwu_min_page_size=12288, self._block_size=53559296, self._chunk_size=26779648, self._shard_size=3347456
[0, 9, 34]
self._net_total_param_size=160671360, self._total_param_size=160677888, dwu_min_page_size=12288, self._block_size=53559296, self._chunk_size=26779648, self._shard_size=3347456
self._net_total_param_size=160671360, self._total_param_size=160677888, dwu_min_page_size=12288, self._block_size=53559296, self._chunk_size=26779648, self._shard_size=3347456
[0, 9, 34]
[0, 9, 34]
self._net_total_param_size=160671360, self._total_param_size=160677888, dwu_min_page_size=12288, self._block_size=53559296, self._chunk_size=26779648, self._shard_size=3347456
self._net_total_param_size=160671360, self._total_param_size=160677888, dwu_min_page_size=12288, self._block_size=53559296, self._chunk_size=26779648, self._shard_size=3347456
[0, 9, 34]
[0, 9, 34]
self._net_total_param_size=160671360, self._total_param_size=160677888, dwu_min_page_size=12288, self._block_size=53559296, self._chunk_size=26779648, self._shard_size=3347456
[0, 9, 34]
self._net_total_param_size=160671360, self._total_param_size=160677888, dwu_min_page_size=12288, self._block_size=53559296, self._chunk_size=26779648, self._shard_size=3347456
self._net_total_param_size=160671360, self._total_param_size=160677888, dwu_min_page_size=12288, self._block_size=53559296, self._chunk_size=26779648, self._shard_size=3347456
[0, 9, 34]
[0, 9, 34]
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3129344]), new_param_packed_fragment.size()=torch.Size([3129344]), master_param_fragment.size()=torch.Size([3129344])
model_param_fragment.size()=torch.Size([218112]), new_param_packed_fragment.size()=torch.Size([218112]), master_param_fragment.size()=torch.Size([218112])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
self._net_total_param_size=160671360, self._total_param_size=160677888, dwu_min_page_size=12288, self._block_size=53559296, self._chunk_size=26779648, self._shard_size=3347456
[0, 9, 34]
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3129344]), new_param_packed_fragment.size()=torch.Size([3129344]), master_param_fragment.size()=torch.Size([3129344])
model_param_fragment.size()=torch.Size([218112]), new_param_packed_fragment.size()=torch.Size([218112]), master_param_fragment.size()=torch.Size([218112])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
self._net_total_param_size=160671360, self._total_param_size=160677888, dwu_min_page_size=12288, self._block_size=53559296, self._chunk_size=26779648, self._shard_size=3347456
[0, 9, 34]
self._net_total_param_size=160671360, self._total_param_size=160677888, dwu_min_page_size=12288, self._block_size=53559296, self._chunk_size=26779648, self._shard_size=3347456
[0, 9, 34]
self._net_total_param_size=160671360, self._total_param_size=160677888, dwu_min_page_size=12288, self._block_size=53559296, self._chunk_size=26779648, self._shard_size=3347456
[0, 9, 34]
self._net_total_param_size=160671360, self._total_param_size=160677888, dwu_min_page_size=12288, self._block_size=53559296, self._chunk_size=26779648, self._shard_size=3347456
[0, 9, 34]
self._net_total_param_size=160671360, self._total_param_size=160677888, dwu_min_page_size=12288, self._block_size=53559296, self._chunk_size=26779648, self._shard_size=3347456
[0, 9, 34]
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
self._net_total_param_size=160671360, self._total_param_size=160677888, dwu_min_page_size=12288, self._block_size=53559296, self._chunk_size=26779648, self._shard_size=3347456
[0, 9, 34]
self._net_total_param_size=160671360, self._total_param_size=160677888, dwu_min_page_size=12288, self._block_size=53559296, self._chunk_size=26779648, self._shard_size=3347456
[0, 9, 34]
model_param_fragment.size()=torch.Size([2169856]), new_param_packed_fragment.size()=torch.Size([2169856]), master_param_fragment.size()=torch.Size([2169856])
model_param_fragment.size()=torch.Size([1177600]), new_param_packed_fragment.size()=torch.Size([1177600]), master_param_fragment.size()=torch.Size([1177600])
model_param_fragment.size()=torch.Size([572416]), new_param_packed_fragment.size()=torch.Size([572416]), master_param_fragment.size()=torch.Size([572416])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
self._net_total_param_size=160671360, self._total_param_size=160677888, dwu_min_page_size=12288, self._block_size=53559296, self._chunk_size=26779648, self._shard_size=3347456
model_param_fragment.size()=torch.Size([2766848]), new_param_packed_fragment.size()=torch.Size([2766848]), master_param_fragment.size()=torch.Size([2766848])
model_param_fragment.size()=torch.Size([1082432]), new_param_packed_fragment.size()=torch.Size([1082432]), master_param_fragment.size()=torch.Size([1082432])
[0, 9, 34]
model_param_fragment.size()=torch.Size([2265024]), new_param_packed_fragment.size()=torch.Size([2265024]), master_param_fragment.size()=torch.Size([2265024])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
self._net_total_param_size=160671360, self._total_param_size=160677888, dwu_min_page_size=12288, self._block_size=53559296, self._chunk_size=26779648, self._shard_size=3347456
[0, 9, 34]
self._net_total_param_size=160671360, self._total_param_size=160677888, dwu_min_page_size=12288, self._block_size=53559296, self._chunk_size=26779648, self._shard_size=3347456
self._net_total_param_size=160671360, self._total_param_size=160677888, dwu_min_page_size=12288, self._block_size=53559296, self._chunk_size=26779648, self._shard_size=3347456
[0, 9, 34]
[0, 9, 34]
self._net_total_param_size=160671360, self._total_param_size=160677888, dwu_min_page_size=12288, self._block_size=53559296, self._chunk_size=26779648, self._shard_size=3347456
[0, 9, 34]
self._net_total_param_size=160671360, self._total_param_size=160677888, dwu_min_page_size=12288, self._block_size=53559296, self._chunk_size=26779648, self._shard_size=3347456
[0, 9, 34]
NCCL version 2.7.5+cuda11.0
self._net_total_param_size=160671360, self._total_param_size=160677888, dwu_min_page_size=12288, self._block_size=53559296, self._chunk_size=26779648, self._shard_size=3347456
[0, 9, 34]
self._net_total_param_size=160671360, self._total_param_size=160677888, dwu_min_page_size=12288, self._block_size=53559296, self._chunk_size=26779648, self._shard_size=3347456
[0, 9, 34]
self._net_total_param_size=160671360, self._total_param_size=160677888, dwu_min_page_size=12288, self._block_size=53559296, self._chunk_size=26779648, self._shard_size=3347456
[0, 9, 34]
self._net_total_param_size=160671360, self._total_param_size=160677888, dwu_min_page_size=12288, self._block_size=53559296, self._chunk_size=26779648, self._shard_size=3347456
[0, 9, 34]
self._net_total_param_size=160671360, self._total_param_size=160677888, dwu_min_page_size=12288, self._block_size=53559296, self._chunk_size=26779648, self._shard_size=3347456
[0, 9, 34]
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
self._net_total_param_size=160671360, self._total_param_size=160677888, dwu_min_page_size=12288, self._block_size=53559296, self._chunk_size=26779648, self._shard_size=3347456
[0, 9, 34]
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
self._net_total_param_size=160671360, self._total_param_size=160677888, dwu_min_page_size=12288, self._block_size=53559296, self._chunk_size=26779648, self._shard_size=3347456
model_param_fragment.size()=torch.Size([3129344]), new_param_packed_fragment.size()=torch.Size([3129344]), master_param_fragment.size()=torch.Size([3129344])
[0, 9, 34]
model_param_fragment.size()=torch.Size([218112]), new_param_packed_fragment.size()=torch.Size([218112]), master_param_fragment.size()=torch.Size([218112])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
self._net_total_param_size=160671360, self._total_param_size=160677888, dwu_min_page_size=12288, self._block_size=53559296, self._chunk_size=26779648, self._shard_size=3347456
model_param_fragment.size()=torch.Size([1314816]), new_param_packed_fragment.size()=torch.Size([1314816]), master_param_fragment.size()=torch.Size([1314816])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
[0, 9, 34]
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([2024448]), new_param_packed_fragment.size()=torch.Size([2024448]), master_param_fragment.size()=torch.Size([2024448])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([2832448]), new_param_packed_fragment.size()=torch.Size([2832448]), master_param_fragment.size()=torch.Size([2832448])
model_param_fragment.size()=torch.Size([515008]), new_param_packed_fragment.size()=torch.Size([515008]), master_param_fragment.size()=torch.Size([515008])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
self._net_total_param_size=160671360, self._total_param_size=160677888, dwu_min_page_size=12288, self._block_size=53559296, self._chunk_size=26779648, self._shard_size=3347456
[0, 9, 34]
self._net_total_param_size=160671360, self._total_param_size=160677888, dwu_min_page_size=12288, self._block_size=53559296, self._chunk_size=26779648, self._shard_size=3347456
[0, 9, 34]
self._net_total_param_size=160671360, self._total_param_size=160677888, dwu_min_page_size=12288, self._block_size=53559296, self._chunk_size=26779648, self._shard_size=3347456
self._net_total_param_size=160671360, self._total_param_size=160677888, dwu_min_page_size=12288, self._block_size=53559296, self._chunk_size=26779648, self._shard_size=3347456
[0, 9, 34]
[0, 9, 34]
self._net_total_param_size=160671360, self._total_param_size=160677888, dwu_min_page_size=12288, self._block_size=53559296, self._chunk_size=26779648, self._shard_size=3347456
[0, 9, 34]
self._net_total_param_size=160671360, self._total_param_size=160677888, dwu_min_page_size=12288, self._block_size=53559296, self._chunk_size=26779648, self._shard_size=3347456
[0, 9, 34]
self._net_total_param_size=160671360, self._total_param_size=160677888, dwu_min_page_size=12288, self._block_size=53559296, self._chunk_size=26779648, self._shard_size=3347456
[0, 9, 34]
self._net_total_param_size=160671360, self._total_param_size=160677888, dwu_min_page_size=12288, self._block_size=53559296, self._chunk_size=26779648, self._shard_size=3347456
[0, 9, 34]
self._net_total_param_size=160671360, self._total_param_size=160677888, dwu_min_page_size=12288, self._block_size=53559296, self._chunk_size=26779648, self._shard_size=3347456
[0, 9, 34]
self._net_total_param_size=160671360, self._total_param_size=160677888, dwu_min_page_size=12288, self._block_size=53559296, self._chunk_size=26779648, self._shard_size=3347456
[0, 9, 34]
self._net_total_param_size=160671360, self._total_param_size=160677888, dwu_min_page_size=12288, self._block_size=53559296, self._chunk_size=26779648, self._shard_size=3347456
[0, 9, 34]
self._net_total_param_size=160671360, self._total_param_size=160677888, dwu_min_page_size=12288, self._block_size=53559296, self._chunk_size=26779648, self._shard_size=3347456
[0, 9, 34]
self._net_total_param_size=160671360, self._total_param_size=160677888, dwu_min_page_size=12288, self._block_size=53559296, self._chunk_size=26779648, self._shard_size=3347456
[0, 9, 34]
self._net_total_param_size=160671360, self._total_param_size=160677888, dwu_min_page_size=12288, self._block_size=53559296, self._chunk_size=26779648, self._shard_size=3347456
[0, 9, 34]
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
self._net_total_param_size=160671360, self._total_param_size=160677888, dwu_min_page_size=12288, self._block_size=53559296, self._chunk_size=26779648, self._shard_size=3347456
[0, 9, 34]
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
self._net_total_param_size=160671360, self._total_param_size=160677888, dwu_min_page_size=12288, self._block_size=53559296, self._chunk_size=26779648, self._shard_size=3347456
[0, 9, 34]
model_param_fragment.size()=torch.Size([3129344]), new_param_packed_fragment.size()=torch.Size([3129344]), master_param_fragment.size()=torch.Size([3129344])
model_param_fragment.size()=torch.Size([218112]), new_param_packed_fragment.size()=torch.Size([218112]), master_param_fragment.size()=torch.Size([218112])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
self._net_total_param_size=160671360, self._total_param_size=160677888, dwu_min_page_size=12288, self._block_size=53559296, self._chunk_size=26779648, self._shard_size=3347456
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
[0, 9, 34]
self._net_total_param_size=160671360, self._total_param_size=160677888, dwu_min_page_size=12288, self._block_size=53559296, self._chunk_size=26779648, self._shard_size=3347456
[0, 9, 34]
self._net_total_param_size=160671360, self._total_param_size=160677888, dwu_min_page_size=12288, self._block_size=53559296, self._chunk_size=26779648, self._shard_size=3347456
[0, 9, 34]
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
self._net_total_param_size=160671360, self._total_param_size=160677888, dwu_min_page_size=12288, self._block_size=53559296, self._chunk_size=26779648, self._shard_size=3347456
[0, 9, 34]
model_param_fragment.size()=torch.Size([3016704]), new_param_packed_fragment.size()=torch.Size([3016704]), master_param_fragment.size()=torch.Size([3016704])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([322560]), new_param_packed_fragment.size()=torch.Size([322560]), master_param_fragment.size()=torch.Size([322560])
model_param_fragment.size()=torch.Size([1427456]), new_param_packed_fragment.size()=torch.Size([1427456]), master_param_fragment.size()=torch.Size([1427456])
model_param_fragment.size()=torch.Size([1920000]), new_param_packed_fragment.size()=torch.Size([1920000]), master_param_fragment.size()=torch.Size([1920000])
self._net_total_param_size=160671360, self._total_param_size=160677888, dwu_min_page_size=12288, self._block_size=53559296, self._chunk_size=26779648, self._shard_size=3347456
model_param_fragment.size()=torch.Size([1929280]), new_param_packed_fragment.size()=torch.Size([1929280]), master_param_fragment.size()=torch.Size([1929280])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
[0, 9, 34]
model_param_fragment.size()=torch.Size([1409984]), new_param_packed_fragment.size()=torch.Size([1409984]), master_param_fragment.size()=torch.Size([1409984])
model_param_fragment.size()=torch.Size([331840]), new_param_packed_fragment.size()=torch.Size([331840]), master_param_fragment.size()=torch.Size([331840])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([3007424]), new_param_packed_fragment.size()=torch.Size([3007424]), master_param_fragment.size()=torch.Size([3007424])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
self._net_total_param_size=160671360, self._total_param_size=160677888, dwu_min_page_size=12288, self._block_size=53559296, self._chunk_size=26779648, self._shard_size=3347456
[0, 9, 34]
self._net_total_param_size=160671360, self._total_param_size=160677888, dwu_min_page_size=12288, self._block_size=53559296, self._chunk_size=26779648, self._shard_size=3347456
[0, 9, 34]
self._net_total_param_size=160671360, self._total_param_size=160677888, dwu_min_page_size=12288, self._block_size=53559296, self._chunk_size=26779648, self._shard_size=3347456
[0, 9, 34]
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([2169856]), new_param_packed_fragment.size()=torch.Size([2169856]), master_param_fragment.size()=torch.Size([2169856])
self._net_total_param_size=160671360, self._total_param_size=160677888, dwu_min_page_size=12288, self._block_size=53559296, self._chunk_size=26779648, self._shard_size=3347456
[0, 9, 34]
model_param_fragment.size()=torch.Size([1177600]), new_param_packed_fragment.size()=torch.Size([1177600]), master_param_fragment.size()=torch.Size([1177600])
model_param_fragment.size()=torch.Size([572416]), new_param_packed_fragment.size()=torch.Size([572416]), master_param_fragment.size()=torch.Size([572416])
model_param_fragment.size()=torch.Size([1314816]), new_param_packed_fragment.size()=torch.Size([1314816]), master_param_fragment.size()=torch.Size([1314816])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([2024448]), new_param_packed_fragment.size()=torch.Size([2024448]), master_param_fragment.size()=torch.Size([2024448])
model_param_fragment.size()=torch.Size([2766848]), new_param_packed_fragment.size()=torch.Size([2766848]), master_param_fragment.size()=torch.Size([2766848])
model_param_fragment.size()=torch.Size([1082432]), new_param_packed_fragment.size()=torch.Size([1082432]), master_param_fragment.size()=torch.Size([1082432])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([2265024]), new_param_packed_fragment.size()=torch.Size([2265024]), master_param_fragment.size()=torch.Size([2265024])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([2832448]), new_param_packed_fragment.size()=torch.Size([2832448]), master_param_fragment.size()=torch.Size([2832448])
self._net_total_param_size=160671360, self._total_param_size=160677888, dwu_min_page_size=12288, self._block_size=53559296, self._chunk_size=26779648, self._shard_size=3347456
self._net_total_param_size=160671360, self._total_param_size=160677888, dwu_min_page_size=12288, self._block_size=53559296, self._chunk_size=26779648, self._shard_size=3347456
[0, 9, 34]
model_param_fragment.size()=torch.Size([515008]), new_param_packed_fragment.size()=torch.Size([515008]), master_param_fragment.size()=torch.Size([515008])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
[0, 9, 34]
self._net_total_param_size=160671360, self._total_param_size=160677888, dwu_min_page_size=12288, self._block_size=53559296, self._chunk_size=26779648, self._shard_size=3347456
[0, 9, 34]
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
self._net_total_param_size=160671360, self._total_param_size=160677888, dwu_min_page_size=12288, self._block_size=53559296, self._chunk_size=26779648, self._shard_size=3347456
self._net_total_param_size=160671360, self._total_param_size=160677888, dwu_min_page_size=12288, self._block_size=53559296, self._chunk_size=26779648, self._shard_size=3347456
[0, 9, 34]
self._net_total_param_size=160671360, self._total_param_size=160677888, dwu_min_page_size=12288, self._block_size=53559296, self._chunk_size=26779648, self._shard_size=3347456
[0, 9, 34]
[0, 9, 34]
model_param_fragment.size()=torch.Size([3016704]), new_param_packed_fragment.size()=torch.Size([3016704]), master_param_fragment.size()=torch.Size([3016704])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
self._net_total_param_size=160671360, self._total_param_size=160677888, dwu_min_page_size=12288, self._block_size=53559296, self._chunk_size=26779648, self._shard_size=3347456
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
[0, 9, 34]
model_param_fragment.size()=torch.Size([322560]), new_param_packed_fragment.size()=torch.Size([322560]), master_param_fragment.size()=torch.Size([322560])
model_param_fragment.size()=torch.Size([1427456]), new_param_packed_fragment.size()=torch.Size([1427456]), master_param_fragment.size()=torch.Size([1427456])
model_param_fragment.size()=torch.Size([1920000]), new_param_packed_fragment.size()=torch.Size([1920000]), master_param_fragment.size()=torch.Size([1920000])
self._net_total_param_size=160671360, self._total_param_size=160677888, dwu_min_page_size=12288, self._block_size=53559296, self._chunk_size=26779648, self._shard_size=3347456
[0, 9, 34]
model_param_fragment.size()=torch.Size([1929280]), new_param_packed_fragment.size()=torch.Size([1929280]), master_param_fragment.size()=torch.Size([1929280])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([1409984]), new_param_packed_fragment.size()=torch.Size([1409984]), master_param_fragment.size()=torch.Size([1409984])
self._net_total_param_size=160671360, self._total_param_size=160677888, dwu_min_page_size=12288, self._block_size=53559296, self._chunk_size=26779648, self._shard_size=3347456
model_param_fragment.size()=torch.Size([331840]), new_param_packed_fragment.size()=torch.Size([331840]), master_param_fragment.size()=torch.Size([331840])
[0, 9, 34]
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
self._net_total_param_size=160671360, self._total_param_size=160677888, dwu_min_page_size=12288, self._block_size=53559296, self._chunk_size=26779648, self._shard_size=3347456
[0, 9, 34]
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3007424]), new_param_packed_fragment.size()=torch.Size([3007424]), master_param_fragment.size()=torch.Size([3007424])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
self._net_total_param_size=160671360, self._total_param_size=160677888, dwu_min_page_size=12288, self._block_size=53559296, self._chunk_size=26779648, self._shard_size=3347456
[0, 9, 34]
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([2274304]), new_param_packed_fragment.size()=torch.Size([2274304]), master_param_fragment.size()=torch.Size([2274304])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([1064960]), new_param_packed_fragment.size()=torch.Size([1064960]), master_param_fragment.size()=torch.Size([1064960])
self._net_total_param_size=160671360, self._total_param_size=160677888, dwu_min_page_size=12288, self._block_size=53559296, self._chunk_size=26779648, self._shard_size=3347456
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
[0, 9, 34]
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
self._net_total_param_size=160671360, self._total_param_size=160677888, dwu_min_page_size=12288, self._block_size=53559296, self._chunk_size=26779648, self._shard_size=3347456
NCCL version 2.7.5+cuda11.0
model_param_fragment.size()=torch.Size([3308608]), new_param_packed_fragment.size()=torch.Size([3308608]), master_param_fragment.size()=torch.Size([3308608])
[0, 9, 34]
self._net_total_param_size=160671360, self._total_param_size=160677888, dwu_min_page_size=12288, self._block_size=53559296, self._chunk_size=26779648, self._shard_size=3347456
model_param_fragment.size()=torch.Size([32320]), new_param_packed_fragment.size()=torch.Size([32320]), master_param_fragment.size()=torch.Size([32320])
[0, 9, 34]
self._net_total_param_size=160671360, self._total_param_size=160677888, dwu_min_page_size=12288, self._block_size=53559296, self._chunk_size=26779648, self._shard_size=3347456
[0, 9, 34]
self._net_total_param_size=160671360, self._total_param_size=160677888, dwu_min_page_size=12288, self._block_size=53559296, self._chunk_size=26779648, self._shard_size=3347456
self._net_total_param_size=160671360, self._total_param_size=160677888, dwu_min_page_size=12288, self._block_size=53559296, self._chunk_size=26779648, self._shard_size=3347456
[0, 9, 34]
[0, 9, 34]
self._net_total_param_size=160671360, self._total_param_size=160677888, dwu_min_page_size=12288, self._block_size=53559296, self._chunk_size=26779648, self._shard_size=3347456
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
[0, 9, 34]
self._net_total_param_size=160671360, self._total_param_size=160677888, dwu_min_page_size=12288, self._block_size=53559296, self._chunk_size=26779648, self._shard_size=3347456
model_param_fragment.size()=torch.Size([467968]), new_param_packed_fragment.size()=torch.Size([467968]), master_param_fragment.size()=torch.Size([467968])
self._net_total_param_size=160671360, self._total_param_size=160677888, dwu_min_page_size=12288, self._block_size=53559296, self._chunk_size=26779648, self._shard_size=3347456
model_param_fragment.size()=torch.Size([2879488]), new_param_packed_fragment.size()=torch.Size([2879488]), master_param_fragment.size()=torch.Size([2879488])
[0, 9, 34]
[0, 9, 34]
model_param_fragment.size()=torch.Size([3073024]), new_param_packed_fragment.size()=torch.Size([3073024]), master_param_fragment.size()=torch.Size([3073024])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([274432]), new_param_packed_fragment.size()=torch.Size([274432]), master_param_fragment.size()=torch.Size([274432])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
self._net_total_param_size=160671360, self._total_param_size=160677888, dwu_min_page_size=12288, self._block_size=53559296, self._chunk_size=26779648, self._shard_size=3347456
model_param_fragment.size()=torch.Size([3129344]), new_param_packed_fragment.size()=torch.Size([3129344]), master_param_fragment.size()=torch.Size([3129344])
[0, 9, 34]
model_param_fragment.size()=torch.Size([218112]), new_param_packed_fragment.size()=torch.Size([218112]), master_param_fragment.size()=torch.Size([218112])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
self._net_total_param_size=160671360, self._total_param_size=160677888, dwu_min_page_size=12288, self._block_size=53559296, self._chunk_size=26779648, self._shard_size=3347456
[0, 9, 34]
self._net_total_param_size=160671360, self._total_param_size=160677888, dwu_min_page_size=12288, self._block_size=53559296, self._chunk_size=26779648, self._shard_size=3347456
[0, 9, 34]
self._net_total_param_size=160671360, self._total_param_size=160677888, dwu_min_page_size=12288, self._block_size=53559296, self._chunk_size=26779648, self._shard_size=3347456
self._net_total_param_size=160671360, self._total_param_size=160677888, dwu_min_page_size=12288, self._block_size=53559296, self._chunk_size=26779648, self._shard_size=3347456
[0, 9, 34]
[0, 9, 34]
self._net_total_param_size=160671360, self._total_param_size=160677888, dwu_min_page_size=12288, self._block_size=53559296, self._chunk_size=26779648, self._shard_size=3347456
[0, 9, 34]
self._net_total_param_size=160671360, self._total_param_size=160677888, dwu_min_page_size=12288, self._block_size=53559296, self._chunk_size=26779648, self._shard_size=3347456
[0, 9, 34]
self._net_total_param_size=160671360, self._total_param_size=160677888, dwu_min_page_size=12288, self._block_size=53559296, self._chunk_size=26779648, self._shard_size=3347456
self._net_total_param_size=160671360, self._total_param_size=160677888, dwu_min_page_size=12288, self._block_size=53559296, self._chunk_size=26779648, self._shard_size=3347456
[0, 9, 34]
[0, 9, 34]
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([2169856]), new_param_packed_fragment.size()=torch.Size([2169856]), master_param_fragment.size()=torch.Size([2169856])
model_param_fragment.size()=torch.Size([1177600]), new_param_packed_fragment.size()=torch.Size([1177600]), master_param_fragment.size()=torch.Size([1177600])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
self._net_total_param_size=160671360, self._total_param_size=160677888, dwu_min_page_size=12288, self._block_size=53559296, self._chunk_size=26779648, self._shard_size=3347456
model_param_fragment.size()=torch.Size([572416]), new_param_packed_fragment.size()=torch.Size([572416]), master_param_fragment.size()=torch.Size([572416])
self._net_total_param_size=160671360, self._total_param_size=160677888, dwu_min_page_size=12288, self._block_size=53559296, self._chunk_size=26779648, self._shard_size=3347456
[0, 9, 34]
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
[0, 9, 34]
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([2766848]), new_param_packed_fragment.size()=torch.Size([2766848]), master_param_fragment.size()=torch.Size([2766848])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([1082432]), new_param_packed_fragment.size()=torch.Size([1082432]), master_param_fragment.size()=torch.Size([1082432])
model_param_fragment.size()=torch.Size([2265024]), new_param_packed_fragment.size()=torch.Size([2265024]), master_param_fragment.size()=torch.Size([2265024])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
self._net_total_param_size=160671360, self._total_param_size=160677888, dwu_min_page_size=12288, self._block_size=53559296, self._chunk_size=26779648, self._shard_size=3347456
[0, 9, 34]
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3129344]), new_param_packed_fragment.size()=torch.Size([3129344]), master_param_fragment.size()=torch.Size([3129344])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([218112]), new_param_packed_fragment.size()=torch.Size([218112]), master_param_fragment.size()=torch.Size([218112])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
self._net_total_param_size=160671360, self._total_param_size=160677888, dwu_min_page_size=12288, self._block_size=53559296, self._chunk_size=26779648, self._shard_size=3347456
self._net_total_param_size=160671360, self._total_param_size=160677888, dwu_min_page_size=12288, self._block_size=53559296, self._chunk_size=26779648, self._shard_size=3347456
model_param_fragment.size()=torch.Size([467968]), new_param_packed_fragment.size()=torch.Size([467968]), master_param_fragment.size()=torch.Size([467968])
[0, 9, 34]
[0, 9, 34]
model_param_fragment.size()=torch.Size([2879488]), new_param_packed_fragment.size()=torch.Size([2879488]), master_param_fragment.size()=torch.Size([2879488])
model_param_fragment.size()=torch.Size([3073024]), new_param_packed_fragment.size()=torch.Size([3073024]), master_param_fragment.size()=torch.Size([3073024])
model_param_fragment.size()=torch.Size([274432]), new_param_packed_fragment.size()=torch.Size([274432]), master_param_fragment.size()=torch.Size([274432])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
self._net_total_param_size=160671360, self._total_param_size=160677888, dwu_min_page_size=12288, self._block_size=53559296, self._chunk_size=26779648, self._shard_size=3347456
[0, 9, 34]
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
self._net_total_param_size=160671360, self._total_param_size=160677888, dwu_min_page_size=12288, self._block_size=53559296, self._chunk_size=26779648, self._shard_size=3347456
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
[0, 9, 34]
self._net_total_param_size=160671360, self._total_param_size=160677888, dwu_min_page_size=12288, self._block_size=53559296, self._chunk_size=26779648, self._shard_size=3347456
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
self._net_total_param_size=160671360, self._total_param_size=160677888, dwu_min_page_size=12288, self._block_size=53559296, self._chunk_size=26779648, self._shard_size=3347456
[0, 9, 34]
[0, 9, 34]
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([2217984]), new_param_packed_fragment.size()=torch.Size([2217984]), master_param_fragment.size()=torch.Size([2217984])
model_param_fragment.size()=torch.Size([467968]), new_param_packed_fragment.size()=torch.Size([467968]), master_param_fragment.size()=torch.Size([467968])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([2879488]), new_param_packed_fragment.size()=torch.Size([2879488]), master_param_fragment.size()=torch.Size([2879488])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([1121280]), new_param_packed_fragment.size()=torch.Size([1121280]), master_param_fragment.size()=torch.Size([1121280])
model_param_fragment.size()=torch.Size([3129344]), new_param_packed_fragment.size()=torch.Size([3129344]), master_param_fragment.size()=torch.Size([3129344])
model_param_fragment.size()=torch.Size([3073024]), new_param_packed_fragment.size()=torch.Size([3073024]), master_param_fragment.size()=torch.Size([3073024])
self._net_total_param_size=160671360, self._total_param_size=160677888, dwu_min_page_size=12288, self._block_size=53559296, self._chunk_size=26779648, self._shard_size=3347456
model_param_fragment.size()=torch.Size([218112]), new_param_packed_fragment.size()=torch.Size([218112]), master_param_fragment.size()=torch.Size([218112])
model_param_fragment.size()=torch.Size([628736]), new_param_packed_fragment.size()=torch.Size([628736]), master_param_fragment.size()=torch.Size([628736])
model_param_fragment.size()=torch.Size([274432]), new_param_packed_fragment.size()=torch.Size([274432]), master_param_fragment.size()=torch.Size([274432])
model_param_fragment.size()=torch.Size([467968]), new_param_packed_fragment.size()=torch.Size([467968]), master_param_fragment.size()=torch.Size([467968])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([2879488]), new_param_packed_fragment.size()=torch.Size([2879488]), master_param_fragment.size()=torch.Size([2879488])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
[0, 9, 34]
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([1024]), new_param_packed_fragment.size()=torch.Size([1024]), master_param_fragment.size()=torch.Size([1024])
model_param_fragment.size()=torch.Size([1]), new_param_packed_fragment.size()=torch.Size([1]), master_param_fragment.size()=torch.Size([1])
model_param_fragment.size()=torch.Size([3073024]), new_param_packed_fragment.size()=torch.Size([3073024]), master_param_fragment.size()=torch.Size([3073024])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([1024]), new_param_packed_fragment.size()=torch.Size([1024]), master_param_fragment.size()=torch.Size([1024])
model_param_fragment.size()=torch.Size([274432]), new_param_packed_fragment.size()=torch.Size([274432]), master_param_fragment.size()=torch.Size([274432])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([1048576]), new_param_packed_fragment.size()=torch.Size([1048576]), master_param_fragment.size()=torch.Size([1048576])
model_param_fragment.size()=torch.Size([1048576]), new_param_packed_fragment.size()=torch.Size([1048576]), master_param_fragment.size()=torch.Size([1048576])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([611264]), new_param_packed_fragment.size()=torch.Size([611264]), master_param_fragment.size()=torch.Size([611264])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([1130560]), new_param_packed_fragment.size()=torch.Size([1130560]), master_param_fragment.size()=torch.Size([1130560])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([2208704]), new_param_packed_fragment.size()=torch.Size([2208704]), master_param_fragment.size()=torch.Size([2208704])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
self._net_total_param_size=160671360, self._total_param_size=160677888, dwu_min_page_size=12288, self._block_size=53559296, self._chunk_size=26779648, self._shard_size=3347456
[0, 9, 34]
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
self._net_total_param_size=160671360, self._total_param_size=160677888, dwu_min_page_size=12288, self._block_size=53559296, self._chunk_size=26779648, self._shard_size=3347456
[0, 9, 34]
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
self._net_total_param_size=160671360, self._total_param_size=160677888, dwu_min_page_size=12288, self._block_size=53559296, self._chunk_size=26779648, self._shard_size=3347456
[0, 9, 34]
model_param_fragment.size()=torch.Size([2217984]), new_param_packed_fragment.size()=torch.Size([2217984]), master_param_fragment.size()=torch.Size([2217984])
self._net_total_param_size=160671360, self._total_param_size=160677888, dwu_min_page_size=12288, self._block_size=53559296, self._chunk_size=26779648, self._shard_size=3347456
self._net_total_param_size=160671360, self._total_param_size=160677888, dwu_min_page_size=12288, self._block_size=53559296, self._chunk_size=26779648, self._shard_size=3347456
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([1121280]), new_param_packed_fragment.size()=torch.Size([1121280]), master_param_fragment.size()=torch.Size([1121280])
[0, 9, 34]
[0, 9, 34]
model_param_fragment.size()=torch.Size([628736]), new_param_packed_fragment.size()=torch.Size([628736]), master_param_fragment.size()=torch.Size([628736])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([1024]), new_param_packed_fragment.size()=torch.Size([1024]), master_param_fragment.size()=torch.Size([1024])
model_param_fragment.size()=torch.Size([1]), new_param_packed_fragment.size()=torch.Size([1]), master_param_fragment.size()=torch.Size([1])
model_param_fragment.size()=torch.Size([1024]), new_param_packed_fragment.size()=torch.Size([1024]), master_param_fragment.size()=torch.Size([1024])
self._net_total_param_size=160671360, self._total_param_size=160677888, dwu_min_page_size=12288, self._block_size=53559296, self._chunk_size=26779648, self._shard_size=3347456
model_param_fragment.size()=torch.Size([1048576]), new_param_packed_fragment.size()=torch.Size([1048576]), master_param_fragment.size()=torch.Size([1048576])
[0, 9, 34]
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([1048576]), new_param_packed_fragment.size()=torch.Size([1048576]), master_param_fragment.size()=torch.Size([1048576])
model_param_fragment.size()=torch.Size([611264]), new_param_packed_fragment.size()=torch.Size([611264]), master_param_fragment.size()=torch.Size([611264])
model_param_fragment.size()=torch.Size([1130560]), new_param_packed_fragment.size()=torch.Size([1130560]), master_param_fragment.size()=torch.Size([1130560])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([2208704]), new_param_packed_fragment.size()=torch.Size([2208704]), master_param_fragment.size()=torch.Size([2208704])
model_param_fragment.size()=torch.Size([2274304]), new_param_packed_fragment.size()=torch.Size([2274304]), master_param_fragment.size()=torch.Size([2274304])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
self._net_total_param_size=160671360, self._total_param_size=160677888, dwu_min_page_size=12288, self._block_size=53559296, self._chunk_size=26779648, self._shard_size=3347456
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
self._net_total_param_size=160671360, self._total_param_size=160677888, dwu_min_page_size=12288, self._block_size=53559296, self._chunk_size=26779648, self._shard_size=3347456
model_param_fragment.size()=torch.Size([3016704]), new_param_packed_fragment.size()=torch.Size([3016704]), master_param_fragment.size()=torch.Size([3016704])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([1064960]), new_param_packed_fragment.size()=torch.Size([1064960]), master_param_fragment.size()=torch.Size([1064960])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
[0, 9, 34]
self._net_total_param_size=160671360, self._total_param_size=160677888, dwu_min_page_size=12288, self._block_size=53559296, self._chunk_size=26779648, self._shard_size=3347456
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
[0, 9, 34]
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
[0, 9, 34]
0: Initializing dwu fp16 optimizer
model_param_fragment.size()=torch.Size([322560]), new_param_packed_fragment.size()=torch.Size([322560]), master_param_fragment.size()=torch.Size([322560])
model_param_fragment.size()=torch.Size([1314816]), new_param_packed_fragment.size()=torch.Size([1314816]), master_param_fragment.size()=torch.Size([1314816])
model_param_fragment.size()=torch.Size([1427456]), new_param_packed_fragment.size()=torch.Size([1427456]), master_param_fragment.size()=torch.Size([1427456])
model_param_fragment.size()=torch.Size([3308608]), new_param_packed_fragment.size()=torch.Size([3308608]), master_param_fragment.size()=torch.Size([3308608])
model_param_fragment.size()=torch.Size([1920000]), new_param_packed_fragment.size()=torch.Size([1920000]), master_param_fragment.size()=torch.Size([1920000])
model_param_fragment.size()=torch.Size([32320]), new_param_packed_fragment.size()=torch.Size([32320]), master_param_fragment.size()=torch.Size([32320])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([1929280]), new_param_packed_fragment.size()=torch.Size([1929280]), master_param_fragment.size()=torch.Size([1929280])
self._net_total_param_size=160671360, self._total_param_size=160677888, dwu_min_page_size=12288, self._block_size=53559296, self._chunk_size=26779648, self._shard_size=3347456
self._net_total_param_size=160671360, self._total_param_size=160677888, dwu_min_page_size=12288, self._block_size=53559296, self._chunk_size=26779648, self._shard_size=3347456
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
[0, 9, 34]
[0, 9, 34]
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([1409984]), new_param_packed_fragment.size()=torch.Size([1409984]), master_param_fragment.size()=torch.Size([1409984])
model_param_fragment.size()=torch.Size([331840]), new_param_packed_fragment.size()=torch.Size([331840]), master_param_fragment.size()=torch.Size([331840])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
self._net_total_param_size=160671360, self._total_param_size=160677888, dwu_min_page_size=12288, self._block_size=53559296, self._chunk_size=26779648, self._shard_size=3347456
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([3007424]), new_param_packed_fragment.size()=torch.Size([3007424]), master_param_fragment.size()=torch.Size([3007424])
[0, 9, 34]
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
self._net_total_param_size=160671360, self._total_param_size=160677888, dwu_min_page_size=12288, self._block_size=53559296, self._chunk_size=26779648, self._shard_size=3347456
[0, 9, 34]
self._net_total_param_size=160671360, self._total_param_size=160677888, dwu_min_page_size=12288, self._block_size=53559296, self._chunk_size=26779648, self._shard_size=3347456
model_param_fragment.size()=torch.Size([2024448]), new_param_packed_fragment.size()=torch.Size([2024448]), master_param_fragment.size()=torch.Size([2024448])
[0, 9, 34]
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
self._net_total_param_size=160671360, self._total_param_size=160677888, dwu_min_page_size=12288, self._block_size=53559296, self._chunk_size=26779648, self._shard_size=3347456
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([2832448]), new_param_packed_fragment.size()=torch.Size([2832448]), master_param_fragment.size()=torch.Size([2832448])
model_param_fragment.size()=torch.Size([515008]), new_param_packed_fragment.size()=torch.Size([515008]), master_param_fragment.size()=torch.Size([515008])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
self._net_total_param_size=160671360, self._total_param_size=160677888, dwu_min_page_size=12288, self._block_size=53559296, self._chunk_size=26779648, self._shard_size=3347456
model_param_fragment.size()=torch.Size([2217984]), new_param_packed_fragment.size()=torch.Size([2217984]), master_param_fragment.size()=torch.Size([2217984])
[0, 9, 34]
self._net_total_param_size=160671360, self._total_param_size=160677888, dwu_min_page_size=12288, self._block_size=53559296, self._chunk_size=26779648, self._shard_size=3347456
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
[0, 9, 34]
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
[0, 9, 34]
model_param_fragment.size()=torch.Size([1121280]), new_param_packed_fragment.size()=torch.Size([1121280]), master_param_fragment.size()=torch.Size([1121280])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([628736]), new_param_packed_fragment.size()=torch.Size([628736]), master_param_fragment.size()=torch.Size([628736])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([1024]), new_param_packed_fragment.size()=torch.Size([1024]), master_param_fragment.size()=torch.Size([1024])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([1]), new_param_packed_fragment.size()=torch.Size([1]), master_param_fragment.size()=torch.Size([1])
model_param_fragment.size()=torch.Size([1024]), new_param_packed_fragment.size()=torch.Size([1024]), master_param_fragment.size()=torch.Size([1024])
self._net_total_param_size=160671360, self._total_param_size=160677888, dwu_min_page_size=12288, self._block_size=53559296, self._chunk_size=26779648, self._shard_size=3347456
model_param_fragment.size()=torch.Size([1048576]), new_param_packed_fragment.size()=torch.Size([1048576]), master_param_fragment.size()=torch.Size([1048576])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([1314816]), new_param_packed_fragment.size()=torch.Size([1314816]), master_param_fragment.size()=torch.Size([1314816])
model_param_fragment.size()=torch.Size([1048576]), new_param_packed_fragment.size()=torch.Size([1048576]), master_param_fragment.size()=torch.Size([1048576])
model_param_fragment.size()=torch.Size([611264]), new_param_packed_fragment.size()=torch.Size([611264]), master_param_fragment.size()=torch.Size([611264])
model_param_fragment.size()=torch.Size([2217984]), new_param_packed_fragment.size()=torch.Size([2217984]), master_param_fragment.size()=torch.Size([2217984])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
[0, 9, 34]
model_param_fragment.size()=torch.Size([467968]), new_param_packed_fragment.size()=torch.Size([467968]), master_param_fragment.size()=torch.Size([467968])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
NCCL version 2.7.5+cuda11.0
model_param_fragment.size()=torch.Size([1130560]), new_param_packed_fragment.size()=torch.Size([1130560]), master_param_fragment.size()=torch.Size([1130560])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([2024448]), new_param_packed_fragment.size()=torch.Size([2024448]), master_param_fragment.size()=torch.Size([2024448])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
self._net_total_param_size=160671360, self._total_param_size=160677888, dwu_min_page_size=12288, self._block_size=53559296, self._chunk_size=26779648, self._shard_size=3347456
model_param_fragment.size()=torch.Size([2879488]), new_param_packed_fragment.size()=torch.Size([2879488]), master_param_fragment.size()=torch.Size([2879488])
[0, 9, 34]
model_param_fragment.size()=torch.Size([1121280]), new_param_packed_fragment.size()=torch.Size([1121280]), master_param_fragment.size()=torch.Size([1121280])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([3073024]), new_param_packed_fragment.size()=torch.Size([3073024]), master_param_fragment.size()=torch.Size([3073024])
model_param_fragment.size()=torch.Size([2208704]), new_param_packed_fragment.size()=torch.Size([2208704]), master_param_fragment.size()=torch.Size([2208704])
model_param_fragment.size()=torch.Size([628736]), new_param_packed_fragment.size()=torch.Size([628736]), master_param_fragment.size()=torch.Size([628736])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([2832448]), new_param_packed_fragment.size()=torch.Size([2832448]), master_param_fragment.size()=torch.Size([2832448])
model_param_fragment.size()=torch.Size([274432]), new_param_packed_fragment.size()=torch.Size([274432]), master_param_fragment.size()=torch.Size([274432])
self._net_total_param_size=160671360, self._total_param_size=160677888, dwu_min_page_size=12288, self._block_size=53559296, self._chunk_size=26779648, self._shard_size=3347456
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
[0, 9, 34]
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([515008]), new_param_packed_fragment.size()=torch.Size([515008]), master_param_fragment.size()=torch.Size([515008])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([1024]), new_param_packed_fragment.size()=torch.Size([1024]), master_param_fragment.size()=torch.Size([1024])
self._net_total_param_size=160671360, self._total_param_size=160677888, dwu_min_page_size=12288, self._block_size=53559296, self._chunk_size=26779648, self._shard_size=3347456
model_param_fragment.size()=torch.Size([1]), new_param_packed_fragment.size()=torch.Size([1]), master_param_fragment.size()=torch.Size([1])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
[0, 9, 34]
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([1024]), new_param_packed_fragment.size()=torch.Size([1024]), master_param_fragment.size()=torch.Size([1024])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([1048576]), new_param_packed_fragment.size()=torch.Size([1048576]), master_param_fragment.size()=torch.Size([1048576])
model_param_fragment.size()=torch.Size([1048576]), new_param_packed_fragment.size()=torch.Size([1048576]), master_param_fragment.size()=torch.Size([1048576])
model_param_fragment.size()=torch.Size([611264]), new_param_packed_fragment.size()=torch.Size([611264]), master_param_fragment.size()=torch.Size([611264])
model_param_fragment.size()=torch.Size([1130560]), new_param_packed_fragment.size()=torch.Size([1130560]), master_param_fragment.size()=torch.Size([1130560])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([2208704]), new_param_packed_fragment.size()=torch.Size([2208704]), master_param_fragment.size()=torch.Size([2208704])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([2968576]), new_param_packed_fragment.size()=torch.Size([2968576]), master_param_fragment.size()=torch.Size([2968576])
model_param_fragment.size()=torch.Size([378880]), new_param_packed_fragment.size()=torch.Size([378880]), master_param_fragment.size()=torch.Size([378880])
self._net_total_param_size=160671360, self._total_param_size=160677888, dwu_min_page_size=12288, self._block_size=53559296, self._chunk_size=26779648, self._shard_size=3347456
[0, 9, 34]
model_param_fragment.size()=torch.Size([1371136]), new_param_packed_fragment.size()=torch.Size([1371136]), master_param_fragment.size()=torch.Size([1371136])
model_param_fragment.size()=torch.Size([1976320]), new_param_packed_fragment.size()=torch.Size([1976320]), master_param_fragment.size()=torch.Size([1976320])
self._net_total_param_size=160671360, self._total_param_size=160677888, dwu_min_page_size=12288, self._block_size=53559296, self._chunk_size=26779648, self._shard_size=3347456
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([283712]), new_param_packed_fragment.size()=torch.Size([283712]), master_param_fragment.size()=torch.Size([283712])
[0, 9, 34]
model_param_fragment.size()=torch.Size([3063744]), new_param_packed_fragment.size()=torch.Size([3063744]), master_param_fragment.size()=torch.Size([3063744])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
self._net_total_param_size=160671360, self._total_param_size=160677888, dwu_min_page_size=12288, self._block_size=53559296, self._chunk_size=26779648, self._shard_size=3347456
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
[0, 9, 34]
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
self._net_total_param_size=160671360, self._total_param_size=160677888, dwu_min_page_size=12288, self._block_size=53559296, self._chunk_size=26779648, self._shard_size=3347456
model_param_fragment.size()=torch.Size([1314816]), new_param_packed_fragment.size()=torch.Size([1314816]), master_param_fragment.size()=torch.Size([1314816])
[0, 9, 34]
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([2024448]), new_param_packed_fragment.size()=torch.Size([2024448]), master_param_fragment.size()=torch.Size([2024448])
model_param_fragment.size()=torch.Size([3129344]), new_param_packed_fragment.size()=torch.Size([3129344]), master_param_fragment.size()=torch.Size([3129344])
model_param_fragment.size()=torch.Size([218112]), new_param_packed_fragment.size()=torch.Size([218112]), master_param_fragment.size()=torch.Size([218112])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([2832448]), new_param_packed_fragment.size()=torch.Size([2832448]), master_param_fragment.size()=torch.Size([2832448])
model_param_fragment.size()=torch.Size([515008]), new_param_packed_fragment.size()=torch.Size([515008]), master_param_fragment.size()=torch.Size([515008])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
self._net_total_param_size=160671360, self._total_param_size=160677888, dwu_min_page_size=12288, self._block_size=53559296, self._chunk_size=26779648, self._shard_size=3347456
model_param_fragment.size()=torch.Size([2169856]), new_param_packed_fragment.size()=torch.Size([2169856]), master_param_fragment.size()=torch.Size([2169856])
self._net_total_param_size=160671360, self._total_param_size=160677888, dwu_min_page_size=12288, self._block_size=53559296, self._chunk_size=26779648, self._shard_size=3347456
model_param_fragment.size()=torch.Size([1177600]), new_param_packed_fragment.size()=torch.Size([1177600]), master_param_fragment.size()=torch.Size([1177600])
[0, 9, 34]
model_param_fragment.size()=torch.Size([572416]), new_param_packed_fragment.size()=torch.Size([572416]), master_param_fragment.size()=torch.Size([572416])
self._net_total_param_size=160671360, self._total_param_size=160677888, dwu_min_page_size=12288, self._block_size=53559296, self._chunk_size=26779648, self._shard_size=3347456
[0, 9, 34]
[0, 9, 34]
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([2766848]), new_param_packed_fragment.size()=torch.Size([2766848]), master_param_fragment.size()=torch.Size([2766848])
model_param_fragment.size()=torch.Size([1082432]), new_param_packed_fragment.size()=torch.Size([1082432]), master_param_fragment.size()=torch.Size([1082432])
model_param_fragment.size()=torch.Size([2265024]), new_param_packed_fragment.size()=torch.Size([2265024]), master_param_fragment.size()=torch.Size([2265024])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([2169856]), new_param_packed_fragment.size()=torch.Size([2169856]), master_param_fragment.size()=torch.Size([2169856])
model_param_fragment.size()=torch.Size([1177600]), new_param_packed_fragment.size()=torch.Size([1177600]), master_param_fragment.size()=torch.Size([1177600])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([572416]), new_param_packed_fragment.size()=torch.Size([572416]), master_param_fragment.size()=torch.Size([572416])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
self._net_total_param_size=160671360, self._total_param_size=160677888, dwu_min_page_size=12288, self._block_size=53559296, self._chunk_size=26779648, self._shard_size=3347456
[0, 9, 34]
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
self._net_total_param_size=160671360, self._total_param_size=160677888, dwu_min_page_size=12288, self._block_size=53559296, self._chunk_size=26779648, self._shard_size=3347456
self._net_total_param_size=160671360, self._total_param_size=160677888, dwu_min_page_size=12288, self._block_size=53559296, self._chunk_size=26779648, self._shard_size=3347456
model_param_fragment.size()=torch.Size([2766848]), new_param_packed_fragment.size()=torch.Size([2766848]), master_param_fragment.size()=torch.Size([2766848])
[0, 9, 34]
[0, 9, 34]
model_param_fragment.size()=torch.Size([2968576]), new_param_packed_fragment.size()=torch.Size([2968576]), master_param_fragment.size()=torch.Size([2968576])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([1082432]), new_param_packed_fragment.size()=torch.Size([1082432]), master_param_fragment.size()=torch.Size([1082432])
self._net_total_param_size=160671360, self._total_param_size=160677888, dwu_min_page_size=12288, self._block_size=53559296, self._chunk_size=26779648, self._shard_size=3347456
model_param_fragment.size()=torch.Size([2265024]), new_param_packed_fragment.size()=torch.Size([2265024]), master_param_fragment.size()=torch.Size([2265024])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
[0, 9, 34]
model_param_fragment.size()=torch.Size([378880]), new_param_packed_fragment.size()=torch.Size([378880]), master_param_fragment.size()=torch.Size([378880])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([2217984]), new_param_packed_fragment.size()=torch.Size([2217984]), master_param_fragment.size()=torch.Size([2217984])
model_param_fragment.size()=torch.Size([1371136]), new_param_packed_fragment.size()=torch.Size([1371136]), master_param_fragment.size()=torch.Size([1371136])
model_param_fragment.size()=torch.Size([1976320]), new_param_packed_fragment.size()=torch.Size([1976320]), master_param_fragment.size()=torch.Size([1976320])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([1121280]), new_param_packed_fragment.size()=torch.Size([1121280]), master_param_fragment.size()=torch.Size([1121280])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([283712]), new_param_packed_fragment.size()=torch.Size([283712]), master_param_fragment.size()=torch.Size([283712])
model_param_fragment.size()=torch.Size([3063744]), new_param_packed_fragment.size()=torch.Size([3063744]), master_param_fragment.size()=torch.Size([3063744])
model_param_fragment.size()=torch.Size([628736]), new_param_packed_fragment.size()=torch.Size([628736]), master_param_fragment.size()=torch.Size([628736])
self._net_total_param_size=160671360, self._total_param_size=160677888, dwu_min_page_size=12288, self._block_size=53559296, self._chunk_size=26779648, self._shard_size=3347456
[0, 9, 34]
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([1024]), new_param_packed_fragment.size()=torch.Size([1024]), master_param_fragment.size()=torch.Size([1024])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
self._net_total_param_size=160671360, self._total_param_size=160677888, dwu_min_page_size=12288, self._block_size=53559296, self._chunk_size=26779648, self._shard_size=3347456
model_param_fragment.size()=torch.Size([1]), new_param_packed_fragment.size()=torch.Size([1]), master_param_fragment.size()=torch.Size([1])
[0, 9, 34]
model_param_fragment.size()=torch.Size([1024]), new_param_packed_fragment.size()=torch.Size([1024]), master_param_fragment.size()=torch.Size([1024])
model_param_fragment.size()=torch.Size([1048576]), new_param_packed_fragment.size()=torch.Size([1048576]), master_param_fragment.size()=torch.Size([1048576])
model_param_fragment.size()=torch.Size([1048576]), new_param_packed_fragment.size()=torch.Size([1048576]), master_param_fragment.size()=torch.Size([1048576])
model_param_fragment.size()=torch.Size([611264]), new_param_packed_fragment.size()=torch.Size([611264]), master_param_fragment.size()=torch.Size([611264])
model_param_fragment.size()=torch.Size([2169856]), new_param_packed_fragment.size()=torch.Size([2169856]), master_param_fragment.size()=torch.Size([2169856])
self._net_total_param_size=160671360, self._total_param_size=160677888, dwu_min_page_size=12288, self._block_size=53559296, self._chunk_size=26779648, self._shard_size=3347456
model_param_fragment.size()=torch.Size([1177600]), new_param_packed_fragment.size()=torch.Size([1177600]), master_param_fragment.size()=torch.Size([1177600])
[0, 9, 34]
model_param_fragment.size()=torch.Size([1130560]), new_param_packed_fragment.size()=torch.Size([1130560]), master_param_fragment.size()=torch.Size([1130560])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([572416]), new_param_packed_fragment.size()=torch.Size([572416]), master_param_fragment.size()=torch.Size([572416])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([2208704]), new_param_packed_fragment.size()=torch.Size([2208704]), master_param_fragment.size()=torch.Size([2208704])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([2766848]), new_param_packed_fragment.size()=torch.Size([2766848]), master_param_fragment.size()=torch.Size([2766848])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([1082432]), new_param_packed_fragment.size()=torch.Size([1082432]), master_param_fragment.size()=torch.Size([1082432])
model_param_fragment.size()=torch.Size([2265024]), new_param_packed_fragment.size()=torch.Size([2265024]), master_param_fragment.size()=torch.Size([2265024])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([2968576]), new_param_packed_fragment.size()=torch.Size([2968576]), master_param_fragment.size()=torch.Size([2968576])
self._net_total_param_size=160671360, self._total_param_size=160677888, dwu_min_page_size=12288, self._block_size=53559296, self._chunk_size=26779648, self._shard_size=3347456
model_param_fragment.size()=torch.Size([378880]), new_param_packed_fragment.size()=torch.Size([378880]), master_param_fragment.size()=torch.Size([378880])
model_param_fragment.size()=torch.Size([1371136]), new_param_packed_fragment.size()=torch.Size([1371136]), master_param_fragment.size()=torch.Size([1371136])
model_param_fragment.size()=torch.Size([1976320]), new_param_packed_fragment.size()=torch.Size([1976320]), master_param_fragment.size()=torch.Size([1976320])
[0, 9, 34]
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
self._net_total_param_size=160671360, self._total_param_size=160677888, dwu_min_page_size=12288, self._block_size=53559296, self._chunk_size=26779648, self._shard_size=3347456
[0, 9, 34]
model_param_fragment.size()=torch.Size([283712]), new_param_packed_fragment.size()=torch.Size([283712]), master_param_fragment.size()=torch.Size([283712])
self._net_total_param_size=160671360, self._total_param_size=160677888, dwu_min_page_size=12288, self._block_size=53559296, self._chunk_size=26779648, self._shard_size=3347456
model_param_fragment.size()=torch.Size([3063744]), new_param_packed_fragment.size()=torch.Size([3063744]), master_param_fragment.size()=torch.Size([3063744])
[0, 9, 34]
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
self._net_total_param_size=160671360, self._total_param_size=160677888, dwu_min_page_size=12288, self._block_size=53559296, self._chunk_size=26779648, self._shard_size=3347456
self._net_total_param_size=160671360, self._total_param_size=160677888, dwu_min_page_size=12288, self._block_size=53559296, self._chunk_size=26779648, self._shard_size=3347456
[0, 9, 34]
[0, 9, 34]
self._net_total_param_size=160671360, self._total_param_size=160677888, dwu_min_page_size=12288, self._block_size=53559296, self._chunk_size=26779648, self._shard_size=3347456
[0, 9, 34]
self._net_total_param_size=160671360, self._total_param_size=160677888, dwu_min_page_size=12288, self._block_size=53559296, self._chunk_size=26779648, self._shard_size=3347456
[0, 9, 34]
self._net_total_param_size=160671360, self._total_param_size=160677888, dwu_min_page_size=12288, self._block_size=53559296, self._chunk_size=26779648, self._shard_size=3347456
[0, 9, 34]
self._net_total_param_size=160671360, self._total_param_size=160677888, dwu_min_page_size=12288, self._block_size=53559296, self._chunk_size=26779648, self._shard_size=3347456
[0, 9, 34]
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([1314816]), new_param_packed_fragment.size()=torch.Size([1314816]), master_param_fragment.size()=torch.Size([1314816])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3016704]), new_param_packed_fragment.size()=torch.Size([3016704]), master_param_fragment.size()=torch.Size([3016704])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([2024448]), new_param_packed_fragment.size()=torch.Size([2024448]), master_param_fragment.size()=torch.Size([2024448])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([1314816]), new_param_packed_fragment.size()=torch.Size([1314816]), master_param_fragment.size()=torch.Size([1314816])
self._net_total_param_size=160671360, self._total_param_size=160677888, dwu_min_page_size=12288, self._block_size=53559296, self._chunk_size=26779648, self._shard_size=3347456
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([322560]), new_param_packed_fragment.size()=torch.Size([322560]), master_param_fragment.size()=torch.Size([322560])
[0, 9, 34]
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([1427456]), new_param_packed_fragment.size()=torch.Size([1427456]), master_param_fragment.size()=torch.Size([1427456])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([1920000]), new_param_packed_fragment.size()=torch.Size([1920000]), master_param_fragment.size()=torch.Size([1920000])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([2832448]), new_param_packed_fragment.size()=torch.Size([2832448]), master_param_fragment.size()=torch.Size([2832448])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([2024448]), new_param_packed_fragment.size()=torch.Size([2024448]), master_param_fragment.size()=torch.Size([2024448])
model_param_fragment.size()=torch.Size([515008]), new_param_packed_fragment.size()=torch.Size([515008]), master_param_fragment.size()=torch.Size([515008])
model_param_fragment.size()=torch.Size([3129344]), new_param_packed_fragment.size()=torch.Size([3129344]), master_param_fragment.size()=torch.Size([3129344])
model_param_fragment.size()=torch.Size([1929280]), new_param_packed_fragment.size()=torch.Size([1929280]), master_param_fragment.size()=torch.Size([1929280])
self._net_total_param_size=160671360, self._total_param_size=160677888, dwu_min_page_size=12288, self._block_size=53559296, self._chunk_size=26779648, self._shard_size=3347456
model_param_fragment.size()=torch.Size([218112]), new_param_packed_fragment.size()=torch.Size([218112]), master_param_fragment.size()=torch.Size([218112])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
[0, 9, 34]
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([1409984]), new_param_packed_fragment.size()=torch.Size([1409984]), master_param_fragment.size()=torch.Size([1409984])
model_param_fragment.size()=torch.Size([2832448]), new_param_packed_fragment.size()=torch.Size([2832448]), master_param_fragment.size()=torch.Size([2832448])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([331840]), new_param_packed_fragment.size()=torch.Size([331840]), master_param_fragment.size()=torch.Size([331840])
self._net_total_param_size=160671360, self._total_param_size=160677888, dwu_min_page_size=12288, self._block_size=53559296, self._chunk_size=26779648, self._shard_size=3347456
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
[0, 9, 34]
model_param_fragment.size()=torch.Size([515008]), new_param_packed_fragment.size()=torch.Size([515008]), master_param_fragment.size()=torch.Size([515008])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3016704]), new_param_packed_fragment.size()=torch.Size([3016704]), master_param_fragment.size()=torch.Size([3016704])
model_param_fragment.size()=torch.Size([2169856]), new_param_packed_fragment.size()=torch.Size([2169856]), master_param_fragment.size()=torch.Size([2169856])
model_param_fragment.size()=torch.Size([3007424]), new_param_packed_fragment.size()=torch.Size([3007424]), master_param_fragment.size()=torch.Size([3007424])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([1177600]), new_param_packed_fragment.size()=torch.Size([1177600]), master_param_fragment.size()=torch.Size([1177600])
model_param_fragment.size()=torch.Size([572416]), new_param_packed_fragment.size()=torch.Size([572416]), master_param_fragment.size()=torch.Size([572416])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
self._net_total_param_size=160671360, self._total_param_size=160677888, dwu_min_page_size=12288, self._block_size=53559296, self._chunk_size=26779648, self._shard_size=3347456
model_param_fragment.size()=torch.Size([322560]), new_param_packed_fragment.size()=torch.Size([322560]), master_param_fragment.size()=torch.Size([322560])
self._net_total_param_size=160671360, self._total_param_size=160677888, dwu_min_page_size=12288, self._block_size=53559296, self._chunk_size=26779648, self._shard_size=3347456
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
self._net_total_param_size=160671360, self._total_param_size=160677888, dwu_min_page_size=12288, self._block_size=53559296, self._chunk_size=26779648, self._shard_size=3347456
[0, 9, 34]
[0, 9, 34]
self._net_total_param_size=160671360, self._total_param_size=160677888, dwu_min_page_size=12288, self._block_size=53559296, self._chunk_size=26779648, self._shard_size=3347456
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
[0, 9, 34]
model_param_fragment.size()=torch.Size([2766848]), new_param_packed_fragment.size()=torch.Size([2766848]), master_param_fragment.size()=torch.Size([2766848])
[0, 9, 34]
model_param_fragment.size()=torch.Size([1427456]), new_param_packed_fragment.size()=torch.Size([1427456]), master_param_fragment.size()=torch.Size([1427456])
model_param_fragment.size()=torch.Size([1082432]), new_param_packed_fragment.size()=torch.Size([1082432]), master_param_fragment.size()=torch.Size([1082432])
model_param_fragment.size()=torch.Size([1920000]), new_param_packed_fragment.size()=torch.Size([1920000]), master_param_fragment.size()=torch.Size([1920000])
model_param_fragment.size()=torch.Size([1929280]), new_param_packed_fragment.size()=torch.Size([1929280]), master_param_fragment.size()=torch.Size([1929280])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([2265024]), new_param_packed_fragment.size()=torch.Size([2265024]), master_param_fragment.size()=torch.Size([2265024])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([2217984]), new_param_packed_fragment.size()=torch.Size([2217984]), master_param_fragment.size()=torch.Size([2217984])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([1409984]), new_param_packed_fragment.size()=torch.Size([1409984]), master_param_fragment.size()=torch.Size([1409984])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([331840]), new_param_packed_fragment.size()=torch.Size([331840]), master_param_fragment.size()=torch.Size([331840])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([1314816]), new_param_packed_fragment.size()=torch.Size([1314816]), master_param_fragment.size()=torch.Size([1314816])
self._net_total_param_size=160671360, self._total_param_size=160677888, dwu_min_page_size=12288, self._block_size=53559296, self._chunk_size=26779648, self._shard_size=3347456
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
self._net_total_param_size=160671360, self._total_param_size=160677888, dwu_min_page_size=12288, self._block_size=53559296, self._chunk_size=26779648, self._shard_size=3347456
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3007424]), new_param_packed_fragment.size()=torch.Size([3007424]), master_param_fragment.size()=torch.Size([3007424])
model_param_fragment.size()=torch.Size([1121280]), new_param_packed_fragment.size()=torch.Size([1121280]), master_param_fragment.size()=torch.Size([1121280])
[0, 9, 34]
[0, 9, 34]
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([628736]), new_param_packed_fragment.size()=torch.Size([628736]), master_param_fragment.size()=torch.Size([628736])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([3016704]), new_param_packed_fragment.size()=torch.Size([3016704]), master_param_fragment.size()=torch.Size([3016704])
model_param_fragment.size()=torch.Size([2024448]), new_param_packed_fragment.size()=torch.Size([2024448]), master_param_fragment.size()=torch.Size([2024448])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([1024]), new_param_packed_fragment.size()=torch.Size([1024]), master_param_fragment.size()=torch.Size([1024])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([1]), new_param_packed_fragment.size()=torch.Size([1]), master_param_fragment.size()=torch.Size([1])
model_param_fragment.size()=torch.Size([1314816]), new_param_packed_fragment.size()=torch.Size([1314816]), master_param_fragment.size()=torch.Size([1314816])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([1024]), new_param_packed_fragment.size()=torch.Size([1024]), master_param_fragment.size()=torch.Size([1024])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([1048576]), new_param_packed_fragment.size()=torch.Size([1048576]), master_param_fragment.size()=torch.Size([1048576])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([2832448]), new_param_packed_fragment.size()=torch.Size([2832448]), master_param_fragment.size()=torch.Size([2832448])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([322560]), new_param_packed_fragment.size()=torch.Size([322560]), master_param_fragment.size()=torch.Size([322560])
model_param_fragment.size()=torch.Size([1048576]), new_param_packed_fragment.size()=torch.Size([1048576]), master_param_fragment.size()=torch.Size([1048576])
model_param_fragment.size()=torch.Size([515008]), new_param_packed_fragment.size()=torch.Size([515008]), master_param_fragment.size()=torch.Size([515008])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([611264]), new_param_packed_fragment.size()=torch.Size([611264]), master_param_fragment.size()=torch.Size([611264])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([2024448]), new_param_packed_fragment.size()=torch.Size([2024448]), master_param_fragment.size()=torch.Size([2024448])
model_param_fragment.size()=torch.Size([1130560]), new_param_packed_fragment.size()=torch.Size([1130560]), master_param_fragment.size()=torch.Size([1130560])
model_param_fragment.size()=torch.Size([1427456]), new_param_packed_fragment.size()=torch.Size([1427456]), master_param_fragment.size()=torch.Size([1427456])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([1314816]), new_param_packed_fragment.size()=torch.Size([1314816]), master_param_fragment.size()=torch.Size([1314816])
model_param_fragment.size()=torch.Size([1920000]), new_param_packed_fragment.size()=torch.Size([1920000]), master_param_fragment.size()=torch.Size([1920000])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([1929280]), new_param_packed_fragment.size()=torch.Size([1929280]), master_param_fragment.size()=torch.Size([1929280])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
self._net_total_param_size=160671360, self._total_param_size=160677888, dwu_min_page_size=12288, self._block_size=53559296, self._chunk_size=26779648, self._shard_size=3347456
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([2208704]), new_param_packed_fragment.size()=torch.Size([2208704]), master_param_fragment.size()=torch.Size([2208704])
[0, 9, 34]
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([2832448]), new_param_packed_fragment.size()=torch.Size([2832448]), master_param_fragment.size()=torch.Size([2832448])
model_param_fragment.size()=torch.Size([2024448]), new_param_packed_fragment.size()=torch.Size([2024448]), master_param_fragment.size()=torch.Size([2024448])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([515008]), new_param_packed_fragment.size()=torch.Size([515008]), master_param_fragment.size()=torch.Size([515008])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([1409984]), new_param_packed_fragment.size()=torch.Size([1409984]), master_param_fragment.size()=torch.Size([1409984])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([331840]), new_param_packed_fragment.size()=torch.Size([331840]), master_param_fragment.size()=torch.Size([331840])
self._net_total_param_size=160671360, self._total_param_size=160677888, dwu_min_page_size=12288, self._block_size=53559296, self._chunk_size=26779648, self._shard_size=3347456
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
[0, 9, 34]
model_param_fragment.size()=torch.Size([2832448]), new_param_packed_fragment.size()=torch.Size([2832448]), master_param_fragment.size()=torch.Size([2832448])
self._net_total_param_size=160671360, self._total_param_size=160677888, dwu_min_page_size=12288, self._block_size=53559296, self._chunk_size=26779648, self._shard_size=3347456
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
[0, 9, 34]
model_param_fragment.size()=torch.Size([515008]), new_param_packed_fragment.size()=torch.Size([515008]), master_param_fragment.size()=torch.Size([515008])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([3007424]), new_param_packed_fragment.size()=torch.Size([3007424]), master_param_fragment.size()=torch.Size([3007424])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([1314816]), new_param_packed_fragment.size()=torch.Size([1314816]), master_param_fragment.size()=torch.Size([1314816])
self._net_total_param_size=160671360, self._total_param_size=160677888, dwu_min_page_size=12288, self._block_size=53559296, self._chunk_size=26779648, self._shard_size=3347456
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
self._net_total_param_size=160671360, self._total_param_size=160677888, dwu_min_page_size=12288, self._block_size=53559296, self._chunk_size=26779648, self._shard_size=3347456
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
[0, 9, 34]
[0, 9, 34]
model_param_fragment.size()=torch.Size([2024448]), new_param_packed_fragment.size()=torch.Size([2024448]), master_param_fragment.size()=torch.Size([2024448])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
self._net_total_param_size=160671360, self._total_param_size=160677888, dwu_min_page_size=12288, self._block_size=53559296, self._chunk_size=26779648, self._shard_size=3347456
model_param_fragment.size()=torch.Size([2832448]), new_param_packed_fragment.size()=torch.Size([2832448]), master_param_fragment.size()=torch.Size([2832448])
model_param_fragment.size()=torch.Size([515008]), new_param_packed_fragment.size()=torch.Size([515008]), master_param_fragment.size()=torch.Size([515008])
[0, 9, 34]
model_param_fragment.size()=torch.Size([2169856]), new_param_packed_fragment.size()=torch.Size([2169856]), master_param_fragment.size()=torch.Size([2169856])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([1177600]), new_param_packed_fragment.size()=torch.Size([1177600]), master_param_fragment.size()=torch.Size([1177600])
model_param_fragment.size()=torch.Size([572416]), new_param_packed_fragment.size()=torch.Size([572416]), master_param_fragment.size()=torch.Size([572416])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([2766848]), new_param_packed_fragment.size()=torch.Size([2766848]), master_param_fragment.size()=torch.Size([2766848])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([1082432]), new_param_packed_fragment.size()=torch.Size([1082432]), master_param_fragment.size()=torch.Size([1082432])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([2265024]), new_param_packed_fragment.size()=torch.Size([2265024]), master_param_fragment.size()=torch.Size([2265024])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3129344]), new_param_packed_fragment.size()=torch.Size([3129344]), master_param_fragment.size()=torch.Size([3129344])
self._net_total_param_size=160671360, self._total_param_size=160677888, dwu_min_page_size=12288, self._block_size=53559296, self._chunk_size=26779648, self._shard_size=3347456
model_param_fragment.size()=torch.Size([218112]), new_param_packed_fragment.size()=torch.Size([218112]), master_param_fragment.size()=torch.Size([218112])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
[0, 9, 34]
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
self._net_total_param_size=160671360, self._total_param_size=160677888, dwu_min_page_size=12288, self._block_size=53559296, self._chunk_size=26779648, self._shard_size=3347456
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3016704]), new_param_packed_fragment.size()=torch.Size([3016704]), master_param_fragment.size()=torch.Size([3016704])
[0, 9, 34]
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([322560]), new_param_packed_fragment.size()=torch.Size([322560]), master_param_fragment.size()=torch.Size([322560])
self._net_total_param_size=160671360, self._total_param_size=160677888, dwu_min_page_size=12288, self._block_size=53559296, self._chunk_size=26779648, self._shard_size=3347456
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([1427456]), new_param_packed_fragment.size()=torch.Size([1427456]), master_param_fragment.size()=torch.Size([1427456])
[0, 9, 34]
model_param_fragment.size()=torch.Size([3129344]), new_param_packed_fragment.size()=torch.Size([3129344]), master_param_fragment.size()=torch.Size([3129344])
model_param_fragment.size()=torch.Size([1920000]), new_param_packed_fragment.size()=torch.Size([1920000]), master_param_fragment.size()=torch.Size([1920000])
model_param_fragment.size()=torch.Size([218112]), new_param_packed_fragment.size()=torch.Size([218112]), master_param_fragment.size()=torch.Size([218112])
model_param_fragment.size()=torch.Size([1929280]), new_param_packed_fragment.size()=torch.Size([1929280]), master_param_fragment.size()=torch.Size([1929280])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([1409984]), new_param_packed_fragment.size()=torch.Size([1409984]), master_param_fragment.size()=torch.Size([1409984])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([331840]), new_param_packed_fragment.size()=torch.Size([331840]), master_param_fragment.size()=torch.Size([331840])
self._net_total_param_size=160671360, self._total_param_size=160677888, dwu_min_page_size=12288, self._block_size=53559296, self._chunk_size=26779648, self._shard_size=3347456
[0, 9, 34]
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
self._net_total_param_size=160671360, self._total_param_size=160677888, dwu_min_page_size=12288, self._block_size=53559296, self._chunk_size=26779648, self._shard_size=3347456
model_param_fragment.size()=torch.Size([3007424]), new_param_packed_fragment.size()=torch.Size([3007424]), master_param_fragment.size()=torch.Size([3007424])
[0, 9, 34]
self._net_total_param_size=160671360, self._total_param_size=160677888, dwu_min_page_size=12288, self._block_size=53559296, self._chunk_size=26779648, self._shard_size=3347456
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
[0, 9, 34]
model_param_fragment.size()=torch.Size([2968576]), new_param_packed_fragment.size()=torch.Size([2968576]), master_param_fragment.size()=torch.Size([2968576])
model_param_fragment.size()=torch.Size([378880]), new_param_packed_fragment.size()=torch.Size([378880]), master_param_fragment.size()=torch.Size([378880])
model_param_fragment.size()=torch.Size([1371136]), new_param_packed_fragment.size()=torch.Size([1371136]), master_param_fragment.size()=torch.Size([1371136])
model_param_fragment.size()=torch.Size([1976320]), new_param_packed_fragment.size()=torch.Size([1976320]), master_param_fragment.size()=torch.Size([1976320])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([283712]), new_param_packed_fragment.size()=torch.Size([283712]), master_param_fragment.size()=torch.Size([283712])
model_param_fragment.size()=torch.Size([3063744]), new_param_packed_fragment.size()=torch.Size([3063744]), master_param_fragment.size()=torch.Size([3063744])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3016704]), new_param_packed_fragment.size()=torch.Size([3016704]), master_param_fragment.size()=torch.Size([3016704])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([322560]), new_param_packed_fragment.size()=torch.Size([322560]), master_param_fragment.size()=torch.Size([322560])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([1427456]), new_param_packed_fragment.size()=torch.Size([1427456]), master_param_fragment.size()=torch.Size([1427456])
model_param_fragment.size()=torch.Size([1920000]), new_param_packed_fragment.size()=torch.Size([1920000]), master_param_fragment.size()=torch.Size([1920000])
model_param_fragment.size()=torch.Size([1929280]), new_param_packed_fragment.size()=torch.Size([1929280]), master_param_fragment.size()=torch.Size([1929280])
model_param_fragment.size()=torch.Size([2217984]), new_param_packed_fragment.size()=torch.Size([2217984]), master_param_fragment.size()=torch.Size([2217984])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([1121280]), new_param_packed_fragment.size()=torch.Size([1121280]), master_param_fragment.size()=torch.Size([1121280])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([1409984]), new_param_packed_fragment.size()=torch.Size([1409984]), master_param_fragment.size()=torch.Size([1409984])
model_param_fragment.size()=torch.Size([331840]), new_param_packed_fragment.size()=torch.Size([331840]), master_param_fragment.size()=torch.Size([331840])
model_param_fragment.size()=torch.Size([628736]), new_param_packed_fragment.size()=torch.Size([628736]), master_param_fragment.size()=torch.Size([628736])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([3007424]), new_param_packed_fragment.size()=torch.Size([3007424]), master_param_fragment.size()=torch.Size([3007424])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([1024]), new_param_packed_fragment.size()=torch.Size([1024]), master_param_fragment.size()=torch.Size([1024])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([1]), new_param_packed_fragment.size()=torch.Size([1]), master_param_fragment.size()=torch.Size([1])
model_param_fragment.size()=torch.Size([1024]), new_param_packed_fragment.size()=torch.Size([1024]), master_param_fragment.size()=torch.Size([1024])
model_param_fragment.size()=torch.Size([1048576]), new_param_packed_fragment.size()=torch.Size([1048576]), master_param_fragment.size()=torch.Size([1048576])
model_param_fragment.size()=torch.Size([1048576]), new_param_packed_fragment.size()=torch.Size([1048576]), master_param_fragment.size()=torch.Size([1048576])
model_param_fragment.size()=torch.Size([611264]), new_param_packed_fragment.size()=torch.Size([611264]), master_param_fragment.size()=torch.Size([611264])
model_param_fragment.size()=torch.Size([1314816]), new_param_packed_fragment.size()=torch.Size([1314816]), master_param_fragment.size()=torch.Size([1314816])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([1130560]), new_param_packed_fragment.size()=torch.Size([1130560]), master_param_fragment.size()=torch.Size([1130560])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([2024448]), new_param_packed_fragment.size()=torch.Size([2024448]), master_param_fragment.size()=torch.Size([2024448])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([3129344]), new_param_packed_fragment.size()=torch.Size([3129344]), master_param_fragment.size()=torch.Size([3129344])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([2208704]), new_param_packed_fragment.size()=torch.Size([2208704]), master_param_fragment.size()=torch.Size([2208704])
model_param_fragment.size()=torch.Size([218112]), new_param_packed_fragment.size()=torch.Size([218112]), master_param_fragment.size()=torch.Size([218112])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([2832448]), new_param_packed_fragment.size()=torch.Size([2832448]), master_param_fragment.size()=torch.Size([2832448])
model_param_fragment.size()=torch.Size([515008]), new_param_packed_fragment.size()=torch.Size([515008]), master_param_fragment.size()=torch.Size([515008])
self._net_total_param_size=160671360, self._total_param_size=160677888, dwu_min_page_size=12288, self._block_size=53559296, self._chunk_size=26779648, self._shard_size=3347456
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([2217984]), new_param_packed_fragment.size()=torch.Size([2217984]), master_param_fragment.size()=torch.Size([2217984])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
[0, 9, 34]
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([1121280]), new_param_packed_fragment.size()=torch.Size([1121280]), master_param_fragment.size()=torch.Size([1121280])
model_param_fragment.size()=torch.Size([628736]), new_param_packed_fragment.size()=torch.Size([628736]), master_param_fragment.size()=torch.Size([628736])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([1024]), new_param_packed_fragment.size()=torch.Size([1024]), master_param_fragment.size()=torch.Size([1024])
model_param_fragment.size()=torch.Size([2217984]), new_param_packed_fragment.size()=torch.Size([2217984]), master_param_fragment.size()=torch.Size([2217984])
model_param_fragment.size()=torch.Size([1]), new_param_packed_fragment.size()=torch.Size([1]), master_param_fragment.size()=torch.Size([1])
self._net_total_param_size=160671360, self._total_param_size=160677888, dwu_min_page_size=12288, self._block_size=53559296, self._chunk_size=26779648, self._shard_size=3347456
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([1024]), new_param_packed_fragment.size()=torch.Size([1024]), master_param_fragment.size()=torch.Size([1024])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([1048576]), new_param_packed_fragment.size()=torch.Size([1048576]), master_param_fragment.size()=torch.Size([1048576])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([1314816]), new_param_packed_fragment.size()=torch.Size([1314816]), master_param_fragment.size()=torch.Size([1314816])
[0, 9, 34]
model_param_fragment.size()=torch.Size([1048576]), new_param_packed_fragment.size()=torch.Size([1048576]), master_param_fragment.size()=torch.Size([1048576])
model_param_fragment.size()=torch.Size([1121280]), new_param_packed_fragment.size()=torch.Size([1121280]), master_param_fragment.size()=torch.Size([1121280])
model_param_fragment.size()=torch.Size([611264]), new_param_packed_fragment.size()=torch.Size([611264]), master_param_fragment.size()=torch.Size([611264])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([628736]), new_param_packed_fragment.size()=torch.Size([628736]), master_param_fragment.size()=torch.Size([628736])
model_param_fragment.size()=torch.Size([1130560]), new_param_packed_fragment.size()=torch.Size([1130560]), master_param_fragment.size()=torch.Size([1130560])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([467968]), new_param_packed_fragment.size()=torch.Size([467968]), master_param_fragment.size()=torch.Size([467968])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([2879488]), new_param_packed_fragment.size()=torch.Size([2879488]), master_param_fragment.size()=torch.Size([2879488])
model_param_fragment.size()=torch.Size([2024448]), new_param_packed_fragment.size()=torch.Size([2024448]), master_param_fragment.size()=torch.Size([2024448])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([1024]), new_param_packed_fragment.size()=torch.Size([1024]), master_param_fragment.size()=torch.Size([1024])
model_param_fragment.size()=torch.Size([1]), new_param_packed_fragment.size()=torch.Size([1]), master_param_fragment.size()=torch.Size([1])
model_param_fragment.size()=torch.Size([3073024]), new_param_packed_fragment.size()=torch.Size([3073024]), master_param_fragment.size()=torch.Size([3073024])
model_param_fragment.size()=torch.Size([2208704]), new_param_packed_fragment.size()=torch.Size([2208704]), master_param_fragment.size()=torch.Size([2208704])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([1024]), new_param_packed_fragment.size()=torch.Size([1024]), master_param_fragment.size()=torch.Size([1024])
model_param_fragment.size()=torch.Size([3016704]), new_param_packed_fragment.size()=torch.Size([3016704]), master_param_fragment.size()=torch.Size([3016704])
model_param_fragment.size()=torch.Size([274432]), new_param_packed_fragment.size()=torch.Size([274432]), master_param_fragment.size()=torch.Size([274432])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([1048576]), new_param_packed_fragment.size()=torch.Size([1048576]), master_param_fragment.size()=torch.Size([1048576])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([1048576]), new_param_packed_fragment.size()=torch.Size([1048576]), master_param_fragment.size()=torch.Size([1048576])
model_param_fragment.size()=torch.Size([2169856]), new_param_packed_fragment.size()=torch.Size([2169856]), master_param_fragment.size()=torch.Size([2169856])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([2832448]), new_param_packed_fragment.size()=torch.Size([2832448]), master_param_fragment.size()=torch.Size([2832448])
model_param_fragment.size()=torch.Size([515008]), new_param_packed_fragment.size()=torch.Size([515008]), master_param_fragment.size()=torch.Size([515008])
model_param_fragment.size()=torch.Size([611264]), new_param_packed_fragment.size()=torch.Size([611264]), master_param_fragment.size()=torch.Size([611264])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([322560]), new_param_packed_fragment.size()=torch.Size([322560]), master_param_fragment.size()=torch.Size([322560])
model_param_fragment.size()=torch.Size([1130560]), new_param_packed_fragment.size()=torch.Size([1130560]), master_param_fragment.size()=torch.Size([1130560])
model_param_fragment.size()=torch.Size([1427456]), new_param_packed_fragment.size()=torch.Size([1427456]), master_param_fragment.size()=torch.Size([1427456])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([1177600]), new_param_packed_fragment.size()=torch.Size([1177600]), master_param_fragment.size()=torch.Size([1177600])
NCCL version 2.7.5+cuda11.0
model_param_fragment.size()=torch.Size([1920000]), new_param_packed_fragment.size()=torch.Size([1920000]), master_param_fragment.size()=torch.Size([1920000])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([572416]), new_param_packed_fragment.size()=torch.Size([572416]), master_param_fragment.size()=torch.Size([572416])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([1929280]), new_param_packed_fragment.size()=torch.Size([1929280]), master_param_fragment.size()=torch.Size([1929280])
model_param_fragment.size()=torch.Size([2208704]), new_param_packed_fragment.size()=torch.Size([2208704]), master_param_fragment.size()=torch.Size([2208704])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([1409984]), new_param_packed_fragment.size()=torch.Size([1409984]), master_param_fragment.size()=torch.Size([1409984])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
self._net_total_param_size=160671360, self._total_param_size=160677888, dwu_min_page_size=12288, self._block_size=53559296, self._chunk_size=26779648, self._shard_size=3347456
model_param_fragment.size()=torch.Size([2766848]), new_param_packed_fragment.size()=torch.Size([2766848]), master_param_fragment.size()=torch.Size([2766848])
model_param_fragment.size()=torch.Size([331840]), new_param_packed_fragment.size()=torch.Size([331840]), master_param_fragment.size()=torch.Size([331840])
[0, 9, 34]
model_param_fragment.size()=torch.Size([1082432]), new_param_packed_fragment.size()=torch.Size([1082432]), master_param_fragment.size()=torch.Size([1082432])
model_param_fragment.size()=torch.Size([2265024]), new_param_packed_fragment.size()=torch.Size([2265024]), master_param_fragment.size()=torch.Size([2265024])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
self._net_total_param_size=160671360, self._total_param_size=160677888, dwu_min_page_size=12288, self._block_size=53559296, self._chunk_size=26779648, self._shard_size=3347456
model_param_fragment.size()=torch.Size([3007424]), new_param_packed_fragment.size()=torch.Size([3007424]), master_param_fragment.size()=torch.Size([3007424])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
[0, 9, 34]
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([467968]), new_param_packed_fragment.size()=torch.Size([467968]), master_param_fragment.size()=torch.Size([467968])
self._net_total_param_size=160671360, self._total_param_size=160677888, dwu_min_page_size=12288, self._block_size=53559296, self._chunk_size=26779648, self._shard_size=3347456
model_param_fragment.size()=torch.Size([2879488]), new_param_packed_fragment.size()=torch.Size([2879488]), master_param_fragment.size()=torch.Size([2879488])
self._net_total_param_size=160671360, self._total_param_size=160677888, dwu_min_page_size=12288, self._block_size=53559296, self._chunk_size=26779648, self._shard_size=3347456
[0, 9, 34]
model_param_fragment.size()=torch.Size([3073024]), new_param_packed_fragment.size()=torch.Size([3073024]), master_param_fragment.size()=torch.Size([3073024])
[0, 9, 34]
model_param_fragment.size()=torch.Size([274432]), new_param_packed_fragment.size()=torch.Size([274432]), master_param_fragment.size()=torch.Size([274432])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
self._net_total_param_size=160671360, self._total_param_size=160677888, dwu_min_page_size=12288, self._block_size=53559296, self._chunk_size=26779648, self._shard_size=3347456
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
[0, 9, 34]
model_param_fragment.size()=torch.Size([3016704]), new_param_packed_fragment.size()=torch.Size([3016704]), master_param_fragment.size()=torch.Size([3016704])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([322560]), new_param_packed_fragment.size()=torch.Size([322560]), master_param_fragment.size()=torch.Size([322560])
model_param_fragment.size()=torch.Size([1427456]), new_param_packed_fragment.size()=torch.Size([1427456]), master_param_fragment.size()=torch.Size([1427456])
model_param_fragment.size()=torch.Size([1920000]), new_param_packed_fragment.size()=torch.Size([1920000]), master_param_fragment.size()=torch.Size([1920000])
self._net_total_param_size=160671360, self._total_param_size=160677888, dwu_min_page_size=12288, self._block_size=53559296, self._chunk_size=26779648, self._shard_size=3347456
[0, 9, 34]
model_param_fragment.size()=torch.Size([1929280]), new_param_packed_fragment.size()=torch.Size([1929280]), master_param_fragment.size()=torch.Size([1929280])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([1409984]), new_param_packed_fragment.size()=torch.Size([1409984]), master_param_fragment.size()=torch.Size([1409984])
model_param_fragment.size()=torch.Size([331840]), new_param_packed_fragment.size()=torch.Size([331840]), master_param_fragment.size()=torch.Size([331840])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([3007424]), new_param_packed_fragment.size()=torch.Size([3007424]), master_param_fragment.size()=torch.Size([3007424])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([2217984]), new_param_packed_fragment.size()=torch.Size([2217984]), master_param_fragment.size()=torch.Size([2217984])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([1121280]), new_param_packed_fragment.size()=torch.Size([1121280]), master_param_fragment.size()=torch.Size([1121280])
self._net_total_param_size=160671360, self._total_param_size=160677888, dwu_min_page_size=12288, self._block_size=53559296, self._chunk_size=26779648, self._shard_size=3347456
model_param_fragment.size()=torch.Size([628736]), new_param_packed_fragment.size()=torch.Size([628736]), master_param_fragment.size()=torch.Size([628736])
model_param_fragment.size()=torch.Size([2968576]), new_param_packed_fragment.size()=torch.Size([2968576]), master_param_fragment.size()=torch.Size([2968576])
model_param_fragment.size()=torch.Size([3016704]), new_param_packed_fragment.size()=torch.Size([3016704]), master_param_fragment.size()=torch.Size([3016704])
[0, 9, 34]
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([378880]), new_param_packed_fragment.size()=torch.Size([378880]), master_param_fragment.size()=torch.Size([378880])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([1371136]), new_param_packed_fragment.size()=torch.Size([1371136]), master_param_fragment.size()=torch.Size([1371136])
model_param_fragment.size()=torch.Size([1024]), new_param_packed_fragment.size()=torch.Size([1024]), master_param_fragment.size()=torch.Size([1024])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
self._net_total_param_size=160671360, self._total_param_size=160677888, dwu_min_page_size=12288, self._block_size=53559296, self._chunk_size=26779648, self._shard_size=3347456
model_param_fragment.size()=torch.Size([1976320]), new_param_packed_fragment.size()=torch.Size([1976320]), master_param_fragment.size()=torch.Size([1976320])
model_param_fragment.size()=torch.Size([1]), new_param_packed_fragment.size()=torch.Size([1]), master_param_fragment.size()=torch.Size([1])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([322560]), new_param_packed_fragment.size()=torch.Size([322560]), master_param_fragment.size()=torch.Size([322560])
model_param_fragment.size()=torch.Size([1024]), new_param_packed_fragment.size()=torch.Size([1024]), master_param_fragment.size()=torch.Size([1024])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
[0, 9, 34]
model_param_fragment.size()=torch.Size([1427456]), new_param_packed_fragment.size()=torch.Size([1427456]), master_param_fragment.size()=torch.Size([1427456])
model_param_fragment.size()=torch.Size([1048576]), new_param_packed_fragment.size()=torch.Size([1048576]), master_param_fragment.size()=torch.Size([1048576])
model_param_fragment.size()=torch.Size([1048576]), new_param_packed_fragment.size()=torch.Size([1048576]), master_param_fragment.size()=torch.Size([1048576])
model_param_fragment.size()=torch.Size([283712]), new_param_packed_fragment.size()=torch.Size([283712]), master_param_fragment.size()=torch.Size([283712])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([611264]), new_param_packed_fragment.size()=torch.Size([611264]), master_param_fragment.size()=torch.Size([611264])
model_param_fragment.size()=torch.Size([3063744]), new_param_packed_fragment.size()=torch.Size([3063744]), master_param_fragment.size()=torch.Size([3063744])
model_param_fragment.size()=torch.Size([1920000]), new_param_packed_fragment.size()=torch.Size([1920000]), master_param_fragment.size()=torch.Size([1920000])
model_param_fragment.size()=torch.Size([1130560]), new_param_packed_fragment.size()=torch.Size([1130560]), master_param_fragment.size()=torch.Size([1130560])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([1929280]), new_param_packed_fragment.size()=torch.Size([1929280]), master_param_fragment.size()=torch.Size([1929280])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([2169856]), new_param_packed_fragment.size()=torch.Size([2169856]), master_param_fragment.size()=torch.Size([2169856])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([3129344]), new_param_packed_fragment.size()=torch.Size([3129344]), master_param_fragment.size()=torch.Size([3129344])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([1177600]), new_param_packed_fragment.size()=torch.Size([1177600]), master_param_fragment.size()=torch.Size([1177600])
model_param_fragment.size()=torch.Size([2208704]), new_param_packed_fragment.size()=torch.Size([2208704]), master_param_fragment.size()=torch.Size([2208704])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([218112]), new_param_packed_fragment.size()=torch.Size([218112]), master_param_fragment.size()=torch.Size([218112])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([572416]), new_param_packed_fragment.size()=torch.Size([572416]), master_param_fragment.size()=torch.Size([572416])
model_param_fragment.size()=torch.Size([1409984]), new_param_packed_fragment.size()=torch.Size([1409984]), master_param_fragment.size()=torch.Size([1409984])
model_param_fragment.size()=torch.Size([331840]), new_param_packed_fragment.size()=torch.Size([331840]), master_param_fragment.size()=torch.Size([331840])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([2766848]), new_param_packed_fragment.size()=torch.Size([2766848]), master_param_fragment.size()=torch.Size([2766848])
model_param_fragment.size()=torch.Size([1314816]), new_param_packed_fragment.size()=torch.Size([1314816]), master_param_fragment.size()=torch.Size([1314816])
model_param_fragment.size()=torch.Size([3007424]), new_param_packed_fragment.size()=torch.Size([3007424]), master_param_fragment.size()=torch.Size([3007424])
self._net_total_param_size=160671360, self._total_param_size=160677888, dwu_min_page_size=12288, self._block_size=53559296, self._chunk_size=26779648, self._shard_size=3347456
self._net_total_param_size=160671360, self._total_param_size=160677888, dwu_min_page_size=12288, self._block_size=53559296, self._chunk_size=26779648, self._shard_size=3347456
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([1082432]), new_param_packed_fragment.size()=torch.Size([1082432]), master_param_fragment.size()=torch.Size([1082432])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([2265024]), new_param_packed_fragment.size()=torch.Size([2265024]), master_param_fragment.size()=torch.Size([2265024])
[0, 9, 34]
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([2024448]), new_param_packed_fragment.size()=torch.Size([2024448]), master_param_fragment.size()=torch.Size([2024448])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
[0, 9, 34]
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3016704]), new_param_packed_fragment.size()=torch.Size([3016704]), master_param_fragment.size()=torch.Size([3016704])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([2832448]), new_param_packed_fragment.size()=torch.Size([2832448]), master_param_fragment.size()=torch.Size([2832448])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([515008]), new_param_packed_fragment.size()=torch.Size([515008]), master_param_fragment.size()=torch.Size([515008])
model_param_fragment.size()=torch.Size([322560]), new_param_packed_fragment.size()=torch.Size([322560]), master_param_fragment.size()=torch.Size([322560])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3129344]), new_param_packed_fragment.size()=torch.Size([3129344]), master_param_fragment.size()=torch.Size([3129344])
model_param_fragment.size()=torch.Size([1427456]), new_param_packed_fragment.size()=torch.Size([1427456]), master_param_fragment.size()=torch.Size([1427456])
model_param_fragment.size()=torch.Size([218112]), new_param_packed_fragment.size()=torch.Size([218112]), master_param_fragment.size()=torch.Size([218112])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
self._net_total_param_size=160671360, self._total_param_size=160677888, dwu_min_page_size=12288, self._block_size=53559296, self._chunk_size=26779648, self._shard_size=3347456
model_param_fragment.size()=torch.Size([1920000]), new_param_packed_fragment.size()=torch.Size([1920000]), master_param_fragment.size()=torch.Size([1920000])
[0, 9, 34]
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([1929280]), new_param_packed_fragment.size()=torch.Size([1929280]), master_param_fragment.size()=torch.Size([1929280])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([2169856]), new_param_packed_fragment.size()=torch.Size([2169856]), master_param_fragment.size()=torch.Size([2169856])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([1409984]), new_param_packed_fragment.size()=torch.Size([1409984]), master_param_fragment.size()=torch.Size([1409984])
model_param_fragment.size()=torch.Size([331840]), new_param_packed_fragment.size()=torch.Size([331840]), master_param_fragment.size()=torch.Size([331840])
model_param_fragment.size()=torch.Size([1177600]), new_param_packed_fragment.size()=torch.Size([1177600]), master_param_fragment.size()=torch.Size([1177600])
model_param_fragment.size()=torch.Size([3129344]), new_param_packed_fragment.size()=torch.Size([3129344]), master_param_fragment.size()=torch.Size([3129344])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([218112]), new_param_packed_fragment.size()=torch.Size([218112]), master_param_fragment.size()=torch.Size([218112])
model_param_fragment.size()=torch.Size([572416]), new_param_packed_fragment.size()=torch.Size([572416]), master_param_fragment.size()=torch.Size([572416])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([3007424]), new_param_packed_fragment.size()=torch.Size([3007424]), master_param_fragment.size()=torch.Size([3007424])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
self._net_total_param_size=160671360, self._total_param_size=160677888, dwu_min_page_size=12288, self._block_size=53559296, self._chunk_size=26779648, self._shard_size=3347456
self._net_total_param_size=160671360, self._total_param_size=160677888, dwu_min_page_size=12288, self._block_size=53559296, self._chunk_size=26779648, self._shard_size=3347456
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
[0, 9, 34]
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
[0, 9, 34]
model_param_fragment.size()=torch.Size([2766848]), new_param_packed_fragment.size()=torch.Size([2766848]), master_param_fragment.size()=torch.Size([2766848])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([467968]), new_param_packed_fragment.size()=torch.Size([467968]), master_param_fragment.size()=torch.Size([467968])
model_param_fragment.size()=torch.Size([1082432]), new_param_packed_fragment.size()=torch.Size([1082432]), master_param_fragment.size()=torch.Size([1082432])
model_param_fragment.size()=torch.Size([2274304]), new_param_packed_fragment.size()=torch.Size([2274304]), master_param_fragment.size()=torch.Size([2274304])
model_param_fragment.size()=torch.Size([2169856]), new_param_packed_fragment.size()=torch.Size([2169856]), master_param_fragment.size()=torch.Size([2169856])
model_param_fragment.size()=torch.Size([2879488]), new_param_packed_fragment.size()=torch.Size([2879488]), master_param_fragment.size()=torch.Size([2879488])
model_param_fragment.size()=torch.Size([2265024]), new_param_packed_fragment.size()=torch.Size([2265024]), master_param_fragment.size()=torch.Size([2265024])
model_param_fragment.size()=torch.Size([467968]), new_param_packed_fragment.size()=torch.Size([467968]), master_param_fragment.size()=torch.Size([467968])
model_param_fragment.size()=torch.Size([2274304]), new_param_packed_fragment.size()=torch.Size([2274304]), master_param_fragment.size()=torch.Size([2274304])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([1177600]), new_param_packed_fragment.size()=torch.Size([1177600]), master_param_fragment.size()=torch.Size([1177600])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([3073024]), new_param_packed_fragment.size()=torch.Size([3073024]), master_param_fragment.size()=torch.Size([3073024])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([2879488]), new_param_packed_fragment.size()=torch.Size([2879488]), master_param_fragment.size()=torch.Size([2879488])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([2968576]), new_param_packed_fragment.size()=torch.Size([2968576]), master_param_fragment.size()=torch.Size([2968576])
model_param_fragment.size()=torch.Size([274432]), new_param_packed_fragment.size()=torch.Size([274432]), master_param_fragment.size()=torch.Size([274432])
model_param_fragment.size()=torch.Size([572416]), new_param_packed_fragment.size()=torch.Size([572416]), master_param_fragment.size()=torch.Size([572416])
model_param_fragment.size()=torch.Size([3073024]), new_param_packed_fragment.size()=torch.Size([3073024]), master_param_fragment.size()=torch.Size([3073024])
model_param_fragment.size()=torch.Size([1064960]), new_param_packed_fragment.size()=torch.Size([1064960]), master_param_fragment.size()=torch.Size([1064960])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([378880]), new_param_packed_fragment.size()=torch.Size([378880]), master_param_fragment.size()=torch.Size([378880])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([274432]), new_param_packed_fragment.size()=torch.Size([274432]), master_param_fragment.size()=torch.Size([274432])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([1064960]), new_param_packed_fragment.size()=torch.Size([1064960]), master_param_fragment.size()=torch.Size([1064960])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([1371136]), new_param_packed_fragment.size()=torch.Size([1371136]), master_param_fragment.size()=torch.Size([1371136])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([2766848]), new_param_packed_fragment.size()=torch.Size([2766848]), master_param_fragment.size()=torch.Size([2766848])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([1976320]), new_param_packed_fragment.size()=torch.Size([1976320]), master_param_fragment.size()=torch.Size([1976320])
model_param_fragment.size()=torch.Size([2217984]), new_param_packed_fragment.size()=torch.Size([2217984]), master_param_fragment.size()=torch.Size([2217984])
model_param_fragment.size()=torch.Size([3308608]), new_param_packed_fragment.size()=torch.Size([3308608]), master_param_fragment.size()=torch.Size([3308608])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([1082432]), new_param_packed_fragment.size()=torch.Size([1082432]), master_param_fragment.size()=torch.Size([1082432])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([32320]), new_param_packed_fragment.size()=torch.Size([32320]), master_param_fragment.size()=torch.Size([32320])
model_param_fragment.size()=torch.Size([3308608]), new_param_packed_fragment.size()=torch.Size([3308608]), master_param_fragment.size()=torch.Size([3308608])
model_param_fragment.size()=torch.Size([2265024]), new_param_packed_fragment.size()=torch.Size([2265024]), master_param_fragment.size()=torch.Size([2265024])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([283712]), new_param_packed_fragment.size()=torch.Size([283712]), master_param_fragment.size()=torch.Size([283712])
model_param_fragment.size()=torch.Size([1121280]), new_param_packed_fragment.size()=torch.Size([1121280]), master_param_fragment.size()=torch.Size([1121280])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([32320]), new_param_packed_fragment.size()=torch.Size([32320]), master_param_fragment.size()=torch.Size([32320])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3063744]), new_param_packed_fragment.size()=torch.Size([3063744]), master_param_fragment.size()=torch.Size([3063744])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([628736]), new_param_packed_fragment.size()=torch.Size([628736]), master_param_fragment.size()=torch.Size([628736])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([1024]), new_param_packed_fragment.size()=torch.Size([1024]), master_param_fragment.size()=torch.Size([1024])
model_param_fragment.size()=torch.Size([1]), new_param_packed_fragment.size()=torch.Size([1]), master_param_fragment.size()=torch.Size([1])
model_param_fragment.size()=torch.Size([1024]), new_param_packed_fragment.size()=torch.Size([1024]), master_param_fragment.size()=torch.Size([1024])
model_param_fragment.size()=torch.Size([1048576]), new_param_packed_fragment.size()=torch.Size([1048576]), master_param_fragment.size()=torch.Size([1048576])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([1048576]), new_param_packed_fragment.size()=torch.Size([1048576]), master_param_fragment.size()=torch.Size([1048576])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([611264]), new_param_packed_fragment.size()=torch.Size([611264]), master_param_fragment.size()=torch.Size([611264])
model_param_fragment.size()=torch.Size([2274304]), new_param_packed_fragment.size()=torch.Size([2274304]), master_param_fragment.size()=torch.Size([2274304])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([1130560]), new_param_packed_fragment.size()=torch.Size([1130560]), master_param_fragment.size()=torch.Size([1130560])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([1064960]), new_param_packed_fragment.size()=torch.Size([1064960]), master_param_fragment.size()=torch.Size([1064960])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([2208704]), new_param_packed_fragment.size()=torch.Size([2208704]), master_param_fragment.size()=torch.Size([2208704])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
self._net_total_param_size=160671360, self._total_param_size=160677888, dwu_min_page_size=12288, self._block_size=53559296, self._chunk_size=26779648, self._shard_size=3347456
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
[0, 9, 34]
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3308608]), new_param_packed_fragment.size()=torch.Size([3308608]), master_param_fragment.size()=torch.Size([3308608])
model_param_fragment.size()=torch.Size([32320]), new_param_packed_fragment.size()=torch.Size([32320]), master_param_fragment.size()=torch.Size([32320])
model_param_fragment.size()=torch.Size([2217984]), new_param_packed_fragment.size()=torch.Size([2217984]), master_param_fragment.size()=torch.Size([2217984])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([3016704]), new_param_packed_fragment.size()=torch.Size([3016704]), master_param_fragment.size()=torch.Size([3016704])
model_param_fragment.size()=torch.Size([1121280]), new_param_packed_fragment.size()=torch.Size([1121280]), master_param_fragment.size()=torch.Size([1121280])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([628736]), new_param_packed_fragment.size()=torch.Size([628736]), master_param_fragment.size()=torch.Size([628736])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([322560]), new_param_packed_fragment.size()=torch.Size([322560]), master_param_fragment.size()=torch.Size([322560])
model_param_fragment.size()=torch.Size([1427456]), new_param_packed_fragment.size()=torch.Size([1427456]), master_param_fragment.size()=torch.Size([1427456])
model_param_fragment.size()=torch.Size([1920000]), new_param_packed_fragment.size()=torch.Size([1920000]), master_param_fragment.size()=torch.Size([1920000])
model_param_fragment.size()=torch.Size([1024]), new_param_packed_fragment.size()=torch.Size([1024]), master_param_fragment.size()=torch.Size([1024])
model_param_fragment.size()=torch.Size([1]), new_param_packed_fragment.size()=torch.Size([1]), master_param_fragment.size()=torch.Size([1])
model_param_fragment.size()=torch.Size([1929280]), new_param_packed_fragment.size()=torch.Size([1929280]), master_param_fragment.size()=torch.Size([1929280])
model_param_fragment.size()=torch.Size([1024]), new_param_packed_fragment.size()=torch.Size([1024]), master_param_fragment.size()=torch.Size([1024])
self._net_total_param_size=160671360, self._total_param_size=160677888, dwu_min_page_size=12288, self._block_size=53559296, self._chunk_size=26779648, self._shard_size=3347456
model_param_fragment.size()=torch.Size([1048576]), new_param_packed_fragment.size()=torch.Size([1048576]), master_param_fragment.size()=torch.Size([1048576])
model_param_fragment.size()=torch.Size([1048576]), new_param_packed_fragment.size()=torch.Size([1048576]), master_param_fragment.size()=torch.Size([1048576])
[0, 9, 34]
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([611264]), new_param_packed_fragment.size()=torch.Size([611264]), master_param_fragment.size()=torch.Size([611264])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([1130560]), new_param_packed_fragment.size()=torch.Size([1130560]), master_param_fragment.size()=torch.Size([1130560])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([1409984]), new_param_packed_fragment.size()=torch.Size([1409984]), master_param_fragment.size()=torch.Size([1409984])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([331840]), new_param_packed_fragment.size()=torch.Size([331840]), master_param_fragment.size()=torch.Size([331840])
model_param_fragment.size()=torch.Size([2208704]), new_param_packed_fragment.size()=torch.Size([2208704]), master_param_fragment.size()=torch.Size([2208704])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
self._net_total_param_size=160671360, self._total_param_size=160677888, dwu_min_page_size=12288, self._block_size=53559296, self._chunk_size=26779648, self._shard_size=3347456
model_param_fragment.size()=torch.Size([3007424]), new_param_packed_fragment.size()=torch.Size([3007424]), master_param_fragment.size()=torch.Size([3007424])
[0, 9, 34]
model_param_fragment.size()=torch.Size([2968576]), new_param_packed_fragment.size()=torch.Size([2968576]), master_param_fragment.size()=torch.Size([2968576])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([378880]), new_param_packed_fragment.size()=torch.Size([378880]), master_param_fragment.size()=torch.Size([378880])
model_param_fragment.size()=torch.Size([1371136]), new_param_packed_fragment.size()=torch.Size([1371136]), master_param_fragment.size()=torch.Size([1371136])
model_param_fragment.size()=torch.Size([2968576]), new_param_packed_fragment.size()=torch.Size([2968576]), master_param_fragment.size()=torch.Size([2968576])
model_param_fragment.size()=torch.Size([1976320]), new_param_packed_fragment.size()=torch.Size([1976320]), master_param_fragment.size()=torch.Size([1976320])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([378880]), new_param_packed_fragment.size()=torch.Size([378880]), master_param_fragment.size()=torch.Size([378880])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([2274304]), new_param_packed_fragment.size()=torch.Size([2274304]), master_param_fragment.size()=torch.Size([2274304])
NCCL version 2.7.5+cuda11.0
model_param_fragment.size()=torch.Size([1371136]), new_param_packed_fragment.size()=torch.Size([1371136]), master_param_fragment.size()=torch.Size([1371136])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([1976320]), new_param_packed_fragment.size()=torch.Size([1976320]), master_param_fragment.size()=torch.Size([1976320])
model_param_fragment.size()=torch.Size([283712]), new_param_packed_fragment.size()=torch.Size([283712]), master_param_fragment.size()=torch.Size([283712])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([3063744]), new_param_packed_fragment.size()=torch.Size([3063744]), master_param_fragment.size()=torch.Size([3063744])
model_param_fragment.size()=torch.Size([1064960]), new_param_packed_fragment.size()=torch.Size([1064960]), master_param_fragment.size()=torch.Size([1064960])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([283712]), new_param_packed_fragment.size()=torch.Size([283712]), master_param_fragment.size()=torch.Size([283712])
model_param_fragment.size()=torch.Size([3063744]), new_param_packed_fragment.size()=torch.Size([3063744]), master_param_fragment.size()=torch.Size([3063744])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([1314816]), new_param_packed_fragment.size()=torch.Size([1314816]), master_param_fragment.size()=torch.Size([1314816])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3308608]), new_param_packed_fragment.size()=torch.Size([3308608]), master_param_fragment.size()=torch.Size([3308608])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([32320]), new_param_packed_fragment.size()=torch.Size([32320]), master_param_fragment.size()=torch.Size([32320])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([2024448]), new_param_packed_fragment.size()=torch.Size([2024448]), master_param_fragment.size()=torch.Size([2024448])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
self._net_total_param_size=160671360, self._total_param_size=160677888, dwu_min_page_size=12288, self._block_size=53559296, self._chunk_size=26779648, self._shard_size=3347456
model_param_fragment.size()=torch.Size([2169856]), new_param_packed_fragment.size()=torch.Size([2169856]), master_param_fragment.size()=torch.Size([2169856])
[0, 9, 34]
model_param_fragment.size()=torch.Size([2832448]), new_param_packed_fragment.size()=torch.Size([2832448]), master_param_fragment.size()=torch.Size([2832448])
model_param_fragment.size()=torch.Size([1177600]), new_param_packed_fragment.size()=torch.Size([1177600]), master_param_fragment.size()=torch.Size([1177600])
model_param_fragment.size()=torch.Size([515008]), new_param_packed_fragment.size()=torch.Size([515008]), master_param_fragment.size()=torch.Size([515008])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([572416]), new_param_packed_fragment.size()=torch.Size([572416]), master_param_fragment.size()=torch.Size([572416])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([2968576]), new_param_packed_fragment.size()=torch.Size([2968576]), master_param_fragment.size()=torch.Size([2968576])
self._net_total_param_size=160671360, self._total_param_size=160677888, dwu_min_page_size=12288, self._block_size=53559296, self._chunk_size=26779648, self._shard_size=3347456
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
NCCL version 2.7.5+cuda11.0
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([378880]), new_param_packed_fragment.size()=torch.Size([378880]), master_param_fragment.size()=torch.Size([378880])
[0, 9, 34]
model_param_fragment.size()=torch.Size([2766848]), new_param_packed_fragment.size()=torch.Size([2766848]), master_param_fragment.size()=torch.Size([2766848])
model_param_fragment.size()=torch.Size([1371136]), new_param_packed_fragment.size()=torch.Size([1371136]), master_param_fragment.size()=torch.Size([1371136])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([2169856]), new_param_packed_fragment.size()=torch.Size([2169856]), master_param_fragment.size()=torch.Size([2169856])
model_param_fragment.size()=torch.Size([1082432]), new_param_packed_fragment.size()=torch.Size([1082432]), master_param_fragment.size()=torch.Size([1082432])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([1976320]), new_param_packed_fragment.size()=torch.Size([1976320]), master_param_fragment.size()=torch.Size([1976320])
model_param_fragment.size()=torch.Size([2265024]), new_param_packed_fragment.size()=torch.Size([2265024]), master_param_fragment.size()=torch.Size([2265024])
model_param_fragment.size()=torch.Size([1177600]), new_param_packed_fragment.size()=torch.Size([1177600]), master_param_fragment.size()=torch.Size([1177600])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
self._net_total_param_size=160671360, self._total_param_size=160677888, dwu_min_page_size=12288, self._block_size=53559296, self._chunk_size=26779648, self._shard_size=3347456
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3129344]), new_param_packed_fragment.size()=torch.Size([3129344]), master_param_fragment.size()=torch.Size([3129344])
model_param_fragment.size()=torch.Size([467968]), new_param_packed_fragment.size()=torch.Size([467968]), master_param_fragment.size()=torch.Size([467968])
model_param_fragment.size()=torch.Size([572416]), new_param_packed_fragment.size()=torch.Size([572416]), master_param_fragment.size()=torch.Size([572416])
model_param_fragment.size()=torch.Size([2879488]), new_param_packed_fragment.size()=torch.Size([2879488]), master_param_fragment.size()=torch.Size([2879488])
model_param_fragment.size()=torch.Size([218112]), new_param_packed_fragment.size()=torch.Size([218112]), master_param_fragment.size()=torch.Size([218112])
model_param_fragment.size()=torch.Size([467968]), new_param_packed_fragment.size()=torch.Size([467968]), master_param_fragment.size()=torch.Size([467968])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([283712]), new_param_packed_fragment.size()=torch.Size([283712]), master_param_fragment.size()=torch.Size([283712])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3063744]), new_param_packed_fragment.size()=torch.Size([3063744]), master_param_fragment.size()=torch.Size([3063744])
model_param_fragment.size()=torch.Size([2879488]), new_param_packed_fragment.size()=torch.Size([2879488]), master_param_fragment.size()=torch.Size([2879488])
model_param_fragment.size()=torch.Size([3073024]), new_param_packed_fragment.size()=torch.Size([3073024]), master_param_fragment.size()=torch.Size([3073024])
[0, 9, 34]
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([274432]), new_param_packed_fragment.size()=torch.Size([274432]), master_param_fragment.size()=torch.Size([274432])
model_param_fragment.size()=torch.Size([3073024]), new_param_packed_fragment.size()=torch.Size([3073024]), master_param_fragment.size()=torch.Size([3073024])
model_param_fragment.size()=torch.Size([2766848]), new_param_packed_fragment.size()=torch.Size([2766848]), master_param_fragment.size()=torch.Size([2766848])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([274432]), new_param_packed_fragment.size()=torch.Size([274432]), master_param_fragment.size()=torch.Size([274432])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([1082432]), new_param_packed_fragment.size()=torch.Size([1082432]), master_param_fragment.size()=torch.Size([1082432])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([2265024]), new_param_packed_fragment.size()=torch.Size([2265024]), master_param_fragment.size()=torch.Size([2265024])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
self._net_total_param_size=160671360, self._total_param_size=160677888, dwu_min_page_size=12288, self._block_size=53559296, self._chunk_size=26779648, self._shard_size=3347456
model_param_fragment.size()=torch.Size([2274304]), new_param_packed_fragment.size()=torch.Size([2274304]), master_param_fragment.size()=torch.Size([2274304])
[0, 9, 34]
model_param_fragment.size()=torch.Size([3129344]), new_param_packed_fragment.size()=torch.Size([3129344]), master_param_fragment.size()=torch.Size([3129344])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([2274304]), new_param_packed_fragment.size()=torch.Size([2274304]), master_param_fragment.size()=torch.Size([2274304])
model_param_fragment.size()=torch.Size([218112]), new_param_packed_fragment.size()=torch.Size([218112]), master_param_fragment.size()=torch.Size([218112])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([1064960]), new_param_packed_fragment.size()=torch.Size([1064960]), master_param_fragment.size()=torch.Size([1064960])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([1064960]), new_param_packed_fragment.size()=torch.Size([1064960]), master_param_fragment.size()=torch.Size([1064960])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([2169856]), new_param_packed_fragment.size()=torch.Size([2169856]), master_param_fragment.size()=torch.Size([2169856])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3308608]), new_param_packed_fragment.size()=torch.Size([3308608]), master_param_fragment.size()=torch.Size([3308608])
model_param_fragment.size()=torch.Size([1177600]), new_param_packed_fragment.size()=torch.Size([1177600]), master_param_fragment.size()=torch.Size([1177600])
model_param_fragment.size()=torch.Size([32320]), new_param_packed_fragment.size()=torch.Size([32320]), master_param_fragment.size()=torch.Size([32320])
model_param_fragment.size()=torch.Size([3308608]), new_param_packed_fragment.size()=torch.Size([3308608]), master_param_fragment.size()=torch.Size([3308608])
model_param_fragment.size()=torch.Size([572416]), new_param_packed_fragment.size()=torch.Size([572416]), master_param_fragment.size()=torch.Size([572416])
model_param_fragment.size()=torch.Size([32320]), new_param_packed_fragment.size()=torch.Size([32320]), master_param_fragment.size()=torch.Size([32320])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([2169856]), new_param_packed_fragment.size()=torch.Size([2169856]), master_param_fragment.size()=torch.Size([2169856])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([1177600]), new_param_packed_fragment.size()=torch.Size([1177600]), master_param_fragment.size()=torch.Size([1177600])
model_param_fragment.size()=torch.Size([2766848]), new_param_packed_fragment.size()=torch.Size([2766848]), master_param_fragment.size()=torch.Size([2766848])
model_param_fragment.size()=torch.Size([572416]), new_param_packed_fragment.size()=torch.Size([572416]), master_param_fragment.size()=torch.Size([572416])
model_param_fragment.size()=torch.Size([1082432]), new_param_packed_fragment.size()=torch.Size([1082432]), master_param_fragment.size()=torch.Size([1082432])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([2265024]), new_param_packed_fragment.size()=torch.Size([2265024]), master_param_fragment.size()=torch.Size([2265024])
self._net_total_param_size=160671360, self._total_param_size=160677888, dwu_min_page_size=12288, self._block_size=53559296, self._chunk_size=26779648, self._shard_size=3347456
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([2169856]), new_param_packed_fragment.size()=torch.Size([2169856]), master_param_fragment.size()=torch.Size([2169856])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([2766848]), new_param_packed_fragment.size()=torch.Size([2766848]), master_param_fragment.size()=torch.Size([2766848])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([1082432]), new_param_packed_fragment.size()=torch.Size([1082432]), master_param_fragment.size()=torch.Size([1082432])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
[0, 9, 34]
model_param_fragment.size()=torch.Size([2265024]), new_param_packed_fragment.size()=torch.Size([2265024]), master_param_fragment.size()=torch.Size([2265024])
model_param_fragment.size()=torch.Size([1177600]), new_param_packed_fragment.size()=torch.Size([1177600]), master_param_fragment.size()=torch.Size([1177600])
model_param_fragment.size()=torch.Size([572416]), new_param_packed_fragment.size()=torch.Size([572416]), master_param_fragment.size()=torch.Size([572416])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([1314816]), new_param_packed_fragment.size()=torch.Size([1314816]), master_param_fragment.size()=torch.Size([1314816])
model_param_fragment.size()=torch.Size([2766848]), new_param_packed_fragment.size()=torch.Size([2766848]), master_param_fragment.size()=torch.Size([2766848])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([1082432]), new_param_packed_fragment.size()=torch.Size([1082432]), master_param_fragment.size()=torch.Size([1082432])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([2968576]), new_param_packed_fragment.size()=torch.Size([2968576]), master_param_fragment.size()=torch.Size([2968576])
model_param_fragment.size()=torch.Size([378880]), new_param_packed_fragment.size()=torch.Size([378880]), master_param_fragment.size()=torch.Size([378880])
self._net_total_param_size=160671360, self._total_param_size=160677888, dwu_min_page_size=12288, self._block_size=53559296, self._chunk_size=26779648, self._shard_size=3347456
model_param_fragment.size()=torch.Size([1371136]), new_param_packed_fragment.size()=torch.Size([1371136]), master_param_fragment.size()=torch.Size([1371136])
model_param_fragment.size()=torch.Size([1976320]), new_param_packed_fragment.size()=torch.Size([1976320]), master_param_fragment.size()=torch.Size([1976320])
[0, 9, 34]
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([2265024]), new_param_packed_fragment.size()=torch.Size([2265024]), master_param_fragment.size()=torch.Size([2265024])
model_param_fragment.size()=torch.Size([2024448]), new_param_packed_fragment.size()=torch.Size([2024448]), master_param_fragment.size()=torch.Size([2024448])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([2832448]), new_param_packed_fragment.size()=torch.Size([2832448]), master_param_fragment.size()=torch.Size([2832448])
model_param_fragment.size()=torch.Size([515008]), new_param_packed_fragment.size()=torch.Size([515008]), master_param_fragment.size()=torch.Size([515008])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([283712]), new_param_packed_fragment.size()=torch.Size([283712]), master_param_fragment.size()=torch.Size([283712])
model_param_fragment.size()=torch.Size([3063744]), new_param_packed_fragment.size()=torch.Size([3063744]), master_param_fragment.size()=torch.Size([3063744])
model_param_fragment.size()=torch.Size([3016704]), new_param_packed_fragment.size()=torch.Size([3016704]), master_param_fragment.size()=torch.Size([3016704])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([322560]), new_param_packed_fragment.size()=torch.Size([322560]), master_param_fragment.size()=torch.Size([322560])
self._net_total_param_size=160671360, self._total_param_size=160677888, dwu_min_page_size=12288, self._block_size=53559296, self._chunk_size=26779648, self._shard_size=3347456
model_param_fragment.size()=torch.Size([2169856]), new_param_packed_fragment.size()=torch.Size([2169856]), master_param_fragment.size()=torch.Size([2169856])
[0, 9, 34]
model_param_fragment.size()=torch.Size([1177600]), new_param_packed_fragment.size()=torch.Size([1177600]), master_param_fragment.size()=torch.Size([1177600])
model_param_fragment.size()=torch.Size([1427456]), new_param_packed_fragment.size()=torch.Size([1427456]), master_param_fragment.size()=torch.Size([1427456])
model_param_fragment.size()=torch.Size([1920000]), new_param_packed_fragment.size()=torch.Size([1920000]), master_param_fragment.size()=torch.Size([1920000])
model_param_fragment.size()=torch.Size([572416]), new_param_packed_fragment.size()=torch.Size([572416]), master_param_fragment.size()=torch.Size([572416])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
self._net_total_param_size=160671360, self._total_param_size=160677888, dwu_min_page_size=12288, self._block_size=53559296, self._chunk_size=26779648, self._shard_size=3347456
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([1929280]), new_param_packed_fragment.size()=torch.Size([1929280]), master_param_fragment.size()=torch.Size([1929280])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([2766848]), new_param_packed_fragment.size()=torch.Size([2766848]), master_param_fragment.size()=torch.Size([2766848])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
[0, 9, 34]
model_param_fragment.size()=torch.Size([3016704]), new_param_packed_fragment.size()=torch.Size([3016704]), master_param_fragment.size()=torch.Size([3016704])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([1082432]), new_param_packed_fragment.size()=torch.Size([1082432]), master_param_fragment.size()=torch.Size([1082432])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([2265024]), new_param_packed_fragment.size()=torch.Size([2265024]), master_param_fragment.size()=torch.Size([2265024])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([1409984]), new_param_packed_fragment.size()=torch.Size([1409984]), master_param_fragment.size()=torch.Size([1409984])
model_param_fragment.size()=torch.Size([331840]), new_param_packed_fragment.size()=torch.Size([331840]), master_param_fragment.size()=torch.Size([331840])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([467968]), new_param_packed_fragment.size()=torch.Size([467968]), master_param_fragment.size()=torch.Size([467968])
model_param_fragment.size()=torch.Size([322560]), new_param_packed_fragment.size()=torch.Size([322560]), master_param_fragment.size()=torch.Size([322560])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([2968576]), new_param_packed_fragment.size()=torch.Size([2968576]), master_param_fragment.size()=torch.Size([2968576])
model_param_fragment.size()=torch.Size([2879488]), new_param_packed_fragment.size()=torch.Size([2879488]), master_param_fragment.size()=torch.Size([2879488])
model_param_fragment.size()=torch.Size([3007424]), new_param_packed_fragment.size()=torch.Size([3007424]), master_param_fragment.size()=torch.Size([3007424])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3129344]), new_param_packed_fragment.size()=torch.Size([3129344]), master_param_fragment.size()=torch.Size([3129344])
model_param_fragment.size()=torch.Size([378880]), new_param_packed_fragment.size()=torch.Size([378880]), master_param_fragment.size()=torch.Size([378880])
model_param_fragment.size()=torch.Size([1427456]), new_param_packed_fragment.size()=torch.Size([1427456]), master_param_fragment.size()=torch.Size([1427456])
model_param_fragment.size()=torch.Size([3073024]), new_param_packed_fragment.size()=torch.Size([3073024]), master_param_fragment.size()=torch.Size([3073024])
model_param_fragment.size()=torch.Size([218112]), new_param_packed_fragment.size()=torch.Size([218112]), master_param_fragment.size()=torch.Size([218112])
model_param_fragment.size()=torch.Size([1920000]), new_param_packed_fragment.size()=torch.Size([1920000]), master_param_fragment.size()=torch.Size([1920000])
model_param_fragment.size()=torch.Size([274432]), new_param_packed_fragment.size()=torch.Size([274432]), master_param_fragment.size()=torch.Size([274432])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([1371136]), new_param_packed_fragment.size()=torch.Size([1371136]), master_param_fragment.size()=torch.Size([1371136])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([1929280]), new_param_packed_fragment.size()=torch.Size([1929280]), master_param_fragment.size()=torch.Size([1929280])
model_param_fragment.size()=torch.Size([1976320]), new_param_packed_fragment.size()=torch.Size([1976320]), master_param_fragment.size()=torch.Size([1976320])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([2968576]), new_param_packed_fragment.size()=torch.Size([2968576]), master_param_fragment.size()=torch.Size([2968576])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([1409984]), new_param_packed_fragment.size()=torch.Size([1409984]), master_param_fragment.size()=torch.Size([1409984])
model_param_fragment.size()=torch.Size([378880]), new_param_packed_fragment.size()=torch.Size([378880]), master_param_fragment.size()=torch.Size([378880])
model_param_fragment.size()=torch.Size([283712]), new_param_packed_fragment.size()=torch.Size([283712]), master_param_fragment.size()=torch.Size([283712])
model_param_fragment.size()=torch.Size([2169856]), new_param_packed_fragment.size()=torch.Size([2169856]), master_param_fragment.size()=torch.Size([2169856])
model_param_fragment.size()=torch.Size([331840]), new_param_packed_fragment.size()=torch.Size([331840]), master_param_fragment.size()=torch.Size([331840])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3063744]), new_param_packed_fragment.size()=torch.Size([3063744]), master_param_fragment.size()=torch.Size([3063744])
model_param_fragment.size()=torch.Size([1177600]), new_param_packed_fragment.size()=torch.Size([1177600]), master_param_fragment.size()=torch.Size([1177600])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([1371136]), new_param_packed_fragment.size()=torch.Size([1371136]), master_param_fragment.size()=torch.Size([1371136])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([1976320]), new_param_packed_fragment.size()=torch.Size([1976320]), master_param_fragment.size()=torch.Size([1976320])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([572416]), new_param_packed_fragment.size()=torch.Size([572416]), master_param_fragment.size()=torch.Size([572416])
model_param_fragment.size()=torch.Size([3007424]), new_param_packed_fragment.size()=torch.Size([3007424]), master_param_fragment.size()=torch.Size([3007424])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([2766848]), new_param_packed_fragment.size()=torch.Size([2766848]), master_param_fragment.size()=torch.Size([2766848])
model_param_fragment.size()=torch.Size([283712]), new_param_packed_fragment.size()=torch.Size([283712]), master_param_fragment.size()=torch.Size([283712])
model_param_fragment.size()=torch.Size([1314816]), new_param_packed_fragment.size()=torch.Size([1314816]), master_param_fragment.size()=torch.Size([1314816])
model_param_fragment.size()=torch.Size([1082432]), new_param_packed_fragment.size()=torch.Size([1082432]), master_param_fragment.size()=torch.Size([1082432])
model_param_fragment.size()=torch.Size([3063744]), new_param_packed_fragment.size()=torch.Size([3063744]), master_param_fragment.size()=torch.Size([3063744])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([2265024]), new_param_packed_fragment.size()=torch.Size([2265024]), master_param_fragment.size()=torch.Size([2265024])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([2024448]), new_param_packed_fragment.size()=torch.Size([2024448]), master_param_fragment.size()=torch.Size([2024448])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([2832448]), new_param_packed_fragment.size()=torch.Size([2832448]), master_param_fragment.size()=torch.Size([2832448])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3129344]), new_param_packed_fragment.size()=torch.Size([3129344]), master_param_fragment.size()=torch.Size([3129344])
model_param_fragment.size()=torch.Size([515008]), new_param_packed_fragment.size()=torch.Size([515008]), master_param_fragment.size()=torch.Size([515008])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([218112]), new_param_packed_fragment.size()=torch.Size([218112]), master_param_fragment.size()=torch.Size([218112])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([2968576]), new_param_packed_fragment.size()=torch.Size([2968576]), master_param_fragment.size()=torch.Size([2968576])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([378880]), new_param_packed_fragment.size()=torch.Size([378880]), master_param_fragment.size()=torch.Size([378880])
model_param_fragment.size()=torch.Size([2169856]), new_param_packed_fragment.size()=torch.Size([2169856]), master_param_fragment.size()=torch.Size([2169856])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([1371136]), new_param_packed_fragment.size()=torch.Size([1371136]), master_param_fragment.size()=torch.Size([1371136])
model_param_fragment.size()=torch.Size([3016704]), new_param_packed_fragment.size()=torch.Size([3016704]), master_param_fragment.size()=torch.Size([3016704])
model_param_fragment.size()=torch.Size([1177600]), new_param_packed_fragment.size()=torch.Size([1177600]), master_param_fragment.size()=torch.Size([1177600])
model_param_fragment.size()=torch.Size([1976320]), new_param_packed_fragment.size()=torch.Size([1976320]), master_param_fragment.size()=torch.Size([1976320])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([467968]), new_param_packed_fragment.size()=torch.Size([467968]), master_param_fragment.size()=torch.Size([467968])
model_param_fragment.size()=torch.Size([572416]), new_param_packed_fragment.size()=torch.Size([572416]), master_param_fragment.size()=torch.Size([572416])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([322560]), new_param_packed_fragment.size()=torch.Size([322560]), master_param_fragment.size()=torch.Size([322560])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([2879488]), new_param_packed_fragment.size()=torch.Size([2879488]), master_param_fragment.size()=torch.Size([2879488])
model_param_fragment.size()=torch.Size([283712]), new_param_packed_fragment.size()=torch.Size([283712]), master_param_fragment.size()=torch.Size([283712])
model_param_fragment.size()=torch.Size([1427456]), new_param_packed_fragment.size()=torch.Size([1427456]), master_param_fragment.size()=torch.Size([1427456])
model_param_fragment.size()=torch.Size([3073024]), new_param_packed_fragment.size()=torch.Size([3073024]), master_param_fragment.size()=torch.Size([3073024])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([1920000]), new_param_packed_fragment.size()=torch.Size([1920000]), master_param_fragment.size()=torch.Size([1920000])
model_param_fragment.size()=torch.Size([2766848]), new_param_packed_fragment.size()=torch.Size([2766848]), master_param_fragment.size()=torch.Size([2766848])
model_param_fragment.size()=torch.Size([3063744]), new_param_packed_fragment.size()=torch.Size([3063744]), master_param_fragment.size()=torch.Size([3063744])
model_param_fragment.size()=torch.Size([274432]), new_param_packed_fragment.size()=torch.Size([274432]), master_param_fragment.size()=torch.Size([274432])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([1929280]), new_param_packed_fragment.size()=torch.Size([1929280]), master_param_fragment.size()=torch.Size([1929280])
model_param_fragment.size()=torch.Size([1082432]), new_param_packed_fragment.size()=torch.Size([1082432]), master_param_fragment.size()=torch.Size([1082432])
model_param_fragment.size()=torch.Size([2217984]), new_param_packed_fragment.size()=torch.Size([2217984]), master_param_fragment.size()=torch.Size([2217984])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([2265024]), new_param_packed_fragment.size()=torch.Size([2265024]), master_param_fragment.size()=torch.Size([2265024])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([1409984]), new_param_packed_fragment.size()=torch.Size([1409984]), master_param_fragment.size()=torch.Size([1409984])
model_param_fragment.size()=torch.Size([1121280]), new_param_packed_fragment.size()=torch.Size([1121280]), master_param_fragment.size()=torch.Size([1121280])
model_param_fragment.size()=torch.Size([331840]), new_param_packed_fragment.size()=torch.Size([331840]), master_param_fragment.size()=torch.Size([331840])
model_param_fragment.size()=torch.Size([628736]), new_param_packed_fragment.size()=torch.Size([628736]), master_param_fragment.size()=torch.Size([628736])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([3129344]), new_param_packed_fragment.size()=torch.Size([3129344]), master_param_fragment.size()=torch.Size([3129344])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([218112]), new_param_packed_fragment.size()=torch.Size([218112]), master_param_fragment.size()=torch.Size([218112])
model_param_fragment.size()=torch.Size([3016704]), new_param_packed_fragment.size()=torch.Size([3016704]), master_param_fragment.size()=torch.Size([3016704])
model_param_fragment.size()=torch.Size([1024]), new_param_packed_fragment.size()=torch.Size([1024]), master_param_fragment.size()=torch.Size([1024])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([1]), new_param_packed_fragment.size()=torch.Size([1]), master_param_fragment.size()=torch.Size([1])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
self._net_total_param_size=160671360, self._total_param_size=160677888, dwu_min_page_size=12288, self._block_size=53559296, self._chunk_size=26779648, self._shard_size=3347456
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([1024]), new_param_packed_fragment.size()=torch.Size([1024]), master_param_fragment.size()=torch.Size([1024])
model_param_fragment.size()=torch.Size([3007424]), new_param_packed_fragment.size()=torch.Size([3007424]), master_param_fragment.size()=torch.Size([3007424])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
[0, 9, 34]
model_param_fragment.size()=torch.Size([322560]), new_param_packed_fragment.size()=torch.Size([322560]), master_param_fragment.size()=torch.Size([322560])
model_param_fragment.size()=torch.Size([1048576]), new_param_packed_fragment.size()=torch.Size([1048576]), master_param_fragment.size()=torch.Size([1048576])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([1048576]), new_param_packed_fragment.size()=torch.Size([1048576]), master_param_fragment.size()=torch.Size([1048576])
model_param_fragment.size()=torch.Size([1427456]), new_param_packed_fragment.size()=torch.Size([1427456]), master_param_fragment.size()=torch.Size([1427456])
model_param_fragment.size()=torch.Size([611264]), new_param_packed_fragment.size()=torch.Size([611264]), master_param_fragment.size()=torch.Size([611264])
model_param_fragment.size()=torch.Size([1920000]), new_param_packed_fragment.size()=torch.Size([1920000]), master_param_fragment.size()=torch.Size([1920000])
model_param_fragment.size()=torch.Size([1130560]), new_param_packed_fragment.size()=torch.Size([1130560]), master_param_fragment.size()=torch.Size([1130560])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3016704]), new_param_packed_fragment.size()=torch.Size([3016704]), master_param_fragment.size()=torch.Size([3016704])
model_param_fragment.size()=torch.Size([467968]), new_param_packed_fragment.size()=torch.Size([467968]), master_param_fragment.size()=torch.Size([467968])
model_param_fragment.size()=torch.Size([1929280]), new_param_packed_fragment.size()=torch.Size([1929280]), master_param_fragment.size()=torch.Size([1929280])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([2879488]), new_param_packed_fragment.size()=torch.Size([2879488]), master_param_fragment.size()=torch.Size([2879488])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([2208704]), new_param_packed_fragment.size()=torch.Size([2208704]), master_param_fragment.size()=torch.Size([2208704])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([3073024]), new_param_packed_fragment.size()=torch.Size([3073024]), master_param_fragment.size()=torch.Size([3073024])
model_param_fragment.size()=torch.Size([1409984]), new_param_packed_fragment.size()=torch.Size([1409984]), master_param_fragment.size()=torch.Size([1409984])
model_param_fragment.size()=torch.Size([322560]), new_param_packed_fragment.size()=torch.Size([322560]), master_param_fragment.size()=torch.Size([322560])
model_param_fragment.size()=torch.Size([274432]), new_param_packed_fragment.size()=torch.Size([274432]), master_param_fragment.size()=torch.Size([274432])
model_param_fragment.size()=torch.Size([1427456]), new_param_packed_fragment.size()=torch.Size([1427456]), master_param_fragment.size()=torch.Size([1427456])
model_param_fragment.size()=torch.Size([331840]), new_param_packed_fragment.size()=torch.Size([331840]), master_param_fragment.size()=torch.Size([331840])
model_param_fragment.size()=torch.Size([2968576]), new_param_packed_fragment.size()=torch.Size([2968576]), master_param_fragment.size()=torch.Size([2968576])
model_param_fragment.size()=torch.Size([1920000]), new_param_packed_fragment.size()=torch.Size([1920000]), master_param_fragment.size()=torch.Size([1920000])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([378880]), new_param_packed_fragment.size()=torch.Size([378880]), master_param_fragment.size()=torch.Size([378880])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([1929280]), new_param_packed_fragment.size()=torch.Size([1929280]), master_param_fragment.size()=torch.Size([1929280])
model_param_fragment.size()=torch.Size([3007424]), new_param_packed_fragment.size()=torch.Size([3007424]), master_param_fragment.size()=torch.Size([3007424])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([1371136]), new_param_packed_fragment.size()=torch.Size([1371136]), master_param_fragment.size()=torch.Size([1371136])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([1976320]), new_param_packed_fragment.size()=torch.Size([1976320]), master_param_fragment.size()=torch.Size([1976320])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([1409984]), new_param_packed_fragment.size()=torch.Size([1409984]), master_param_fragment.size()=torch.Size([1409984])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([331840]), new_param_packed_fragment.size()=torch.Size([331840]), master_param_fragment.size()=torch.Size([331840])
model_param_fragment.size()=torch.Size([283712]), new_param_packed_fragment.size()=torch.Size([283712]), master_param_fragment.size()=torch.Size([283712])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([2217984]), new_param_packed_fragment.size()=torch.Size([2217984]), master_param_fragment.size()=torch.Size([2217984])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([3063744]), new_param_packed_fragment.size()=torch.Size([3063744]), master_param_fragment.size()=torch.Size([3063744])
model_param_fragment.size()=torch.Size([3007424]), new_param_packed_fragment.size()=torch.Size([3007424]), master_param_fragment.size()=torch.Size([3007424])
model_param_fragment.size()=torch.Size([1121280]), new_param_packed_fragment.size()=torch.Size([1121280]), master_param_fragment.size()=torch.Size([1121280])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([628736]), new_param_packed_fragment.size()=torch.Size([628736]), master_param_fragment.size()=torch.Size([628736])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([2274304]), new_param_packed_fragment.size()=torch.Size([2274304]), master_param_fragment.size()=torch.Size([2274304])
model_param_fragment.size()=torch.Size([1024]), new_param_packed_fragment.size()=torch.Size([1024]), master_param_fragment.size()=torch.Size([1024])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([1]), new_param_packed_fragment.size()=torch.Size([1]), master_param_fragment.size()=torch.Size([1])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([1024]), new_param_packed_fragment.size()=torch.Size([1024]), master_param_fragment.size()=torch.Size([1024])
model_param_fragment.size()=torch.Size([1064960]), new_param_packed_fragment.size()=torch.Size([1064960]), master_param_fragment.size()=torch.Size([1064960])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([2968576]), new_param_packed_fragment.size()=torch.Size([2968576]), master_param_fragment.size()=torch.Size([2968576])
model_param_fragment.size()=torch.Size([378880]), new_param_packed_fragment.size()=torch.Size([378880]), master_param_fragment.size()=torch.Size([378880])
model_param_fragment.size()=torch.Size([1371136]), new_param_packed_fragment.size()=torch.Size([1371136]), master_param_fragment.size()=torch.Size([1371136])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([1976320]), new_param_packed_fragment.size()=torch.Size([1976320]), master_param_fragment.size()=torch.Size([1976320])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([283712]), new_param_packed_fragment.size()=torch.Size([283712]), master_param_fragment.size()=torch.Size([283712])
model_param_fragment.size()=torch.Size([3063744]), new_param_packed_fragment.size()=torch.Size([3063744]), master_param_fragment.size()=torch.Size([3063744])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3016704]), new_param_packed_fragment.size()=torch.Size([3016704]), master_param_fragment.size()=torch.Size([3016704])
model_param_fragment.size()=torch.Size([2217984]), new_param_packed_fragment.size()=torch.Size([2217984]), master_param_fragment.size()=torch.Size([2217984])
model_param_fragment.size()=torch.Size([2968576]), new_param_packed_fragment.size()=torch.Size([2968576]), master_param_fragment.size()=torch.Size([2968576])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([378880]), new_param_packed_fragment.size()=torch.Size([378880]), master_param_fragment.size()=torch.Size([378880])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([1371136]), new_param_packed_fragment.size()=torch.Size([1371136]), master_param_fragment.size()=torch.Size([1371136])
model_param_fragment.size()=torch.Size([2968576]), new_param_packed_fragment.size()=torch.Size([2968576]), master_param_fragment.size()=torch.Size([2968576])
model_param_fragment.size()=torch.Size([1976320]), new_param_packed_fragment.size()=torch.Size([1976320]), master_param_fragment.size()=torch.Size([1976320])
model_param_fragment.size()=torch.Size([322560]), new_param_packed_fragment.size()=torch.Size([322560]), master_param_fragment.size()=torch.Size([322560])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([1121280]), new_param_packed_fragment.size()=torch.Size([1121280]), master_param_fragment.size()=torch.Size([1121280])
model_param_fragment.size()=torch.Size([378880]), new_param_packed_fragment.size()=torch.Size([378880]), master_param_fragment.size()=torch.Size([378880])
model_param_fragment.size()=torch.Size([1427456]), new_param_packed_fragment.size()=torch.Size([1427456]), master_param_fragment.size()=torch.Size([1427456])
model_param_fragment.size()=torch.Size([628736]), new_param_packed_fragment.size()=torch.Size([628736]), master_param_fragment.size()=torch.Size([628736])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([1920000]), new_param_packed_fragment.size()=torch.Size([1920000]), master_param_fragment.size()=torch.Size([1920000])
model_param_fragment.size()=torch.Size([1371136]), new_param_packed_fragment.size()=torch.Size([1371136]), master_param_fragment.size()=torch.Size([1371136])
model_param_fragment.size()=torch.Size([283712]), new_param_packed_fragment.size()=torch.Size([283712]), master_param_fragment.size()=torch.Size([283712])
model_param_fragment.size()=torch.Size([1976320]), new_param_packed_fragment.size()=torch.Size([1976320]), master_param_fragment.size()=torch.Size([1976320])
model_param_fragment.size()=torch.Size([1929280]), new_param_packed_fragment.size()=torch.Size([1929280]), master_param_fragment.size()=torch.Size([1929280])
model_param_fragment.size()=torch.Size([3063744]), new_param_packed_fragment.size()=torch.Size([3063744]), master_param_fragment.size()=torch.Size([3063744])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([1024]), new_param_packed_fragment.size()=torch.Size([1024]), master_param_fragment.size()=torch.Size([1024])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([1]), new_param_packed_fragment.size()=torch.Size([1]), master_param_fragment.size()=torch.Size([1])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([1409984]), new_param_packed_fragment.size()=torch.Size([1409984]), master_param_fragment.size()=torch.Size([1409984])
model_param_fragment.size()=torch.Size([283712]), new_param_packed_fragment.size()=torch.Size([283712]), master_param_fragment.size()=torch.Size([283712])
model_param_fragment.size()=torch.Size([3063744]), new_param_packed_fragment.size()=torch.Size([3063744]), master_param_fragment.size()=torch.Size([3063744])
model_param_fragment.size()=torch.Size([331840]), new_param_packed_fragment.size()=torch.Size([331840]), master_param_fragment.size()=torch.Size([331840])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([3007424]), new_param_packed_fragment.size()=torch.Size([3007424]), master_param_fragment.size()=torch.Size([3007424])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([1024]), new_param_packed_fragment.size()=torch.Size([1024]), master_param_fragment.size()=torch.Size([1024])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([1048576]), new_param_packed_fragment.size()=torch.Size([1048576]), master_param_fragment.size()=torch.Size([1048576])
model_param_fragment.size()=torch.Size([1048576]), new_param_packed_fragment.size()=torch.Size([1048576]), master_param_fragment.size()=torch.Size([1048576])
model_param_fragment.size()=torch.Size([611264]), new_param_packed_fragment.size()=torch.Size([611264]), master_param_fragment.size()=torch.Size([611264])
model_param_fragment.size()=torch.Size([1130560]), new_param_packed_fragment.size()=torch.Size([1130560]), master_param_fragment.size()=torch.Size([1130560])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([2208704]), new_param_packed_fragment.size()=torch.Size([2208704]), master_param_fragment.size()=torch.Size([2208704])
model_param_fragment.size()=torch.Size([3016704]), new_param_packed_fragment.size()=torch.Size([3016704]), master_param_fragment.size()=torch.Size([3016704])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
self._net_total_param_size=160671360, self._total_param_size=160677888, dwu_min_page_size=12288, self._block_size=53559296, self._chunk_size=26779648, self._shard_size=3347456
model_param_fragment.size()=torch.Size([2217984]), new_param_packed_fragment.size()=torch.Size([2217984]), master_param_fragment.size()=torch.Size([2217984])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
[0, 9, 34]
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([322560]), new_param_packed_fragment.size()=torch.Size([322560]), master_param_fragment.size()=torch.Size([322560])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([1121280]), new_param_packed_fragment.size()=torch.Size([1121280]), master_param_fragment.size()=torch.Size([1121280])
model_param_fragment.size()=torch.Size([1427456]), new_param_packed_fragment.size()=torch.Size([1427456]), master_param_fragment.size()=torch.Size([1427456])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([628736]), new_param_packed_fragment.size()=torch.Size([628736]), master_param_fragment.size()=torch.Size([628736])
model_param_fragment.size()=torch.Size([1920000]), new_param_packed_fragment.size()=torch.Size([1920000]), master_param_fragment.size()=torch.Size([1920000])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([1929280]), new_param_packed_fragment.size()=torch.Size([1929280]), master_param_fragment.size()=torch.Size([1929280])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([3129344]), new_param_packed_fragment.size()=torch.Size([3129344]), master_param_fragment.size()=torch.Size([3129344])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([1024]), new_param_packed_fragment.size()=torch.Size([1024]), master_param_fragment.size()=torch.Size([1024])
model_param_fragment.size()=torch.Size([218112]), new_param_packed_fragment.size()=torch.Size([218112]), master_param_fragment.size()=torch.Size([218112])
model_param_fragment.size()=torch.Size([1]), new_param_packed_fragment.size()=torch.Size([1]), master_param_fragment.size()=torch.Size([1])
model_param_fragment.size()=torch.Size([1409984]), new_param_packed_fragment.size()=torch.Size([1409984]), master_param_fragment.size()=torch.Size([1409984])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([331840]), new_param_packed_fragment.size()=torch.Size([331840]), master_param_fragment.size()=torch.Size([331840])
model_param_fragment.size()=torch.Size([1024]), new_param_packed_fragment.size()=torch.Size([1024]), master_param_fragment.size()=torch.Size([1024])
model_param_fragment.size()=torch.Size([1048576]), new_param_packed_fragment.size()=torch.Size([1048576]), master_param_fragment.size()=torch.Size([1048576])
model_param_fragment.size()=torch.Size([1048576]), new_param_packed_fragment.size()=torch.Size([1048576]), master_param_fragment.size()=torch.Size([1048576])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([611264]), new_param_packed_fragment.size()=torch.Size([611264]), master_param_fragment.size()=torch.Size([611264])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([1314816]), new_param_packed_fragment.size()=torch.Size([1314816]), master_param_fragment.size()=torch.Size([1314816])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3007424]), new_param_packed_fragment.size()=torch.Size([3007424]), master_param_fragment.size()=torch.Size([3007424])
model_param_fragment.size()=torch.Size([1130560]), new_param_packed_fragment.size()=torch.Size([1130560]), master_param_fragment.size()=torch.Size([1130560])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([2208704]), new_param_packed_fragment.size()=torch.Size([2208704]), master_param_fragment.size()=torch.Size([2208704])
model_param_fragment.size()=torch.Size([2024448]), new_param_packed_fragment.size()=torch.Size([2024448]), master_param_fragment.size()=torch.Size([2024448])
model_param_fragment.size()=torch.Size([2217984]), new_param_packed_fragment.size()=torch.Size([2217984]), master_param_fragment.size()=torch.Size([2217984])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([1121280]), new_param_packed_fragment.size()=torch.Size([1121280]), master_param_fragment.size()=torch.Size([1121280])
model_param_fragment.size()=torch.Size([2832448]), new_param_packed_fragment.size()=torch.Size([2832448]), master_param_fragment.size()=torch.Size([2832448])
model_param_fragment.size()=torch.Size([515008]), new_param_packed_fragment.size()=torch.Size([515008]), master_param_fragment.size()=torch.Size([515008])
model_param_fragment.size()=torch.Size([628736]), new_param_packed_fragment.size()=torch.Size([628736]), master_param_fragment.size()=torch.Size([628736])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([1024]), new_param_packed_fragment.size()=torch.Size([1024]), master_param_fragment.size()=torch.Size([1024])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([1]), new_param_packed_fragment.size()=torch.Size([1]), master_param_fragment.size()=torch.Size([1])
model_param_fragment.size()=torch.Size([1024]), new_param_packed_fragment.size()=torch.Size([1024]), master_param_fragment.size()=torch.Size([1024])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([2274304]), new_param_packed_fragment.size()=torch.Size([2274304]), master_param_fragment.size()=torch.Size([2274304])
model_param_fragment.size()=torch.Size([1048576]), new_param_packed_fragment.size()=torch.Size([1048576]), master_param_fragment.size()=torch.Size([1048576])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([1048576]), new_param_packed_fragment.size()=torch.Size([1048576]), master_param_fragment.size()=torch.Size([1048576])
model_param_fragment.size()=torch.Size([611264]), new_param_packed_fragment.size()=torch.Size([611264]), master_param_fragment.size()=torch.Size([611264])
model_param_fragment.size()=torch.Size([3016704]), new_param_packed_fragment.size()=torch.Size([3016704]), master_param_fragment.size()=torch.Size([3016704])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([1064960]), new_param_packed_fragment.size()=torch.Size([1064960]), master_param_fragment.size()=torch.Size([1064960])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([1130560]), new_param_packed_fragment.size()=torch.Size([1130560]), master_param_fragment.size()=torch.Size([1130560])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([322560]), new_param_packed_fragment.size()=torch.Size([322560]), master_param_fragment.size()=torch.Size([322560])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([2274304]), new_param_packed_fragment.size()=torch.Size([2274304]), master_param_fragment.size()=torch.Size([2274304])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([2208704]), new_param_packed_fragment.size()=torch.Size([2208704]), master_param_fragment.size()=torch.Size([2208704])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([2217984]), new_param_packed_fragment.size()=torch.Size([2217984]), master_param_fragment.size()=torch.Size([2217984])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([1427456]), new_param_packed_fragment.size()=torch.Size([1427456]), master_param_fragment.size()=torch.Size([1427456])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
self._net_total_param_size=160671360, self._total_param_size=160677888, dwu_min_page_size=12288, self._block_size=53559296, self._chunk_size=26779648, self._shard_size=3347456
model_param_fragment.size()=torch.Size([3308608]), new_param_packed_fragment.size()=torch.Size([3308608]), master_param_fragment.size()=torch.Size([3308608])
model_param_fragment.size()=torch.Size([1920000]), new_param_packed_fragment.size()=torch.Size([1920000]), master_param_fragment.size()=torch.Size([1920000])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
[0, 9, 34]
model_param_fragment.size()=torch.Size([1929280]), new_param_packed_fragment.size()=torch.Size([1929280]), master_param_fragment.size()=torch.Size([1929280])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([1064960]), new_param_packed_fragment.size()=torch.Size([1064960]), master_param_fragment.size()=torch.Size([1064960])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([1121280]), new_param_packed_fragment.size()=torch.Size([1121280]), master_param_fragment.size()=torch.Size([1121280])
model_param_fragment.size()=torch.Size([32320]), new_param_packed_fragment.size()=torch.Size([32320]), master_param_fragment.size()=torch.Size([32320])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([628736]), new_param_packed_fragment.size()=torch.Size([628736]), master_param_fragment.size()=torch.Size([628736])
model_param_fragment.size()=torch.Size([467968]), new_param_packed_fragment.size()=torch.Size([467968]), master_param_fragment.size()=torch.Size([467968])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([1409984]), new_param_packed_fragment.size()=torch.Size([1409984]), master_param_fragment.size()=torch.Size([1409984])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([2879488]), new_param_packed_fragment.size()=torch.Size([2879488]), master_param_fragment.size()=torch.Size([2879488])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([331840]), new_param_packed_fragment.size()=torch.Size([331840]), master_param_fragment.size()=torch.Size([331840])
model_param_fragment.size()=torch.Size([1024]), new_param_packed_fragment.size()=torch.Size([1024]), master_param_fragment.size()=torch.Size([1024])
model_param_fragment.size()=torch.Size([3073024]), new_param_packed_fragment.size()=torch.Size([3073024]), master_param_fragment.size()=torch.Size([3073024])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([3308608]), new_param_packed_fragment.size()=torch.Size([3308608]), master_param_fragment.size()=torch.Size([3308608])
model_param_fragment.size()=torch.Size([1]), new_param_packed_fragment.size()=torch.Size([1]), master_param_fragment.size()=torch.Size([1])
model_param_fragment.size()=torch.Size([274432]), new_param_packed_fragment.size()=torch.Size([274432]), master_param_fragment.size()=torch.Size([274432])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([32320]), new_param_packed_fragment.size()=torch.Size([32320]), master_param_fragment.size()=torch.Size([32320])
model_param_fragment.size()=torch.Size([1024]), new_param_packed_fragment.size()=torch.Size([1024]), master_param_fragment.size()=torch.Size([1024])
model_param_fragment.size()=torch.Size([3007424]), new_param_packed_fragment.size()=torch.Size([3007424]), master_param_fragment.size()=torch.Size([3007424])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([1048576]), new_param_packed_fragment.size()=torch.Size([1048576]), master_param_fragment.size()=torch.Size([1048576])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3016704]), new_param_packed_fragment.size()=torch.Size([3016704]), master_param_fragment.size()=torch.Size([3016704])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([1048576]), new_param_packed_fragment.size()=torch.Size([1048576]), master_param_fragment.size()=torch.Size([1048576])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([611264]), new_param_packed_fragment.size()=torch.Size([611264]), master_param_fragment.size()=torch.Size([611264])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([1130560]), new_param_packed_fragment.size()=torch.Size([1130560]), master_param_fragment.size()=torch.Size([1130560])
model_param_fragment.size()=torch.Size([322560]), new_param_packed_fragment.size()=torch.Size([322560]), master_param_fragment.size()=torch.Size([322560])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([1427456]), new_param_packed_fragment.size()=torch.Size([1427456]), master_param_fragment.size()=torch.Size([1427456])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([1920000]), new_param_packed_fragment.size()=torch.Size([1920000]), master_param_fragment.size()=torch.Size([1920000])
model_param_fragment.size()=torch.Size([2208704]), new_param_packed_fragment.size()=torch.Size([2208704]), master_param_fragment.size()=torch.Size([2208704])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([1929280]), new_param_packed_fragment.size()=torch.Size([1929280]), master_param_fragment.size()=torch.Size([1929280])
model_param_fragment.size()=torch.Size([1314816]), new_param_packed_fragment.size()=torch.Size([1314816]), master_param_fragment.size()=torch.Size([1314816])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([2968576]), new_param_packed_fragment.size()=torch.Size([2968576]), master_param_fragment.size()=torch.Size([2968576])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([378880]), new_param_packed_fragment.size()=torch.Size([378880]), master_param_fragment.size()=torch.Size([378880])
model_param_fragment.size()=torch.Size([1409984]), new_param_packed_fragment.size()=torch.Size([1409984]), master_param_fragment.size()=torch.Size([1409984])
model_param_fragment.size()=torch.Size([2024448]), new_param_packed_fragment.size()=torch.Size([2024448]), master_param_fragment.size()=torch.Size([2024448])
model_param_fragment.size()=torch.Size([1371136]), new_param_packed_fragment.size()=torch.Size([1371136]), master_param_fragment.size()=torch.Size([1371136])
model_param_fragment.size()=torch.Size([331840]), new_param_packed_fragment.size()=torch.Size([331840]), master_param_fragment.size()=torch.Size([331840])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([1976320]), new_param_packed_fragment.size()=torch.Size([1976320]), master_param_fragment.size()=torch.Size([1976320])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
self._net_total_param_size=160671360, self._total_param_size=160677888, dwu_min_page_size=12288, self._block_size=53559296, self._chunk_size=26779648, self._shard_size=3347456
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
[0, 9, 34]
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3129344]), new_param_packed_fragment.size()=torch.Size([3129344]), master_param_fragment.size()=torch.Size([3129344])
model_param_fragment.size()=torch.Size([2832448]), new_param_packed_fragment.size()=torch.Size([2832448]), master_param_fragment.size()=torch.Size([2832448])
model_param_fragment.size()=torch.Size([3007424]), new_param_packed_fragment.size()=torch.Size([3007424]), master_param_fragment.size()=torch.Size([3007424])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([283712]), new_param_packed_fragment.size()=torch.Size([283712]), master_param_fragment.size()=torch.Size([283712])
model_param_fragment.size()=torch.Size([515008]), new_param_packed_fragment.size()=torch.Size([515008]), master_param_fragment.size()=torch.Size([515008])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3063744]), new_param_packed_fragment.size()=torch.Size([3063744]), master_param_fragment.size()=torch.Size([3063744])
model_param_fragment.size()=torch.Size([218112]), new_param_packed_fragment.size()=torch.Size([218112]), master_param_fragment.size()=torch.Size([218112])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3016704]), new_param_packed_fragment.size()=torch.Size([3016704]), master_param_fragment.size()=torch.Size([3016704])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([322560]), new_param_packed_fragment.size()=torch.Size([322560]), master_param_fragment.size()=torch.Size([322560])
model_param_fragment.size()=torch.Size([3129344]), new_param_packed_fragment.size()=torch.Size([3129344]), master_param_fragment.size()=torch.Size([3129344])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([1427456]), new_param_packed_fragment.size()=torch.Size([1427456]), master_param_fragment.size()=torch.Size([1427456])
model_param_fragment.size()=torch.Size([218112]), new_param_packed_fragment.size()=torch.Size([218112]), master_param_fragment.size()=torch.Size([218112])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([1920000]), new_param_packed_fragment.size()=torch.Size([1920000]), master_param_fragment.size()=torch.Size([1920000])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([467968]), new_param_packed_fragment.size()=torch.Size([467968]), master_param_fragment.size()=torch.Size([467968])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([2879488]), new_param_packed_fragment.size()=torch.Size([2879488]), master_param_fragment.size()=torch.Size([2879488])
model_param_fragment.size()=torch.Size([1929280]), new_param_packed_fragment.size()=torch.Size([1929280]), master_param_fragment.size()=torch.Size([1929280])
model_param_fragment.size()=torch.Size([2274304]), new_param_packed_fragment.size()=torch.Size([2274304]), master_param_fragment.size()=torch.Size([2274304])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([3073024]), new_param_packed_fragment.size()=torch.Size([3073024]), master_param_fragment.size()=torch.Size([3073024])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([274432]), new_param_packed_fragment.size()=torch.Size([274432]), master_param_fragment.size()=torch.Size([274432])
model_param_fragment.size()=torch.Size([1064960]), new_param_packed_fragment.size()=torch.Size([1064960]), master_param_fragment.size()=torch.Size([1064960])
model_param_fragment.size()=torch.Size([1409984]), new_param_packed_fragment.size()=torch.Size([1409984]), master_param_fragment.size()=torch.Size([1409984])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([331840]), new_param_packed_fragment.size()=torch.Size([331840]), master_param_fragment.size()=torch.Size([331840])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([1048576]), new_param_packed_fragment.size()=torch.Size([1048576]), master_param_fragment.size()=torch.Size([1048576])
model_param_fragment.size()=torch.Size([3308608]), new_param_packed_fragment.size()=torch.Size([3308608]), master_param_fragment.size()=torch.Size([3308608])
model_param_fragment.size()=torch.Size([3007424]), new_param_packed_fragment.size()=torch.Size([3007424]), master_param_fragment.size()=torch.Size([3007424])
model_param_fragment.size()=torch.Size([1048576]), new_param_packed_fragment.size()=torch.Size([1048576]), master_param_fragment.size()=torch.Size([1048576])
model_param_fragment.size()=torch.Size([611264]), new_param_packed_fragment.size()=torch.Size([611264]), master_param_fragment.size()=torch.Size([611264])
model_param_fragment.size()=torch.Size([1130560]), new_param_packed_fragment.size()=torch.Size([1130560]), master_param_fragment.size()=torch.Size([1130560])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([2208704]), new_param_packed_fragment.size()=torch.Size([2208704]), master_param_fragment.size()=torch.Size([2208704])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([32320]), new_param_packed_fragment.size()=torch.Size([32320]), master_param_fragment.size()=torch.Size([32320])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3016704]), new_param_packed_fragment.size()=torch.Size([3016704]), master_param_fragment.size()=torch.Size([3016704])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([2274304]), new_param_packed_fragment.size()=torch.Size([2274304]), master_param_fragment.size()=torch.Size([2274304])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([322560]), new_param_packed_fragment.size()=torch.Size([322560]), master_param_fragment.size()=torch.Size([322560])
self._net_total_param_size=160671360, self._total_param_size=160677888, dwu_min_page_size=12288, self._block_size=53559296, self._chunk_size=26779648, self._shard_size=3347456
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
[0, 9, 34]
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([1427456]), new_param_packed_fragment.size()=torch.Size([1427456]), master_param_fragment.size()=torch.Size([1427456])
model_param_fragment.size()=torch.Size([1064960]), new_param_packed_fragment.size()=torch.Size([1064960]), master_param_fragment.size()=torch.Size([1064960])
model_param_fragment.size()=torch.Size([1920000]), new_param_packed_fragment.size()=torch.Size([1920000]), master_param_fragment.size()=torch.Size([1920000])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([1929280]), new_param_packed_fragment.size()=torch.Size([1929280]), master_param_fragment.size()=torch.Size([1929280])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([3308608]), new_param_packed_fragment.size()=torch.Size([3308608]), master_param_fragment.size()=torch.Size([3308608])
model_param_fragment.size()=torch.Size([2217984]), new_param_packed_fragment.size()=torch.Size([2217984]), master_param_fragment.size()=torch.Size([2217984])
model_param_fragment.size()=torch.Size([1409984]), new_param_packed_fragment.size()=torch.Size([1409984]), master_param_fragment.size()=torch.Size([1409984])
model_param_fragment.size()=torch.Size([32320]), new_param_packed_fragment.size()=torch.Size([32320]), master_param_fragment.size()=torch.Size([32320])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([331840]), new_param_packed_fragment.size()=torch.Size([331840]), master_param_fragment.size()=torch.Size([331840])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([1121280]), new_param_packed_fragment.size()=torch.Size([1121280]), master_param_fragment.size()=torch.Size([1121280])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
self._net_total_param_size=160671360, self._total_param_size=160677888, dwu_min_page_size=12288, self._block_size=53559296, self._chunk_size=26779648, self._shard_size=3347456
[0, 9, 34]
model_param_fragment.size()=torch.Size([628736]), new_param_packed_fragment.size()=torch.Size([628736]), master_param_fragment.size()=torch.Size([628736])
model_param_fragment.size()=torch.Size([3007424]), new_param_packed_fragment.size()=torch.Size([3007424]), master_param_fragment.size()=torch.Size([3007424])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
self._net_total_param_size=160671360, self._total_param_size=160677888, dwu_min_page_size=12288, self._block_size=53559296, self._chunk_size=26779648, self._shard_size=3347456
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
[0, 9, 34]
model_param_fragment.size()=torch.Size([1024]), new_param_packed_fragment.size()=torch.Size([1024]), master_param_fragment.size()=torch.Size([1024])
model_param_fragment.size()=torch.Size([1]), new_param_packed_fragment.size()=torch.Size([1]), master_param_fragment.size()=torch.Size([1])
model_param_fragment.size()=torch.Size([2169856]), new_param_packed_fragment.size()=torch.Size([2169856]), master_param_fragment.size()=torch.Size([2169856])
model_param_fragment.size()=torch.Size([1024]), new_param_packed_fragment.size()=torch.Size([1024]), master_param_fragment.size()=torch.Size([1024])
model_param_fragment.size()=torch.Size([1048576]), new_param_packed_fragment.size()=torch.Size([1048576]), master_param_fragment.size()=torch.Size([1048576])
model_param_fragment.size()=torch.Size([1177600]), new_param_packed_fragment.size()=torch.Size([1177600]), master_param_fragment.size()=torch.Size([1177600])
model_param_fragment.size()=torch.Size([1048576]), new_param_packed_fragment.size()=torch.Size([1048576]), master_param_fragment.size()=torch.Size([1048576])
model_param_fragment.size()=torch.Size([572416]), new_param_packed_fragment.size()=torch.Size([572416]), master_param_fragment.size()=torch.Size([572416])
model_param_fragment.size()=torch.Size([611264]), new_param_packed_fragment.size()=torch.Size([611264]), master_param_fragment.size()=torch.Size([611264])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([1130560]), new_param_packed_fragment.size()=torch.Size([1130560]), master_param_fragment.size()=torch.Size([1130560])
self._net_total_param_size=160671360, self._total_param_size=160677888, dwu_min_page_size=12288, self._block_size=53559296, self._chunk_size=26779648, self._shard_size=3347456
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([2766848]), new_param_packed_fragment.size()=torch.Size([2766848]), master_param_fragment.size()=torch.Size([2766848])
[0, 9, 34]
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([1082432]), new_param_packed_fragment.size()=torch.Size([1082432]), master_param_fragment.size()=torch.Size([1082432])
model_param_fragment.size()=torch.Size([2208704]), new_param_packed_fragment.size()=torch.Size([2208704]), master_param_fragment.size()=torch.Size([2208704])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([2265024]), new_param_packed_fragment.size()=torch.Size([2265024]), master_param_fragment.size()=torch.Size([2265024])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([2274304]), new_param_packed_fragment.size()=torch.Size([2274304]), master_param_fragment.size()=torch.Size([2274304])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([1064960]), new_param_packed_fragment.size()=torch.Size([1064960]), master_param_fragment.size()=torch.Size([1064960])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3308608]), new_param_packed_fragment.size()=torch.Size([3308608]), master_param_fragment.size()=torch.Size([3308608])
model_param_fragment.size()=torch.Size([32320]), new_param_packed_fragment.size()=torch.Size([32320]), master_param_fragment.size()=torch.Size([32320])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([2217984]), new_param_packed_fragment.size()=torch.Size([2217984]), master_param_fragment.size()=torch.Size([2217984])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([2169856]), new_param_packed_fragment.size()=torch.Size([2169856]), master_param_fragment.size()=torch.Size([2169856])
model_param_fragment.size()=torch.Size([1121280]), new_param_packed_fragment.size()=torch.Size([1121280]), master_param_fragment.size()=torch.Size([1121280])
model_param_fragment.size()=torch.Size([1177600]), new_param_packed_fragment.size()=torch.Size([1177600]), master_param_fragment.size()=torch.Size([1177600])
model_param_fragment.size()=torch.Size([2968576]), new_param_packed_fragment.size()=torch.Size([2968576]), master_param_fragment.size()=torch.Size([2968576])
model_param_fragment.size()=torch.Size([628736]), new_param_packed_fragment.size()=torch.Size([628736]), master_param_fragment.size()=torch.Size([628736])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([572416]), new_param_packed_fragment.size()=torch.Size([572416]), master_param_fragment.size()=torch.Size([572416])
model_param_fragment.size()=torch.Size([378880]), new_param_packed_fragment.size()=torch.Size([378880]), master_param_fragment.size()=torch.Size([378880])
model_param_fragment.size()=torch.Size([3308608]), new_param_packed_fragment.size()=torch.Size([3308608]), master_param_fragment.size()=torch.Size([3308608])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([32320]), new_param_packed_fragment.size()=torch.Size([32320]), master_param_fragment.size()=torch.Size([32320])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([1371136]), new_param_packed_fragment.size()=torch.Size([1371136]), master_param_fragment.size()=torch.Size([1371136])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([3129344]), new_param_packed_fragment.size()=torch.Size([3129344]), master_param_fragment.size()=torch.Size([3129344])
model_param_fragment.size()=torch.Size([218112]), new_param_packed_fragment.size()=torch.Size([218112]), master_param_fragment.size()=torch.Size([218112])
model_param_fragment.size()=torch.Size([1976320]), new_param_packed_fragment.size()=torch.Size([1976320]), master_param_fragment.size()=torch.Size([1976320])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([1024]), new_param_packed_fragment.size()=torch.Size([1024]), master_param_fragment.size()=torch.Size([1024])
model_param_fragment.size()=torch.Size([2766848]), new_param_packed_fragment.size()=torch.Size([2766848]), master_param_fragment.size()=torch.Size([2766848])
model_param_fragment.size()=torch.Size([1314816]), new_param_packed_fragment.size()=torch.Size([1314816]), master_param_fragment.size()=torch.Size([1314816])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
self._net_total_param_size=160671360, self._total_param_size=160677888, dwu_min_page_size=12288, self._block_size=53559296, self._chunk_size=26779648, self._shard_size=3347456
[0, 9, 34]
model_param_fragment.size()=torch.Size([1082432]), new_param_packed_fragment.size()=torch.Size([1082432]), master_param_fragment.size()=torch.Size([1082432])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([1]), new_param_packed_fragment.size()=torch.Size([1]), master_param_fragment.size()=torch.Size([1])
model_param_fragment.size()=torch.Size([1024]), new_param_packed_fragment.size()=torch.Size([1024]), master_param_fragment.size()=torch.Size([1024])
model_param_fragment.size()=torch.Size([2265024]), new_param_packed_fragment.size()=torch.Size([2265024]), master_param_fragment.size()=torch.Size([2265024])
model_param_fragment.size()=torch.Size([283712]), new_param_packed_fragment.size()=torch.Size([283712]), master_param_fragment.size()=torch.Size([283712])
model_param_fragment.size()=torch.Size([1048576]), new_param_packed_fragment.size()=torch.Size([1048576]), master_param_fragment.size()=torch.Size([1048576])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3063744]), new_param_packed_fragment.size()=torch.Size([3063744]), master_param_fragment.size()=torch.Size([3063744])
model_param_fragment.size()=torch.Size([2024448]), new_param_packed_fragment.size()=torch.Size([2024448]), master_param_fragment.size()=torch.Size([2024448])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([2832448]), new_param_packed_fragment.size()=torch.Size([2832448]), master_param_fragment.size()=torch.Size([2832448])
model_param_fragment.size()=torch.Size([515008]), new_param_packed_fragment.size()=torch.Size([515008]), master_param_fragment.size()=torch.Size([515008])
self._net_total_param_size=160671360, self._total_param_size=160677888, dwu_min_page_size=12288, self._block_size=53559296, self._chunk_size=26779648, self._shard_size=3347456
[0, 9, 34]
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([2217984]), new_param_packed_fragment.size()=torch.Size([2217984]), master_param_fragment.size()=torch.Size([2217984])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([1121280]), new_param_packed_fragment.size()=torch.Size([1121280]), master_param_fragment.size()=torch.Size([1121280])
model_param_fragment.size()=torch.Size([628736]), new_param_packed_fragment.size()=torch.Size([628736]), master_param_fragment.size()=torch.Size([628736])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([1024]), new_param_packed_fragment.size()=torch.Size([1024]), master_param_fragment.size()=torch.Size([1024])
model_param_fragment.size()=torch.Size([1]), new_param_packed_fragment.size()=torch.Size([1]), master_param_fragment.size()=torch.Size([1])
model_param_fragment.size()=torch.Size([1024]), new_param_packed_fragment.size()=torch.Size([1024]), master_param_fragment.size()=torch.Size([1024])
model_param_fragment.size()=torch.Size([1048576]), new_param_packed_fragment.size()=torch.Size([1048576]), master_param_fragment.size()=torch.Size([1048576])
model_param_fragment.size()=torch.Size([467968]), new_param_packed_fragment.size()=torch.Size([467968]), master_param_fragment.size()=torch.Size([467968])
model_param_fragment.size()=torch.Size([2879488]), new_param_packed_fragment.size()=torch.Size([2879488]), master_param_fragment.size()=torch.Size([2879488])
model_param_fragment.size()=torch.Size([3073024]), new_param_packed_fragment.size()=torch.Size([3073024]), master_param_fragment.size()=torch.Size([3073024])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([1048576]), new_param_packed_fragment.size()=torch.Size([1048576]), master_param_fragment.size()=torch.Size([1048576])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([274432]), new_param_packed_fragment.size()=torch.Size([274432]), master_param_fragment.size()=torch.Size([274432])
model_param_fragment.size()=torch.Size([611264]), new_param_packed_fragment.size()=torch.Size([611264]), master_param_fragment.size()=torch.Size([611264])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([1130560]), new_param_packed_fragment.size()=torch.Size([1130560]), master_param_fragment.size()=torch.Size([1130560])
self._net_total_param_size=160671360, self._total_param_size=160677888, dwu_min_page_size=12288, self._block_size=53559296, self._chunk_size=26779648, self._shard_size=3347456
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
[0, 9, 34]
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([2208704]), new_param_packed_fragment.size()=torch.Size([2208704]), master_param_fragment.size()=torch.Size([2208704])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([2968576]), new_param_packed_fragment.size()=torch.Size([2968576]), master_param_fragment.size()=torch.Size([2968576])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([1314816]), new_param_packed_fragment.size()=torch.Size([1314816]), master_param_fragment.size()=torch.Size([1314816])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([2217984]), new_param_packed_fragment.size()=torch.Size([2217984]), master_param_fragment.size()=torch.Size([2217984])
model_param_fragment.size()=torch.Size([378880]), new_param_packed_fragment.size()=torch.Size([378880]), master_param_fragment.size()=torch.Size([378880])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([2217984]), new_param_packed_fragment.size()=torch.Size([2217984]), master_param_fragment.size()=torch.Size([2217984])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([1121280]), new_param_packed_fragment.size()=torch.Size([1121280]), master_param_fragment.size()=torch.Size([1121280])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([2024448]), new_param_packed_fragment.size()=torch.Size([2024448]), master_param_fragment.size()=torch.Size([2024448])
model_param_fragment.size()=torch.Size([2968576]), new_param_packed_fragment.size()=torch.Size([2968576]), master_param_fragment.size()=torch.Size([2968576])
model_param_fragment.size()=torch.Size([1371136]), new_param_packed_fragment.size()=torch.Size([1371136]), master_param_fragment.size()=torch.Size([1371136])
model_param_fragment.size()=torch.Size([1976320]), new_param_packed_fragment.size()=torch.Size([1976320]), master_param_fragment.size()=torch.Size([1976320])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([378880]), new_param_packed_fragment.size()=torch.Size([378880]), master_param_fragment.size()=torch.Size([378880])
model_param_fragment.size()=torch.Size([1121280]), new_param_packed_fragment.size()=torch.Size([1121280]), master_param_fragment.size()=torch.Size([1121280])
model_param_fragment.size()=torch.Size([628736]), new_param_packed_fragment.size()=torch.Size([628736]), master_param_fragment.size()=torch.Size([628736])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([628736]), new_param_packed_fragment.size()=torch.Size([628736]), master_param_fragment.size()=torch.Size([628736])
model_param_fragment.size()=torch.Size([1024]), new_param_packed_fragment.size()=torch.Size([1024]), master_param_fragment.size()=torch.Size([1024])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([1371136]), new_param_packed_fragment.size()=torch.Size([1371136]), master_param_fragment.size()=torch.Size([1371136])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([2832448]), new_param_packed_fragment.size()=torch.Size([2832448]), master_param_fragment.size()=torch.Size([2832448])
model_param_fragment.size()=torch.Size([283712]), new_param_packed_fragment.size()=torch.Size([283712]), master_param_fragment.size()=torch.Size([283712])
model_param_fragment.size()=torch.Size([1976320]), new_param_packed_fragment.size()=torch.Size([1976320]), master_param_fragment.size()=torch.Size([1976320])
model_param_fragment.size()=torch.Size([3063744]), new_param_packed_fragment.size()=torch.Size([3063744]), master_param_fragment.size()=torch.Size([3063744])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([515008]), new_param_packed_fragment.size()=torch.Size([515008]), master_param_fragment.size()=torch.Size([515008])
model_param_fragment.size()=torch.Size([1024]), new_param_packed_fragment.size()=torch.Size([1024]), master_param_fragment.size()=torch.Size([1024])
model_param_fragment.size()=torch.Size([1]), new_param_packed_fragment.size()=torch.Size([1]), master_param_fragment.size()=torch.Size([1])
model_param_fragment.size()=torch.Size([3129344]), new_param_packed_fragment.size()=torch.Size([3129344]), master_param_fragment.size()=torch.Size([3129344])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([1]), new_param_packed_fragment.size()=torch.Size([1]), master_param_fragment.size()=torch.Size([1])
model_param_fragment.size()=torch.Size([1024]), new_param_packed_fragment.size()=torch.Size([1024]), master_param_fragment.size()=torch.Size([1024])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([1048576]), new_param_packed_fragment.size()=torch.Size([1048576]), master_param_fragment.size()=torch.Size([1048576])
model_param_fragment.size()=torch.Size([1048576]), new_param_packed_fragment.size()=torch.Size([1048576]), master_param_fragment.size()=torch.Size([1048576])
model_param_fragment.size()=torch.Size([218112]), new_param_packed_fragment.size()=torch.Size([218112]), master_param_fragment.size()=torch.Size([218112])
model_param_fragment.size()=torch.Size([611264]), new_param_packed_fragment.size()=torch.Size([611264]), master_param_fragment.size()=torch.Size([611264])
model_param_fragment.size()=torch.Size([283712]), new_param_packed_fragment.size()=torch.Size([283712]), master_param_fragment.size()=torch.Size([283712])
model_param_fragment.size()=torch.Size([1024]), new_param_packed_fragment.size()=torch.Size([1024]), master_param_fragment.size()=torch.Size([1024])
model_param_fragment.size()=torch.Size([1048576]), new_param_packed_fragment.size()=torch.Size([1048576]), master_param_fragment.size()=torch.Size([1048576])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3063744]), new_param_packed_fragment.size()=torch.Size([3063744]), master_param_fragment.size()=torch.Size([3063744])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([1048576]), new_param_packed_fragment.size()=torch.Size([1048576]), master_param_fragment.size()=torch.Size([1048576])
model_param_fragment.size()=torch.Size([611264]), new_param_packed_fragment.size()=torch.Size([611264]), master_param_fragment.size()=torch.Size([611264])
model_param_fragment.size()=torch.Size([1130560]), new_param_packed_fragment.size()=torch.Size([1130560]), master_param_fragment.size()=torch.Size([1130560])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([1130560]), new_param_packed_fragment.size()=torch.Size([1130560]), master_param_fragment.size()=torch.Size([1130560])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([2208704]), new_param_packed_fragment.size()=torch.Size([2208704]), master_param_fragment.size()=torch.Size([2208704])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([2208704]), new_param_packed_fragment.size()=torch.Size([2208704]), master_param_fragment.size()=torch.Size([2208704])
model_param_fragment.size()=torch.Size([1314816]), new_param_packed_fragment.size()=torch.Size([1314816]), master_param_fragment.size()=torch.Size([1314816])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([2024448]), new_param_packed_fragment.size()=torch.Size([2024448]), master_param_fragment.size()=torch.Size([2024448])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([467968]), new_param_packed_fragment.size()=torch.Size([467968]), master_param_fragment.size()=torch.Size([467968])
self._net_total_param_size=160671360, self._total_param_size=160677888, dwu_min_page_size=12288, self._block_size=53559296, self._chunk_size=26779648, self._shard_size=3347456
model_param_fragment.size()=torch.Size([2832448]), new_param_packed_fragment.size()=torch.Size([2832448]), master_param_fragment.size()=torch.Size([2832448])
model_param_fragment.size()=torch.Size([2879488]), new_param_packed_fragment.size()=torch.Size([2879488]), master_param_fragment.size()=torch.Size([2879488])
model_param_fragment.size()=torch.Size([515008]), new_param_packed_fragment.size()=torch.Size([515008]), master_param_fragment.size()=torch.Size([515008])
[0, 9, 34]
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3073024]), new_param_packed_fragment.size()=torch.Size([3073024]), master_param_fragment.size()=torch.Size([3073024])
model_param_fragment.size()=torch.Size([274432]), new_param_packed_fragment.size()=torch.Size([274432]), master_param_fragment.size()=torch.Size([274432])
model_param_fragment.size()=torch.Size([2274304]), new_param_packed_fragment.size()=torch.Size([2274304]), master_param_fragment.size()=torch.Size([2274304])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([1064960]), new_param_packed_fragment.size()=torch.Size([1064960]), master_param_fragment.size()=torch.Size([1064960])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3308608]), new_param_packed_fragment.size()=torch.Size([3308608]), master_param_fragment.size()=torch.Size([3308608])
model_param_fragment.size()=torch.Size([32320]), new_param_packed_fragment.size()=torch.Size([32320]), master_param_fragment.size()=torch.Size([32320])
self._net_total_param_size=160671360, self._total_param_size=160677888, dwu_min_page_size=12288, self._block_size=53559296, self._chunk_size=26779648, self._shard_size=3347456
[0, 9, 34]
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([2169856]), new_param_packed_fragment.size()=torch.Size([2169856]), master_param_fragment.size()=torch.Size([2169856])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([1177600]), new_param_packed_fragment.size()=torch.Size([1177600]), master_param_fragment.size()=torch.Size([1177600])
model_param_fragment.size()=torch.Size([572416]), new_param_packed_fragment.size()=torch.Size([572416]), master_param_fragment.size()=torch.Size([572416])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([2766848]), new_param_packed_fragment.size()=torch.Size([2766848]), master_param_fragment.size()=torch.Size([2766848])
model_param_fragment.size()=torch.Size([467968]), new_param_packed_fragment.size()=torch.Size([467968]), master_param_fragment.size()=torch.Size([467968])
model_param_fragment.size()=torch.Size([1082432]), new_param_packed_fragment.size()=torch.Size([1082432]), master_param_fragment.size()=torch.Size([1082432])
model_param_fragment.size()=torch.Size([2879488]), new_param_packed_fragment.size()=torch.Size([2879488]), master_param_fragment.size()=torch.Size([2879488])
model_param_fragment.size()=torch.Size([2217984]), new_param_packed_fragment.size()=torch.Size([2217984]), master_param_fragment.size()=torch.Size([2217984])
model_param_fragment.size()=torch.Size([2265024]), new_param_packed_fragment.size()=torch.Size([2265024]), master_param_fragment.size()=torch.Size([2265024])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([3073024]), new_param_packed_fragment.size()=torch.Size([3073024]), master_param_fragment.size()=torch.Size([3073024])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([1121280]), new_param_packed_fragment.size()=torch.Size([1121280]), master_param_fragment.size()=torch.Size([1121280])
model_param_fragment.size()=torch.Size([274432]), new_param_packed_fragment.size()=torch.Size([274432]), master_param_fragment.size()=torch.Size([274432])
model_param_fragment.size()=torch.Size([628736]), new_param_packed_fragment.size()=torch.Size([628736]), master_param_fragment.size()=torch.Size([628736])
model_param_fragment.size()=torch.Size([2274304]), new_param_packed_fragment.size()=torch.Size([2274304]), master_param_fragment.size()=torch.Size([2274304])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([1314816]), new_param_packed_fragment.size()=torch.Size([1314816]), master_param_fragment.size()=torch.Size([1314816])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([1064960]), new_param_packed_fragment.size()=torch.Size([1064960]), master_param_fragment.size()=torch.Size([1064960])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([1024]), new_param_packed_fragment.size()=torch.Size([1024]), master_param_fragment.size()=torch.Size([1024])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([1]), new_param_packed_fragment.size()=torch.Size([1]), master_param_fragment.size()=torch.Size([1])
model_param_fragment.size()=torch.Size([1024]), new_param_packed_fragment.size()=torch.Size([1024]), master_param_fragment.size()=torch.Size([1024])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([1048576]), new_param_packed_fragment.size()=torch.Size([1048576]), master_param_fragment.size()=torch.Size([1048576])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([1048576]), new_param_packed_fragment.size()=torch.Size([1048576]), master_param_fragment.size()=torch.Size([1048576])
model_param_fragment.size()=torch.Size([2024448]), new_param_packed_fragment.size()=torch.Size([2024448]), master_param_fragment.size()=torch.Size([2024448])
model_param_fragment.size()=torch.Size([3308608]), new_param_packed_fragment.size()=torch.Size([3308608]), master_param_fragment.size()=torch.Size([3308608])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([32320]), new_param_packed_fragment.size()=torch.Size([32320]), master_param_fragment.size()=torch.Size([32320])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([611264]), new_param_packed_fragment.size()=torch.Size([611264]), master_param_fragment.size()=torch.Size([611264])
model_param_fragment.size()=torch.Size([2274304]), new_param_packed_fragment.size()=torch.Size([2274304]), master_param_fragment.size()=torch.Size([2274304])
model_param_fragment.size()=torch.Size([1130560]), new_param_packed_fragment.size()=torch.Size([1130560]), master_param_fragment.size()=torch.Size([1130560])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([2832448]), new_param_packed_fragment.size()=torch.Size([2832448]), master_param_fragment.size()=torch.Size([2832448])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([515008]), new_param_packed_fragment.size()=torch.Size([515008]), master_param_fragment.size()=torch.Size([515008])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([2208704]), new_param_packed_fragment.size()=torch.Size([2208704]), master_param_fragment.size()=torch.Size([2208704])
model_param_fragment.size()=torch.Size([1064960]), new_param_packed_fragment.size()=torch.Size([1064960]), master_param_fragment.size()=torch.Size([1064960])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3308608]), new_param_packed_fragment.size()=torch.Size([3308608]), master_param_fragment.size()=torch.Size([3308608])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([32320]), new_param_packed_fragment.size()=torch.Size([32320]), master_param_fragment.size()=torch.Size([32320])
model_param_fragment.size()=torch.Size([2274304]), new_param_packed_fragment.size()=torch.Size([2274304]), master_param_fragment.size()=torch.Size([2274304])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([1064960]), new_param_packed_fragment.size()=torch.Size([1064960]), master_param_fragment.size()=torch.Size([1064960])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3308608]), new_param_packed_fragment.size()=torch.Size([3308608]), master_param_fragment.size()=torch.Size([3308608])
model_param_fragment.size()=torch.Size([32320]), new_param_packed_fragment.size()=torch.Size([32320]), master_param_fragment.size()=torch.Size([32320])
model_param_fragment.size()=torch.Size([2274304]), new_param_packed_fragment.size()=torch.Size([2274304]), master_param_fragment.size()=torch.Size([2274304])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([2968576]), new_param_packed_fragment.size()=torch.Size([2968576]), master_param_fragment.size()=torch.Size([2968576])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([1064960]), new_param_packed_fragment.size()=torch.Size([1064960]), master_param_fragment.size()=torch.Size([1064960])
model_param_fragment.size()=torch.Size([378880]), new_param_packed_fragment.size()=torch.Size([378880]), master_param_fragment.size()=torch.Size([378880])
model_param_fragment.size()=torch.Size([1371136]), new_param_packed_fragment.size()=torch.Size([1371136]), master_param_fragment.size()=torch.Size([1371136])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([1048576]), new_param_packed_fragment.size()=torch.Size([1048576]), master_param_fragment.size()=torch.Size([1048576])
model_param_fragment.size()=torch.Size([1976320]), new_param_packed_fragment.size()=torch.Size([1976320]), master_param_fragment.size()=torch.Size([1976320])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([611264]), new_param_packed_fragment.size()=torch.Size([611264]), master_param_fragment.size()=torch.Size([611264])
model_param_fragment.size()=torch.Size([1130560]), new_param_packed_fragment.size()=torch.Size([1130560]), master_param_fragment.size()=torch.Size([1130560])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([2208704]), new_param_packed_fragment.size()=torch.Size([2208704]), master_param_fragment.size()=torch.Size([2208704])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([2169856]), new_param_packed_fragment.size()=torch.Size([2169856]), master_param_fragment.size()=torch.Size([2169856])
model_param_fragment.size()=torch.Size([1177600]), new_param_packed_fragment.size()=torch.Size([1177600]), master_param_fragment.size()=torch.Size([1177600])
model_param_fragment.size()=torch.Size([283712]), new_param_packed_fragment.size()=torch.Size([283712]), master_param_fragment.size()=torch.Size([283712])
model_param_fragment.size()=torch.Size([3308608]), new_param_packed_fragment.size()=torch.Size([3308608]), master_param_fragment.size()=torch.Size([3308608])
model_param_fragment.size()=torch.Size([572416]), new_param_packed_fragment.size()=torch.Size([572416]), master_param_fragment.size()=torch.Size([572416])
model_param_fragment.size()=torch.Size([32320]), new_param_packed_fragment.size()=torch.Size([32320]), master_param_fragment.size()=torch.Size([32320])
model_param_fragment.size()=torch.Size([3063744]), new_param_packed_fragment.size()=torch.Size([3063744]), master_param_fragment.size()=torch.Size([3063744])
model_param_fragment.size()=torch.Size([1314816]), new_param_packed_fragment.size()=torch.Size([1314816]), master_param_fragment.size()=torch.Size([1314816])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([2766848]), new_param_packed_fragment.size()=torch.Size([2766848]), master_param_fragment.size()=torch.Size([2766848])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([2024448]), new_param_packed_fragment.size()=torch.Size([2024448]), master_param_fragment.size()=torch.Size([2024448])
model_param_fragment.size()=torch.Size([1082432]), new_param_packed_fragment.size()=torch.Size([1082432]), master_param_fragment.size()=torch.Size([1082432])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([2265024]), new_param_packed_fragment.size()=torch.Size([2265024]), master_param_fragment.size()=torch.Size([2265024])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([2832448]), new_param_packed_fragment.size()=torch.Size([2832448]), master_param_fragment.size()=torch.Size([2832448])
model_param_fragment.size()=torch.Size([515008]), new_param_packed_fragment.size()=torch.Size([515008]), master_param_fragment.size()=torch.Size([515008])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([2274304]), new_param_packed_fragment.size()=torch.Size([2274304]), master_param_fragment.size()=torch.Size([2274304])
model_param_fragment.size()=torch.Size([467968]), new_param_packed_fragment.size()=torch.Size([467968]), master_param_fragment.size()=torch.Size([467968])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([2879488]), new_param_packed_fragment.size()=torch.Size([2879488]), master_param_fragment.size()=torch.Size([2879488])
model_param_fragment.size()=torch.Size([1064960]), new_param_packed_fragment.size()=torch.Size([1064960]), master_param_fragment.size()=torch.Size([1064960])
model_param_fragment.size()=torch.Size([3073024]), new_param_packed_fragment.size()=torch.Size([3073024]), master_param_fragment.size()=torch.Size([3073024])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([274432]), new_param_packed_fragment.size()=torch.Size([274432]), master_param_fragment.size()=torch.Size([274432])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3308608]), new_param_packed_fragment.size()=torch.Size([3308608]), master_param_fragment.size()=torch.Size([3308608])
model_param_fragment.size()=torch.Size([32320]), new_param_packed_fragment.size()=torch.Size([32320]), master_param_fragment.size()=torch.Size([32320])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([467968]), new_param_packed_fragment.size()=torch.Size([467968]), master_param_fragment.size()=torch.Size([467968])
model_param_fragment.size()=torch.Size([2879488]), new_param_packed_fragment.size()=torch.Size([2879488]), master_param_fragment.size()=torch.Size([2879488])
model_param_fragment.size()=torch.Size([2968576]), new_param_packed_fragment.size()=torch.Size([2968576]), master_param_fragment.size()=torch.Size([2968576])
model_param_fragment.size()=torch.Size([378880]), new_param_packed_fragment.size()=torch.Size([378880]), master_param_fragment.size()=torch.Size([378880])
model_param_fragment.size()=torch.Size([3073024]), new_param_packed_fragment.size()=torch.Size([3073024]), master_param_fragment.size()=torch.Size([3073024])
model_param_fragment.size()=torch.Size([274432]), new_param_packed_fragment.size()=torch.Size([274432]), master_param_fragment.size()=torch.Size([274432])
model_param_fragment.size()=torch.Size([1371136]), new_param_packed_fragment.size()=torch.Size([1371136]), master_param_fragment.size()=torch.Size([1371136])
model_param_fragment.size()=torch.Size([1976320]), new_param_packed_fragment.size()=torch.Size([1976320]), master_param_fragment.size()=torch.Size([1976320])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([283712]), new_param_packed_fragment.size()=torch.Size([283712]), master_param_fragment.size()=torch.Size([283712])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3063744]), new_param_packed_fragment.size()=torch.Size([3063744]), master_param_fragment.size()=torch.Size([3063744])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3129344]), new_param_packed_fragment.size()=torch.Size([3129344]), master_param_fragment.size()=torch.Size([3129344])
model_param_fragment.size()=torch.Size([218112]), new_param_packed_fragment.size()=torch.Size([218112]), master_param_fragment.size()=torch.Size([218112])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([2217984]), new_param_packed_fragment.size()=torch.Size([2217984]), master_param_fragment.size()=torch.Size([2217984])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([1121280]), new_param_packed_fragment.size()=torch.Size([1121280]), master_param_fragment.size()=torch.Size([1121280])
model_param_fragment.size()=torch.Size([628736]), new_param_packed_fragment.size()=torch.Size([628736]), master_param_fragment.size()=torch.Size([628736])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([1024]), new_param_packed_fragment.size()=torch.Size([1024]), master_param_fragment.size()=torch.Size([1024])
model_param_fragment.size()=torch.Size([1]), new_param_packed_fragment.size()=torch.Size([1]), master_param_fragment.size()=torch.Size([1])
model_param_fragment.size()=torch.Size([1024]), new_param_packed_fragment.size()=torch.Size([1024]), master_param_fragment.size()=torch.Size([1024])
model_param_fragment.size()=torch.Size([1048576]), new_param_packed_fragment.size()=torch.Size([1048576]), master_param_fragment.size()=torch.Size([1048576])
self._net_total_param_size=160671360, self._total_param_size=160677888, dwu_min_page_size=12288, self._block_size=53559296, self._chunk_size=26779648, self._shard_size=3347456
model_param_fragment.size()=torch.Size([1048576]), new_param_packed_fragment.size()=torch.Size([1048576]), master_param_fragment.size()=torch.Size([1048576])
[0, 9, 34]
model_param_fragment.size()=torch.Size([611264]), new_param_packed_fragment.size()=torch.Size([611264]), master_param_fragment.size()=torch.Size([611264])
model_param_fragment.size()=torch.Size([1130560]), new_param_packed_fragment.size()=torch.Size([1130560]), master_param_fragment.size()=torch.Size([1130560])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([2208704]), new_param_packed_fragment.size()=torch.Size([2208704]), master_param_fragment.size()=torch.Size([2208704])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
self._net_total_param_size=160671360, self._total_param_size=160677888, dwu_min_page_size=12288, self._block_size=53559296, self._chunk_size=26779648, self._shard_size=3347456
[0, 9, 34]
model_param_fragment.size()=torch.Size([467968]), new_param_packed_fragment.size()=torch.Size([467968]), master_param_fragment.size()=torch.Size([467968])
model_param_fragment.size()=torch.Size([2879488]), new_param_packed_fragment.size()=torch.Size([2879488]), master_param_fragment.size()=torch.Size([2879488])
model_param_fragment.size()=torch.Size([3073024]), new_param_packed_fragment.size()=torch.Size([3073024]), master_param_fragment.size()=torch.Size([3073024])
model_param_fragment.size()=torch.Size([274432]), new_param_packed_fragment.size()=torch.Size([274432]), master_param_fragment.size()=torch.Size([274432])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([2968576]), new_param_packed_fragment.size()=torch.Size([2968576]), master_param_fragment.size()=torch.Size([2968576])
model_param_fragment.size()=torch.Size([378880]), new_param_packed_fragment.size()=torch.Size([378880]), master_param_fragment.size()=torch.Size([378880])
model_param_fragment.size()=torch.Size([1371136]), new_param_packed_fragment.size()=torch.Size([1371136]), master_param_fragment.size()=torch.Size([1371136])
model_param_fragment.size()=torch.Size([1976320]), new_param_packed_fragment.size()=torch.Size([1976320]), master_param_fragment.size()=torch.Size([1976320])
model_param_fragment.size()=torch.Size([1314816]), new_param_packed_fragment.size()=torch.Size([1314816]), master_param_fragment.size()=torch.Size([1314816])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([283712]), new_param_packed_fragment.size()=torch.Size([283712]), master_param_fragment.size()=torch.Size([283712])
model_param_fragment.size()=torch.Size([2024448]), new_param_packed_fragment.size()=torch.Size([2024448]), master_param_fragment.size()=torch.Size([2024448])
model_param_fragment.size()=torch.Size([3063744]), new_param_packed_fragment.size()=torch.Size([3063744]), master_param_fragment.size()=torch.Size([3063744])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([2832448]), new_param_packed_fragment.size()=torch.Size([2832448]), master_param_fragment.size()=torch.Size([2832448])
model_param_fragment.size()=torch.Size([515008]), new_param_packed_fragment.size()=torch.Size([515008]), master_param_fragment.size()=torch.Size([515008])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([2968576]), new_param_packed_fragment.size()=torch.Size([2968576]), master_param_fragment.size()=torch.Size([2968576])
model_param_fragment.size()=torch.Size([378880]), new_param_packed_fragment.size()=torch.Size([378880]), master_param_fragment.size()=torch.Size([378880])
model_param_fragment.size()=torch.Size([1371136]), new_param_packed_fragment.size()=torch.Size([1371136]), master_param_fragment.size()=torch.Size([1371136])
model_param_fragment.size()=torch.Size([1976320]), new_param_packed_fragment.size()=torch.Size([1976320]), master_param_fragment.size()=torch.Size([1976320])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([283712]), new_param_packed_fragment.size()=torch.Size([283712]), master_param_fragment.size()=torch.Size([283712])
model_param_fragment.size()=torch.Size([3063744]), new_param_packed_fragment.size()=torch.Size([3063744]), master_param_fragment.size()=torch.Size([3063744])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([2274304]), new_param_packed_fragment.size()=torch.Size([2274304]), master_param_fragment.size()=torch.Size([2274304])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([1064960]), new_param_packed_fragment.size()=torch.Size([1064960]), master_param_fragment.size()=torch.Size([1064960])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3308608]), new_param_packed_fragment.size()=torch.Size([3308608]), master_param_fragment.size()=torch.Size([3308608])
model_param_fragment.size()=torch.Size([2274304]), new_param_packed_fragment.size()=torch.Size([2274304]), master_param_fragment.size()=torch.Size([2274304])
model_param_fragment.size()=torch.Size([32320]), new_param_packed_fragment.size()=torch.Size([32320]), master_param_fragment.size()=torch.Size([32320])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([1064960]), new_param_packed_fragment.size()=torch.Size([1064960]), master_param_fragment.size()=torch.Size([1064960])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3308608]), new_param_packed_fragment.size()=torch.Size([3308608]), master_param_fragment.size()=torch.Size([3308608])
self._net_total_param_size=160671360, self._total_param_size=160677888, dwu_min_page_size=12288, self._block_size=53559296, self._chunk_size=26779648, self._shard_size=3347456
model_param_fragment.size()=torch.Size([32320]), new_param_packed_fragment.size()=torch.Size([32320]), master_param_fragment.size()=torch.Size([32320])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
[0, 9, 34]
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([2169856]), new_param_packed_fragment.size()=torch.Size([2169856]), master_param_fragment.size()=torch.Size([2169856])
model_param_fragment.size()=torch.Size([1177600]), new_param_packed_fragment.size()=torch.Size([1177600]), master_param_fragment.size()=torch.Size([1177600])
model_param_fragment.size()=torch.Size([572416]), new_param_packed_fragment.size()=torch.Size([572416]), master_param_fragment.size()=torch.Size([572416])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([1314816]), new_param_packed_fragment.size()=torch.Size([1314816]), master_param_fragment.size()=torch.Size([1314816])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([2766848]), new_param_packed_fragment.size()=torch.Size([2766848]), master_param_fragment.size()=torch.Size([2766848])
model_param_fragment.size()=torch.Size([2024448]), new_param_packed_fragment.size()=torch.Size([2024448]), master_param_fragment.size()=torch.Size([2024448])
model_param_fragment.size()=torch.Size([1082432]), new_param_packed_fragment.size()=torch.Size([1082432]), master_param_fragment.size()=torch.Size([1082432])
model_param_fragment.size()=torch.Size([2265024]), new_param_packed_fragment.size()=torch.Size([2265024]), master_param_fragment.size()=torch.Size([2265024])
model_param_fragment.size()=torch.Size([2169856]), new_param_packed_fragment.size()=torch.Size([2169856]), master_param_fragment.size()=torch.Size([2169856])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([1177600]), new_param_packed_fragment.size()=torch.Size([1177600]), master_param_fragment.size()=torch.Size([1177600])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([572416]), new_param_packed_fragment.size()=torch.Size([572416]), master_param_fragment.size()=torch.Size([572416])
model_param_fragment.size()=torch.Size([2832448]), new_param_packed_fragment.size()=torch.Size([2832448]), master_param_fragment.size()=torch.Size([2832448])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([515008]), new_param_packed_fragment.size()=torch.Size([515008]), master_param_fragment.size()=torch.Size([515008])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([3016704]), new_param_packed_fragment.size()=torch.Size([3016704]), master_param_fragment.size()=torch.Size([3016704])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([2766848]), new_param_packed_fragment.size()=torch.Size([2766848]), master_param_fragment.size()=torch.Size([2766848])
model_param_fragment.size()=torch.Size([1082432]), new_param_packed_fragment.size()=torch.Size([1082432]), master_param_fragment.size()=torch.Size([1082432])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([2265024]), new_param_packed_fragment.size()=torch.Size([2265024]), master_param_fragment.size()=torch.Size([2265024])
model_param_fragment.size()=torch.Size([322560]), new_param_packed_fragment.size()=torch.Size([322560]), master_param_fragment.size()=torch.Size([322560])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([2274304]), new_param_packed_fragment.size()=torch.Size([2274304]), master_param_fragment.size()=torch.Size([2274304])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([1427456]), new_param_packed_fragment.size()=torch.Size([1427456]), master_param_fragment.size()=torch.Size([1427456])
model_param_fragment.size()=torch.Size([1920000]), new_param_packed_fragment.size()=torch.Size([1920000]), master_param_fragment.size()=torch.Size([1920000])
model_param_fragment.size()=torch.Size([1929280]), new_param_packed_fragment.size()=torch.Size([1929280]), master_param_fragment.size()=torch.Size([1929280])
model_param_fragment.size()=torch.Size([1064960]), new_param_packed_fragment.size()=torch.Size([1064960]), master_param_fragment.size()=torch.Size([1064960])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([467968]), new_param_packed_fragment.size()=torch.Size([467968]), master_param_fragment.size()=torch.Size([467968])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([1409984]), new_param_packed_fragment.size()=torch.Size([1409984]), master_param_fragment.size()=torch.Size([1409984])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([2879488]), new_param_packed_fragment.size()=torch.Size([2879488]), master_param_fragment.size()=torch.Size([2879488])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3073024]), new_param_packed_fragment.size()=torch.Size([3073024]), master_param_fragment.size()=torch.Size([3073024])
model_param_fragment.size()=torch.Size([3308608]), new_param_packed_fragment.size()=torch.Size([3308608]), master_param_fragment.size()=torch.Size([3308608])
model_param_fragment.size()=torch.Size([331840]), new_param_packed_fragment.size()=torch.Size([331840]), master_param_fragment.size()=torch.Size([331840])
model_param_fragment.size()=torch.Size([274432]), new_param_packed_fragment.size()=torch.Size([274432]), master_param_fragment.size()=torch.Size([274432])
model_param_fragment.size()=torch.Size([32320]), new_param_packed_fragment.size()=torch.Size([32320]), master_param_fragment.size()=torch.Size([32320])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([3007424]), new_param_packed_fragment.size()=torch.Size([3007424]), master_param_fragment.size()=torch.Size([3007424])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([2274304]), new_param_packed_fragment.size()=torch.Size([2274304]), master_param_fragment.size()=torch.Size([2274304])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([1064960]), new_param_packed_fragment.size()=torch.Size([1064960]), master_param_fragment.size()=torch.Size([1064960])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3308608]), new_param_packed_fragment.size()=torch.Size([3308608]), master_param_fragment.size()=torch.Size([3308608])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([32320]), new_param_packed_fragment.size()=torch.Size([32320]), master_param_fragment.size()=torch.Size([32320])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([2968576]), new_param_packed_fragment.size()=torch.Size([2968576]), master_param_fragment.size()=torch.Size([2968576])
model_param_fragment.size()=torch.Size([378880]), new_param_packed_fragment.size()=torch.Size([378880]), master_param_fragment.size()=torch.Size([378880])
model_param_fragment.size()=torch.Size([1371136]), new_param_packed_fragment.size()=torch.Size([1371136]), master_param_fragment.size()=torch.Size([1371136])
model_param_fragment.size()=torch.Size([1976320]), new_param_packed_fragment.size()=torch.Size([1976320]), master_param_fragment.size()=torch.Size([1976320])
model_param_fragment.size()=torch.Size([467968]), new_param_packed_fragment.size()=torch.Size([467968]), master_param_fragment.size()=torch.Size([467968])
model_param_fragment.size()=torch.Size([2879488]), new_param_packed_fragment.size()=torch.Size([2879488]), master_param_fragment.size()=torch.Size([2879488])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3073024]), new_param_packed_fragment.size()=torch.Size([3073024]), master_param_fragment.size()=torch.Size([3073024])
model_param_fragment.size()=torch.Size([283712]), new_param_packed_fragment.size()=torch.Size([283712]), master_param_fragment.size()=torch.Size([283712])
model_param_fragment.size()=torch.Size([274432]), new_param_packed_fragment.size()=torch.Size([274432]), master_param_fragment.size()=torch.Size([274432])
model_param_fragment.size()=torch.Size([3063744]), new_param_packed_fragment.size()=torch.Size([3063744]), master_param_fragment.size()=torch.Size([3063744])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([2274304]), new_param_packed_fragment.size()=torch.Size([2274304]), master_param_fragment.size()=torch.Size([2274304])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([1064960]), new_param_packed_fragment.size()=torch.Size([1064960]), master_param_fragment.size()=torch.Size([1064960])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3308608]), new_param_packed_fragment.size()=torch.Size([3308608]), master_param_fragment.size()=torch.Size([3308608])
model_param_fragment.size()=torch.Size([32320]), new_param_packed_fragment.size()=torch.Size([32320]), master_param_fragment.size()=torch.Size([32320])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([2169856]), new_param_packed_fragment.size()=torch.Size([2169856]), master_param_fragment.size()=torch.Size([2169856])
model_param_fragment.size()=torch.Size([1177600]), new_param_packed_fragment.size()=torch.Size([1177600]), master_param_fragment.size()=torch.Size([1177600])
model_param_fragment.size()=torch.Size([572416]), new_param_packed_fragment.size()=torch.Size([572416]), master_param_fragment.size()=torch.Size([572416])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([2766848]), new_param_packed_fragment.size()=torch.Size([2766848]), master_param_fragment.size()=torch.Size([2766848])
model_param_fragment.size()=torch.Size([1082432]), new_param_packed_fragment.size()=torch.Size([1082432]), master_param_fragment.size()=torch.Size([1082432])
model_param_fragment.size()=torch.Size([2265024]), new_param_packed_fragment.size()=torch.Size([2265024]), master_param_fragment.size()=torch.Size([2265024])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
self._net_total_param_size=160671360, self._total_param_size=160677888, dwu_min_page_size=12288, self._block_size=53559296, self._chunk_size=26779648, self._shard_size=3347456
[0, 9, 34]
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([467968]), new_param_packed_fragment.size()=torch.Size([467968]), master_param_fragment.size()=torch.Size([467968])
model_param_fragment.size()=torch.Size([2879488]), new_param_packed_fragment.size()=torch.Size([2879488]), master_param_fragment.size()=torch.Size([2879488])
model_param_fragment.size()=torch.Size([3073024]), new_param_packed_fragment.size()=torch.Size([3073024]), master_param_fragment.size()=torch.Size([3073024])
model_param_fragment.size()=torch.Size([274432]), new_param_packed_fragment.size()=torch.Size([274432]), master_param_fragment.size()=torch.Size([274432])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
self._net_total_param_size=160671360, self._total_param_size=160677888, dwu_min_page_size=12288, self._block_size=53559296, self._chunk_size=26779648, self._shard_size=3347456
[0, 9, 34]
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3016704]), new_param_packed_fragment.size()=torch.Size([3016704]), master_param_fragment.size()=torch.Size([3016704])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([322560]), new_param_packed_fragment.size()=torch.Size([322560]), master_param_fragment.size()=torch.Size([322560])
model_param_fragment.size()=torch.Size([467968]), new_param_packed_fragment.size()=torch.Size([467968]), master_param_fragment.size()=torch.Size([467968])
model_param_fragment.size()=torch.Size([1427456]), new_param_packed_fragment.size()=torch.Size([1427456]), master_param_fragment.size()=torch.Size([1427456])
model_param_fragment.size()=torch.Size([2879488]), new_param_packed_fragment.size()=torch.Size([2879488]), master_param_fragment.size()=torch.Size([2879488])
model_param_fragment.size()=torch.Size([1920000]), new_param_packed_fragment.size()=torch.Size([1920000]), master_param_fragment.size()=torch.Size([1920000])
model_param_fragment.size()=torch.Size([1929280]), new_param_packed_fragment.size()=torch.Size([1929280]), master_param_fragment.size()=torch.Size([1929280])
model_param_fragment.size()=torch.Size([3073024]), new_param_packed_fragment.size()=torch.Size([3073024]), master_param_fragment.size()=torch.Size([3073024])
model_param_fragment.size()=torch.Size([274432]), new_param_packed_fragment.size()=torch.Size([274432]), master_param_fragment.size()=torch.Size([274432])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([1409984]), new_param_packed_fragment.size()=torch.Size([1409984]), master_param_fragment.size()=torch.Size([1409984])
model_param_fragment.size()=torch.Size([331840]), new_param_packed_fragment.size()=torch.Size([331840]), master_param_fragment.size()=torch.Size([331840])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([3007424]), new_param_packed_fragment.size()=torch.Size([3007424]), master_param_fragment.size()=torch.Size([3007424])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3016704]), new_param_packed_fragment.size()=torch.Size([3016704]), master_param_fragment.size()=torch.Size([3016704])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([322560]), new_param_packed_fragment.size()=torch.Size([322560]), master_param_fragment.size()=torch.Size([322560])
model_param_fragment.size()=torch.Size([1427456]), new_param_packed_fragment.size()=torch.Size([1427456]), master_param_fragment.size()=torch.Size([1427456])
model_param_fragment.size()=torch.Size([1920000]), new_param_packed_fragment.size()=torch.Size([1920000]), master_param_fragment.size()=torch.Size([1920000])
model_param_fragment.size()=torch.Size([1929280]), new_param_packed_fragment.size()=torch.Size([1929280]), master_param_fragment.size()=torch.Size([1929280])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([1409984]), new_param_packed_fragment.size()=torch.Size([1409984]), master_param_fragment.size()=torch.Size([1409984])
model_param_fragment.size()=torch.Size([331840]), new_param_packed_fragment.size()=torch.Size([331840]), master_param_fragment.size()=torch.Size([331840])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([3007424]), new_param_packed_fragment.size()=torch.Size([3007424]), master_param_fragment.size()=torch.Size([3007424])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([1314816]), new_param_packed_fragment.size()=torch.Size([1314816]), master_param_fragment.size()=torch.Size([1314816])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([1314816]), new_param_packed_fragment.size()=torch.Size([1314816]), master_param_fragment.size()=torch.Size([1314816])
model_param_fragment.size()=torch.Size([2024448]), new_param_packed_fragment.size()=torch.Size([2024448]), master_param_fragment.size()=torch.Size([2024448])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([2024448]), new_param_packed_fragment.size()=torch.Size([2024448]), master_param_fragment.size()=torch.Size([2024448])
model_param_fragment.size()=torch.Size([2832448]), new_param_packed_fragment.size()=torch.Size([2832448]), master_param_fragment.size()=torch.Size([2832448])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([515008]), new_param_packed_fragment.size()=torch.Size([515008]), master_param_fragment.size()=torch.Size([515008])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([2832448]), new_param_packed_fragment.size()=torch.Size([2832448]), master_param_fragment.size()=torch.Size([2832448])
model_param_fragment.size()=torch.Size([515008]), new_param_packed_fragment.size()=torch.Size([515008]), master_param_fragment.size()=torch.Size([515008])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([2217984]), new_param_packed_fragment.size()=torch.Size([2217984]), master_param_fragment.size()=torch.Size([2217984])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([1121280]), new_param_packed_fragment.size()=torch.Size([1121280]), master_param_fragment.size()=torch.Size([1121280])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([628736]), new_param_packed_fragment.size()=torch.Size([628736]), master_param_fragment.size()=torch.Size([628736])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([2274304]), new_param_packed_fragment.size()=torch.Size([2274304]), master_param_fragment.size()=torch.Size([2274304])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([1024]), new_param_packed_fragment.size()=torch.Size([1024]), master_param_fragment.size()=torch.Size([1024])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([1]), new_param_packed_fragment.size()=torch.Size([1]), master_param_fragment.size()=torch.Size([1])
model_param_fragment.size()=torch.Size([1064960]), new_param_packed_fragment.size()=torch.Size([1064960]), master_param_fragment.size()=torch.Size([1064960])
model_param_fragment.size()=torch.Size([1024]), new_param_packed_fragment.size()=torch.Size([1024]), master_param_fragment.size()=torch.Size([1024])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([1048576]), new_param_packed_fragment.size()=torch.Size([1048576]), master_param_fragment.size()=torch.Size([1048576])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([1048576]), new_param_packed_fragment.size()=torch.Size([1048576]), master_param_fragment.size()=torch.Size([1048576])
model_param_fragment.size()=torch.Size([611264]), new_param_packed_fragment.size()=torch.Size([611264]), master_param_fragment.size()=torch.Size([611264])
model_param_fragment.size()=torch.Size([1130560]), new_param_packed_fragment.size()=torch.Size([1130560]), master_param_fragment.size()=torch.Size([1130560])
model_param_fragment.size()=torch.Size([3308608]), new_param_packed_fragment.size()=torch.Size([3308608]), master_param_fragment.size()=torch.Size([3308608])
model_param_fragment.size()=torch.Size([32320]), new_param_packed_fragment.size()=torch.Size([32320]), master_param_fragment.size()=torch.Size([32320])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([2208704]), new_param_packed_fragment.size()=torch.Size([2208704]), master_param_fragment.size()=torch.Size([2208704])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([2968576]), new_param_packed_fragment.size()=torch.Size([2968576]), master_param_fragment.size()=torch.Size([2968576])
model_param_fragment.size()=torch.Size([378880]), new_param_packed_fragment.size()=torch.Size([378880]), master_param_fragment.size()=torch.Size([378880])
model_param_fragment.size()=torch.Size([2274304]), new_param_packed_fragment.size()=torch.Size([2274304]), master_param_fragment.size()=torch.Size([2274304])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([1371136]), new_param_packed_fragment.size()=torch.Size([1371136]), master_param_fragment.size()=torch.Size([1371136])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([1976320]), new_param_packed_fragment.size()=torch.Size([1976320]), master_param_fragment.size()=torch.Size([1976320])
model_param_fragment.size()=torch.Size([1064960]), new_param_packed_fragment.size()=torch.Size([1064960]), master_param_fragment.size()=torch.Size([1064960])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([283712]), new_param_packed_fragment.size()=torch.Size([283712]), master_param_fragment.size()=torch.Size([283712])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3063744]), new_param_packed_fragment.size()=torch.Size([3063744]), master_param_fragment.size()=torch.Size([3063744])
model_param_fragment.size()=torch.Size([3308608]), new_param_packed_fragment.size()=torch.Size([3308608]), master_param_fragment.size()=torch.Size([3308608])
model_param_fragment.size()=torch.Size([3129344]), new_param_packed_fragment.size()=torch.Size([3129344]), master_param_fragment.size()=torch.Size([3129344])
model_param_fragment.size()=torch.Size([32320]), new_param_packed_fragment.size()=torch.Size([32320]), master_param_fragment.size()=torch.Size([32320])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([218112]), new_param_packed_fragment.size()=torch.Size([218112]), master_param_fragment.size()=torch.Size([218112])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
self._net_total_param_size=160671360, self._total_param_size=160677888, dwu_min_page_size=12288, self._block_size=53559296, self._chunk_size=26779648, self._shard_size=3347456
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
[0, 9, 34]
self._net_total_param_size=160671360, self._total_param_size=160677888, dwu_min_page_size=12288, self._block_size=53559296, self._chunk_size=26779648, self._shard_size=3347456
[0, 9, 34]
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([467968]), new_param_packed_fragment.size()=torch.Size([467968]), master_param_fragment.size()=torch.Size([467968])
model_param_fragment.size()=torch.Size([2879488]), new_param_packed_fragment.size()=torch.Size([2879488]), master_param_fragment.size()=torch.Size([2879488])
model_param_fragment.size()=torch.Size([3073024]), new_param_packed_fragment.size()=torch.Size([3073024]), master_param_fragment.size()=torch.Size([3073024])
model_param_fragment.size()=torch.Size([274432]), new_param_packed_fragment.size()=torch.Size([274432]), master_param_fragment.size()=torch.Size([274432])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3129344]), new_param_packed_fragment.size()=torch.Size([3129344]), master_param_fragment.size()=torch.Size([3129344])
model_param_fragment.size()=torch.Size([218112]), new_param_packed_fragment.size()=torch.Size([218112]), master_param_fragment.size()=torch.Size([218112])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([2217984]), new_param_packed_fragment.size()=torch.Size([2217984]), master_param_fragment.size()=torch.Size([2217984])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([1121280]), new_param_packed_fragment.size()=torch.Size([1121280]), master_param_fragment.size()=torch.Size([1121280])
model_param_fragment.size()=torch.Size([628736]), new_param_packed_fragment.size()=torch.Size([628736]), master_param_fragment.size()=torch.Size([628736])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([1024]), new_param_packed_fragment.size()=torch.Size([1024]), master_param_fragment.size()=torch.Size([1024])
model_param_fragment.size()=torch.Size([1]), new_param_packed_fragment.size()=torch.Size([1]), master_param_fragment.size()=torch.Size([1])
model_param_fragment.size()=torch.Size([1024]), new_param_packed_fragment.size()=torch.Size([1024]), master_param_fragment.size()=torch.Size([1024])
model_param_fragment.size()=torch.Size([1048576]), new_param_packed_fragment.size()=torch.Size([1048576]), master_param_fragment.size()=torch.Size([1048576])
model_param_fragment.size()=torch.Size([1048576]), new_param_packed_fragment.size()=torch.Size([1048576]), master_param_fragment.size()=torch.Size([1048576])
model_param_fragment.size()=torch.Size([611264]), new_param_packed_fragment.size()=torch.Size([611264]), master_param_fragment.size()=torch.Size([611264])
model_param_fragment.size()=torch.Size([1130560]), new_param_packed_fragment.size()=torch.Size([1130560]), master_param_fragment.size()=torch.Size([1130560])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([2208704]), new_param_packed_fragment.size()=torch.Size([2208704]), master_param_fragment.size()=torch.Size([2208704])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([2968576]), new_param_packed_fragment.size()=torch.Size([2968576]), master_param_fragment.size()=torch.Size([2968576])
model_param_fragment.size()=torch.Size([378880]), new_param_packed_fragment.size()=torch.Size([378880]), master_param_fragment.size()=torch.Size([378880])
model_param_fragment.size()=torch.Size([1371136]), new_param_packed_fragment.size()=torch.Size([1371136]), master_param_fragment.size()=torch.Size([1371136])
model_param_fragment.size()=torch.Size([1976320]), new_param_packed_fragment.size()=torch.Size([1976320]), master_param_fragment.size()=torch.Size([1976320])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([283712]), new_param_packed_fragment.size()=torch.Size([283712]), master_param_fragment.size()=torch.Size([283712])
model_param_fragment.size()=torch.Size([3063744]), new_param_packed_fragment.size()=torch.Size([3063744]), master_param_fragment.size()=torch.Size([3063744])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
self._net_total_param_size=160671360, self._total_param_size=160677888, dwu_min_page_size=12288, self._block_size=53559296, self._chunk_size=26779648, self._shard_size=3347456
[0, 9, 34]
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([2968576]), new_param_packed_fragment.size()=torch.Size([2968576]), master_param_fragment.size()=torch.Size([2968576])
model_param_fragment.size()=torch.Size([378880]), new_param_packed_fragment.size()=torch.Size([378880]), master_param_fragment.size()=torch.Size([378880])
model_param_fragment.size()=torch.Size([1371136]), new_param_packed_fragment.size()=torch.Size([1371136]), master_param_fragment.size()=torch.Size([1371136])
model_param_fragment.size()=torch.Size([1976320]), new_param_packed_fragment.size()=torch.Size([1976320]), master_param_fragment.size()=torch.Size([1976320])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([283712]), new_param_packed_fragment.size()=torch.Size([283712]), master_param_fragment.size()=torch.Size([283712])
model_param_fragment.size()=torch.Size([3063744]), new_param_packed_fragment.size()=torch.Size([3063744]), master_param_fragment.size()=torch.Size([3063744])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([1314816]), new_param_packed_fragment.size()=torch.Size([1314816]), master_param_fragment.size()=torch.Size([1314816])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([2024448]), new_param_packed_fragment.size()=torch.Size([2024448]), master_param_fragment.size()=torch.Size([2024448])
self._net_total_param_size=160671360, self._total_param_size=160677888, dwu_min_page_size=12288, self._block_size=53559296, self._chunk_size=26779648, self._shard_size=3347456
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
[0, 9, 34]
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([2832448]), new_param_packed_fragment.size()=torch.Size([2832448]), master_param_fragment.size()=torch.Size([2832448])
model_param_fragment.size()=torch.Size([515008]), new_param_packed_fragment.size()=torch.Size([515008]), master_param_fragment.size()=torch.Size([515008])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
self._net_total_param_size=160671360, self._total_param_size=160677888, dwu_min_page_size=12288, self._block_size=53559296, self._chunk_size=26779648, self._shard_size=3347456
[0, 9, 34]
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([2169856]), new_param_packed_fragment.size()=torch.Size([2169856]), master_param_fragment.size()=torch.Size([2169856])
model_param_fragment.size()=torch.Size([1177600]), new_param_packed_fragment.size()=torch.Size([1177600]), master_param_fragment.size()=torch.Size([1177600])
model_param_fragment.size()=torch.Size([572416]), new_param_packed_fragment.size()=torch.Size([572416]), master_param_fragment.size()=torch.Size([572416])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([2766848]), new_param_packed_fragment.size()=torch.Size([2766848]), master_param_fragment.size()=torch.Size([2766848])
model_param_fragment.size()=torch.Size([1082432]), new_param_packed_fragment.size()=torch.Size([1082432]), master_param_fragment.size()=torch.Size([1082432])
model_param_fragment.size()=torch.Size([2265024]), new_param_packed_fragment.size()=torch.Size([2265024]), master_param_fragment.size()=torch.Size([2265024])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([1314816]), new_param_packed_fragment.size()=torch.Size([1314816]), master_param_fragment.size()=torch.Size([1314816])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([2024448]), new_param_packed_fragment.size()=torch.Size([2024448]), master_param_fragment.size()=torch.Size([2024448])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([2832448]), new_param_packed_fragment.size()=torch.Size([2832448]), master_param_fragment.size()=torch.Size([2832448])
model_param_fragment.size()=torch.Size([515008]), new_param_packed_fragment.size()=torch.Size([515008]), master_param_fragment.size()=torch.Size([515008])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
self._net_total_param_size=160671360, self._total_param_size=160677888, dwu_min_page_size=12288, self._block_size=53559296, self._chunk_size=26779648, self._shard_size=3347456
[0, 9, 34]
self._net_total_param_size=160671360, self._total_param_size=160677888, dwu_min_page_size=12288, self._block_size=53559296, self._chunk_size=26779648, self._shard_size=3347456
[0, 9, 34]
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([2274304]), new_param_packed_fragment.size()=torch.Size([2274304]), master_param_fragment.size()=torch.Size([2274304])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([1064960]), new_param_packed_fragment.size()=torch.Size([1064960]), master_param_fragment.size()=torch.Size([1064960])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3308608]), new_param_packed_fragment.size()=torch.Size([3308608]), master_param_fragment.size()=torch.Size([3308608])
model_param_fragment.size()=torch.Size([32320]), new_param_packed_fragment.size()=torch.Size([32320]), master_param_fragment.size()=torch.Size([32320])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3016704]), new_param_packed_fragment.size()=torch.Size([3016704]), master_param_fragment.size()=torch.Size([3016704])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([322560]), new_param_packed_fragment.size()=torch.Size([322560]), master_param_fragment.size()=torch.Size([322560])
model_param_fragment.size()=torch.Size([1427456]), new_param_packed_fragment.size()=torch.Size([1427456]), master_param_fragment.size()=torch.Size([1427456])
model_param_fragment.size()=torch.Size([1920000]), new_param_packed_fragment.size()=torch.Size([1920000]), master_param_fragment.size()=torch.Size([1920000])
model_param_fragment.size()=torch.Size([1929280]), new_param_packed_fragment.size()=torch.Size([1929280]), master_param_fragment.size()=torch.Size([1929280])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([1409984]), new_param_packed_fragment.size()=torch.Size([1409984]), master_param_fragment.size()=torch.Size([1409984])
model_param_fragment.size()=torch.Size([331840]), new_param_packed_fragment.size()=torch.Size([331840]), master_param_fragment.size()=torch.Size([331840])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([3007424]), new_param_packed_fragment.size()=torch.Size([3007424]), master_param_fragment.size()=torch.Size([3007424])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([2274304]), new_param_packed_fragment.size()=torch.Size([2274304]), master_param_fragment.size()=torch.Size([2274304])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([1064960]), new_param_packed_fragment.size()=torch.Size([1064960]), master_param_fragment.size()=torch.Size([1064960])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3308608]), new_param_packed_fragment.size()=torch.Size([3308608]), master_param_fragment.size()=torch.Size([3308608])
model_param_fragment.size()=torch.Size([32320]), new_param_packed_fragment.size()=torch.Size([32320]), master_param_fragment.size()=torch.Size([32320])
NCCL version 2.7.5+cuda11.0
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3129344]), new_param_packed_fragment.size()=torch.Size([3129344]), master_param_fragment.size()=torch.Size([3129344])
model_param_fragment.size()=torch.Size([218112]), new_param_packed_fragment.size()=torch.Size([218112]), master_param_fragment.size()=torch.Size([218112])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3016704]), new_param_packed_fragment.size()=torch.Size([3016704]), master_param_fragment.size()=torch.Size([3016704])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([322560]), new_param_packed_fragment.size()=torch.Size([322560]), master_param_fragment.size()=torch.Size([322560])
model_param_fragment.size()=torch.Size([1427456]), new_param_packed_fragment.size()=torch.Size([1427456]), master_param_fragment.size()=torch.Size([1427456])
model_param_fragment.size()=torch.Size([1920000]), new_param_packed_fragment.size()=torch.Size([1920000]), master_param_fragment.size()=torch.Size([1920000])
model_param_fragment.size()=torch.Size([1929280]), new_param_packed_fragment.size()=torch.Size([1929280]), master_param_fragment.size()=torch.Size([1929280])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([1409984]), new_param_packed_fragment.size()=torch.Size([1409984]), master_param_fragment.size()=torch.Size([1409984])
model_param_fragment.size()=torch.Size([331840]), new_param_packed_fragment.size()=torch.Size([331840]), master_param_fragment.size()=torch.Size([331840])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([3007424]), new_param_packed_fragment.size()=torch.Size([3007424]), master_param_fragment.size()=torch.Size([3007424])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
self._net_total_param_size=160671360, self._total_param_size=160677888, dwu_min_page_size=12288, self._block_size=53559296, self._chunk_size=26779648, self._shard_size=3347456
[0, 9, 34]
self._net_total_param_size=160671360, self._total_param_size=160677888, dwu_min_page_size=12288, self._block_size=53559296, self._chunk_size=26779648, self._shard_size=3347456
[0, 9, 34]
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([2274304]), new_param_packed_fragment.size()=torch.Size([2274304]), master_param_fragment.size()=torch.Size([2274304])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([1064960]), new_param_packed_fragment.size()=torch.Size([1064960]), master_param_fragment.size()=torch.Size([1064960])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3308608]), new_param_packed_fragment.size()=torch.Size([3308608]), master_param_fragment.size()=torch.Size([3308608])
model_param_fragment.size()=torch.Size([32320]), new_param_packed_fragment.size()=torch.Size([32320]), master_param_fragment.size()=torch.Size([32320])
self._net_total_param_size=160671360, self._total_param_size=160677888, dwu_min_page_size=12288, self._block_size=53559296, self._chunk_size=26779648, self._shard_size=3347456
[0, 9, 34]
self._net_total_param_size=160671360, self._total_param_size=160677888, dwu_min_page_size=12288, self._block_size=53559296, self._chunk_size=26779648, self._shard_size=3347456
[0, 9, 34]
self._net_total_param_size=160671360, self._total_param_size=160677888, dwu_min_page_size=12288, self._block_size=53559296, self._chunk_size=26779648, self._shard_size=3347456
[0, 9, 34]
self._net_total_param_size=160671360, self._total_param_size=160677888, dwu_min_page_size=12288, self._block_size=53559296, self._chunk_size=26779648, self._shard_size=3347456
[0, 9, 34]
self._net_total_param_size=160671360, self._total_param_size=160677888, dwu_min_page_size=12288, self._block_size=53559296, self._chunk_size=26779648, self._shard_size=3347456
[0, 9, 34]
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3016704]), new_param_packed_fragment.size()=torch.Size([3016704]), master_param_fragment.size()=torch.Size([3016704])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([322560]), new_param_packed_fragment.size()=torch.Size([322560]), master_param_fragment.size()=torch.Size([322560])
model_param_fragment.size()=torch.Size([1427456]), new_param_packed_fragment.size()=torch.Size([1427456]), master_param_fragment.size()=torch.Size([1427456])
model_param_fragment.size()=torch.Size([1920000]), new_param_packed_fragment.size()=torch.Size([1920000]), master_param_fragment.size()=torch.Size([1920000])
model_param_fragment.size()=torch.Size([1929280]), new_param_packed_fragment.size()=torch.Size([1929280]), master_param_fragment.size()=torch.Size([1929280])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([1409984]), new_param_packed_fragment.size()=torch.Size([1409984]), master_param_fragment.size()=torch.Size([1409984])
model_param_fragment.size()=torch.Size([331840]), new_param_packed_fragment.size()=torch.Size([331840]), master_param_fragment.size()=torch.Size([331840])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([3007424]), new_param_packed_fragment.size()=torch.Size([3007424]), master_param_fragment.size()=torch.Size([3007424])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([2217984]), new_param_packed_fragment.size()=torch.Size([2217984]), master_param_fragment.size()=torch.Size([2217984])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([1121280]), new_param_packed_fragment.size()=torch.Size([1121280]), master_param_fragment.size()=torch.Size([1121280])
model_param_fragment.size()=torch.Size([628736]), new_param_packed_fragment.size()=torch.Size([628736]), master_param_fragment.size()=torch.Size([628736])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([1024]), new_param_packed_fragment.size()=torch.Size([1024]), master_param_fragment.size()=torch.Size([1024])
model_param_fragment.size()=torch.Size([1]), new_param_packed_fragment.size()=torch.Size([1]), master_param_fragment.size()=torch.Size([1])
model_param_fragment.size()=torch.Size([1024]), new_param_packed_fragment.size()=torch.Size([1024]), master_param_fragment.size()=torch.Size([1024])
model_param_fragment.size()=torch.Size([1048576]), new_param_packed_fragment.size()=torch.Size([1048576]), master_param_fragment.size()=torch.Size([1048576])
model_param_fragment.size()=torch.Size([1048576]), new_param_packed_fragment.size()=torch.Size([1048576]), master_param_fragment.size()=torch.Size([1048576])
model_param_fragment.size()=torch.Size([611264]), new_param_packed_fragment.size()=torch.Size([611264]), master_param_fragment.size()=torch.Size([611264])
model_param_fragment.size()=torch.Size([1130560]), new_param_packed_fragment.size()=torch.Size([1130560]), master_param_fragment.size()=torch.Size([1130560])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([2208704]), new_param_packed_fragment.size()=torch.Size([2208704]), master_param_fragment.size()=torch.Size([2208704])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([2169856]), new_param_packed_fragment.size()=torch.Size([2169856]), master_param_fragment.size()=torch.Size([2169856])
model_param_fragment.size()=torch.Size([1177600]), new_param_packed_fragment.size()=torch.Size([1177600]), master_param_fragment.size()=torch.Size([1177600])
model_param_fragment.size()=torch.Size([572416]), new_param_packed_fragment.size()=torch.Size([572416]), master_param_fragment.size()=torch.Size([572416])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([2766848]), new_param_packed_fragment.size()=torch.Size([2766848]), master_param_fragment.size()=torch.Size([2766848])
model_param_fragment.size()=torch.Size([1082432]), new_param_packed_fragment.size()=torch.Size([1082432]), master_param_fragment.size()=torch.Size([1082432])
model_param_fragment.size()=torch.Size([2265024]), new_param_packed_fragment.size()=torch.Size([2265024]), master_param_fragment.size()=torch.Size([2265024])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([467968]), new_param_packed_fragment.size()=torch.Size([467968]), master_param_fragment.size()=torch.Size([467968])
model_param_fragment.size()=torch.Size([2879488]), new_param_packed_fragment.size()=torch.Size([2879488]), master_param_fragment.size()=torch.Size([2879488])
model_param_fragment.size()=torch.Size([3073024]), new_param_packed_fragment.size()=torch.Size([3073024]), master_param_fragment.size()=torch.Size([3073024])
model_param_fragment.size()=torch.Size([274432]), new_param_packed_fragment.size()=torch.Size([274432]), master_param_fragment.size()=torch.Size([274432])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([2968576]), new_param_packed_fragment.size()=torch.Size([2968576]), master_param_fragment.size()=torch.Size([2968576])
model_param_fragment.size()=torch.Size([378880]), new_param_packed_fragment.size()=torch.Size([378880]), master_param_fragment.size()=torch.Size([378880])
model_param_fragment.size()=torch.Size([1371136]), new_param_packed_fragment.size()=torch.Size([1371136]), master_param_fragment.size()=torch.Size([1371136])
model_param_fragment.size()=torch.Size([467968]), new_param_packed_fragment.size()=torch.Size([467968]), master_param_fragment.size()=torch.Size([467968])
model_param_fragment.size()=torch.Size([1976320]), new_param_packed_fragment.size()=torch.Size([1976320]), master_param_fragment.size()=torch.Size([1976320])
model_param_fragment.size()=torch.Size([2879488]), new_param_packed_fragment.size()=torch.Size([2879488]), master_param_fragment.size()=torch.Size([2879488])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3073024]), new_param_packed_fragment.size()=torch.Size([3073024]), master_param_fragment.size()=torch.Size([3073024])
model_param_fragment.size()=torch.Size([274432]), new_param_packed_fragment.size()=torch.Size([274432]), master_param_fragment.size()=torch.Size([274432])
model_param_fragment.size()=torch.Size([283712]), new_param_packed_fragment.size()=torch.Size([283712]), master_param_fragment.size()=torch.Size([283712])
model_param_fragment.size()=torch.Size([3063744]), new_param_packed_fragment.size()=torch.Size([3063744]), master_param_fragment.size()=torch.Size([3063744])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
self._net_total_param_size=160671360, self._total_param_size=160677888, dwu_min_page_size=12288, self._block_size=53559296, self._chunk_size=26779648, self._shard_size=3347456
[0, 9, 34]
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([2217984]), new_param_packed_fragment.size()=torch.Size([2217984]), master_param_fragment.size()=torch.Size([2217984])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([1121280]), new_param_packed_fragment.size()=torch.Size([1121280]), master_param_fragment.size()=torch.Size([1121280])
model_param_fragment.size()=torch.Size([628736]), new_param_packed_fragment.size()=torch.Size([628736]), master_param_fragment.size()=torch.Size([628736])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([1024]), new_param_packed_fragment.size()=torch.Size([1024]), master_param_fragment.size()=torch.Size([1024])
model_param_fragment.size()=torch.Size([1]), new_param_packed_fragment.size()=torch.Size([1]), master_param_fragment.size()=torch.Size([1])
model_param_fragment.size()=torch.Size([1024]), new_param_packed_fragment.size()=torch.Size([1024]), master_param_fragment.size()=torch.Size([1024])
model_param_fragment.size()=torch.Size([1048576]), new_param_packed_fragment.size()=torch.Size([1048576]), master_param_fragment.size()=torch.Size([1048576])
model_param_fragment.size()=torch.Size([1048576]), new_param_packed_fragment.size()=torch.Size([1048576]), master_param_fragment.size()=torch.Size([1048576])
model_param_fragment.size()=torch.Size([611264]), new_param_packed_fragment.size()=torch.Size([611264]), master_param_fragment.size()=torch.Size([611264])
model_param_fragment.size()=torch.Size([1130560]), new_param_packed_fragment.size()=torch.Size([1130560]), master_param_fragment.size()=torch.Size([1130560])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([2208704]), new_param_packed_fragment.size()=torch.Size([2208704]), master_param_fragment.size()=torch.Size([2208704])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
NCCL version 2.7.5+cuda11.0
NCCL version 2.7.5+cuda11.0
NCCL version 2.7.5+cuda11.0
NCCL version 2.7.5+cuda11.0
NCCL version 2.7.5+cuda11.0
NCCL version 2.7.5+cuda11.0
NCCL version 2.7.5+cuda11.0
NCCL version 2.7.5+cuda11.0
NCCL version 2.7.5+cuda11.0
NCCL version 2.7.5+cuda11.0
NCCL version 2.7.5+cuda11.0
NCCL version 2.7.5+cuda11.0
NCCL version 2.7.5+cuda11.0
NCCL version 2.7.5+cuda11.0
NCCL version 2.7.5+cuda11.0
NCCL version 2.7.5+cuda11.0
NCCL version 2.7.5+cuda11.0
NCCL version 2.7.5+cuda11.0
NCCL version 2.7.5+cuda11.0
NCCL version 2.7.5+cuda11.0
NCCL version 2.7.5+cuda11.0
NCCL version 2.7.5+cuda11.0
NCCL version 2.7.5+cuda11.0
NCCL version 2.7.5+cuda11.0
NCCL version 2.7.5+cuda11.0
NCCL version 2.7.5+cuda11.0
NCCL version 2.7.5+cuda11.0
NCCL version 2.7.5+cuda11.0
NCCL version 2.7.5+cuda11.0
NCCL version 2.7.5+cuda11.0
NCCL version 2.7.5+cuda11.0
0: Using optimizer: DistributedFusedAdam (
Parameter Group 0
    betas: (0.9, 0.999)
    bias_correction: True
    eps: 1e-08
    lr: 0.004
    max_grad_norm: 5.0
    weight_decay: 0.0
)
:::MLLOG {"namespace": "", "time_ms": 1592471637118, "event_type": "POINT_IN_TIME", "key": "opt_name", "value": "adam", "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/trainer.py", "lineno": 176}}
:::MLLOG {"namespace": "", "time_ms": 1592471637118, "event_type": "POINT_IN_TIME", "key": "opt_base_learning_rate", "value": 0.004, "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/trainer.py", "lineno": 178}}
:::MLLOG {"namespace": "", "time_ms": 1592471637118, "event_type": "POINT_IN_TIME", "key": "opt_adam_beta_1", "value": 0.9, "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/trainer.py", "lineno": 180}}
:::MLLOG {"namespace": "", "time_ms": 1592471637118, "event_type": "POINT_IN_TIME", "key": "opt_adam_beta_2", "value": 0.999, "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/trainer.py", "lineno": 182}}
:::MLLOG {"namespace": "", "time_ms": 1592471637119, "event_type": "POINT_IN_TIME", "key": "opt_adam_epsilon", "value": 1e-08, "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/trainer.py", "lineno": 184}}
0: Executing preallocation
:::MLLOG {"namespace": "", "time_ms": 1592471638688, "event_type": "INTERVAL_END", "key": "init_stop", "value": null, "metadata": {"file": "train.py", "lineno": 429}}
:::MLLOG {"namespace": "", "time_ms": 1592471638689, "event_type": "INTERVAL_START", "key": "run_start", "value": null, "metadata": {"file": "train.py", "lineno": 430}}
:::MLLOG {"namespace": "", "time_ms": 1592471638689, "event_type": "POINT_IN_TIME", "key": "max_sequence_length", "value": 75, "metadata": {"file": "train.py", "lineno": 435, "method": "discard"}}
0: Reading preprocessed data file from /preproc_data/75/training.bin
0: Preprocessed data: length: 3975116 min length: 0 max length: 75
0: Opening preprocessed data /preproc_data/75/training.bin for reading
0: Processing data from /data/newstest2014.tok.bpe.32000.en
0: Filtering data, min len: 0, max len: 150
0: Pairs before: 3003, after: 3003
:::MLLOG {"namespace": "", "time_ms": 1592471638940, "event_type": "POINT_IN_TIME", "key": "global_batch_size", "value": 8192, "metadata": {"file": "train.py", "lineno": 480}}
:::MLLOG {"namespace": "", "time_ms": 1592471638941, "event_type": "POINT_IN_TIME", "key": "train_samples", "value": 3964928, "metadata": {"file": "train.py", "lineno": 488}}
:::MLLOG {"namespace": "", "time_ms": 1592471638941, "event_type": "POINT_IN_TIME", "key": "eval_samples", "value": 3003, "metadata": {"file": "train.py", "lineno": 490}}
0: Training LR schedule config: {'warmup_steps': 200, 'remain_steps': 2259, 'decay_interval': 283, 'decay_steps': 4, 'decay_factor': 0.5}
0: Scheduler warmup steps: 200
0: Scheduler remain steps: 2259
0: Scheduler decay interval: 283
0: Scheduler decay factor: 0.5
0: Scheduler max decay steps: 4
:::MLLOG {"namespace": "", "time_ms": 1592471638942, "event_type": "POINT_IN_TIME", "key": "opt_learning_rate_alt_decay_func", "value": true, "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/lr_scheduler.py", "lineno": 78}}
:::MLLOG {"namespace": "", "time_ms": 1592471638942, "event_type": "POINT_IN_TIME", "key": "opt_learning_rate_alt_warmup_func", "value": true, "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/lr_scheduler.py", "lineno": 80}}
:::MLLOG {"namespace": "", "time_ms": 1592471638942, "event_type": "POINT_IN_TIME", "key": "opt_learning_rate_decay_interval", "value": 283, "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/lr_scheduler.py", "lineno": 83}}
:::MLLOG {"namespace": "", "time_ms": 1592471638942, "event_type": "POINT_IN_TIME", "key": "opt_learning_rate_decay_factor", "value": 0.5, "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/lr_scheduler.py", "lineno": 85}}
:::MLLOG {"namespace": "", "time_ms": 1592471638942, "event_type": "POINT_IN_TIME", "key": "opt_learning_rate_decay_steps", "value": 4, "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/lr_scheduler.py", "lineno": 87}}
:::MLLOG {"namespace": "", "time_ms": 1592471638942, "event_type": "POINT_IN_TIME", "key": "opt_learning_rate_remain_steps", "value": 2259, "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/lr_scheduler.py", "lineno": 89}}
:::MLLOG {"namespace": "", "time_ms": 1592471638942, "event_type": "POINT_IN_TIME", "key": "opt_learning_rate_warmup_steps", "value": 200, "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/lr_scheduler.py", "lineno": 91}}
:::MLLOG {"namespace": "", "time_ms": 1592471638943, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "train.py", "lineno": 538, "first_epoch_num": 1, "epoch_count": 1}}
:::MLLOG {"namespace": "", "time_ms": 1592471638943, "event_type": "INTERVAL_START", "key": "epoch_start", "value": null, "metadata": {"file": "train.py", "lineno": 541, "epoch_num": 1}}
0: Starting epoch 0
0: Sampler for epoch 0 uses seed 2230060666
0: Gradient norm: inf
0: Skipped batch, new scale: 512.0
0: TRAIN [0][0/484]	Time 0.633 (0.633)	Data 1.73e-01 (1.73e-01)	Tok/s 3367 (3367)	Loss/tok 10.6673 (10.6673)	LR 4.000e-05
0: Gradient norm: inf
0: Skipped batch, new scale: 256.0
0: Gradient norm: inf
0: Skipped batch, new scale: 128.0
0: Gradient norm: inf
0: Skipped batch, new scale: 64.0
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
0: TRAIN [0][10/484]	Time 0.016 (0.074)	Data 5.67e-05 (1.58e-02)	Tok/s 80647 (96477)	Loss/tok 9.5295 (10.3025)	LR 4.700e-05
0: Gradient norm: inf
0: Skipped batch, new scale: 32.0
0: TRAIN [0][20/484]	Time 0.022 (0.048)	Data 5.65e-05 (8.29e-03)	Tok/s 129259 (98927)	Loss/tok 9.2353 (9.8646)	LR 5.782e-05
0: TRAIN [0][30/484]	Time 0.026 (0.038)	Data 6.18e-05 (5.64e-03)	Tok/s 141254 (100258)	Loss/tok 8.8734 (9.5764)	LR 7.279e-05
0: TRAIN [0][40/484]	Time 0.013 (0.035)	Data 5.65e-05 (4.27e-03)	Tok/s 51567 (98451)	Loss/tok 8.2785 (9.3532)	LR 9.163e-05
0: Gradient norm: inf
0: Skipped batch, new scale: 16.0
0: TRAIN [0][50/484]	Time 0.026 (0.032)	Data 5.63e-05 (3.45e-03)	Tok/s 146156 (98226)	Loss/tok 8.5842 (9.1698)	LR 1.127e-04
0: Gradient norm: inf
0: Skipped batch, new scale: 8.0
0: TRAIN [0][60/484]	Time 0.015 (0.029)	Data 6.22e-05 (2.89e-03)	Tok/s 87103 (99365)	Loss/tok 7.9849 (9.0158)	LR 1.387e-04
0: TRAIN [0][70/484]	Time 0.019 (0.028)	Data 5.63e-05 (2.49e-03)	Tok/s 110602 (101304)	Loss/tok 8.0081 (8.8556)	LR 1.746e-04
0: TRAIN [0][80/484]	Time 0.019 (0.027)	Data 5.53e-05 (2.19e-03)	Tok/s 114768 (102237)	Loss/tok 7.9454 (8.7451)	LR 2.198e-04
0: TRAIN [0][90/484]	Time 0.019 (0.026)	Data 5.48e-05 (1.96e-03)	Tok/s 108458 (101307)	Loss/tok 7.8979 (8.6654)	LR 2.767e-04
0: TRAIN [0][100/484]	Time 0.019 (0.025)	Data 5.46e-05 (1.77e-03)	Tok/s 114852 (100928)	Loss/tok 7.9570 (8.5943)	LR 3.484e-04
0: TRAIN [0][110/484]	Time 0.019 (0.024)	Data 5.44e-05 (1.61e-03)	Tok/s 108377 (100864)	Loss/tok 7.9002 (8.5300)	LR 4.386e-04
0: TRAIN [0][120/484]	Time 0.019 (0.024)	Data 5.44e-05 (1.49e-03)	Tok/s 107714 (100804)	Loss/tok 7.8976 (8.4701)	LR 5.522e-04
0: TRAIN [0][130/484]	Time 0.026 (0.024)	Data 5.53e-05 (1.38e-03)	Tok/s 141750 (100675)	Loss/tok 7.8654 (8.4202)	LR 6.951e-04
0: TRAIN [0][140/484]	Time 0.016 (0.023)	Data 5.46e-05 (1.28e-03)	Tok/s 89836 (100263)	Loss/tok 7.4645 (8.3657)	LR 8.751e-04
0: TRAIN [0][150/484]	Time 0.023 (0.023)	Data 5.36e-05 (1.20e-03)	Tok/s 126041 (100113)	Loss/tok 7.3548 (8.3080)	LR 1.102e-03
0: TRAIN [0][160/484]	Time 0.026 (0.023)	Data 5.34e-05 (1.13e-03)	Tok/s 143455 (100156)	Loss/tok 7.3044 (8.2397)	LR 1.387e-03
0: TRAIN [0][170/484]	Time 0.019 (0.022)	Data 5.63e-05 (1.07e-03)	Tok/s 111362 (100380)	Loss/tok 7.0016 (8.1707)	LR 1.746e-03
0: TRAIN [0][180/484]	Time 0.013 (0.022)	Data 5.48e-05 (1.01e-03)	Tok/s 52135 (100139)	Loss/tok 6.5318 (8.1103)	LR 2.198e-03
0: Upscaling, new scale: 16.0
0: TRAIN [0][190/484]	Time 0.019 (0.022)	Data 5.51e-05 (9.62e-04)	Tok/s 114488 (100450)	Loss/tok 6.6377 (8.0388)	LR 2.767e-03
0: TRAIN [0][200/484]	Time 0.023 (0.022)	Data 5.46e-05 (9.16e-04)	Tok/s 131802 (100454)	Loss/tok 6.7926 (7.9707)	LR 3.484e-03
0: TRAIN [0][210/484]	Time 0.016 (0.022)	Data 5.36e-05 (8.76e-04)	Tok/s 83795 (100587)	Loss/tok 6.0625 (7.8992)	LR 4.000e-03
0: TRAIN [0][220/484]	Time 0.015 (0.021)	Data 5.53e-05 (8.38e-04)	Tok/s 85884 (100114)	Loss/tok 6.1366 (7.8365)	LR 4.000e-03
0: TRAIN [0][230/484]	Time 0.015 (0.021)	Data 5.53e-05 (8.05e-04)	Tok/s 79538 (100352)	Loss/tok 5.8445 (7.7533)	LR 4.000e-03
0: TRAIN [0][240/484]	Time 0.019 (0.021)	Data 5.48e-05 (7.73e-04)	Tok/s 104460 (100275)	Loss/tok 5.7432 (7.6814)	LR 4.000e-03
0: TRAIN [0][250/484]	Time 0.016 (0.021)	Data 5.41e-05 (7.45e-04)	Tok/s 80301 (99748)	Loss/tok 5.8167 (7.6239)	LR 4.000e-03
0: TRAIN [0][260/484]	Time 0.016 (0.021)	Data 5.72e-05 (7.18e-04)	Tok/s 83784 (99227)	Loss/tok 5.0880 (7.5560)	LR 4.000e-03
0: TRAIN [0][270/484]	Time 0.016 (0.021)	Data 5.32e-05 (6.94e-04)	Tok/s 83139 (99183)	Loss/tok 5.3367 (7.4716)	LR 4.000e-03
0: TRAIN [0][280/484]	Time 0.015 (0.021)	Data 5.44e-05 (6.71e-04)	Tok/s 79379 (99169)	Loss/tok 4.8266 (7.3976)	LR 4.000e-03
0: TRAIN [0][290/484]	Time 0.013 (0.021)	Data 5.53e-05 (6.50e-04)	Tok/s 50789 (98713)	Loss/tok 4.3190 (7.3339)	LR 4.000e-03
0: TRAIN [0][300/484]	Time 0.016 (0.021)	Data 5.34e-05 (6.30e-04)	Tok/s 79296 (98328)	Loss/tok 4.5547 (7.2708)	LR 4.000e-03
0: TRAIN [0][310/484]	Time 0.016 (0.020)	Data 5.46e-05 (6.12e-04)	Tok/s 80459 (98230)	Loss/tok 4.4894 (7.1986)	LR 4.000e-03
0: Upscaling, new scale: 32.0
0: TRAIN [0][320/484]	Time 0.016 (0.020)	Data 5.44e-05 (5.94e-04)	Tok/s 84989 (97821)	Loss/tok 4.6430 (7.1373)	LR 4.000e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 16.0
0: TRAIN [0][330/484]	Time 0.019 (0.020)	Data 5.56e-05 (5.78e-04)	Tok/s 112670 (97416)	Loss/tok 4.5185 (7.0742)	LR 4.000e-03
0: TRAIN [0][340/484]	Time 0.019 (0.020)	Data 5.53e-05 (5.63e-04)	Tok/s 116385 (97459)	Loss/tok 4.7482 (6.9999)	LR 4.000e-03
0: TRAIN [0][350/484]	Time 0.019 (0.020)	Data 5.48e-05 (5.49e-04)	Tok/s 115005 (97501)	Loss/tok 4.5540 (6.9295)	LR 4.000e-03
0: TRAIN [0][360/484]	Time 0.013 (0.020)	Data 5.51e-05 (5.35e-04)	Tok/s 53062 (97541)	Loss/tok 3.1955 (6.8587)	LR 4.000e-03
0: TRAIN [0][370/484]	Time 0.015 (0.020)	Data 5.51e-05 (5.22e-04)	Tok/s 82941 (97562)	Loss/tok 3.9627 (6.7905)	LR 4.000e-03
0: TRAIN [0][380/484]	Time 0.023 (0.020)	Data 5.53e-05 (5.10e-04)	Tok/s 128060 (97602)	Loss/tok 4.5512 (6.7241)	LR 4.000e-03
0: TRAIN [0][390/484]	Time 0.022 (0.020)	Data 5.70e-05 (4.98e-04)	Tok/s 131730 (97605)	Loss/tok 4.3330 (6.6576)	LR 4.000e-03
0: TRAIN [0][400/484]	Time 0.019 (0.020)	Data 5.65e-05 (4.87e-04)	Tok/s 106085 (97533)	Loss/tok 4.0972 (6.5980)	LR 4.000e-03
0: TRAIN [0][410/484]	Time 0.026 (0.020)	Data 5.46e-05 (4.77e-04)	Tok/s 143871 (97252)	Loss/tok 4.0485 (6.5463)	LR 4.000e-03
0: TRAIN [0][420/484]	Time 0.019 (0.020)	Data 5.67e-05 (4.67e-04)	Tok/s 111484 (97213)	Loss/tok 4.4462 (6.4919)	LR 4.000e-03
0: TRAIN [0][430/484]	Time 0.016 (0.020)	Data 5.56e-05 (4.57e-04)	Tok/s 80853 (97338)	Loss/tok 3.6955 (6.4321)	LR 4.000e-03
0: TRAIN [0][440/484]	Time 0.013 (0.020)	Data 5.77e-05 (4.48e-04)	Tok/s 51693 (97222)	Loss/tok 3.0235 (6.3808)	LR 4.000e-03
0: Upscaling, new scale: 32.0
0: TRAIN [0][450/484]	Time 0.019 (0.020)	Data 5.58e-05 (4.39e-04)	Tok/s 111005 (97155)	Loss/tok 4.2856 (6.3331)	LR 4.000e-03
0: TRAIN [0][460/484]	Time 0.016 (0.019)	Data 5.36e-05 (4.31e-04)	Tok/s 82165 (97181)	Loss/tok 3.5095 (6.2840)	LR 4.000e-03
0: TRAIN [0][470/484]	Time 0.016 (0.019)	Data 5.65e-05 (4.23e-04)	Tok/s 86534 (96959)	Loss/tok 3.5772 (6.2427)	LR 4.000e-03
0: TRAIN [0][480/484]	Time 0.019 (0.019)	Data 5.51e-05 (4.15e-04)	Tok/s 112688 (97129)	Loss/tok 4.1380 (6.1946)	LR 4.000e-03
:::MLLOG {"namespace": "", "time_ms": 1592471648399, "event_type": "INTERVAL_END", "key": "epoch_stop", "value": null, "metadata": {"file": "train.py", "lineno": 551, "epoch_num": 1}}
:::MLLOG {"namespace": "", "time_ms": 1592471648399, "event_type": "INTERVAL_START", "key": "eval_start", "value": null, "metadata": {"file": "train.py", "lineno": 556, "epoch_num": 1}}
0: Running evaluation on test set
0: TEST [0][0/1]	Time 0.315 (0.315)	Decoder iters 149.0 (149.0)	Tok/s 8416 (8416)
0: Running moses detokenizer
0: BLEU(score=14.152362401213775, counts=[29394, 11985, 5878, 3004], totals=[67345, 64342, 61339, 58341], precisions=[43.646892865097634, 18.627024338690124, 9.582810283832472, 5.149037555064192], bp=1.0, sys_len=67345, ref_len=64676)
0: Finished evaluation on test set
:::MLLOG {"namespace": "", "time_ms": 1592471648908, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.14150000000000001, "metadata": {"file": "train.py", "lineno": 562, "epoch_num": 1}}
:::MLLOG {"namespace": "", "time_ms": 1592471648909, "event_type": "INTERVAL_END", "key": "eval_stop", "value": null, "metadata": {"file": "train.py", "lineno": 565, "epoch_num": 1}}
0: Summary: Epoch: 0	Training Loss: 6.1758	Test BLEU: 14.15
0: Performance: Epoch: 0	Training: 24871080 Tok/s
0: Finished epoch 0
:::MLLOG {"namespace": "", "time_ms": 1592471648909, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "train.py", "lineno": 584, "first_epoch_num": 1}}
:::MLLOG {"namespace": "", "time_ms": 1592471648909, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "train.py", "lineno": 538, "first_epoch_num": 2, "epoch_count": 1}}
:::MLLOG {"namespace": "", "time_ms": 1592471648909, "event_type": "INTERVAL_START", "key": "epoch_start", "value": null, "metadata": {"file": "train.py", "lineno": 541, "epoch_num": 2}}
0: Starting epoch 1
0: Sampler for epoch 1 uses seed 1292568754
0: TRAIN [1][0/484]	Time 0.239 (0.239)	Data 1.69e-01 (1.69e-01)	Tok/s 8688 (8688)	Loss/tok 3.9336 (3.9336)	LR 4.000e-03
0: TRAIN [1][10/484]	Time 0.016 (0.037)	Data 5.36e-05 (1.54e-02)	Tok/s 76876 (82587)	Loss/tok 3.5489 (3.7107)	LR 4.000e-03
0: TRAIN [1][20/484]	Time 0.019 (0.028)	Data 5.53e-05 (8.09e-03)	Tok/s 110967 (93914)	Loss/tok 3.8399 (3.7750)	LR 4.000e-03
0: TRAIN [1][30/484]	Time 0.016 (0.025)	Data 5.53e-05 (5.50e-03)	Tok/s 82320 (95401)	Loss/tok 3.6860 (3.7745)	LR 4.000e-03
0: TRAIN [1][40/484]	Time 0.019 (0.024)	Data 6.87e-05 (4.17e-03)	Tok/s 114584 (96186)	Loss/tok 3.5117 (3.7828)	LR 4.000e-03
0: TRAIN [1][50/484]	Time 0.019 (0.023)	Data 5.01e-05 (3.37e-03)	Tok/s 115869 (97197)	Loss/tok 4.0042 (3.7946)	LR 4.000e-03
0: TRAIN [1][60/484]	Time 0.031 (0.022)	Data 4.96e-05 (2.82e-03)	Tok/s 67261 (97287)	Loss/tok 3.7700 (3.7910)	LR 4.000e-03
0: TRAIN [1][70/484]	Time 0.013 (0.021)	Data 8.44e-05 (2.43e-03)	Tok/s 49639 (95522)	Loss/tok 2.9005 (3.7589)	LR 4.000e-03
0: TRAIN [1][80/484]	Time 0.023 (0.021)	Data 4.98e-05 (2.14e-03)	Tok/s 127072 (96282)	Loss/tok 4.0150 (3.7761)	LR 4.000e-03
0: TRAIN [1][90/484]	Time 0.013 (0.021)	Data 4.96e-05 (1.91e-03)	Tok/s 48210 (94892)	Loss/tok 3.0371 (3.7685)	LR 4.000e-03
0: Upscaling, new scale: 64.0
0: TRAIN [1][100/484]	Time 0.016 (0.020)	Data 5.03e-05 (1.73e-03)	Tok/s 82342 (94015)	Loss/tok 3.4692 (3.7546)	LR 4.000e-03
0: TRAIN [1][110/484]	Time 0.019 (0.020)	Data 5.03e-05 (1.58e-03)	Tok/s 108233 (95217)	Loss/tok 3.8837 (3.7533)	LR 4.000e-03
0: TRAIN [1][120/484]	Time 0.023 (0.020)	Data 5.56e-05 (1.45e-03)	Tok/s 129995 (94563)	Loss/tok 3.9722 (3.7495)	LR 4.000e-03
0: TRAIN [1][130/484]	Time 0.019 (0.020)	Data 5.53e-05 (1.34e-03)	Tok/s 107867 (95251)	Loss/tok 3.8855 (3.7506)	LR 4.000e-03
0: TRAIN [1][140/484]	Time 0.016 (0.020)	Data 5.58e-05 (1.25e-03)	Tok/s 87230 (95610)	Loss/tok 3.1598 (3.7470)	LR 4.000e-03
0: TRAIN [1][150/484]	Time 0.019 (0.020)	Data 5.34e-05 (1.17e-03)	Tok/s 108461 (95936)	Loss/tok 3.6455 (3.7431)	LR 4.000e-03
0: TRAIN [1][160/484]	Time 0.019 (0.019)	Data 5.03e-05 (1.10e-03)	Tok/s 112965 (95606)	Loss/tok 3.4138 (3.7410)	LR 4.000e-03
0: TRAIN [1][170/484]	Time 0.026 (0.019)	Data 5.03e-05 (1.04e-03)	Tok/s 141441 (96138)	Loss/tok 4.3133 (3.7408)	LR 4.000e-03
0: TRAIN [1][180/484]	Time 0.015 (0.019)	Data 5.53e-05 (9.88e-04)	Tok/s 78961 (96375)	Loss/tok 3.5262 (3.7392)	LR 4.000e-03
0: TRAIN [1][190/484]	Time 0.016 (0.019)	Data 4.96e-05 (9.39e-04)	Tok/s 85300 (96446)	Loss/tok 3.2429 (3.7312)	LR 4.000e-03
0: TRAIN [1][200/484]	Time 0.016 (0.019)	Data 5.05e-05 (8.95e-04)	Tok/s 80915 (96070)	Loss/tok 3.3304 (3.7190)	LR 4.000e-03
0: TRAIN [1][210/484]	Time 0.026 (0.019)	Data 5.20e-05 (8.55e-04)	Tok/s 138169 (97150)	Loss/tok 4.2467 (3.7201)	LR 4.000e-03
0: TRAIN [1][220/484]	Time 0.022 (0.019)	Data 4.89e-05 (8.19e-04)	Tok/s 126946 (97337)	Loss/tok 3.8752 (3.7126)	LR 4.000e-03
0: Upscaling, new scale: 128.0
0: Gradient norm: inf
0: Skipped batch, new scale: 64.0
0: TRAIN [1][230/484]	Time 0.019 (0.019)	Data 5.10e-05 (7.86e-04)	Tok/s 113835 (97205)	Loss/tok 3.6430 (3.7093)	LR 4.000e-03
0: TRAIN [1][240/484]	Time 0.019 (0.019)	Data 4.98e-05 (7.55e-04)	Tok/s 109628 (97201)	Loss/tok 3.7499 (3.7038)	LR 4.000e-03
0: TRAIN [1][250/484]	Time 0.019 (0.019)	Data 4.86e-05 (7.27e-04)	Tok/s 113136 (96932)	Loss/tok 3.5820 (3.7000)	LR 4.000e-03
0: TRAIN [1][260/484]	Time 0.014 (0.019)	Data 4.94e-05 (7.01e-04)	Tok/s 43021 (96532)	Loss/tok 2.8404 (3.6973)	LR 4.000e-03
0: TRAIN [1][270/484]	Time 0.019 (0.019)	Data 5.44e-05 (6.78e-04)	Tok/s 111624 (96639)	Loss/tok 3.5298 (3.6905)	LR 4.000e-03
0: TRAIN [1][280/484]	Time 0.016 (0.019)	Data 5.46e-05 (6.56e-04)	Tok/s 82407 (96647)	Loss/tok 3.4982 (3.6900)	LR 4.000e-03
0: TRAIN [1][290/484]	Time 0.019 (0.019)	Data 5.60e-05 (6.35e-04)	Tok/s 109076 (96811)	Loss/tok 3.8648 (3.6859)	LR 4.000e-03
0: TRAIN [1][300/484]	Time 0.019 (0.019)	Data 5.32e-05 (6.16e-04)	Tok/s 110259 (96994)	Loss/tok 3.5842 (3.6793)	LR 4.000e-03
0: TRAIN [1][310/484]	Time 0.015 (0.019)	Data 5.75e-05 (5.98e-04)	Tok/s 79637 (96869)	Loss/tok 3.3625 (3.6780)	LR 4.000e-03
0: TRAIN [1][320/484]	Time 0.016 (0.019)	Data 5.46e-05 (5.81e-04)	Tok/s 78924 (96821)	Loss/tok 3.3532 (3.6729)	LR 4.000e-03
0: TRAIN [1][330/484]	Time 0.016 (0.019)	Data 5.48e-05 (5.65e-04)	Tok/s 80485 (96883)	Loss/tok 3.2578 (3.6718)	LR 4.000e-03
0: TRAIN [1][340/484]	Time 0.019 (0.019)	Data 5.36e-05 (5.50e-04)	Tok/s 114799 (97434)	Loss/tok 3.5366 (3.6726)	LR 4.000e-03
0: TRAIN [1][350/484]	Time 0.015 (0.019)	Data 5.51e-05 (5.36e-04)	Tok/s 83872 (97551)	Loss/tok 3.3615 (3.6687)	LR 4.000e-03
0: Upscaling, new scale: 128.0
0: TRAIN [1][360/484]	Time 0.015 (0.019)	Data 5.56e-05 (5.23e-04)	Tok/s 79756 (97127)	Loss/tok 3.3143 (3.6675)	LR 4.000e-03
0: TRAIN [1][370/484]	Time 0.031 (0.019)	Data 5.34e-05 (5.11e-04)	Tok/s 92005 (97064)	Loss/tok 3.7095 (3.6641)	LR 4.000e-03
0: TRAIN [1][380/484]	Time 0.023 (0.019)	Data 5.36e-05 (4.99e-04)	Tok/s 128960 (97112)	Loss/tok 3.7710 (3.6633)	LR 4.000e-03
0: TRAIN [1][390/484]	Time 0.013 (0.019)	Data 5.48e-05 (4.88e-04)	Tok/s 47007 (97180)	Loss/tok 2.7382 (3.6638)	LR 4.000e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 64.0
0: TRAIN [1][400/484]	Time 0.015 (0.019)	Data 8.75e-05 (4.77e-04)	Tok/s 87181 (97621)	Loss/tok 3.0050 (3.6685)	LR 4.000e-03
0: TRAIN [1][410/484]	Time 0.023 (0.019)	Data 5.29e-05 (4.67e-04)	Tok/s 135385 (97196)	Loss/tok 3.4288 (3.6632)	LR 4.000e-03
0: TRAIN [1][420/484]	Time 0.019 (0.019)	Data 5.46e-05 (4.57e-04)	Tok/s 114033 (97394)	Loss/tok 3.4130 (3.6604)	LR 4.000e-03
0: TRAIN [1][430/484]	Time 0.015 (0.019)	Data 5.41e-05 (4.48e-04)	Tok/s 87139 (97649)	Loss/tok 3.4216 (3.6550)	LR 4.000e-03
0: TRAIN [1][440/484]	Time 0.013 (0.019)	Data 5.27e-05 (4.39e-04)	Tok/s 50115 (97311)	Loss/tok 3.0666 (3.6499)	LR 4.000e-03
0: TRAIN [1][450/484]	Time 0.016 (0.019)	Data 5.34e-05 (4.30e-04)	Tok/s 81382 (97035)	Loss/tok 3.1096 (3.6456)	LR 4.000e-03
0: TRAIN [1][460/484]	Time 0.023 (0.019)	Data 6.87e-05 (4.22e-04)	Tok/s 128295 (97269)	Loss/tok 3.6440 (3.6430)	LR 4.000e-03
0: TRAIN [1][470/484]	Time 0.014 (0.019)	Data 5.20e-05 (4.14e-04)	Tok/s 47896 (97169)	Loss/tok 2.7522 (3.6361)	LR 4.000e-03
0: TRAIN [1][480/484]	Time 0.016 (0.019)	Data 5.20e-05 (4.07e-04)	Tok/s 79689 (96957)	Loss/tok 2.9000 (3.6320)	LR 4.000e-03
:::MLLOG {"namespace": "", "time_ms": 1592471657959, "event_type": "INTERVAL_END", "key": "epoch_stop", "value": null, "metadata": {"file": "train.py", "lineno": 551, "epoch_num": 2}}
:::MLLOG {"namespace": "", "time_ms": 1592471657960, "event_type": "INTERVAL_START", "key": "eval_start", "value": null, "metadata": {"file": "train.py", "lineno": 556, "epoch_num": 2}}
0: Running evaluation on test set
0: TEST [1][0/1]	Time 0.322 (0.322)	Decoder iters 149.0 (149.0)	Tok/s 7643 (7643)
0: Running moses detokenizer
0: BLEU(score=20.926330801255407, counts=[34894, 16321, 8920, 5081], totals=[65166, 62163, 59160, 56164], precisions=[53.546327839670994, 26.255167865128776, 15.077755240027045, 9.046720319065594], bp=1.0, sys_len=65166, ref_len=64676)
0: Finished evaluation on test set
:::MLLOG {"namespace": "", "time_ms": 1592471658468, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.20929999999999999, "metadata": {"file": "train.py", "lineno": 562, "epoch_num": 2}}
:::MLLOG {"namespace": "", "time_ms": 1592471658468, "event_type": "INTERVAL_END", "key": "eval_stop", "value": null, "metadata": {"file": "train.py", "lineno": 565, "epoch_num": 2}}
0: Summary: Epoch: 1	Training Loss: 3.6315	Test BLEU: 20.93
0: Performance: Epoch: 1	Training: 24805898 Tok/s
0: Finished epoch 1
:::MLLOG {"namespace": "", "time_ms": 1592471658469, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "train.py", "lineno": 584, "first_epoch_num": 2}}
:::MLLOG {"namespace": "", "time_ms": 1592471658469, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "train.py", "lineno": 538, "first_epoch_num": 3, "epoch_count": 1}}
:::MLLOG {"namespace": "", "time_ms": 1592471658469, "event_type": "INTERVAL_START", "key": "epoch_start", "value": null, "metadata": {"file": "train.py", "lineno": 541, "epoch_num": 3}}
0: Starting epoch 2
0: Sampler for epoch 2 uses seed 2092821154
0: TRAIN [2][0/484]	Time 0.217 (0.217)	Data 1.64e-01 (1.64e-01)	Tok/s 6045 (6045)	Loss/tok 3.1244 (3.1244)	LR 4.000e-03
0: TRAIN [2][10/484]	Time 0.019 (0.036)	Data 5.17e-05 (1.50e-02)	Tok/s 110436 (93481)	Loss/tok 3.5640 (3.4467)	LR 4.000e-03
0: TRAIN [2][20/484]	Time 0.022 (0.028)	Data 5.01e-05 (7.86e-03)	Tok/s 126408 (97323)	Loss/tok 3.7798 (3.4463)	LR 4.000e-03
0: TRAIN [2][30/484]	Time 0.023 (0.025)	Data 5.01e-05 (5.34e-03)	Tok/s 127617 (99357)	Loss/tok 3.4858 (3.3979)	LR 4.000e-03
0: Upscaling, new scale: 128.0
0: TRAIN [2][40/484]	Time 0.019 (0.023)	Data 5.05e-05 (4.05e-03)	Tok/s 116473 (102048)	Loss/tok 3.3303 (3.4031)	LR 4.000e-03
0: TRAIN [2][50/484]	Time 0.023 (0.022)	Data 4.94e-05 (3.27e-03)	Tok/s 124025 (99997)	Loss/tok 3.6791 (3.3870)	LR 4.000e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 64.0
0: TRAIN [2][60/484]	Time 0.019 (0.022)	Data 4.82e-05 (2.74e-03)	Tok/s 115776 (101677)	Loss/tok 3.6867 (3.4247)	LR 4.000e-03
0: TRAIN [2][70/484]	Time 0.013 (0.021)	Data 5.01e-05 (2.36e-03)	Tok/s 47939 (101175)	Loss/tok 2.8062 (3.4277)	LR 4.000e-03
0: TRAIN [2][80/484]	Time 0.023 (0.021)	Data 5.10e-05 (2.08e-03)	Tok/s 133171 (100548)	Loss/tok 3.2221 (3.4094)	LR 4.000e-03
0: TRAIN [2][90/484]	Time 0.022 (0.020)	Data 5.03e-05 (1.85e-03)	Tok/s 129386 (98988)	Loss/tok 3.5241 (3.4011)	LR 4.000e-03
0: TRAIN [2][100/484]	Time 0.019 (0.020)	Data 5.05e-05 (1.68e-03)	Tok/s 114284 (99697)	Loss/tok 3.1136 (3.4003)	LR 4.000e-03
0: TRAIN [2][110/484]	Time 0.023 (0.020)	Data 4.98e-05 (1.53e-03)	Tok/s 131243 (99708)	Loss/tok 3.5202 (3.3999)	LR 4.000e-03
0: TRAIN [2][120/484]	Time 0.023 (0.020)	Data 4.96e-05 (1.41e-03)	Tok/s 130430 (100168)	Loss/tok 3.6093 (3.4076)	LR 4.000e-03
0: TRAIN [2][130/484]	Time 0.023 (0.020)	Data 4.89e-05 (1.30e-03)	Tok/s 125469 (100297)	Loss/tok 3.6767 (3.4097)	LR 4.000e-03
0: TRAIN [2][140/484]	Time 0.015 (0.020)	Data 8.11e-05 (1.21e-03)	Tok/s 83473 (100431)	Loss/tok 2.7300 (3.4036)	LR 4.000e-03
0: TRAIN [2][150/484]	Time 0.015 (0.020)	Data 4.94e-05 (1.14e-03)	Tok/s 82072 (99433)	Loss/tok 3.4166 (3.3964)	LR 4.000e-03
0: TRAIN [2][160/484]	Time 0.019 (0.020)	Data 4.98e-05 (1.07e-03)	Tok/s 106269 (98987)	Loss/tok 3.4938 (3.3895)	LR 4.000e-03
0: TRAIN [2][170/484]	Time 0.013 (0.019)	Data 4.98e-05 (1.01e-03)	Tok/s 50581 (98233)	Loss/tok 3.1085 (3.3899)	LR 4.000e-03
0: Upscaling, new scale: 128.0
0: TRAIN [2][180/484]	Time 0.027 (0.019)	Data 5.01e-05 (9.57e-04)	Tok/s 76380 (97836)	Loss/tok 3.3451 (3.3887)	LR 4.000e-03
0: TRAIN [2][190/484]	Time 0.019 (0.019)	Data 5.27e-05 (9.10e-04)	Tok/s 106690 (98181)	Loss/tok 3.2906 (3.3883)	LR 4.000e-03
0: TRAIN [2][200/484]	Time 0.016 (0.019)	Data 5.17e-05 (8.67e-04)	Tok/s 79795 (97721)	Loss/tok 3.3363 (3.3829)	LR 4.000e-03
0: TRAIN [2][210/484]	Time 0.013 (0.019)	Data 5.17e-05 (8.29e-04)	Tok/s 53795 (97438)	Loss/tok 3.1730 (3.3793)	LR 4.000e-03
0: TRAIN [2][220/484]	Time 0.019 (0.019)	Data 5.03e-05 (7.94e-04)	Tok/s 107823 (97550)	Loss/tok 3.3016 (3.3746)	LR 4.000e-03
0: TRAIN [2][230/484]	Time 0.016 (0.019)	Data 5.32e-05 (7.62e-04)	Tok/s 81176 (96818)	Loss/tok 3.1884 (3.3711)	LR 4.000e-03
0: TRAIN [2][240/484]	Time 0.019 (0.019)	Data 5.17e-05 (7.32e-04)	Tok/s 107248 (97028)	Loss/tok 3.0019 (3.3706)	LR 4.000e-03
0: TRAIN [2][250/484]	Time 0.015 (0.019)	Data 5.36e-05 (7.05e-04)	Tok/s 77770 (96683)	Loss/tok 3.2244 (3.3694)	LR 4.000e-03
0: TRAIN [2][260/484]	Time 0.022 (0.019)	Data 5.10e-05 (6.80e-04)	Tok/s 129211 (96861)	Loss/tok 3.5153 (3.3681)	LR 4.000e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 64.0
0: TRAIN [2][270/484]	Time 0.026 (0.019)	Data 5.17e-05 (6.57e-04)	Tok/s 137957 (97133)	Loss/tok 3.9174 (3.3728)	LR 4.000e-03
0: TRAIN [2][280/484]	Time 0.013 (0.019)	Data 5.41e-05 (6.36e-04)	Tok/s 50631 (96953)	Loss/tok 2.9338 (3.3768)	LR 4.000e-03
0: TRAIN [2][290/484]	Time 0.019 (0.019)	Data 6.94e-05 (6.16e-04)	Tok/s 110194 (96776)	Loss/tok 3.1546 (3.3709)	LR 4.000e-03
0: TRAIN [2][300/484]	Time 0.017 (0.019)	Data 5.27e-05 (5.97e-04)	Tok/s 72097 (96569)	Loss/tok 3.4152 (3.3730)	LR 4.000e-03
0: TRAIN [2][310/484]	Time 0.014 (0.019)	Data 5.29e-05 (5.79e-04)	Tok/s 48608 (96454)	Loss/tok 2.8592 (3.3721)	LR 4.000e-03
0: TRAIN [2][320/484]	Time 0.016 (0.019)	Data 5.17e-05 (5.63e-04)	Tok/s 82215 (95868)	Loss/tok 3.4955 (3.3727)	LR 4.000e-03
0: TRAIN [2][330/484]	Time 0.016 (0.019)	Data 5.25e-05 (5.48e-04)	Tok/s 80841 (95767)	Loss/tok 3.2694 (3.3738)	LR 4.000e-03
0: TRAIN [2][340/484]	Time 0.016 (0.018)	Data 5.29e-05 (5.33e-04)	Tok/s 84224 (95999)	Loss/tok 3.4955 (3.3755)	LR 4.000e-03
0: TRAIN [2][350/484]	Time 0.014 (0.018)	Data 5.65e-05 (5.20e-04)	Tok/s 45496 (95658)	Loss/tok 2.2789 (3.3695)	LR 4.000e-03
0: TRAIN [2][360/484]	Time 0.015 (0.018)	Data 5.41e-05 (5.07e-04)	Tok/s 85342 (95960)	Loss/tok 3.0111 (3.3694)	LR 4.000e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 32.0
0: TRAIN [2][370/484]	Time 0.016 (0.018)	Data 5.41e-05 (4.94e-04)	Tok/s 83351 (95894)	Loss/tok 3.3197 (3.3707)	LR 4.000e-03
0: TRAIN [2][380/484]	Time 0.019 (0.018)	Data 5.34e-05 (4.83e-04)	Tok/s 112349 (95984)	Loss/tok 3.4709 (3.3689)	LR 4.000e-03
0: TRAIN [2][390/484]	Time 0.026 (0.018)	Data 5.25e-05 (4.72e-04)	Tok/s 144670 (96383)	Loss/tok 3.2258 (3.3721)	LR 4.000e-03
0: TRAIN [2][400/484]	Time 0.019 (0.018)	Data 5.41e-05 (4.61e-04)	Tok/s 109890 (96747)	Loss/tok 3.1466 (3.3711)	LR 4.000e-03
0: TRAIN [2][410/484]	Time 0.016 (0.019)	Data 5.60e-05 (4.52e-04)	Tok/s 84949 (97213)	Loss/tok 3.2518 (3.3731)	LR 4.000e-03
0: TRAIN [2][420/484]	Time 0.013 (0.018)	Data 5.58e-05 (4.42e-04)	Tok/s 51730 (96945)	Loss/tok 2.9434 (3.3718)	LR 4.000e-03
0: TRAIN [2][430/484]	Time 0.041 (0.019)	Data 5.29e-05 (4.33e-04)	Tok/s 91416 (97152)	Loss/tok 3.6425 (3.3769)	LR 4.000e-03
0: TRAIN [2][440/484]	Time 0.022 (0.019)	Data 5.36e-05 (4.25e-04)	Tok/s 129976 (96936)	Loss/tok 3.6833 (3.3754)	LR 4.000e-03
0: TRAIN [2][450/484]	Time 0.016 (0.019)	Data 5.20e-05 (4.16e-04)	Tok/s 78502 (96699)	Loss/tok 3.1996 (3.3741)	LR 4.000e-03
0: TRAIN [2][460/484]	Time 0.018 (0.019)	Data 5.70e-05 (4.09e-04)	Tok/s 114078 (96670)	Loss/tok 2.9459 (3.3721)	LR 4.000e-03
0: TRAIN [2][470/484]	Time 0.016 (0.019)	Data 5.41e-05 (4.01e-04)	Tok/s 79693 (96762)	Loss/tok 3.3730 (3.3718)	LR 4.000e-03
0: TRAIN [2][480/484]	Time 0.023 (0.019)	Data 5.44e-05 (3.94e-04)	Tok/s 129076 (96890)	Loss/tok 3.7011 (3.3755)	LR 4.000e-03
:::MLLOG {"namespace": "", "time_ms": 1592471667490, "event_type": "INTERVAL_END", "key": "epoch_stop", "value": null, "metadata": {"file": "train.py", "lineno": 551, "epoch_num": 3}}
:::MLLOG {"namespace": "", "time_ms": 1592471667490, "event_type": "INTERVAL_START", "key": "eval_start", "value": null, "metadata": {"file": "train.py", "lineno": 556, "epoch_num": 3}}
0: Running evaluation on test set
0: TEST [2][0/1]	Time 0.318 (0.318)	Decoder iters 149.0 (149.0)	Tok/s 8154 (8154)
0: Running moses detokenizer
0: BLEU(score=20.70744941858633, counts=[35165, 16629, 9096, 5183], totals=[66820, 63817, 60814, 57817], precisions=[52.62645914396887, 26.057320149803346, 14.957082250797514, 8.964491412560319], bp=1.0, sys_len=66820, ref_len=64676)
0: Finished evaluation on test set
:::MLLOG {"namespace": "", "time_ms": 1592471668002, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.2071, "metadata": {"file": "train.py", "lineno": 562, "epoch_num": 3}}
:::MLLOG {"namespace": "", "time_ms": 1592471668003, "event_type": "INTERVAL_END", "key": "eval_stop", "value": null, "metadata": {"file": "train.py", "lineno": 565, "epoch_num": 3}}
0: Summary: Epoch: 2	Training Loss: 3.3803	Test BLEU: 20.71
0: Performance: Epoch: 2	Training: 24829706 Tok/s
0: Finished epoch 2
:::MLLOG {"namespace": "", "time_ms": 1592471668003, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "train.py", "lineno": 584, "first_epoch_num": 3}}
:::MLLOG {"namespace": "", "time_ms": 1592471668003, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "train.py", "lineno": 538, "first_epoch_num": 4, "epoch_count": 1}}
:::MLLOG {"namespace": "", "time_ms": 1592471668003, "event_type": "INTERVAL_START", "key": "epoch_start", "value": null, "metadata": {"file": "train.py", "lineno": 541, "epoch_num": 4}}
0: Starting epoch 3
0: Sampler for epoch 3 uses seed 1819092639
0: TRAIN [3][0/484]	Time 0.215 (0.215)	Data 1.66e-01 (1.66e-01)	Tok/s 6377 (6377)	Loss/tok 3.1510 (3.1510)	LR 4.000e-03
0: Upscaling, new scale: 64.0
0: TRAIN [3][10/484]	Time 0.013 (0.038)	Data 5.05e-05 (1.51e-02)	Tok/s 50879 (103737)	Loss/tok 2.5255 (3.4786)	LR 4.000e-03
0: TRAIN [3][20/484]	Time 0.013 (0.028)	Data 4.94e-05 (7.95e-03)	Tok/s 50476 (99442)	Loss/tok 2.7207 (3.3490)	LR 4.000e-03
0: TRAIN [3][30/484]	Time 0.023 (0.025)	Data 5.01e-05 (5.40e-03)	Tok/s 129074 (99969)	Loss/tok 3.2271 (3.3493)	LR 4.000e-03
0: TRAIN [3][40/484]	Time 0.022 (0.023)	Data 7.34e-05 (4.10e-03)	Tok/s 130635 (100014)	Loss/tok 3.5371 (3.3394)	LR 4.000e-03
0: TRAIN [3][50/484]	Time 0.019 (0.023)	Data 5.08e-05 (3.30e-03)	Tok/s 112021 (101450)	Loss/tok 3.1977 (3.3414)	LR 4.000e-03
0: TRAIN [3][60/484]	Time 0.014 (0.022)	Data 4.94e-05 (2.77e-03)	Tok/s 48929 (100236)	Loss/tok 2.6140 (3.3530)	LR 4.000e-03
0: TRAIN [3][70/484]	Time 0.019 (0.022)	Data 4.91e-05 (2.39e-03)	Tok/s 118083 (99245)	Loss/tok 3.2070 (3.3434)	LR 4.000e-03
0: TRAIN [3][80/484]	Time 0.016 (0.021)	Data 4.89e-05 (2.10e-03)	Tok/s 76851 (98806)	Loss/tok 3.0158 (3.3229)	LR 4.000e-03
0: TRAIN [3][90/484]	Time 0.018 (0.021)	Data 4.84e-05 (1.87e-03)	Tok/s 112253 (99281)	Loss/tok 3.4010 (3.3229)	LR 4.000e-03
0: TRAIN [3][100/484]	Time 0.016 (0.020)	Data 4.79e-05 (1.69e-03)	Tok/s 84232 (97855)	Loss/tok 3.3027 (3.3135)	LR 4.000e-03
0: TRAIN [3][110/484]	Time 0.015 (0.020)	Data 4.96e-05 (1.54e-03)	Tok/s 83425 (98162)	Loss/tok 3.1795 (3.3289)	LR 4.000e-03
0: TRAIN [3][120/484]	Time 0.016 (0.020)	Data 5.03e-05 (1.42e-03)	Tok/s 86130 (98863)	Loss/tok 3.1857 (3.3319)	LR 4.000e-03
0: TRAIN [3][130/484]	Time 0.013 (0.020)	Data 4.74e-05 (1.32e-03)	Tok/s 49902 (98835)	Loss/tok 2.6831 (3.3303)	LR 4.000e-03
0: Upscaling, new scale: 128.0
0: TRAIN [3][140/484]	Time 0.016 (0.020)	Data 4.94e-05 (1.23e-03)	Tok/s 84073 (98741)	Loss/tok 3.0397 (3.3211)	LR 4.000e-03
0: TRAIN [3][150/484]	Time 0.015 (0.020)	Data 4.82e-05 (1.15e-03)	Tok/s 83171 (98277)	Loss/tok 2.9361 (3.3180)	LR 4.000e-03
0: TRAIN [3][160/484]	Time 0.016 (0.019)	Data 4.79e-05 (1.08e-03)	Tok/s 81016 (98370)	Loss/tok 3.0237 (3.3164)	LR 4.000e-03
0: TRAIN [3][170/484]	Time 0.016 (0.019)	Data 5.05e-05 (1.02e-03)	Tok/s 79083 (98086)	Loss/tok 3.0564 (3.3134)	LR 4.000e-03
0: TRAIN [3][180/484]	Time 0.016 (0.019)	Data 4.89e-05 (9.68e-04)	Tok/s 81529 (97678)	Loss/tok 2.8604 (3.3050)	LR 4.000e-03
0: TRAIN [3][190/484]	Time 0.016 (0.019)	Data 5.13e-05 (9.20e-04)	Tok/s 86970 (97336)	Loss/tok 3.2668 (3.3020)	LR 4.000e-03
0: TRAIN [3][200/484]	Time 0.022 (0.019)	Data 4.86e-05 (8.76e-04)	Tok/s 132314 (97342)	Loss/tok 3.4517 (3.2992)	LR 4.000e-03
0: TRAIN [3][210/484]	Time 0.016 (0.019)	Data 4.91e-05 (8.37e-04)	Tok/s 80946 (97365)	Loss/tok 3.0610 (3.2992)	LR 4.000e-03
0: TRAIN [3][220/484]	Time 0.019 (0.019)	Data 5.05e-05 (8.02e-04)	Tok/s 113640 (97990)	Loss/tok 2.7701 (3.2994)	LR 4.000e-03
0: TRAIN [3][230/484]	Time 0.015 (0.019)	Data 4.91e-05 (7.69e-04)	Tok/s 84325 (97870)	Loss/tok 2.7046 (3.2915)	LR 4.000e-03
0: TRAIN [3][240/484]	Time 0.019 (0.019)	Data 4.86e-05 (7.39e-04)	Tok/s 110090 (97995)	Loss/tok 3.1044 (3.2872)	LR 4.000e-03
0: TRAIN [3][250/484]	Time 0.019 (0.019)	Data 5.15e-05 (7.12e-04)	Tok/s 67379 (97970)	Loss/tok 3.3433 (3.2829)	LR 4.000e-03
0: TRAIN [3][260/484]	Time 0.016 (0.019)	Data 4.91e-05 (6.87e-04)	Tok/s 81270 (97973)	Loss/tok 3.0430 (3.2824)	LR 4.000e-03
0: Upscaling, new scale: 256.0
0: TRAIN [3][270/484]	Time 0.019 (0.019)	Data 5.01e-05 (6.63e-04)	Tok/s 113354 (98291)	Loss/tok 3.3021 (3.2870)	LR 4.000e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 128.0
0: TRAIN [3][280/484]	Time 0.018 (0.019)	Data 5.67e-05 (6.41e-04)	Tok/s 116250 (98508)	Loss/tok 3.2591 (3.2905)	LR 4.000e-03
0: TRAIN [3][290/484]	Time 0.013 (0.019)	Data 8.30e-05 (6.21e-04)	Tok/s 49292 (98125)	Loss/tok 2.5367 (3.2892)	LR 4.000e-03
0: TRAIN [3][300/484]	Time 0.026 (0.019)	Data 5.01e-05 (6.02e-04)	Tok/s 141587 (97634)	Loss/tok 3.6475 (3.2881)	LR 4.000e-03
0: TRAIN [3][310/484]	Time 0.019 (0.019)	Data 5.01e-05 (5.85e-04)	Tok/s 111697 (98153)	Loss/tok 3.0268 (3.2892)	LR 4.000e-03
0: TRAIN [3][320/484]	Time 0.016 (0.019)	Data 4.98e-05 (5.68e-04)	Tok/s 83667 (97697)	Loss/tok 3.1488 (3.2837)	LR 4.000e-03
0: TRAIN [3][330/484]	Time 0.013 (0.019)	Data 4.91e-05 (5.53e-04)	Tok/s 51130 (97193)	Loss/tok 2.9248 (3.2826)	LR 4.000e-03
0: TRAIN [3][340/484]	Time 0.016 (0.019)	Data 4.96e-05 (5.38e-04)	Tok/s 86938 (97088)	Loss/tok 3.0942 (3.2824)	LR 4.000e-03
0: TRAIN [3][350/484]	Time 0.023 (0.019)	Data 8.03e-05 (5.24e-04)	Tok/s 126929 (97012)	Loss/tok 3.5173 (3.2824)	LR 4.000e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 64.0
0: TRAIN [3][360/484]	Time 0.016 (0.019)	Data 5.03e-05 (5.11e-04)	Tok/s 83703 (97088)	Loss/tok 3.0017 (3.2796)	LR 4.000e-03
0: TRAIN [3][370/484]	Time 0.019 (0.019)	Data 4.91e-05 (4.99e-04)	Tok/s 117541 (97121)	Loss/tok 2.9822 (3.2795)	LR 4.000e-03
0: TRAIN [3][380/484]	Time 0.019 (0.019)	Data 4.70e-05 (4.87e-04)	Tok/s 110699 (97444)	Loss/tok 3.3273 (3.2801)	LR 4.000e-03
0: TRAIN [3][390/484]	Time 0.016 (0.019)	Data 4.91e-05 (4.76e-04)	Tok/s 80308 (97466)	Loss/tok 3.3668 (3.2790)	LR 4.000e-03
0: TRAIN [3][400/484]	Time 0.016 (0.019)	Data 4.91e-05 (4.65e-04)	Tok/s 81791 (97611)	Loss/tok 3.2772 (3.2787)	LR 4.000e-03
0: TRAIN [3][410/484]	Time 0.019 (0.018)	Data 4.91e-05 (4.55e-04)	Tok/s 107259 (97688)	Loss/tok 3.3322 (3.2800)	LR 4.000e-03
0: TRAIN [3][420/484]	Time 0.016 (0.018)	Data 4.86e-05 (4.46e-04)	Tok/s 79441 (97019)	Loss/tok 2.9576 (3.2757)	LR 4.000e-03
0: TRAIN [3][430/484]	Time 0.023 (0.018)	Data 5.05e-05 (4.37e-04)	Tok/s 131757 (97289)	Loss/tok 3.1939 (3.2763)	LR 4.000e-03
0: TRAIN [3][440/484]	Time 0.016 (0.018)	Data 4.96e-05 (4.28e-04)	Tok/s 83471 (97237)	Loss/tok 3.0716 (3.2755)	LR 4.000e-03
0: TRAIN [3][450/484]	Time 0.022 (0.018)	Data 4.79e-05 (4.20e-04)	Tok/s 128148 (97475)	Loss/tok 3.2514 (3.2775)	LR 4.000e-03
0: TRAIN [3][460/484]	Time 0.019 (0.018)	Data 4.98e-05 (4.12e-04)	Tok/s 106639 (97675)	Loss/tok 3.2225 (3.2786)	LR 4.000e-03
0: TRAIN [3][470/484]	Time 0.015 (0.018)	Data 5.05e-05 (4.04e-04)	Tok/s 82847 (97732)	Loss/tok 3.1911 (3.2818)	LR 4.000e-03
0: TRAIN [3][480/484]	Time 0.014 (0.018)	Data 4.86e-05 (3.97e-04)	Tok/s 50839 (97584)	Loss/tok 2.6053 (3.2798)	LR 4.000e-03
:::MLLOG {"namespace": "", "time_ms": 1592471676973, "event_type": "INTERVAL_END", "key": "epoch_stop", "value": null, "metadata": {"file": "train.py", "lineno": 551, "epoch_num": 4}}
:::MLLOG {"namespace": "", "time_ms": 1592471676973, "event_type": "INTERVAL_START", "key": "eval_start", "value": null, "metadata": {"file": "train.py", "lineno": 556, "epoch_num": 4}}
0: Running evaluation on test set
0: TEST [3][0/1]	Time 0.227 (0.227)	Decoder iters 104.0 (104.0)	Tok/s 10924 (10924)
0: Running moses detokenizer
0: BLEU(score=22.697509719398813, counts=[35906, 17532, 9717, 5639], totals=[64134, 61131, 58128, 55132], precisions=[55.985904512427105, 28.67939343377337, 16.71655656482246, 10.228179641587463], bp=0.9915845541840296, sys_len=64134, ref_len=64676)
0: Finished evaluation on test set
:::MLLOG {"namespace": "", "time_ms": 1592471677468, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.22699999999999998, "metadata": {"file": "train.py", "lineno": 562, "epoch_num": 4}}
:::MLLOG {"namespace": "", "time_ms": 1592471677468, "event_type": "INTERVAL_END", "key": "eval_stop", "value": null, "metadata": {"file": "train.py", "lineno": 565, "epoch_num": 4}}
0: Summary: Epoch: 3	Training Loss: 3.2750	Test BLEU: 22.70
0: Performance: Epoch: 3	Training: 24944428 Tok/s
0: Finished epoch 3
:::MLLOG {"namespace": "", "time_ms": 1592471677469, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "train.py", "lineno": 584, "first_epoch_num": 4}}
:::MLLOG {"namespace": "", "time_ms": 1592471677469, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "train.py", "lineno": 538, "first_epoch_num": 5, "epoch_count": 1}}
:::MLLOG {"namespace": "", "time_ms": 1592471677469, "event_type": "INTERVAL_START", "key": "epoch_start", "value": null, "metadata": {"file": "train.py", "lineno": 541, "epoch_num": 5}}
0: Starting epoch 4
0: Sampler for epoch 4 uses seed 42286537
0: TRAIN [4][0/484]	Time 0.216 (0.216)	Data 1.70e-01 (1.70e-01)	Tok/s 9372 (9372)	Loss/tok 3.3154 (3.3154)	LR 4.000e-03
0: Upscaling, new scale: 128.0
0: TRAIN [4][10/484]	Time 0.016 (0.037)	Data 5.17e-05 (1.55e-02)	Tok/s 81905 (95159)	Loss/tok 2.8924 (3.1890)	LR 4.000e-03
0: TRAIN [4][20/484]	Time 0.016 (0.028)	Data 4.94e-05 (8.17e-03)	Tok/s 85678 (92634)	Loss/tok 3.3035 (3.2056)	LR 4.000e-03
0: TRAIN [4][30/484]	Time 0.016 (0.025)	Data 5.03e-05 (5.55e-03)	Tok/s 85190 (92214)	Loss/tok 3.1723 (3.1940)	LR 4.000e-03
0: TRAIN [4][40/484]	Time 0.023 (0.023)	Data 4.91e-05 (4.21e-03)	Tok/s 128589 (92057)	Loss/tok 3.1171 (3.1654)	LR 4.000e-03
0: TRAIN [4][50/484]	Time 0.019 (0.022)	Data 4.94e-05 (3.39e-03)	Tok/s 112971 (93072)	Loss/tok 3.1505 (3.1783)	LR 4.000e-03
0: TRAIN [4][60/484]	Time 0.019 (0.022)	Data 5.17e-05 (2.85e-03)	Tok/s 108355 (95538)	Loss/tok 3.2782 (3.2063)	LR 4.000e-03
0: TRAIN [4][70/484]	Time 0.013 (0.021)	Data 4.98e-05 (2.45e-03)	Tok/s 48774 (94200)	Loss/tok 2.3075 (3.1934)	LR 4.000e-03
0: TRAIN [4][80/484]	Time 0.016 (0.021)	Data 5.53e-05 (2.16e-03)	Tok/s 89307 (94917)	Loss/tok 3.1497 (3.1963)	LR 4.000e-03
0: TRAIN [4][90/484]	Time 0.019 (0.020)	Data 5.70e-05 (1.93e-03)	Tok/s 107427 (94527)	Loss/tok 3.1965 (3.1876)	LR 4.000e-03
0: TRAIN [4][100/484]	Time 0.027 (0.020)	Data 5.63e-05 (1.74e-03)	Tok/s 24769 (94014)	Loss/tok 2.6121 (3.1812)	LR 4.000e-03
0: TRAIN [4][110/484]	Time 0.013 (0.020)	Data 5.56e-05 (1.59e-03)	Tok/s 49456 (94966)	Loss/tok 2.6603 (3.2054)	LR 4.000e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 64.0
0: TRAIN [4][120/484]	Time 0.019 (0.020)	Data 5.39e-05 (1.46e-03)	Tok/s 112362 (96338)	Loss/tok 2.9597 (3.2108)	LR 4.000e-03
0: TRAIN [4][130/484]	Time 0.015 (0.020)	Data 5.44e-05 (1.35e-03)	Tok/s 84388 (95918)	Loss/tok 3.1594 (3.2159)	LR 4.000e-03
0: TRAIN [4][140/484]	Time 0.013 (0.019)	Data 5.53e-05 (1.26e-03)	Tok/s 46348 (95727)	Loss/tok 2.6417 (3.2193)	LR 4.000e-03
0: TRAIN [4][150/484]	Time 0.019 (0.019)	Data 5.63e-05 (1.18e-03)	Tok/s 109978 (96684)	Loss/tok 3.0415 (3.2197)	LR 4.000e-03
0: TRAIN [4][160/484]	Time 0.019 (0.019)	Data 5.70e-05 (1.11e-03)	Tok/s 109669 (97034)	Loss/tok 3.5054 (3.2239)	LR 4.000e-03
0: TRAIN [4][170/484]	Time 0.022 (0.019)	Data 6.03e-05 (1.05e-03)	Tok/s 128889 (97314)	Loss/tok 3.4082 (3.2255)	LR 4.000e-03
0: TRAIN [4][180/484]	Time 0.016 (0.019)	Data 5.56e-05 (9.96e-04)	Tok/s 82969 (97123)	Loss/tok 3.4216 (3.2258)	LR 4.000e-03
0: TRAIN [4][190/484]	Time 0.016 (0.019)	Data 5.01e-05 (9.47e-04)	Tok/s 82471 (97044)	Loss/tok 2.9141 (3.2338)	LR 4.000e-03
0: TRAIN [4][200/484]	Time 0.016 (0.019)	Data 5.20e-05 (9.02e-04)	Tok/s 80235 (96430)	Loss/tok 2.8564 (3.2314)	LR 4.000e-03
0: TRAIN [4][210/484]	Time 0.019 (0.019)	Data 5.01e-05 (8.62e-04)	Tok/s 112432 (96513)	Loss/tok 3.2837 (3.2258)	LR 4.000e-03
0: TRAIN [4][220/484]	Time 0.015 (0.019)	Data 4.91e-05 (8.25e-04)	Tok/s 79644 (96078)	Loss/tok 2.9129 (3.2239)	LR 4.000e-03
0: TRAIN [4][230/484]	Time 0.016 (0.019)	Data 4.86e-05 (7.91e-04)	Tok/s 77943 (96267)	Loss/tok 3.1186 (3.2228)	LR 4.000e-03
0: Upscaling, new scale: 128.0
0: TRAIN [4][240/484]	Time 0.015 (0.019)	Data 4.82e-05 (7.61e-04)	Tok/s 88561 (95645)	Loss/tok 2.7774 (3.2168)	LR 4.000e-03
0: TRAIN [4][250/484]	Time 0.026 (0.019)	Data 4.98e-05 (7.33e-04)	Tok/s 143195 (96772)	Loss/tok 3.4986 (3.2275)	LR 4.000e-03
0: TRAIN [4][260/484]	Time 0.016 (0.019)	Data 5.01e-05 (7.07e-04)	Tok/s 81266 (96599)	Loss/tok 2.8424 (3.2309)	LR 4.000e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 64.0
0: TRAIN [4][270/484]	Time 0.015 (0.019)	Data 4.94e-05 (6.83e-04)	Tok/s 83931 (97142)	Loss/tok 3.2047 (3.2340)	LR 4.000e-03
0: TRAIN [4][280/484]	Time 0.016 (0.019)	Data 5.10e-05 (6.60e-04)	Tok/s 77746 (96890)	Loss/tok 2.8838 (3.2327)	LR 4.000e-03
0: TRAIN [4][290/484]	Time 0.013 (0.019)	Data 4.86e-05 (6.39e-04)	Tok/s 46631 (96549)	Loss/tok 2.4977 (3.2296)	LR 4.000e-03
0: TRAIN [4][300/484]	Time 0.037 (0.019)	Data 4.98e-05 (6.20e-04)	Tok/s 57950 (96606)	Loss/tok 3.5201 (3.2302)	LR 4.000e-03
0: TRAIN [4][310/484]	Time 0.027 (0.019)	Data 5.10e-05 (6.01e-04)	Tok/s 76193 (96305)	Loss/tok 3.1343 (3.2279)	LR 4.000e-03
0: TRAIN [4][320/484]	Time 0.018 (0.019)	Data 8.34e-05 (5.84e-04)	Tok/s 72745 (96383)	Loss/tok 3.0775 (3.2234)	LR 4.000e-03
0: TRAIN [4][330/484]	Time 0.014 (0.019)	Data 5.60e-05 (5.68e-04)	Tok/s 44918 (96180)	Loss/tok 2.6481 (3.2209)	LR 4.000e-03
0: TRAIN [4][340/484]	Time 0.019 (0.019)	Data 5.56e-05 (5.54e-04)	Tok/s 112692 (96735)	Loss/tok 3.2811 (3.2230)	LR 2.000e-03
0: TRAIN [4][350/484]	Time 0.023 (0.019)	Data 5.39e-05 (5.39e-04)	Tok/s 131458 (96863)	Loss/tok 3.1283 (3.2217)	LR 2.000e-03
0: TRAIN [4][360/484]	Time 0.019 (0.019)	Data 5.44e-05 (5.26e-04)	Tok/s 112246 (97050)	Loss/tok 3.1668 (3.2200)	LR 2.000e-03
0: TRAIN [4][370/484]	Time 0.015 (0.019)	Data 7.92e-05 (5.13e-04)	Tok/s 87204 (96685)	Loss/tok 2.7118 (3.2144)	LR 2.000e-03
0: TRAIN [4][380/484]	Time 0.018 (0.019)	Data 5.39e-05 (5.01e-04)	Tok/s 112355 (96564)	Loss/tok 3.2210 (3.2123)	LR 2.000e-03
0: Upscaling, new scale: 128.0
0: TRAIN [4][390/484]	Time 0.026 (0.019)	Data 5.58e-05 (4.90e-04)	Tok/s 142119 (96633)	Loss/tok 3.4182 (3.2125)	LR 2.000e-03
0: TRAIN [4][400/484]	Time 0.018 (0.019)	Data 6.34e-05 (4.79e-04)	Tok/s 109048 (96793)	Loss/tok 3.4402 (3.2138)	LR 2.000e-03
0: TRAIN [4][410/484]	Time 0.026 (0.019)	Data 5.58e-05 (4.69e-04)	Tok/s 146142 (96812)	Loss/tok 3.5627 (3.2136)	LR 2.000e-03
0: TRAIN [4][420/484]	Time 0.022 (0.019)	Data 9.06e-05 (4.60e-04)	Tok/s 126739 (96780)	Loss/tok 3.5492 (3.2101)	LR 2.000e-03
0: TRAIN [4][430/484]	Time 0.016 (0.019)	Data 9.06e-05 (4.50e-04)	Tok/s 82054 (96921)	Loss/tok 3.2273 (3.2120)	LR 2.000e-03
0: TRAIN [4][440/484]	Time 0.022 (0.018)	Data 5.77e-05 (4.42e-04)	Tok/s 128477 (96594)	Loss/tok 3.1710 (3.2104)	LR 2.000e-03
0: TRAIN [4][450/484]	Time 0.016 (0.018)	Data 5.32e-05 (4.33e-04)	Tok/s 79726 (96532)	Loss/tok 3.0401 (3.2095)	LR 2.000e-03
0: TRAIN [4][460/484]	Time 0.019 (0.018)	Data 5.53e-05 (4.25e-04)	Tok/s 109506 (96616)	Loss/tok 2.8609 (3.2067)	LR 2.000e-03
0: TRAIN [4][470/484]	Time 0.023 (0.018)	Data 5.46e-05 (4.17e-04)	Tok/s 128394 (96814)	Loss/tok 3.4223 (3.2088)	LR 2.000e-03
0: TRAIN [4][480/484]	Time 0.019 (0.018)	Data 5.72e-05 (4.10e-04)	Tok/s 111489 (97112)	Loss/tok 3.2340 (3.2100)	LR 2.000e-03
:::MLLOG {"namespace": "", "time_ms": 1592471686482, "event_type": "INTERVAL_END", "key": "epoch_stop", "value": null, "metadata": {"file": "train.py", "lineno": 551, "epoch_num": 5}}
:::MLLOG {"namespace": "", "time_ms": 1592471686482, "event_type": "INTERVAL_START", "key": "eval_start", "value": null, "metadata": {"file": "train.py", "lineno": 556, "epoch_num": 5}}
0: Running evaluation on test set
0: TEST [4][0/1]	Time 0.239 (0.239)	Decoder iters 111.0 (111.0)	Tok/s 10863 (10863)
0: Running moses detokenizer
0: BLEU(score=23.21375778146722, counts=[36741, 18050, 10100, 5916], totals=[65374, 62371, 59368, 56371], precisions=[56.20124208400893, 28.939731606034854, 17.012532003773078, 10.49475794291391], bp=1.0, sys_len=65374, ref_len=64676)
0: Finished evaluation on test set
:::MLLOG {"namespace": "", "time_ms": 1592471686988, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.2321, "metadata": {"file": "train.py", "lineno": 562, "epoch_num": 5}}
:::MLLOG {"namespace": "", "time_ms": 1592471686989, "event_type": "INTERVAL_END", "key": "eval_stop", "value": null, "metadata": {"file": "train.py", "lineno": 565, "epoch_num": 5}}
0: Summary: Epoch: 4	Training Loss: 3.2008	Test BLEU: 23.21
0: Performance: Epoch: 4	Training: 24838320 Tok/s
0: Finished epoch 4
:::MLLOG {"namespace": "", "time_ms": 1592471686989, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "train.py", "lineno": 584, "first_epoch_num": 5}}
:::MLLOG {"namespace": "", "time_ms": 1592471686989, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "train.py", "lineno": 538, "first_epoch_num": 6, "epoch_count": 1}}
:::MLLOG {"namespace": "", "time_ms": 1592471686989, "event_type": "INTERVAL_START", "key": "epoch_start", "value": null, "metadata": {"file": "train.py", "lineno": 541, "epoch_num": 6}}
0: Starting epoch 5
0: Sampler for epoch 5 uses seed 3537644001
0: TRAIN [5][0/484]	Time 0.225 (0.225)	Data 1.68e-01 (1.68e-01)	Tok/s 9345 (9345)	Loss/tok 2.9684 (2.9684)	LR 2.000e-03
0: TRAIN [5][10/484]	Time 0.015 (0.037)	Data 8.34e-05 (1.53e-02)	Tok/s 80889 (87295)	Loss/tok 2.8275 (3.1377)	LR 2.000e-03
0: TRAIN [5][20/484]	Time 0.015 (0.027)	Data 8.32e-05 (8.04e-03)	Tok/s 85657 (91109)	Loss/tok 2.9639 (3.0902)	LR 2.000e-03
0: TRAIN [5][30/484]	Time 0.023 (0.025)	Data 6.25e-05 (5.47e-03)	Tok/s 123517 (93704)	Loss/tok 3.3551 (3.1003)	LR 2.000e-03
0: Upscaling, new scale: 256.0
0: TRAIN [5][40/484]	Time 0.016 (0.023)	Data 4.89e-05 (4.15e-03)	Tok/s 81756 (94895)	Loss/tok 2.7685 (3.0840)	LR 2.000e-03
0: TRAIN [5][50/484]	Time 0.015 (0.022)	Data 4.67e-05 (3.34e-03)	Tok/s 81401 (94678)	Loss/tok 3.2692 (3.1144)	LR 2.000e-03
0: TRAIN [5][60/484]	Time 0.019 (0.021)	Data 4.89e-05 (2.80e-03)	Tok/s 112931 (93134)	Loss/tok 2.9836 (3.0902)	LR 2.000e-03
0: TRAIN [5][70/484]	Time 0.016 (0.021)	Data 4.79e-05 (2.42e-03)	Tok/s 82101 (94209)	Loss/tok 2.8099 (3.0887)	LR 2.000e-03
0: TRAIN [5][80/484]	Time 0.019 (0.020)	Data 4.79e-05 (2.12e-03)	Tok/s 111308 (94142)	Loss/tok 2.9893 (3.0705)	LR 2.000e-03
0: TRAIN [5][90/484]	Time 0.015 (0.020)	Data 4.74e-05 (1.90e-03)	Tok/s 76602 (94475)	Loss/tok 2.9356 (3.0681)	LR 2.000e-03
0: TRAIN [5][100/484]	Time 0.016 (0.020)	Data 4.82e-05 (1.71e-03)	Tok/s 84111 (94131)	Loss/tok 2.8364 (3.0717)	LR 2.000e-03
0: TRAIN [5][110/484]	Time 0.016 (0.019)	Data 4.86e-05 (1.56e-03)	Tok/s 84394 (94247)	Loss/tok 2.8516 (3.0682)	LR 2.000e-03
0: TRAIN [5][120/484]	Time 0.026 (0.019)	Data 4.91e-05 (1.44e-03)	Tok/s 141158 (94907)	Loss/tok 3.2823 (3.0718)	LR 2.000e-03
0: TRAIN [5][130/484]	Time 0.019 (0.019)	Data 4.79e-05 (1.33e-03)	Tok/s 112385 (94977)	Loss/tok 3.1053 (3.0684)	LR 2.000e-03
0: TRAIN [5][140/484]	Time 0.019 (0.019)	Data 4.79e-05 (1.24e-03)	Tok/s 114389 (95815)	Loss/tok 3.1079 (3.0733)	LR 1.000e-03
0: TRAIN [5][150/484]	Time 0.022 (0.019)	Data 4.86e-05 (1.16e-03)	Tok/s 129399 (97063)	Loss/tok 3.1544 (3.0799)	LR 1.000e-03
0: TRAIN [5][160/484]	Time 0.015 (0.019)	Data 4.65e-05 (1.09e-03)	Tok/s 82402 (97267)	Loss/tok 2.8437 (3.0769)	LR 1.000e-03
0: Upscaling, new scale: 512.0
0: TRAIN [5][170/484]	Time 0.015 (0.019)	Data 4.89e-05 (1.03e-03)	Tok/s 83385 (96541)	Loss/tok 2.8426 (3.0751)	LR 1.000e-03
0: TRAIN [5][180/484]	Time 0.019 (0.019)	Data 4.91e-05 (9.79e-04)	Tok/s 109354 (96860)	Loss/tok 3.1000 (3.0752)	LR 1.000e-03
0: TRAIN [5][190/484]	Time 0.019 (0.019)	Data 7.99e-05 (9.30e-04)	Tok/s 108994 (96586)	Loss/tok 3.4120 (3.0755)	LR 1.000e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 256.0
0: TRAIN [5][200/484]	Time 0.019 (0.019)	Data 4.86e-05 (8.86e-04)	Tok/s 111580 (96290)	Loss/tok 3.1687 (3.0779)	LR 1.000e-03
0: TRAIN [5][210/484]	Time 0.019 (0.019)	Data 4.79e-05 (8.47e-04)	Tok/s 112663 (95936)	Loss/tok 3.1163 (3.0717)	LR 1.000e-03
0: TRAIN [5][220/484]	Time 0.012 (0.019)	Data 4.96e-05 (8.11e-04)	Tok/s 53558 (95788)	Loss/tok 2.3418 (3.0755)	LR 1.000e-03
0: TRAIN [5][230/484]	Time 0.016 (0.019)	Data 4.84e-05 (7.78e-04)	Tok/s 81936 (95755)	Loss/tok 2.7718 (3.0702)	LR 1.000e-03
0: TRAIN [5][240/484]	Time 0.016 (0.019)	Data 4.84e-05 (7.48e-04)	Tok/s 82419 (95755)	Loss/tok 3.1701 (3.0695)	LR 1.000e-03
0: TRAIN [5][250/484]	Time 0.023 (0.019)	Data 5.08e-05 (7.20e-04)	Tok/s 126060 (95635)	Loss/tok 3.4140 (3.0725)	LR 1.000e-03
0: TRAIN [5][260/484]	Time 0.023 (0.019)	Data 4.84e-05 (6.95e-04)	Tok/s 127018 (95363)	Loss/tok 3.1177 (3.0703)	LR 1.000e-03
0: TRAIN [5][270/484]	Time 0.016 (0.019)	Data 4.86e-05 (6.71e-04)	Tok/s 88195 (95625)	Loss/tok 3.0603 (3.0669)	LR 1.000e-03
0: TRAIN [5][280/484]	Time 0.016 (0.018)	Data 4.89e-05 (6.49e-04)	Tok/s 86401 (95593)	Loss/tok 2.5972 (3.0646)	LR 1.000e-03
0: TRAIN [5][290/484]	Time 0.016 (0.018)	Data 4.94e-05 (6.29e-04)	Tok/s 80455 (95910)	Loss/tok 2.7922 (3.0675)	LR 1.000e-03
0: TRAIN [5][300/484]	Time 0.022 (0.019)	Data 6.63e-05 (6.09e-04)	Tok/s 128245 (96289)	Loss/tok 3.1994 (3.0708)	LR 1.000e-03
0: TRAIN [5][310/484]	Time 0.023 (0.019)	Data 4.91e-05 (5.91e-04)	Tok/s 127902 (96605)	Loss/tok 3.1815 (3.0700)	LR 1.000e-03
0: TRAIN [5][320/484]	Time 0.022 (0.019)	Data 5.82e-05 (5.75e-04)	Tok/s 128526 (96626)	Loss/tok 3.3260 (3.0685)	LR 1.000e-03
0: Upscaling, new scale: 512.0
0: TRAIN [5][330/484]	Time 0.016 (0.019)	Data 4.94e-05 (5.59e-04)	Tok/s 84936 (96703)	Loss/tok 2.8672 (3.0698)	LR 1.000e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 256.0
0: TRAIN [5][340/484]	Time 0.026 (0.019)	Data 4.86e-05 (5.44e-04)	Tok/s 136383 (97094)	Loss/tok 3.5076 (3.0708)	LR 1.000e-03
0: TRAIN [5][350/484]	Time 0.016 (0.019)	Data 4.94e-05 (5.30e-04)	Tok/s 85126 (97371)	Loss/tok 2.7486 (3.0703)	LR 1.000e-03
0: TRAIN [5][360/484]	Time 0.022 (0.019)	Data 4.84e-05 (5.17e-04)	Tok/s 129055 (97732)	Loss/tok 3.3528 (3.0749)	LR 1.000e-03
0: TRAIN [5][370/484]	Time 0.016 (0.019)	Data 4.82e-05 (5.04e-04)	Tok/s 83702 (97722)	Loss/tok 2.9578 (3.0756)	LR 1.000e-03
0: TRAIN [5][380/484]	Time 0.022 (0.019)	Data 4.89e-05 (4.92e-04)	Tok/s 130679 (97676)	Loss/tok 3.2488 (3.0745)	LR 1.000e-03
0: TRAIN [5][390/484]	Time 0.013 (0.018)	Data 4.84e-05 (4.81e-04)	Tok/s 49948 (97523)	Loss/tok 2.4180 (3.0753)	LR 1.000e-03
0: TRAIN [5][400/484]	Time 0.016 (0.019)	Data 4.77e-05 (4.70e-04)	Tok/s 85166 (97478)	Loss/tok 2.9213 (3.0773)	LR 1.000e-03
0: TRAIN [5][410/484]	Time 0.013 (0.019)	Data 4.74e-05 (4.60e-04)	Tok/s 51269 (97385)	Loss/tok 2.4862 (3.0785)	LR 1.000e-03
0: TRAIN [5][420/484]	Time 0.018 (0.019)	Data 7.06e-05 (4.50e-04)	Tok/s 112738 (97349)	Loss/tok 2.9506 (3.0791)	LR 1.000e-03
0: TRAIN [5][430/484]	Time 0.016 (0.018)	Data 4.67e-05 (4.41e-04)	Tok/s 86400 (97189)	Loss/tok 3.0714 (3.0768)	LR 5.000e-04
0: TRAIN [5][440/484]	Time 0.019 (0.018)	Data 4.94e-05 (4.32e-04)	Tok/s 113079 (97352)	Loss/tok 3.1326 (3.0779)	LR 5.000e-04
0: TRAIN [5][450/484]	Time 0.015 (0.018)	Data 4.82e-05 (4.23e-04)	Tok/s 80322 (97332)	Loss/tok 2.9814 (3.0785)	LR 5.000e-04
0: TRAIN [5][460/484]	Time 0.013 (0.018)	Data 4.74e-05 (4.15e-04)	Tok/s 49544 (97225)	Loss/tok 2.5255 (3.0796)	LR 5.000e-04
0: Upscaling, new scale: 512.0
0: TRAIN [5][470/484]	Time 0.015 (0.018)	Data 4.98e-05 (4.07e-04)	Tok/s 85454 (97449)	Loss/tok 2.8466 (3.0773)	LR 5.000e-04
0: TRAIN [5][480/484]	Time 0.013 (0.018)	Data 4.91e-05 (4.00e-04)	Tok/s 50640 (97225)	Loss/tok 2.3852 (3.0758)	LR 5.000e-04
:::MLLOG {"namespace": "", "time_ms": 1592471695966, "event_type": "INTERVAL_END", "key": "epoch_stop", "value": null, "metadata": {"file": "train.py", "lineno": 551, "epoch_num": 6}}
:::MLLOG {"namespace": "", "time_ms": 1592471695966, "event_type": "INTERVAL_START", "key": "eval_start", "value": null, "metadata": {"file": "train.py", "lineno": 556, "epoch_num": 6}}
0: Running evaluation on test set
0: TEST [5][0/1]	Time 0.217 (0.217)	Decoder iters 100.0 (100.0)	Tok/s 11744 (11744)
0: Running moses detokenizer
0: BLEU(score=24.19936201888501, counts=[36970, 18540, 10591, 6281], totals=[64981, 61978, 58975, 55978], precisions=[56.89355350025392, 29.913840394978862, 17.958456973293767, 11.220479474079102], bp=1.0, sys_len=64981, ref_len=64676)
0: Target accuracy reached
0: Finished evaluation on test set
:::MLLOG {"namespace": "", "time_ms": 1592471696383, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.242, "metadata": {"file": "train.py", "lineno": 562, "epoch_num": 6}}
:::MLLOG {"namespace": "", "time_ms": 1592471696383, "event_type": "INTERVAL_END", "key": "eval_stop", "value": null, "metadata": {"file": "train.py", "lineno": 565, "epoch_num": 6}}
0: Summary: Epoch: 5	Training Loss: 3.0823	Test BLEU: 24.20
0: Performance: Epoch: 5	Training: 24900896 Tok/s
0: Finished epoch 5
:::MLLOG {"namespace": "", "time_ms": 1592471696383, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "train.py", "lineno": 584, "first_epoch_num": 6}}
0: Closing preprocessed data file
:::MLLOG {"namespace": "", "time_ms": 1592471696384, "event_type": "INTERVAL_END", "key": "run_stop", "value": null, "metadata": {"file": "train.py", "lineno": 595, "status": "success"}}
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
ENDING TIMING RUN AT 2020-06-18 02:15:03 AM
RESULT,RNN_TRANSLATOR,,93,nvidia,2020-06-18 02:13:30 AM
ENDING TIMING RUN AT 2020-06-18 02:15:03 AM
RESULT,RNN_TRANSLATOR,,93,nvidia,2020-06-18 02:13:30 AM
ENDING TIMING RUN AT 2020-06-18 02:15:03 AM
RESULT,RNN_TRANSLATOR,,93,nvidia,2020-06-18 02:13:30 AM
ENDING TIMING RUN AT 2020-06-18 02:15:03 AM
RESULT,RNN_TRANSLATOR,,93,nvidia,2020-06-18 02:13:30 AM
ENDING TIMING RUN AT 2020-06-18 02:15:03 AM
RESULT,RNN_TRANSLATOR,,93,nvidia,2020-06-18 02:13:30 AM
ENDING TIMING RUN AT 2020-06-18 02:15:03 AM
RESULT,RNN_TRANSLATOR,,93,nvidia,2020-06-18 02:13:30 AM
ENDING TIMING RUN AT 2020-06-18 02:15:03 AM
RESULT,RNN_TRANSLATOR,,93,nvidia,2020-06-18 02:13:30 AM
ENDING TIMING RUN AT 2020-06-18 02:15:03 AM
RESULT,RNN_TRANSLATOR,,92,nvidia,2020-06-18 02:13:31 AM
ENDING TIMING RUN AT 2020-06-18 02:15:03 AM
RESULT,RNN_TRANSLATOR,,93,nvidia,2020-06-18 02:13:30 AM
ENDING TIMING RUN AT 2020-06-18 02:15:03 AM
RESULT,RNN_TRANSLATOR,,93,nvidia,2020-06-18 02:13:30 AM
ENDING TIMING RUN AT 2020-06-18 02:15:03 AM
RESULT,RNN_TRANSLATOR,,93,nvidia,2020-06-18 02:13:30 AM
ENDING TIMING RUN AT 2020-06-18 02:15:03 AM
RESULT,RNN_TRANSLATOR,,93,nvidia,2020-06-18 02:13:30 AM
ENDING TIMING RUN AT 2020-06-18 02:15:03 AM
RESULT,RNN_TRANSLATOR,,93,nvidia,2020-06-18 02:13:30 AM
ENDING TIMING RUN AT 2020-06-18 02:15:03 AM
RESULT,RNN_TRANSLATOR,,93,nvidia,2020-06-18 02:13:30 AM
ENDING TIMING RUN AT 2020-06-18 02:15:03 AM
RESULT,RNN_TRANSLATOR,,93,nvidia,2020-06-18 02:13:30 AM
ENDING TIMING RUN AT 2020-06-18 02:15:03 AM
RESULT,RNN_TRANSLATOR,,93,nvidia,2020-06-18 02:13:30 AM
ENDING TIMING RUN AT 2020-06-18 02:15:03 AM
RESULT,RNN_TRANSLATOR,,93,nvidia,2020-06-18 02:13:30 AM
ENDING TIMING RUN AT 2020-06-18 02:15:03 AM
RESULT,RNN_TRANSLATOR,,93,nvidia,2020-06-18 02:13:30 AM
ENDING TIMING RUN AT 2020-06-18 02:15:03 AM
RESULT,RNN_TRANSLATOR,,93,nvidia,2020-06-18 02:13:30 AM
ENDING TIMING RUN AT 2020-06-18 02:15:03 AM
RESULT,RNN_TRANSLATOR,,93,nvidia,2020-06-18 02:13:30 AM
ENDING TIMING RUN AT 2020-06-18 02:15:03 AM
RESULT,RNN_TRANSLATOR,,93,nvidia,2020-06-18 02:13:30 AM
ENDING TIMING RUN AT 2020-06-18 02:15:03 AM
RESULT,RNN_TRANSLATOR,,93,nvidia,2020-06-18 02:13:30 AM
ENDING TIMING RUN AT 2020-06-18 02:15:03 AM
RESULT,RNN_TRANSLATOR,,93,nvidia,2020-06-18 02:13:30 AM
ENDING TIMING RUN AT 2020-06-18 02:15:03 AM
RESULT,RNN_TRANSLATOR,,93,nvidia,2020-06-18 02:13:30 AM
ENDING TIMING RUN AT 2020-06-18 02:15:03 AM
RESULT,RNN_TRANSLATOR,,93,nvidia,2020-06-18 02:13:30 AM
ENDING TIMING RUN AT 2020-06-18 02:15:03 AM
RESULT,RNN_TRANSLATOR,,93,nvidia,2020-06-18 02:13:30 AM
ENDING TIMING RUN AT 2020-06-18 02:15:03 AM
ENDING TIMING RUN AT 2020-06-18 02:15:03 AM
RESULT,RNN_TRANSLATOR,,93,nvidia,2020-06-18 02:13:30 AM
RESULT,RNN_TRANSLATOR,,93,nvidia,2020-06-18 02:13:30 AM
ENDING TIMING RUN AT 2020-06-18 02:15:03 AM
RESULT,RNN_TRANSLATOR,,93,nvidia,2020-06-18 02:13:30 AM
ENDING TIMING RUN AT 2020-06-18 02:15:03 AM
RESULT,RNN_TRANSLATOR,,93,nvidia,2020-06-18 02:13:30 AM
ENDING TIMING RUN AT 2020-06-18 02:15:03 AM
RESULT,RNN_TRANSLATOR,,93,nvidia,2020-06-18 02:13:30 AM
ENDING TIMING RUN AT 2020-06-18 02:15:03 AM
RESULT,RNN_TRANSLATOR,,93,nvidia,2020-06-18 02:13:30 AM
ENDING TIMING RUN AT 2020-06-18 02:15:04 AM
RESULT,RNN_TRANSLATOR,,94,nvidia,2020-06-18 02:13:30 AM
ENDING TIMING RUN AT 2020-06-18 02:15:04 AM
RESULT,RNN_TRANSLATOR,,94,nvidia,2020-06-18 02:13:30 AM
ENDING TIMING RUN AT 2020-06-18 02:15:04 AM
RESULT,RNN_TRANSLATOR,,94,nvidia,2020-06-18 02:13:30 AM
ENDING TIMING RUN AT 2020-06-18 02:15:04 AM
ENDING TIMING RUN AT 2020-06-18 02:15:04 AM
ENDING TIMING RUN AT 2020-06-18 02:15:04 AM
RESULT,RNN_TRANSLATOR,,94,nvidia,2020-06-18 02:13:30 AM
RESULT,RNN_TRANSLATOR,,94,nvidia,2020-06-18 02:13:30 AM
RESULT,RNN_TRANSLATOR,,94,nvidia,2020-06-18 02:13:30 AM
ENDING TIMING RUN AT 2020-06-18 02:15:04 AM
RESULT,RNN_TRANSLATOR,,94,nvidia,2020-06-18 02:13:30 AM
ENDING TIMING RUN AT 2020-06-18 02:15:04 AM
RESULT,RNN_TRANSLATOR,,94,nvidia,2020-06-18 02:13:30 AM
ENDING TIMING RUN AT 2020-06-18 02:15:04 AM
RESULT,RNN_TRANSLATOR,,94,nvidia,2020-06-18 02:13:30 AM
ENDING TIMING RUN AT 2020-06-18 02:15:04 AM
ENDING TIMING RUN AT 2020-06-18 02:15:04 AM
RESULT,RNN_TRANSLATOR,,94,nvidia,2020-06-18 02:13:30 AM
RESULT,RNN_TRANSLATOR,,94,nvidia,2020-06-18 02:13:30 AM
ENDING TIMING RUN AT 2020-06-18 02:15:04 AM
RESULT,RNN_TRANSLATOR,,94,nvidia,2020-06-18 02:13:30 AM
ENDING TIMING RUN AT 2020-06-18 02:15:04 AM
RESULT,RNN_TRANSLATOR,,94,nvidia,2020-06-18 02:13:30 AM
ENDING TIMING RUN AT 2020-06-18 02:15:04 AM
RESULT,RNN_TRANSLATOR,,94,nvidia,2020-06-18 02:13:30 AM
ENDING TIMING RUN AT 2020-06-18 02:15:04 AM
RESULT,RNN_TRANSLATOR,,94,nvidia,2020-06-18 02:13:30 AM
ENDING TIMING RUN AT 2020-06-18 02:15:04 AM
RESULT,RNN_TRANSLATOR,,94,nvidia,2020-06-18 02:13:30 AM
ENDING TIMING RUN AT 2020-06-18 02:15:04 AM
RESULT,RNN_TRANSLATOR,,94,nvidia,2020-06-18 02:13:30 AM
ENDING TIMING RUN AT 2020-06-18 02:15:04 AM
RESULT,RNN_TRANSLATOR,,94,nvidia,2020-06-18 02:13:30 AM
ENDING TIMING RUN AT 2020-06-18 02:15:04 AM
RESULT,RNN_TRANSLATOR,,94,nvidia,2020-06-18 02:13:30 AM
ENDING TIMING RUN AT 2020-06-18 02:15:04 AM
RESULT,RNN_TRANSLATOR,,94,nvidia,2020-06-18 02:13:30 AM
ENDING TIMING RUN AT 2020-06-18 02:15:04 AM
RESULT,RNN_TRANSLATOR,,94,nvidia,2020-06-18 02:13:30 AM
ENDING TIMING RUN AT 2020-06-18 02:15:04 AM
RESULT,RNN_TRANSLATOR,,94,nvidia,2020-06-18 02:13:30 AM
ENDING TIMING RUN AT 2020-06-18 02:15:04 AM
RESULT,RNN_TRANSLATOR,,94,nvidia,2020-06-18 02:13:30 AM
ENDING TIMING RUN AT 2020-06-18 02:15:04 AM
RESULT,RNN_TRANSLATOR,,94,nvidia,2020-06-18 02:13:30 AM
ENDING TIMING RUN AT 2020-06-18 02:15:04 AM
RESULT,RNN_TRANSLATOR,,94,nvidia,2020-06-18 02:13:30 AM
ENDING TIMING RUN AT 2020-06-18 02:15:04 AM
RESULT,RNN_TRANSLATOR,,94,nvidia,2020-06-18 02:13:30 AM
ENDING TIMING RUN AT 2020-06-18 02:15:04 AM
ENDING TIMING RUN AT 2020-06-18 02:15:04 AM
RESULT,RNN_TRANSLATOR,,94,nvidia,2020-06-18 02:13:30 AM
RESULT,RNN_TRANSLATOR,,94,nvidia,2020-06-18 02:13:30 AM
ENDING TIMING RUN AT 2020-06-18 02:15:04 AM
RESULT,RNN_TRANSLATOR,,94,nvidia,2020-06-18 02:13:30 AM
ENDING TIMING RUN AT 2020-06-18 02:15:04 AM
RESULT,RNN_TRANSLATOR,,94,nvidia,2020-06-18 02:13:30 AM
ENDING TIMING RUN AT 2020-06-18 02:15:04 AM
RESULT,RNN_TRANSLATOR,,94,nvidia,2020-06-18 02:13:30 AM
ENDING TIMING RUN AT 2020-06-18 02:15:04 AM
RESULT,RNN_TRANSLATOR,,94,nvidia,2020-06-18 02:13:30 AM
ENDING TIMING RUN AT 2020-06-18 02:15:04 AM
RESULT,RNN_TRANSLATOR,,94,nvidia,2020-06-18 02:13:30 AM
ENDING TIMING RUN AT 2020-06-18 02:15:04 AM
RESULT,RNN_TRANSLATOR,,94,nvidia,2020-06-18 02:13:30 AM
ENDING TIMING RUN AT 2020-06-18 02:15:04 AM
RESULT,RNN_TRANSLATOR,,94,nvidia,2020-06-18 02:13:30 AM
ENDING TIMING RUN AT 2020-06-18 02:15:04 AM
RESULT,RNN_TRANSLATOR,,94,nvidia,2020-06-18 02:13:30 AM
ENDING TIMING RUN AT 2020-06-18 02:15:04 AM
ENDING TIMING RUN AT 2020-06-18 02:15:04 AM
ENDING TIMING RUN AT 2020-06-18 02:15:04 AM
RESULT,RNN_TRANSLATOR,,94,nvidia,2020-06-18 02:13:30 AM
RESULT,RNN_TRANSLATOR,,94,nvidia,2020-06-18 02:13:30 AM
RESULT,RNN_TRANSLATOR,,94,nvidia,2020-06-18 02:13:30 AM
ENDING TIMING RUN AT 2020-06-18 02:15:04 AM
RESULT,RNN_TRANSLATOR,,94,nvidia,2020-06-18 02:13:30 AM
ENDING TIMING RUN AT 2020-06-18 02:15:04 AM
RESULT,RNN_TRANSLATOR,,94,nvidia,2020-06-18 02:13:30 AM
ENDING TIMING RUN AT 2020-06-18 02:15:04 AM
RESULT,RNN_TRANSLATOR,,94,nvidia,2020-06-18 02:13:30 AM
ENDING TIMING RUN AT 2020-06-18 02:15:04 AM
RESULT,RNN_TRANSLATOR,,94,nvidia,2020-06-18 02:13:30 AM
ENDING TIMING RUN AT 2020-06-18 02:15:04 AM
ENDING TIMING RUN AT 2020-06-18 02:15:04 AM
RESULT,RNN_TRANSLATOR,,94,nvidia,2020-06-18 02:13:30 AM
RESULT,RNN_TRANSLATOR,,94,nvidia,2020-06-18 02:13:30 AM
ENDING TIMING RUN AT 2020-06-18 02:15:04 AM
RESULT,RNN_TRANSLATOR,,94,nvidia,2020-06-18 02:13:30 AM
ENDING TIMING RUN AT 2020-06-18 02:15:04 AM
RESULT,RNN_TRANSLATOR,,94,nvidia,2020-06-18 02:13:30 AM
ENDING TIMING RUN AT 2020-06-18 02:15:04 AM
RESULT,RNN_TRANSLATOR,,94,nvidia,2020-06-18 02:13:30 AM
ENDING TIMING RUN AT 2020-06-18 02:15:04 AM
RESULT,RNN_TRANSLATOR,,94,nvidia,2020-06-18 02:13:30 AM
ENDING TIMING RUN AT 2020-06-18 02:15:04 AM
RESULT,RNN_TRANSLATOR,,94,nvidia,2020-06-18 02:13:30 AM
ENDING TIMING RUN AT 2020-06-18 02:15:04 AM
RESULT,RNN_TRANSLATOR,,94,nvidia,2020-06-18 02:13:30 AM
ENDING TIMING RUN AT 2020-06-18 02:15:04 AM
ENDING TIMING RUN AT 2020-06-18 02:15:04 AM
RESULT,RNN_TRANSLATOR,,94,nvidia,2020-06-18 02:13:30 AM
RESULT,RNN_TRANSLATOR,,94,nvidia,2020-06-18 02:13:30 AM
ENDING TIMING RUN AT 2020-06-18 02:15:04 AM
RESULT,RNN_TRANSLATOR,,94,nvidia,2020-06-18 02:13:30 AM
ENDING TIMING RUN AT 2020-06-18 02:15:04 AM
RESULT,RNN_TRANSLATOR,,94,nvidia,2020-06-18 02:13:30 AM
ENDING TIMING RUN AT 2020-06-18 02:15:04 AM
RESULT,RNN_TRANSLATOR,,94,nvidia,2020-06-18 02:13:30 AM
ENDING TIMING RUN AT 2020-06-18 02:15:04 AM
RESULT,RNN_TRANSLATOR,,94,nvidia,2020-06-18 02:13:30 AM
ENDING TIMING RUN AT 2020-06-18 02:15:04 AM
RESULT,RNN_TRANSLATOR,,94,nvidia,2020-06-18 02:13:30 AM
ENDING TIMING RUN AT 2020-06-18 02:15:04 AM
ENDING TIMING RUN AT 2020-06-18 02:15:04 AM
RESULT,RNN_TRANSLATOR,,94,nvidia,2020-06-18 02:13:30 AM
RESULT,RNN_TRANSLATOR,,94,nvidia,2020-06-18 02:13:30 AM
ENDING TIMING RUN AT 2020-06-18 02:15:04 AM
RESULT,RNN_TRANSLATOR,,94,nvidia,2020-06-18 02:13:30 AM
ENDING TIMING RUN AT 2020-06-18 02:15:04 AM
RESULT,RNN_TRANSLATOR,,94,nvidia,2020-06-18 02:13:30 AM
ENDING TIMING RUN AT 2020-06-18 02:15:04 AM
ENDING TIMING RUN AT 2020-06-18 02:15:04 AM
ENDING TIMING RUN AT 2020-06-18 02:15:04 AM
RESULT,RNN_TRANSLATOR,,94,nvidia,2020-06-18 02:13:30 AM
ENDING TIMING RUN AT 2020-06-18 02:15:04 AM
RESULT,RNN_TRANSLATOR,,94,nvidia,2020-06-18 02:13:30 AM
RESULT,RNN_TRANSLATOR,,94,nvidia,2020-06-18 02:13:30 AM
RESULT,RNN_TRANSLATOR,,94,nvidia,2020-06-18 02:13:30 AM
ENDING TIMING RUN AT 2020-06-18 02:15:04 AM
RESULT,RNN_TRANSLATOR,,94,nvidia,2020-06-18 02:13:30 AM
ENDING TIMING RUN AT 2020-06-18 02:15:04 AM
RESULT,RNN_TRANSLATOR,,94,nvidia,2020-06-18 02:13:30 AM
ENDING TIMING RUN AT 2020-06-18 02:15:04 AM
ENDING TIMING RUN AT 2020-06-18 02:15:04 AM
RESULT,RNN_TRANSLATOR,,94,nvidia,2020-06-18 02:13:30 AM
RESULT,RNN_TRANSLATOR,,94,nvidia,2020-06-18 02:13:30 AM
ENDING TIMING RUN AT 2020-06-18 02:15:04 AM
RESULT,RNN_TRANSLATOR,,94,nvidia,2020-06-18 02:13:30 AM
ENDING TIMING RUN AT 2020-06-18 02:15:04 AM
RESULT,RNN_TRANSLATOR,,94,nvidia,2020-06-18 02:13:30 AM
ENDING TIMING RUN AT 2020-06-18 02:15:04 AM
RESULT,RNN_TRANSLATOR,,94,nvidia,2020-06-18 02:13:30 AM
ENDING TIMING RUN AT 2020-06-18 02:15:04 AM
RESULT,RNN_TRANSLATOR,,94,nvidia,2020-06-18 02:13:30 AM
ENDING TIMING RUN AT 2020-06-18 02:15:04 AM
RESULT,RNN_TRANSLATOR,,94,nvidia,2020-06-18 02:13:30 AM
ENDING TIMING RUN AT 2020-06-18 02:15:04 AM
RESULT,RNN_TRANSLATOR,,94,nvidia,2020-06-18 02:13:30 AM
ENDING TIMING RUN AT 2020-06-18 02:15:04 AM
ENDING TIMING RUN AT 2020-06-18 02:15:04 AM
RESULT,RNN_TRANSLATOR,,94,nvidia,2020-06-18 02:13:30 AM
RESULT,RNN_TRANSLATOR,,94,nvidia,2020-06-18 02:13:30 AM
ENDING TIMING RUN AT 2020-06-18 02:15:04 AM
RESULT,RNN_TRANSLATOR,,94,nvidia,2020-06-18 02:13:30 AM
ENDING TIMING RUN AT 2020-06-18 02:15:04 AM
ENDING TIMING RUN AT 2020-06-18 02:15:04 AM
RESULT,RNN_TRANSLATOR,,94,nvidia,2020-06-18 02:13:30 AM
RESULT,RNN_TRANSLATOR,,94,nvidia,2020-06-18 02:13:30 AM
ENDING TIMING RUN AT 2020-06-18 02:15:04 AM
ENDING TIMING RUN AT 2020-06-18 02:15:04 AM
RESULT,RNN_TRANSLATOR,,94,nvidia,2020-06-18 02:13:30 AM
RESULT,RNN_TRANSLATOR,,94,nvidia,2020-06-18 02:13:30 AM
ENDING TIMING RUN AT 2020-06-18 02:15:04 AM
RESULT,RNN_TRANSLATOR,,94,nvidia,2020-06-18 02:13:30 AM
ENDING TIMING RUN AT 2020-06-18 02:15:04 AM
RESULT,RNN_TRANSLATOR,,94,nvidia,2020-06-18 02:13:30 AM
ENDING TIMING RUN AT 2020-06-18 02:15:04 AM
RESULT,RNN_TRANSLATOR,,94,nvidia,2020-06-18 02:13:30 AM
ENDING TIMING RUN AT 2020-06-18 02:15:04 AM
RESULT,RNN_TRANSLATOR,,94,nvidia,2020-06-18 02:13:30 AM
ENDING TIMING RUN AT 2020-06-18 02:15:04 AM
ENDING TIMING RUN AT 2020-06-18 02:15:04 AM
RESULT,RNN_TRANSLATOR,,94,nvidia,2020-06-18 02:13:30 AM
RESULT,RNN_TRANSLATOR,,94,nvidia,2020-06-18 02:13:30 AM
ENDING TIMING RUN AT 2020-06-18 02:15:04 AM
RESULT,RNN_TRANSLATOR,,94,nvidia,2020-06-18 02:13:30 AM
ENDING TIMING RUN AT 2020-06-18 02:15:04 AM
ENDING TIMING RUN AT 2020-06-18 02:15:04 AM
RESULT,RNN_TRANSLATOR,,94,nvidia,2020-06-18 02:13:30 AM
RESULT,RNN_TRANSLATOR,,94,nvidia,2020-06-18 02:13:30 AM
ENDING TIMING RUN AT 2020-06-18 02:15:04 AM
RESULT,RNN_TRANSLATOR,,94,nvidia,2020-06-18 02:13:30 AM
ENDING TIMING RUN AT 2020-06-18 02:15:04 AM
RESULT,RNN_TRANSLATOR,,94,nvidia,2020-06-18 02:13:30 AM
ENDING TIMING RUN AT 2020-06-18 02:15:04 AM
RESULT,RNN_TRANSLATOR,,94,nvidia,2020-06-18 02:13:30 AM
ENDING TIMING RUN AT 2020-06-18 02:15:04 AM
RESULT,RNN_TRANSLATOR,,94,nvidia,2020-06-18 02:13:30 AM
ENDING TIMING RUN AT 2020-06-18 02:15:04 AM
RESULT,RNN_TRANSLATOR,,94,nvidia,2020-06-18 02:13:30 AM
ENDING TIMING RUN AT 2020-06-18 02:15:04 AM
RESULT,RNN_TRANSLATOR,,94,nvidia,2020-06-18 02:13:30 AM
ENDING TIMING RUN AT 2020-06-18 02:15:04 AM
ENDING TIMING RUN AT 2020-06-18 02:15:04 AM
ENDING TIMING RUN AT 2020-06-18 02:15:04 AM
RESULT,RNN_TRANSLATOR,,94,nvidia,2020-06-18 02:13:30 AM
RESULT,RNN_TRANSLATOR,,94,nvidia,2020-06-18 02:13:30 AM
RESULT,RNN_TRANSLATOR,,94,nvidia,2020-06-18 02:13:30 AM
ENDING TIMING RUN AT 2020-06-18 02:15:04 AM
RESULT,RNN_TRANSLATOR,,94,nvidia,2020-06-18 02:13:30 AM
ENDING TIMING RUN AT 2020-06-18 02:15:04 AM
ENDING TIMING RUN AT 2020-06-18 02:15:04 AM
RESULT,RNN_TRANSLATOR,,94,nvidia,2020-06-18 02:13:30 AM
ENDING TIMING RUN AT 2020-06-18 02:15:04 AM
RESULT,RNN_TRANSLATOR,,94,nvidia,2020-06-18 02:13:30 AM
RESULT,RNN_TRANSLATOR,,94,nvidia,2020-06-18 02:13:30 AM
ENDING TIMING RUN AT 2020-06-18 02:15:04 AM
RESULT,RNN_TRANSLATOR,,94,nvidia,2020-06-18 02:13:30 AM
ENDING TIMING RUN AT 2020-06-18 02:15:04 AM
RESULT,RNN_TRANSLATOR,,94,nvidia,2020-06-18 02:13:30 AM
ENDING TIMING RUN AT 2020-06-18 02:15:04 AM
RESULT,RNN_TRANSLATOR,,94,nvidia,2020-06-18 02:13:30 AM
ENDING TIMING RUN AT 2020-06-18 02:15:04 AM
RESULT,RNN_TRANSLATOR,,94,nvidia,2020-06-18 02:13:30 AM
ENDING TIMING RUN AT 2020-06-18 02:15:04 AM
RESULT,RNN_TRANSLATOR,,94,nvidia,2020-06-18 02:13:30 AM
ENDING TIMING RUN AT 2020-06-18 02:15:04 AM
ENDING TIMING RUN AT 2020-06-18 02:15:04 AM
RESULT,RNN_TRANSLATOR,,94,nvidia,2020-06-18 02:13:30 AM
RESULT,RNN_TRANSLATOR,,94,nvidia,2020-06-18 02:13:30 AM
slurmstepd: error: _is_a_lwp: open() /proc/182958/status failed: No such file or directory
ENDING TIMING RUN AT 2020-06-18 02:15:04 AM
RESULT,RNN_TRANSLATOR,,94,nvidia,2020-06-18 02:13:30 AM
ENDING TIMING RUN AT 2020-06-18 02:15:04 AM
RESULT,RNN_TRANSLATOR,,94,nvidia,2020-06-18 02:13:30 AM
ENDING TIMING RUN AT 2020-06-18 02:15:04 AM
RESULT,RNN_TRANSLATOR,,94,nvidia,2020-06-18 02:13:30 AM
ENDING TIMING RUN AT 2020-06-18 02:15:04 AM
RESULT,RNN_TRANSLATOR,,93,nvidia,2020-06-18 02:13:31 AM
ENDING TIMING RUN AT 2020-06-18 02:15:04 AM
ENDING TIMING RUN AT 2020-06-18 02:15:04 AM
RESULT,RNN_TRANSLATOR,,93,nvidia,2020-06-18 02:13:31 AM
RESULT,RNN_TRANSLATOR,,93,nvidia,2020-06-18 02:13:31 AM
ENDING TIMING RUN AT 2020-06-18 02:15:04 AM
RESULT,RNN_TRANSLATOR,,93,nvidia,2020-06-18 02:13:31 AM
ENDING TIMING RUN AT 2020-06-18 02:15:04 AM
ENDING TIMING RUN AT 2020-06-18 02:15:04 AM
RESULT,RNN_TRANSLATOR,,93,nvidia,2020-06-18 02:13:31 AM
RESULT,RNN_TRANSLATOR,,93,nvidia,2020-06-18 02:13:31 AM
ENDING TIMING RUN AT 2020-06-18 02:15:04 AM
RESULT,RNN_TRANSLATOR,,94,nvidia,2020-06-18 02:13:30 AM
ENDING TIMING RUN AT 2020-06-18 02:15:04 AM
RESULT,RNN_TRANSLATOR,,94,nvidia,2020-06-18 02:13:30 AM
ENDING TIMING RUN AT 2020-06-18 02:15:04 AM
RESULT,RNN_TRANSLATOR,,94,nvidia,2020-06-18 02:13:30 AM
ENDING TIMING RUN AT 2020-06-18 02:15:04 AM
RESULT,RNN_TRANSLATOR,,94,nvidia,2020-06-18 02:13:30 AM
ENDING TIMING RUN AT 2020-06-18 02:15:04 AM
RESULT,RNN_TRANSLATOR,,94,nvidia,2020-06-18 02:13:30 AM
ENDING TIMING RUN AT 2020-06-18 02:15:04 AM
RESULT,RNN_TRANSLATOR,,94,nvidia,2020-06-18 02:13:30 AM
ENDING TIMING RUN AT 2020-06-18 02:15:04 AM
RESULT,RNN_TRANSLATOR,,94,nvidia,2020-06-18 02:13:30 AM
ENDING TIMING RUN AT 2020-06-18 02:15:04 AM
RESULT,RNN_TRANSLATOR,,94,nvidia,2020-06-18 02:13:30 AM
ENDING TIMING RUN AT 2020-06-18 02:15:04 AM
RESULT,RNN_TRANSLATOR,,94,nvidia,2020-06-18 02:13:30 AM
ENDING TIMING RUN AT 2020-06-18 02:15:04 AM
RESULT,RNN_TRANSLATOR,,94,nvidia,2020-06-18 02:13:30 AM
ENDING TIMING RUN AT 2020-06-18 02:15:04 AM
RESULT,RNN_TRANSLATOR,,94,nvidia,2020-06-18 02:13:30 AM
ENDING TIMING RUN AT 2020-06-18 02:15:04 AM
RESULT,RNN_TRANSLATOR,,94,nvidia,2020-06-18 02:13:30 AM
ENDING TIMING RUN AT 2020-06-18 02:15:04 AM
RESULT,RNN_TRANSLATOR,,94,nvidia,2020-06-18 02:13:30 AM
ENDING TIMING RUN AT 2020-06-18 02:15:04 AM
RESULT,RNN_TRANSLATOR,,94,nvidia,2020-06-18 02:13:30 AM
ENDING TIMING RUN AT 2020-06-18 02:15:04 AM
RESULT,RNN_TRANSLATOR,,94,nvidia,2020-06-18 02:13:30 AM
ENDING TIMING RUN AT 2020-06-18 02:15:04 AM
ENDING TIMING RUN AT 2020-06-18 02:15:04 AM
RESULT,RNN_TRANSLATOR,,94,nvidia,2020-06-18 02:13:30 AM
RESULT,RNN_TRANSLATOR,,94,nvidia,2020-06-18 02:13:30 AM
ENDING TIMING RUN AT 2020-06-18 02:15:04 AM
RESULT,RNN_TRANSLATOR,,94,nvidia,2020-06-18 02:13:30 AM
ENDING TIMING RUN AT 2020-06-18 02:15:04 AM
RESULT,RNN_TRANSLATOR,,94,nvidia,2020-06-18 02:13:30 AM
ENDING TIMING RUN AT 2020-06-18 02:15:04 AM
RESULT,RNN_TRANSLATOR,,94,nvidia,2020-06-18 02:13:30 AM
ENDING TIMING RUN AT 2020-06-18 02:15:04 AM
RESULT,RNN_TRANSLATOR,,94,nvidia,2020-06-18 02:13:30 AM
ENDING TIMING RUN AT 2020-06-18 02:15:04 AM
RESULT,RNN_TRANSLATOR,,94,nvidia,2020-06-18 02:13:30 AM
ENDING TIMING RUN AT 2020-06-18 02:15:04 AM
RESULT,RNN_TRANSLATOR,,94,nvidia,2020-06-18 02:13:30 AM
ENDING TIMING RUN AT 2020-06-18 02:15:04 AM
RESULT,RNN_TRANSLATOR,,94,nvidia,2020-06-18 02:13:30 AM
ENDING TIMING RUN AT 2020-06-18 02:15:04 AM
RESULT,RNN_TRANSLATOR,,94,nvidia,2020-06-18 02:13:30 AM
ENDING TIMING RUN AT 2020-06-18 02:15:04 AM
RESULT,RNN_TRANSLATOR,,94,nvidia,2020-06-18 02:13:30 AM
ENDING TIMING RUN AT 2020-06-18 02:15:04 AM
RESULT,RNN_TRANSLATOR,,94,nvidia,2020-06-18 02:13:30 AM
ENDING TIMING RUN AT 2020-06-18 02:15:04 AM
RESULT,RNN_TRANSLATOR,,94,nvidia,2020-06-18 02:13:30 AM
ENDING TIMING RUN AT 2020-06-18 02:15:04 AM
RESULT,RNN_TRANSLATOR,,94,nvidia,2020-06-18 02:13:30 AM
ENDING TIMING RUN AT 2020-06-18 02:15:04 AM
RESULT,RNN_TRANSLATOR,,94,nvidia,2020-06-18 02:13:30 AM
ENDING TIMING RUN AT 2020-06-18 02:15:04 AM
RESULT,RNN_TRANSLATOR,,94,nvidia,2020-06-18 02:13:30 AM
ENDING TIMING RUN AT 2020-06-18 02:15:04 AM
ENDING TIMING RUN AT 2020-06-18 02:15:04 AM
RESULT,RNN_TRANSLATOR,,94,nvidia,2020-06-18 02:13:30 AM
RESULT,RNN_TRANSLATOR,,94,nvidia,2020-06-18 02:13:30 AM
ENDING TIMING RUN AT 2020-06-18 02:15:04 AM
RESULT,RNN_TRANSLATOR,,94,nvidia,2020-06-18 02:13:30 AM
ENDING TIMING RUN AT 2020-06-18 02:15:04 AM
RESULT,RNN_TRANSLATOR,,94,nvidia,2020-06-18 02:13:30 AM
ENDING TIMING RUN AT 2020-06-18 02:15:04 AM
ENDING TIMING RUN AT 2020-06-18 02:15:04 AM
RESULT,RNN_TRANSLATOR,,94,nvidia,2020-06-18 02:13:30 AM
RESULT,RNN_TRANSLATOR,,94,nvidia,2020-06-18 02:13:30 AM
ENDING TIMING RUN AT 2020-06-18 02:15:04 AM
RESULT,RNN_TRANSLATOR,,94,nvidia,2020-06-18 02:13:30 AM
ENDING TIMING RUN AT 2020-06-18 02:15:04 AM
RESULT,RNN_TRANSLATOR,,94,nvidia,2020-06-18 02:13:30 AM
ENDING TIMING RUN AT 2020-06-18 02:15:04 AM
RESULT,RNN_TRANSLATOR,,94,nvidia,2020-06-18 02:13:30 AM
ENDING TIMING RUN AT 2020-06-18 02:15:04 AM
ENDING TIMING RUN AT 2020-06-18 02:15:04 AM
RESULT,RNN_TRANSLATOR,,94,nvidia,2020-06-18 02:13:30 AM
RESULT,RNN_TRANSLATOR,,94,nvidia,2020-06-18 02:13:30 AM
ENDING TIMING RUN AT 2020-06-18 02:15:04 AM
RESULT,RNN_TRANSLATOR,,94,nvidia,2020-06-18 02:13:30 AM
ENDING TIMING RUN AT 2020-06-18 02:15:04 AM
RESULT,RNN_TRANSLATOR,,94,nvidia,2020-06-18 02:13:30 AM
ENDING TIMING RUN AT 2020-06-18 02:15:04 AM
RESULT,RNN_TRANSLATOR,,94,nvidia,2020-06-18 02:13:30 AM
ENDING TIMING RUN AT 2020-06-18 02:15:04 AM
RESULT,RNN_TRANSLATOR,,94,nvidia,2020-06-18 02:13:30 AM
ENDING TIMING RUN AT 2020-06-18 02:15:04 AM
RESULT,RNN_TRANSLATOR,,94,nvidia,2020-06-18 02:13:30 AM
ENDING TIMING RUN AT 2020-06-18 02:15:04 AM
RESULT,RNN_TRANSLATOR,,94,nvidia,2020-06-18 02:13:30 AM
ENDING TIMING RUN AT 2020-06-18 02:15:04 AM
RESULT,RNN_TRANSLATOR,,94,nvidia,2020-06-18 02:13:30 AM
ENDING TIMING RUN AT 2020-06-18 02:15:04 AM
RESULT,RNN_TRANSLATOR,,94,nvidia,2020-06-18 02:13:30 AM
ENDING TIMING RUN AT 2020-06-18 02:15:04 AM
RESULT,RNN_TRANSLATOR,,94,nvidia,2020-06-18 02:13:30 AM
ENDING TIMING RUN AT 2020-06-18 02:15:04 AM
RESULT,RNN_TRANSLATOR,,94,nvidia,2020-06-18 02:13:30 AM
ENDING TIMING RUN AT 2020-06-18 02:15:04 AM
ENDING TIMING RUN AT 2020-06-18 02:15:04 AM
RESULT,RNN_TRANSLATOR,,94,nvidia,2020-06-18 02:13:30 AM
RESULT,RNN_TRANSLATOR,,94,nvidia,2020-06-18 02:13:30 AM
ENDING TIMING RUN AT 2020-06-18 02:15:04 AM
RESULT,RNN_TRANSLATOR,,94,nvidia,2020-06-18 02:13:30 AM
ENDING TIMING RUN AT 2020-06-18 02:15:04 AM
RESULT,RNN_TRANSLATOR,,94,nvidia,2020-06-18 02:13:30 AM
ENDING TIMING RUN AT 2020-06-18 02:15:05 AM
RESULT,RNN_TRANSLATOR,,94,nvidia,2020-06-18 02:13:30 AM
ENDING TIMING RUN AT 2020-06-18 02:15:05 AM
RESULT,RNN_TRANSLATOR,,95,nvidia,2020-06-18 02:13:30 AM
ENDING TIMING RUN AT 2020-06-18 02:15:05 AM
RESULT,RNN_TRANSLATOR,,95,nvidia,2020-06-18 02:13:30 AM
ENDING TIMING RUN AT 2020-06-18 02:15:05 AM
RESULT,RNN_TRANSLATOR,,95,nvidia,2020-06-18 02:13:30 AM
ENDING TIMING RUN AT 2020-06-18 02:15:05 AM
RESULT,RNN_TRANSLATOR,,95,nvidia,2020-06-18 02:13:30 AM
ENDING TIMING RUN AT 2020-06-18 02:15:05 AM
RESULT,RNN_TRANSLATOR,,95,nvidia,2020-06-18 02:13:30 AM
ENDING TIMING RUN AT 2020-06-18 02:15:05 AM
RESULT,RNN_TRANSLATOR,,95,nvidia,2020-06-18 02:13:30 AM
ENDING TIMING RUN AT 2020-06-18 02:15:05 AM
RESULT,RNN_TRANSLATOR,,95,nvidia,2020-06-18 02:13:30 AM
ENDING TIMING RUN AT 2020-06-18 02:15:05 AM
ENDING TIMING RUN AT 2020-06-18 02:15:05 AM
RESULT,RNN_TRANSLATOR,,95,nvidia,2020-06-18 02:13:30 AM
RESULT,RNN_TRANSLATOR,,95,nvidia,2020-06-18 02:13:30 AM
ENDING TIMING RUN AT 2020-06-18 02:15:05 AM
RESULT,RNN_TRANSLATOR,,95,nvidia,2020-06-18 02:13:30 AM
ENDING TIMING RUN AT 2020-06-18 02:15:05 AM
RESULT,RNN_TRANSLATOR,,95,nvidia,2020-06-18 02:13:30 AM
ENDING TIMING RUN AT 2020-06-18 02:15:05 AM
RESULT,RNN_TRANSLATOR,,95,nvidia,2020-06-18 02:13:30 AM
ENDING TIMING RUN AT 2020-06-18 02:15:05 AM
RESULT,RNN_TRANSLATOR,,95,nvidia,2020-06-18 02:13:30 AM
ENDING TIMING RUN AT 2020-06-18 02:15:05 AM
RESULT,RNN_TRANSLATOR,,95,nvidia,2020-06-18 02:13:30 AM
ENDING TIMING RUN AT 2020-06-18 02:15:05 AM
RESULT,RNN_TRANSLATOR,,95,nvidia,2020-06-18 02:13:30 AM
ENDING TIMING RUN AT 2020-06-18 02:15:05 AM
RESULT,RNN_TRANSLATOR,,95,nvidia,2020-06-18 02:13:30 AM
ENDING TIMING RUN AT 2020-06-18 02:15:05 AM
RESULT,RNN_TRANSLATOR,,95,nvidia,2020-06-18 02:13:30 AM
ENDING TIMING RUN AT 2020-06-18 02:15:05 AM
RESULT,RNN_TRANSLATOR,,95,nvidia,2020-06-18 02:13:30 AM
ENDING TIMING RUN AT 2020-06-18 02:15:05 AM
RESULT,RNN_TRANSLATOR,,95,nvidia,2020-06-18 02:13:30 AM
ENDING TIMING RUN AT 2020-06-18 02:15:05 AM
RESULT,RNN_TRANSLATOR,,95,nvidia,2020-06-18 02:13:30 AM
ENDING TIMING RUN AT 2020-06-18 02:15:05 AM
RESULT,RNN_TRANSLATOR,,95,nvidia,2020-06-18 02:13:30 AM
ENDING TIMING RUN AT 2020-06-18 02:15:05 AM
RESULT,RNN_TRANSLATOR,,95,nvidia,2020-06-18 02:13:30 AM
ENDING TIMING RUN AT 2020-06-18 02:15:05 AM
RESULT,RNN_TRANSLATOR,,95,nvidia,2020-06-18 02:13:30 AM
ENDING TIMING RUN AT 2020-06-18 02:15:05 AM
RESULT,RNN_TRANSLATOR,,95,nvidia,2020-06-18 02:13:30 AM
ENDING TIMING RUN AT 2020-06-18 02:15:05 AM
RESULT,RNN_TRANSLATOR,,95,nvidia,2020-06-18 02:13:30 AM
ENDING TIMING RUN AT 2020-06-18 02:15:05 AM
RESULT,RNN_TRANSLATOR,,95,nvidia,2020-06-18 02:13:30 AM
ENDING TIMING RUN AT 2020-06-18 02:15:05 AM
RESULT,RNN_TRANSLATOR,,95,nvidia,2020-06-18 02:13:30 AM
ENDING TIMING RUN AT 2020-06-18 02:15:05 AM
RESULT,RNN_TRANSLATOR,,95,nvidia,2020-06-18 02:13:30 AM
ENDING TIMING RUN AT 2020-06-18 02:15:05 AM
RESULT,RNN_TRANSLATOR,,95,nvidia,2020-06-18 02:13:30 AM
ENDING TIMING RUN AT 2020-06-18 02:15:05 AM
RESULT,RNN_TRANSLATOR,,95,nvidia,2020-06-18 02:13:30 AM
ENDING TIMING RUN AT 2020-06-18 02:15:05 AM
RESULT,RNN_TRANSLATOR,,95,nvidia,2020-06-18 02:13:30 AM
ENDING TIMING RUN AT 2020-06-18 02:15:05 AM
RESULT,RNN_TRANSLATOR,,95,nvidia,2020-06-18 02:13:30 AM
ENDING TIMING RUN AT 2020-06-18 02:15:05 AM
RESULT,RNN_TRANSLATOR,,95,nvidia,2020-06-18 02:13:30 AM
ENDING TIMING RUN AT 2020-06-18 02:15:05 AM
RESULT,RNN_TRANSLATOR,,94,nvidia,2020-06-18 02:13:31 AM
ENDING TIMING RUN AT 2020-06-18 02:15:05 AM
RESULT,RNN_TRANSLATOR,,95,nvidia,2020-06-18 02:13:30 AM
ENDING TIMING RUN AT 2020-06-18 02:15:05 AM
RESULT,RNN_TRANSLATOR,,95,nvidia,2020-06-18 02:13:30 AM
ENDING TIMING RUN AT 2020-06-18 02:15:05 AM
RESULT,RNN_TRANSLATOR,,95,nvidia,2020-06-18 02:13:30 AM
ENDING TIMING RUN AT 2020-06-18 02:15:05 AM
RESULT,RNN_TRANSLATOR,,95,nvidia,2020-06-18 02:13:30 AM
ENDING TIMING RUN AT 2020-06-18 02:15:05 AM
RESULT,RNN_TRANSLATOR,,95,nvidia,2020-06-18 02:13:30 AM
ENDING TIMING RUN AT 2020-06-18 02:15:05 AM
RESULT,RNN_TRANSLATOR,,95,nvidia,2020-06-18 02:13:30 AM
ENDING TIMING RUN AT 2020-06-18 02:15:05 AM
RESULT,RNN_TRANSLATOR,,95,nvidia,2020-06-18 02:13:30 AM
ENDING TIMING RUN AT 2020-06-18 02:15:05 AM
RESULT,RNN_TRANSLATOR,,95,nvidia,2020-06-18 02:13:30 AM
ENDING TIMING RUN AT 2020-06-18 02:15:05 AM
RESULT,RNN_TRANSLATOR,,95,nvidia,2020-06-18 02:13:30 AM
ENDING TIMING RUN AT 2020-06-18 02:15:05 AM
RESULT,RNN_TRANSLATOR,,95,nvidia,2020-06-18 02:13:30 AM
ENDING TIMING RUN AT 2020-06-18 02:15:05 AM
RESULT,RNN_TRANSLATOR,,95,nvidia,2020-06-18 02:13:30 AM
ENDING TIMING RUN AT 2020-06-18 02:15:05 AM
RESULT,RNN_TRANSLATOR,,95,nvidia,2020-06-18 02:13:30 AM
