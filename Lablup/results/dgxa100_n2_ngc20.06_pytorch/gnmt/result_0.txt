+ echo 'Beginning trial 1 of 5'
Beginning trial 1 of 5
+ srun --ntasks=2 --container-name=rnn_translator python -c '
import mlperf_log_utils
from mlperf_logging.mllog import constants
mlperf_log_utils.mlperf_submission_log(constants.GNMT)'
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
:::MLLOG {"namespace": "", "time_ms": 1593112657411, "event_type": "POINT_IN_TIME", "key": "submission_benchmark", "value": "gnmt", "metadata": {"file": "/workspace/rnn_translator/mlperf_log_utils.py", "lineno": 19}}
:::MLLOG {"namespace": "", "time_ms": 1593112657450, "event_type": "POINT_IN_TIME", "key": "submission_org", "value": "NVIDIA", "metadata": {"file": "/workspace/rnn_translator/mlperf_log_utils.py", "lineno": 24}}
:::MLLOG {"namespace": "", "time_ms": 1593112657450, "event_type": "POINT_IN_TIME", "key": "submission_division", "value": "closed", "metadata": {"file": "/workspace/rnn_translator/mlperf_log_utils.py", "lineno": 28}}
:::MLLOG {"namespace": "", "time_ms": 1593112657451, "event_type": "POINT_IN_TIME", "key": "submission_status", "value": "onprem", "metadata": {"file": "/workspace/rnn_translator/mlperf_log_utils.py", "lineno": 32}}
:::MLLOG {"namespace": "", "time_ms": 1593112657451, "event_type": "POINT_IN_TIME", "key": "submission_platform", "value": "2xNVIDIA DGX A100", "metadata": {"file": "/workspace/rnn_translator/mlperf_log_utils.py", "lineno": 36}}
+ '[' 1 -eq 1 ']'
+ srun --ntasks=2 bash -c 'echo -n '\''Clearing cache on '\'' && hostname && sync && sudo /sbin/sysctl vm.drop_caches=3'
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
Clearing cache on luna-0100
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
Clearing cache on luna-0099
vm.drop_caches = 3
vm.drop_caches = 3
+ srun --ntasks=2 --container-name=rnn_translator python -c '
from mlperf_logging.mllog import constants
from seq2seq.utils import log_event
log_event(key=constants.CACHE_CLEAR, value=True)'
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
:::MLLOG {"namespace": "", "time_ms": 1593112663569, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
:::MLLOG {"namespace": "", "time_ms": 1593112663660, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
+ srun --mpi=none --ntasks=16 --ntasks-per-node=8 --container-name=rnn_translator --container-mounts=/lustre/fsr/datasets/wmt16_de_en:/data,/raid/scratch/svcnvdlfw/gnmt:/preproc_data,/lustre/fsw/mlperf-ci/14251651/results:/results ./run_and_time.sh
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
STARTING TIMING RUN AT 2020-06-25 12:17:45 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=2.875e-3
+ TRAIN_BATCH_SIZE=192
+ TEST_BATCH_SIZE=64
+ WARMUP_STEPS=200
+ REMAIN_STEPS=4054
+ DECAY_INTERVAL=506
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=8
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 3    --dwu-num-chunks 2    --dwu-num-rs-pg 1    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-overlap-reductions    --dwu-grad-norm    '
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 7 ']'
+ '[' 16 -gt 2 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGXA100_multi_2x8x192_dist == *\D\G\X\A\1\0\0* ]]
Using TCMalloc
+ echo 'Using TCMalloc'
+ export LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
running benchmark
+ echo 'running benchmark'
STARTING TIMING RUN AT 2020-06-25 12:17:45 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=2.875e-3
+ TRAIN_BATCH_SIZE=192
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 192 --test-batch-size 64 --optimizer FusedAdam --lr 2.875e-3 --warmup-steps 200 --remain-steps 4054 --decay-interval 506 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
+ TEST_BATCH_SIZE=64
+ WARMUP_STEPS=200
+ REMAIN_STEPS=4054
+ DECAY_INTERVAL=506
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=8
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 3    --dwu-num-chunks 2    --dwu-num-rs-pg 1    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-overlap-reductions    --dwu-grad-norm    '
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 1 ']'
+ '[' 16 -gt 2 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGXA100_multi_2x8x192_dist == *\D\G\X\A\1\0\0* ]]
+ echo 'Using TCMalloc'
Using TCMalloc
+ export LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
running benchmark
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 192 --test-batch-size 64 --optimizer FusedAdam --lr 2.875e-3 --warmup-steps 200 --remain-steps 4054 --decay-interval 506 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
STARTING TIMING RUN AT 2020-06-25 12:17:45 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=2.875e-3
+ TRAIN_BATCH_SIZE=192
+ TEST_BATCH_SIZE=64
+ WARMUP_STEPS=200
+ REMAIN_STEPS=4054
+ DECAY_INTERVAL=506
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=8
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 3    --dwu-num-chunks 2    --dwu-num-rs-pg 1    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-overlap-reductions    --dwu-grad-norm    '
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 3 ']'
+ '[' 16 -gt 2 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
STARTING TIMING RUN AT 2020-06-25 12:17:45 PM
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGXA100_multi_2x8x192_dist == *\D\G\X\A\1\0\0* ]]
Using TCMalloc
+ echo 'Using TCMalloc'
+ export LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
running benchmark
+ echo 'running benchmark'
+ RESULTS_DIR=gnmt_wmt16
+ LR=2.875e-3
+ TRAIN_BATCH_SIZE=192
+ TEST_BATCH_SIZE=64
+ WARMUP_STEPS=200
+ REMAIN_STEPS=4054
+ DECAY_INTERVAL=506
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=8
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 3    --dwu-num-chunks 2    --dwu-num-rs-pg 1    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-overlap-reductions    --dwu-grad-norm    '
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 2 ']'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 192 --test-batch-size 64 --optimizer FusedAdam --lr 2.875e-3 --warmup-steps 200 --remain-steps 4054 --decay-interval 506 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
+ '[' 16 -gt 2 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGXA100_multi_2x8x192_dist == *\D\G\X\A\1\0\0* ]]
Using TCMalloc
+ echo 'Using TCMalloc'
+ export LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
running benchmark
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 192 --test-batch-size 64 --optimizer FusedAdam --lr 2.875e-3 --warmup-steps 200 --remain-steps 4054 --decay-interval 506 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
STARTING TIMING RUN AT 2020-06-25 12:17:45 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=2.875e-3
+ TRAIN_BATCH_SIZE=192
+ TEST_BATCH_SIZE=64
+ WARMUP_STEPS=200
+ REMAIN_STEPS=4054
+ DECAY_INTERVAL=506
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=8
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 3    --dwu-num-chunks 2    --dwu-num-rs-pg 1    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-overlap-reductions    --dwu-grad-norm    '
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 6 ']'
+ '[' 16 -gt 2 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGXA100_multi_2x8x192_dist == *\D\G\X\A\1\0\0* ]]
Using TCMalloc
+ echo 'Using TCMalloc'
+ export LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
running benchmark
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 192 --test-batch-size 64 --optimizer FusedAdam --lr 2.875e-3 --warmup-steps 200 --remain-steps 4054 --decay-interval 506 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
STARTING TIMING RUN AT 2020-06-25 12:17:45 PM
STARTING TIMING RUN AT 2020-06-25 12:17:45 PM
+ DATASET_DIR=/data
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ RESULTS_DIR=gnmt_wmt16
+ LR=2.875e-3
+ LR=2.875e-3
+ TRAIN_BATCH_SIZE=192
+ TRAIN_BATCH_SIZE=192
+ TEST_BATCH_SIZE=64
+ TEST_BATCH_SIZE=64
+ WARMUP_STEPS=200
+ WARMUP_STEPS=200
+ REMAIN_STEPS=4054
+ REMAIN_STEPS=4054
+ DECAY_INTERVAL=506
+ DECAY_INTERVAL=506
+ TARGET=24.0
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=8
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=8
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 3    --dwu-num-chunks 2    --dwu-num-rs-pg 1    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-overlap-reductions    --dwu-grad-norm    '
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 3    --dwu-num-chunks 2    --dwu-num-rs-pg 1    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-overlap-reductions    --dwu-grad-norm    '
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    '
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    '
+ MATH=fp16
+ MATH=fp16
+ declare -a CMD
STARTING TIMING RUN AT 2020-06-25 12:17:45 PM
+ declare -a CMD
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=2.875e-3
+ '[' -n 0 ']'
+ '[' -n 5 ']'
+ TRAIN_BATCH_SIZE=192
+ TEST_BATCH_SIZE=64
+ '[' 16 -gt 2 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ WARMUP_STEPS=200
+ REMAIN_STEPS=4054
+ '[' 16 -gt 2 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DECAY_INTERVAL=506
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=8
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGXA100_multi_2x8x192_dist == *\D\G\X\A\1\0\0* ]]
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 3    --dwu-num-chunks 2    --dwu-num-rs-pg 1    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-overlap-reductions    --dwu-grad-norm    '
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    '
+ [[ DGXA100_multi_2x8x192_dist == *\D\G\X\A\1\0\0* ]]
+ echo 'Using TCMalloc'
+ MATH=fp16
+ declare -a CMD
Using TCMalloc
+ echo 'Using TCMalloc'
+ export LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
Using TCMalloc
running benchmark
+ export LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ echo 'running benchmark'
+ echo 'running benchmark'
running benchmark
+ '[' -n 4 ']'
+ '[' 16 -gt 2 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGXA100_multi_2x8x192_dist == *\D\G\X\A\1\0\0* ]]
Using TCMalloc
+ echo 'Using TCMalloc'
+ export LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
running benchmark
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 192 --test-batch-size 64 --optimizer FusedAdam --lr 2.875e-3 --warmup-steps 200 --remain-steps 4054 --decay-interval 506 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 192 --test-batch-size 64 --optimizer FusedAdam --lr 2.875e-3 --warmup-steps 200 --remain-steps 4054 --decay-interval 506 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 192 --test-batch-size 64 --optimizer FusedAdam --lr 2.875e-3 --warmup-steps 200 --remain-steps 4054 --decay-interval 506 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
STARTING TIMING RUN AT 2020-06-25 12:17:45 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=2.875e-3
+ TRAIN_BATCH_SIZE=192
+ TEST_BATCH_SIZE=64
+ WARMUP_STEPS=200
+ REMAIN_STEPS=4054
+ DECAY_INTERVAL=506
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=8
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 3    --dwu-num-chunks 2    --dwu-num-rs-pg 1    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-overlap-reductions    --dwu-grad-norm    '
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 7 ']'
+ '[' 16 -gt 2 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGXA100_multi_2x8x192_dist == *\D\G\X\A\1\0\0* ]]
+ echo 'Using TCMalloc'
Using TCMalloc
+ export LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
running benchmark
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 192 --test-batch-size 64 --optimizer FusedAdam --lr 2.875e-3 --warmup-steps 200 --remain-steps 4054 --decay-interval 506 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
STARTING TIMING RUN AT 2020-06-25 12:17:45 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=2.875e-3
+ TRAIN_BATCH_SIZE=192
+ TEST_BATCH_SIZE=64
+ WARMUP_STEPS=200
+ REMAIN_STEPS=4054
+ DECAY_INTERVAL=506
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=8
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 3    --dwu-num-chunks 2    --dwu-num-rs-pg 1    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-overlap-reductions    --dwu-grad-norm    '
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 1 ']'
+ '[' 16 -gt 2 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGXA100_multi_2x8x192_dist == *\D\G\X\A\1\0\0* ]]
+ echo 'Using TCMalloc'
Using TCMalloc
+ export LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
running benchmark
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 192 --test-batch-size 64 --optimizer FusedAdam --lr 2.875e-3 --warmup-steps 200 --remain-steps 4054 --decay-interval 506 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
STARTING TIMING RUN AT 2020-06-25 12:17:45 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=2.875e-3
+ TRAIN_BATCH_SIZE=192
+ TEST_BATCH_SIZE=64
STARTING TIMING RUN AT 2020-06-25 12:17:45 PM
+ WARMUP_STEPS=200
+ REMAIN_STEPS=4054
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=2.875e-3
+ TRAIN_BATCH_SIZE=192
+ TEST_BATCH_SIZE=64
+ WARMUP_STEPS=200
+ REMAIN_STEPS=4054
+ DECAY_INTERVAL=506
+ DECAY_INTERVAL=506
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=8
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 3    --dwu-num-chunks 2    --dwu-num-rs-pg 1    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-overlap-reductions    --dwu-grad-norm    '
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    '
+ MATH=fp16
+ declare -a CMD
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=8
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 3    --dwu-num-chunks 2    --dwu-num-rs-pg 1    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-overlap-reductions    --dwu-grad-norm    '
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    '
+ MATH=fp16
+ '[' -n 2 ']'
+ declare -a CMD
+ '[' 16 -gt 2 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' -n 0 ']'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGXA100_multi_2x8x192_dist == *\D\G\X\A\1\0\0* ]]
+ '[' 16 -gt 2 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
Using TCMalloc
+ echo 'Using TCMalloc'
+ export LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
running benchmark
+ echo 'running benchmark'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGXA100_multi_2x8x192_dist == *\D\G\X\A\1\0\0* ]]
+ echo 'Using TCMalloc'
Using TCMalloc
+ export LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
running benchmark
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 192 --test-batch-size 64 --optimizer FusedAdam --lr 2.875e-3 --warmup-steps 200 --remain-steps 4054 --decay-interval 506 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 192 --test-batch-size 64 --optimizer FusedAdam --lr 2.875e-3 --warmup-steps 200 --remain-steps 4054 --decay-interval 506 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
STARTING TIMING RUN AT 2020-06-25 12:17:45 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=2.875e-3
+ TRAIN_BATCH_SIZE=192
+ TEST_BATCH_SIZE=64
+ WARMUP_STEPS=200
+ REMAIN_STEPS=4054
+ DECAY_INTERVAL=506
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=8
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 3    --dwu-num-chunks 2    --dwu-num-rs-pg 1    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-overlap-reductions    --dwu-grad-norm    '
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 4 ']'
+ '[' 16 -gt 2 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGXA100_multi_2x8x192_dist == *\D\G\X\A\1\0\0* ]]
Using TCMalloc
+ echo 'Using TCMalloc'
+ export LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
running benchmark
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 192 --test-batch-size 64 --optimizer FusedAdam --lr 2.875e-3 --warmup-steps 200 --remain-steps 4054 --decay-interval 506 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
STARTING TIMING RUN AT 2020-06-25 12:17:45 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=2.875e-3
+ TRAIN_BATCH_SIZE=192
+ TEST_BATCH_SIZE=64
+ WARMUP_STEPS=200
+ REMAIN_STEPS=4054
+ DECAY_INTERVAL=506
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=8
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 3    --dwu-num-chunks 2    --dwu-num-rs-pg 1    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-overlap-reductions    --dwu-grad-norm    '
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 5 ']'
+ '[' 16 -gt 2 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGXA100_multi_2x8x192_dist == *\D\G\X\A\1\0\0* ]]
Using TCMalloc
+ echo 'Using TCMalloc'
+ export LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
running benchmark
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 192 --test-batch-size 64 --optimizer FusedAdam --lr 2.875e-3 --warmup-steps 200 --remain-steps 4054 --decay-interval 506 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
STARTING TIMING RUN AT 2020-06-25 12:17:45 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=2.875e-3
+ TRAIN_BATCH_SIZE=192
+ TEST_BATCH_SIZE=64
+ WARMUP_STEPS=200
+ REMAIN_STEPS=4054
+ DECAY_INTERVAL=506
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=8
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 3    --dwu-num-chunks 2    --dwu-num-rs-pg 1    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-overlap-reductions    --dwu-grad-norm    '
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 3 ']'
+ '[' 16 -gt 2 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGXA100_multi_2x8x192_dist == *\D\G\X\A\1\0\0* ]]
Using TCMalloc
+ echo 'Using TCMalloc'
+ export LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
running benchmark
+ echo 'running benchmark'
STARTING TIMING RUN AT 2020-06-25 12:17:45 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 192 --test-batch-size 64 --optimizer FusedAdam --lr 2.875e-3 --warmup-steps 200 --remain-steps 4054 --decay-interval 506 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
+ RESULTS_DIR=gnmt_wmt16
+ LR=2.875e-3
+ TRAIN_BATCH_SIZE=192
+ TEST_BATCH_SIZE=64
+ WARMUP_STEPS=200
+ REMAIN_STEPS=4054
+ DECAY_INTERVAL=506
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=8
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 3    --dwu-num-chunks 2    --dwu-num-rs-pg 1    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-overlap-reductions    --dwu-grad-norm    '
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 6 ']'
+ '[' 16 -gt 2 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGXA100_multi_2x8x192_dist == *\D\G\X\A\1\0\0* ]]
+ echo 'Using TCMalloc'
Using TCMalloc
+ export LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
running benchmark
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 192 --test-batch-size 64 --optimizer FusedAdam --lr 2.875e-3 --warmup-steps 200 --remain-steps 4054 --decay-interval 506 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
num_sockets = 2 num_nodes=8 cores_per_socket=64
+ exec numactl --physcpubind=80-95,208-223 --membind=5 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 192 --test-batch-size 64 --optimizer FusedAdam --lr 2.875e-3 --warmup-steps 200 --remain-steps 4054 --decay-interval 506 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
num_sockets = 2 num_nodes=8 cores_per_socket=64
+ exec numactl --physcpubind=112-127,240-255 --membind=7 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 192 --test-batch-size 64 --optimizer FusedAdam --lr 2.875e-3 --warmup-steps 200 --remain-steps 4054 --decay-interval 506 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
num_sockets = 2 num_nodes=8 cores_per_socket=64
+ exec numactl --physcpubind=32-47,160-175 --membind=2 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 192 --test-batch-size 64 --optimizer FusedAdam --lr 2.875e-3 --warmup-steps 200 --remain-steps 4054 --decay-interval 506 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
num_sockets = 2 num_nodes=8 cores_per_socket=64
num_sockets = 2 num_nodes=8 cores_per_socket=64
+ exec numactl --physcpubind=16-31,144-159 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 192 --test-batch-size 64 --optimizer FusedAdam --lr 2.875e-3 --warmup-steps 200 --remain-steps 4054 --decay-interval 506 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
+ exec numactl --physcpubind=96-111,224-239 --membind=6 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 192 --test-batch-size 64 --optimizer FusedAdam --lr 2.875e-3 --warmup-steps 200 --remain-steps 4054 --decay-interval 506 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
num_sockets = 2 num_nodes=8 cores_per_socket=64
num_sockets = 2 num_nodes=8 cores_per_socket=64
num_sockets = 2 num_nodes=8 cores_per_socket=64
+ exec numactl --physcpubind=0-15,128-143 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 192 --test-batch-size 64 --optimizer FusedAdam --lr 2.875e-3 --warmup-steps 200 --remain-steps 4054 --decay-interval 506 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
+ exec numactl --physcpubind=112-127,240-255 --membind=7 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 192 --test-batch-size 64 --optimizer FusedAdam --lr 2.875e-3 --warmup-steps 200 --remain-steps 4054 --decay-interval 506 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
num_sockets = 2 num_nodes=8 cores_per_socket=64
num_sockets = 2 num_nodes=8 cores_per_socket=64
num_sockets = 2 num_nodes=8 cores_per_socket=64
+ exec numactl --physcpubind=48-63,176-191 --membind=3 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 192 --test-batch-size 64 --optimizer FusedAdam --lr 2.875e-3 --warmup-steps 200 --remain-steps 4054 --decay-interval 506 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
+ exec numactl --physcpubind=96-111,224-239 --membind=6 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 192 --test-batch-size 64 --optimizer FusedAdam --lr 2.875e-3 --warmup-steps 200 --remain-steps 4054 --decay-interval 506 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
+ exec numactl --physcpubind=16-31,144-159 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 192 --test-batch-size 64 --optimizer FusedAdam --lr 2.875e-3 --warmup-steps 200 --remain-steps 4054 --decay-interval 506 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
+ exec numactl --physcpubind=64-79,192-207 --membind=4 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 192 --test-batch-size 64 --optimizer FusedAdam --lr 2.875e-3 --warmup-steps 200 --remain-steps 4054 --decay-interval 506 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
num_sockets = 2 num_nodes=8 cores_per_socket=64
num_sockets = 2 num_nodes=8 cores_per_socket=64
num_sockets = 2 num_nodes=8 cores_per_socket=64
num_sockets = 2 num_nodes=8 cores_per_socket=64
num_sockets = 2 num_nodes=8 cores_per_socket=64
+ exec numactl --physcpubind=0-15,128-143 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 192 --test-batch-size 64 --optimizer FusedAdam --lr 2.875e-3 --warmup-steps 200 --remain-steps 4054 --decay-interval 506 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
+ exec numactl --physcpubind=80-95,208-223 --membind=5 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 192 --test-batch-size 64 --optimizer FusedAdam --lr 2.875e-3 --warmup-steps 200 --remain-steps 4054 --decay-interval 506 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
+ exec numactl --physcpubind=64-79,192-207 --membind=4 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 192 --test-batch-size 64 --optimizer FusedAdam --lr 2.875e-3 --warmup-steps 200 --remain-steps 4054 --decay-interval 506 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
+ exec numactl --physcpubind=48-63,176-191 --membind=3 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 192 --test-batch-size 64 --optimizer FusedAdam --lr 2.875e-3 --warmup-steps 200 --remain-steps 4054 --decay-interval 506 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
+ exec numactl --physcpubind=32-47,160-175 --membind=2 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 192 --test-batch-size 64 --optimizer FusedAdam --lr 2.875e-3 --warmup-steps 200 --remain-steps 4054 --decay-interval 506 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
:::MLLOG {"namespace": "", "time_ms": 1593112667803, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593112667948, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593112668002, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593112668006, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593112668021, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593112668028, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593112668035, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593112668036, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593112668039, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593112668051, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593112668053, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593112668053, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593112668057, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593112668062, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593112668063, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593112668088, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
NCCL version 2.7.5+cuda11.0
0: Saving results to: results/gnmt_wmt16
0: Run arguments: Namespace(apex_message_size=10000000.0, apex_num_allreduce_streams=1, batching='bucketing', beam_size=5, cov_penalty_factor=0.1, cuda=True, cudnn=True, dataset_dir='/data', decay_factor=0.5, decay_interval=506, decay_steps=4, distributed_weight_update=2, dropout=0.2, dwu_e5m2_allgather=False, dwu_full_pipeline=False, dwu_grad_norm=True, dwu_group_size=0, dwu_num_ag_pg=0, dwu_num_ar_pg=2, dwu_num_blocks=3, dwu_num_chunks=2, dwu_num_rs_pg=1, dwu_overlap_reductions=True, enable_apex_allreduce_overlap=False, env=False, epochs=8, eval=True, fused_attention=True, fused_xentropy=True, grad_clip=5.0, hidden_size=1024, init_scale=1024, intra_epoch_eval=0, keep_checkpoints=0, len_norm_const=5.0, len_norm_factor=0.6, local_rank=0, log_all_ranks=False, lr=0.002875, math='fp16', max_length_test=150, max_length_train=75, max_size=None, min_length_test=0, min_length_train=0, num_buckets=5, num_layers=4, optimizer='FusedAdam', optimizer_extra='{}', prealloc_mode='once', preproc_data_dir='/preproc_data/75', print_freq=10, rank=0, remain_steps=4054, results_dir='results', resume=None, save='gnmt_wmt16', save_all=False, save_freq=5000, save_path='results/gnmt_wmt16', seed=None, shard_size=80, share_embedding=True, smoothing=0.1, start_epoch=0, target_bleu=24.0, test_batch_size=64, test_loader_workers=0, train_batch_size=192, train_global_batch_size=None, train_iter_size=1, train_loader_workers=1, upscale_interval=128, use_preproc_data=True, warmup_steps=200)
0: L2 promotion: 128B
0: Using random master seed: 3712290243
:::MLLOG {"namespace": "", "time_ms": 1593112678157, "event_type": "POINT_IN_TIME", "key": "seed", "value": 3712290243, "metadata": {"file": "/workspace/rnn_translator/seq2seq/utils.py", "lineno": 144}}
0: Worker 0 is using worker seed: 597646714
0: Building vocabulary from /data/vocab.bpe.32000
0: Size of vocabulary: 32320
0: Fused attention flag set to True
0: GNMT(
  (embedder): Embedding(32320, 1024, padding_idx=0)
  (encoder): ResidualRecurrentEncoder(
    (rnn_layers): ModuleList(
      (0): EmuBidirLSTM(
        (bidir): LSTM(1024, 1024, bidirectional=True)
        (layer1): LSTM(1024, 1024)
        (layer2): LSTM(1024, 1024)
      )
      (1): LSTM(2048, 1024)
      (2): LSTM(1024, 1024)
      (3): LSTM(1024, 1024)
    )
    (dropout): Dropout(p=0.2, inplace=False)
    (embedder): Embedding(32320, 1024, padding_idx=0)
  )
  (decoder): ResidualRecurrentDecoder(
    (att_rnn): RecurrentAttention(
      (rnn): LSTM(1024, 1024)
      (attn): BahdanauAttention(
        (linear_q): Linear(in_features=1024, out_features=1024, bias=False)
        (linear_k): Linear(in_features=1024, out_features=1024, bias=False)
      )
      (dropout): Dropout(p=0.2, inplace=False)
    )
    (rnn_layers): ModuleList(
      (0): LSTM(2048, 1024)
      (1): LSTM(2048, 1024)
      (2): LSTM(2048, 1024)
    )
    (embedder): Embedding(32320, 1024, padding_idx=0)
    (classifier): Classifier(
      (classifier): Linear(in_features=1024, out_features=32320, bias=True)
    )
    (dropout): Dropout(p=0.2, inplace=False)
  )
)
0: Building LabelSmoothingLoss (smoothing: 0.1)
0: Fused xentropy flag set to True
0: Training optimizer config: {'optimizer': 'FusedAdam', 'lr': 0.002875}
0: Number of parameters: 160671297
0: Saving state of the tokenizer
self._net_total_param_size=160671360, self._total_param_size=160677888, dwu_min_page_size=12288, self._block_size=53559296, self._chunk_size=26779648, self._shard_size=3347456
[0, 9, 34]
self._net_total_param_size=160671360, self._total_param_size=160677888, dwu_min_page_size=12288, self._block_size=53559296, self._chunk_size=26779648, self._shard_size=3347456
[0, 9, 34]
0: Initializing dwu fp16 optimizer
self._net_total_param_size=160671360, self._total_param_size=160677888, dwu_min_page_size=12288, self._block_size=53559296, self._chunk_size=26779648, self._shard_size=3347456
[0, 9, 34]
self._net_total_param_size=160671360, self._total_param_size=160677888, dwu_min_page_size=12288, self._block_size=53559296, self._chunk_size=26779648, self._shard_size=3347456
[0, 9, 34]
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([467968]), new_param_packed_fragment.size()=torch.Size([467968]), master_param_fragment.size()=torch.Size([467968])
model_param_fragment.size()=torch.Size([2879488]), new_param_packed_fragment.size()=torch.Size([2879488]), master_param_fragment.size()=torch.Size([2879488])
model_param_fragment.size()=torch.Size([3073024]), new_param_packed_fragment.size()=torch.Size([3073024]), master_param_fragment.size()=torch.Size([3073024])
model_param_fragment.size()=torch.Size([274432]), new_param_packed_fragment.size()=torch.Size([274432]), master_param_fragment.size()=torch.Size([274432])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
NCCL version 2.7.5+cuda11.0
self._net_total_param_size=160671360, self._total_param_size=160677888, dwu_min_page_size=12288, self._block_size=53559296, self._chunk_size=26779648, self._shard_size=3347456
[0, 9, 34]
self._net_total_param_size=160671360, self._total_param_size=160677888, dwu_min_page_size=12288, self._block_size=53559296, self._chunk_size=26779648, self._shard_size=3347456
[0, 9, 34]
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([2217984]), new_param_packed_fragment.size()=torch.Size([2217984]), master_param_fragment.size()=torch.Size([2217984])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([1121280]), new_param_packed_fragment.size()=torch.Size([1121280]), master_param_fragment.size()=torch.Size([1121280])
model_param_fragment.size()=torch.Size([628736]), new_param_packed_fragment.size()=torch.Size([628736]), master_param_fragment.size()=torch.Size([628736])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([1024]), new_param_packed_fragment.size()=torch.Size([1024]), master_param_fragment.size()=torch.Size([1024])
model_param_fragment.size()=torch.Size([1]), new_param_packed_fragment.size()=torch.Size([1]), master_param_fragment.size()=torch.Size([1])
model_param_fragment.size()=torch.Size([1024]), new_param_packed_fragment.size()=torch.Size([1024]), master_param_fragment.size()=torch.Size([1024])
model_param_fragment.size()=torch.Size([1048576]), new_param_packed_fragment.size()=torch.Size([1048576]), master_param_fragment.size()=torch.Size([1048576])
model_param_fragment.size()=torch.Size([1048576]), new_param_packed_fragment.size()=torch.Size([1048576]), master_param_fragment.size()=torch.Size([1048576])
model_param_fragment.size()=torch.Size([611264]), new_param_packed_fragment.size()=torch.Size([611264]), master_param_fragment.size()=torch.Size([611264])
model_param_fragment.size()=torch.Size([1130560]), new_param_packed_fragment.size()=torch.Size([1130560]), master_param_fragment.size()=torch.Size([1130560])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([2208704]), new_param_packed_fragment.size()=torch.Size([2208704]), master_param_fragment.size()=torch.Size([2208704])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
NCCL version 2.7.5+cuda11.0
self._net_total_param_size=160671360, self._total_param_size=160677888, dwu_min_page_size=12288, self._block_size=53559296, self._chunk_size=26779648, self._shard_size=3347456
[0, 9, 34]
self._net_total_param_size=160671360, self._total_param_size=160677888, dwu_min_page_size=12288, self._block_size=53559296, self._chunk_size=26779648, self._shard_size=3347456
[0, 9, 34]
self._net_total_param_size=160671360, self._total_param_size=160677888, dwu_min_page_size=12288, self._block_size=53559296, self._chunk_size=26779648, self._shard_size=3347456
[0, 9, 34]
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3129344]), new_param_packed_fragment.size()=torch.Size([3129344]), master_param_fragment.size()=torch.Size([3129344])
model_param_fragment.size()=torch.Size([218112]), new_param_packed_fragment.size()=torch.Size([218112]), master_param_fragment.size()=torch.Size([218112])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3129344]), new_param_packed_fragment.size()=torch.Size([3129344]), master_param_fragment.size()=torch.Size([3129344])
model_param_fragment.size()=torch.Size([218112]), new_param_packed_fragment.size()=torch.Size([218112]), master_param_fragment.size()=torch.Size([218112])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
self._net_total_param_size=160671360, self._total_param_size=160677888, dwu_min_page_size=12288, self._block_size=53559296, self._chunk_size=26779648, self._shard_size=3347456
[0, 9, 34]
self._net_total_param_size=160671360, self._total_param_size=160677888, dwu_min_page_size=12288, self._block_size=53559296, self._chunk_size=26779648, self._shard_size=3347456
[0, 9, 34]
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([2169856]), new_param_packed_fragment.size()=torch.Size([2169856]), master_param_fragment.size()=torch.Size([2169856])
model_param_fragment.size()=torch.Size([1177600]), new_param_packed_fragment.size()=torch.Size([1177600]), master_param_fragment.size()=torch.Size([1177600])
model_param_fragment.size()=torch.Size([572416]), new_param_packed_fragment.size()=torch.Size([572416]), master_param_fragment.size()=torch.Size([572416])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([2766848]), new_param_packed_fragment.size()=torch.Size([2766848]), master_param_fragment.size()=torch.Size([2766848])
model_param_fragment.size()=torch.Size([1082432]), new_param_packed_fragment.size()=torch.Size([1082432]), master_param_fragment.size()=torch.Size([1082432])
model_param_fragment.size()=torch.Size([2265024]), new_param_packed_fragment.size()=torch.Size([2265024]), master_param_fragment.size()=torch.Size([2265024])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3016704]), new_param_packed_fragment.size()=torch.Size([3016704]), master_param_fragment.size()=torch.Size([3016704])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([322560]), new_param_packed_fragment.size()=torch.Size([322560]), master_param_fragment.size()=torch.Size([322560])
model_param_fragment.size()=torch.Size([1427456]), new_param_packed_fragment.size()=torch.Size([1427456]), master_param_fragment.size()=torch.Size([1427456])
model_param_fragment.size()=torch.Size([1920000]), new_param_packed_fragment.size()=torch.Size([1920000]), master_param_fragment.size()=torch.Size([1920000])
model_param_fragment.size()=torch.Size([1929280]), new_param_packed_fragment.size()=torch.Size([1929280]), master_param_fragment.size()=torch.Size([1929280])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([1409984]), new_param_packed_fragment.size()=torch.Size([1409984]), master_param_fragment.size()=torch.Size([1409984])
model_param_fragment.size()=torch.Size([331840]), new_param_packed_fragment.size()=torch.Size([331840]), master_param_fragment.size()=torch.Size([331840])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
self._net_total_param_size=160671360, self._total_param_size=160677888, dwu_min_page_size=12288, self._block_size=53559296, self._chunk_size=26779648, self._shard_size=3347456
[0, 9, 34]
model_param_fragment.size()=torch.Size([3007424]), new_param_packed_fragment.size()=torch.Size([3007424]), master_param_fragment.size()=torch.Size([3007424])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
self._net_total_param_size=160671360, self._total_param_size=160677888, dwu_min_page_size=12288, self._block_size=53559296, self._chunk_size=26779648, self._shard_size=3347456
[0, 9, 34]
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([2968576]), new_param_packed_fragment.size()=torch.Size([2968576]), master_param_fragment.size()=torch.Size([2968576])
model_param_fragment.size()=torch.Size([378880]), new_param_packed_fragment.size()=torch.Size([378880]), master_param_fragment.size()=torch.Size([378880])
model_param_fragment.size()=torch.Size([1371136]), new_param_packed_fragment.size()=torch.Size([1371136]), master_param_fragment.size()=torch.Size([1371136])
model_param_fragment.size()=torch.Size([1976320]), new_param_packed_fragment.size()=torch.Size([1976320]), master_param_fragment.size()=torch.Size([1976320])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([283712]), new_param_packed_fragment.size()=torch.Size([283712]), master_param_fragment.size()=torch.Size([283712])
model_param_fragment.size()=torch.Size([3063744]), new_param_packed_fragment.size()=torch.Size([3063744]), master_param_fragment.size()=torch.Size([3063744])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([1314816]), new_param_packed_fragment.size()=torch.Size([1314816]), master_param_fragment.size()=torch.Size([1314816])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([2024448]), new_param_packed_fragment.size()=torch.Size([2024448]), master_param_fragment.size()=torch.Size([2024448])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([2832448]), new_param_packed_fragment.size()=torch.Size([2832448]), master_param_fragment.size()=torch.Size([2832448])
model_param_fragment.size()=torch.Size([515008]), new_param_packed_fragment.size()=torch.Size([515008]), master_param_fragment.size()=torch.Size([515008])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
self._net_total_param_size=160671360, self._total_param_size=160677888, dwu_min_page_size=12288, self._block_size=53559296, self._chunk_size=26779648, self._shard_size=3347456
[0, 9, 34]
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([2217984]), new_param_packed_fragment.size()=torch.Size([2217984]), master_param_fragment.size()=torch.Size([2217984])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([1121280]), new_param_packed_fragment.size()=torch.Size([1121280]), master_param_fragment.size()=torch.Size([1121280])
model_param_fragment.size()=torch.Size([628736]), new_param_packed_fragment.size()=torch.Size([628736]), master_param_fragment.size()=torch.Size([628736])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([1024]), new_param_packed_fragment.size()=torch.Size([1024]), master_param_fragment.size()=torch.Size([1024])
model_param_fragment.size()=torch.Size([1]), new_param_packed_fragment.size()=torch.Size([1]), master_param_fragment.size()=torch.Size([1])
model_param_fragment.size()=torch.Size([1024]), new_param_packed_fragment.size()=torch.Size([1024]), master_param_fragment.size()=torch.Size([1024])
model_param_fragment.size()=torch.Size([1048576]), new_param_packed_fragment.size()=torch.Size([1048576]), master_param_fragment.size()=torch.Size([1048576])
model_param_fragment.size()=torch.Size([1048576]), new_param_packed_fragment.size()=torch.Size([1048576]), master_param_fragment.size()=torch.Size([1048576])
model_param_fragment.size()=torch.Size([611264]), new_param_packed_fragment.size()=torch.Size([611264]), master_param_fragment.size()=torch.Size([611264])
model_param_fragment.size()=torch.Size([1130560]), new_param_packed_fragment.size()=torch.Size([1130560]), master_param_fragment.size()=torch.Size([1130560])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([2208704]), new_param_packed_fragment.size()=torch.Size([2208704]), master_param_fragment.size()=torch.Size([2208704])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([2274304]), new_param_packed_fragment.size()=torch.Size([2274304]), master_param_fragment.size()=torch.Size([2274304])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([1064960]), new_param_packed_fragment.size()=torch.Size([1064960]), master_param_fragment.size()=torch.Size([1064960])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3308608]), new_param_packed_fragment.size()=torch.Size([3308608]), master_param_fragment.size()=torch.Size([3308608])
model_param_fragment.size()=torch.Size([32320]), new_param_packed_fragment.size()=torch.Size([32320]), master_param_fragment.size()=torch.Size([32320])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3016704]), new_param_packed_fragment.size()=torch.Size([3016704]), master_param_fragment.size()=torch.Size([3016704])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([322560]), new_param_packed_fragment.size()=torch.Size([322560]), master_param_fragment.size()=torch.Size([322560])
model_param_fragment.size()=torch.Size([1427456]), new_param_packed_fragment.size()=torch.Size([1427456]), master_param_fragment.size()=torch.Size([1427456])
model_param_fragment.size()=torch.Size([1920000]), new_param_packed_fragment.size()=torch.Size([1920000]), master_param_fragment.size()=torch.Size([1920000])
model_param_fragment.size()=torch.Size([1929280]), new_param_packed_fragment.size()=torch.Size([1929280]), master_param_fragment.size()=torch.Size([1929280])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([1409984]), new_param_packed_fragment.size()=torch.Size([1409984]), master_param_fragment.size()=torch.Size([1409984])
model_param_fragment.size()=torch.Size([331840]), new_param_packed_fragment.size()=torch.Size([331840]), master_param_fragment.size()=torch.Size([331840])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([3007424]), new_param_packed_fragment.size()=torch.Size([3007424]), master_param_fragment.size()=torch.Size([3007424])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
NCCL version 2.7.5+cuda11.0
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([1314816]), new_param_packed_fragment.size()=torch.Size([1314816]), master_param_fragment.size()=torch.Size([1314816])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([2024448]), new_param_packed_fragment.size()=torch.Size([2024448]), master_param_fragment.size()=torch.Size([2024448])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([2832448]), new_param_packed_fragment.size()=torch.Size([2832448]), master_param_fragment.size()=torch.Size([2832448])
model_param_fragment.size()=torch.Size([515008]), new_param_packed_fragment.size()=torch.Size([515008]), master_param_fragment.size()=torch.Size([515008])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
NCCL version 2.7.5+cuda11.0
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([2274304]), new_param_packed_fragment.size()=torch.Size([2274304]), master_param_fragment.size()=torch.Size([2274304])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([1064960]), new_param_packed_fragment.size()=torch.Size([1064960]), master_param_fragment.size()=torch.Size([1064960])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3308608]), new_param_packed_fragment.size()=torch.Size([3308608]), master_param_fragment.size()=torch.Size([3308608])
model_param_fragment.size()=torch.Size([32320]), new_param_packed_fragment.size()=torch.Size([32320]), master_param_fragment.size()=torch.Size([32320])
NCCL version 2.7.5+cuda11.0
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([2968576]), new_param_packed_fragment.size()=torch.Size([2968576]), master_param_fragment.size()=torch.Size([2968576])
model_param_fragment.size()=torch.Size([378880]), new_param_packed_fragment.size()=torch.Size([378880]), master_param_fragment.size()=torch.Size([378880])
model_param_fragment.size()=torch.Size([1371136]), new_param_packed_fragment.size()=torch.Size([1371136]), master_param_fragment.size()=torch.Size([1371136])
model_param_fragment.size()=torch.Size([1976320]), new_param_packed_fragment.size()=torch.Size([1976320]), master_param_fragment.size()=torch.Size([1976320])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([283712]), new_param_packed_fragment.size()=torch.Size([283712]), master_param_fragment.size()=torch.Size([283712])
model_param_fragment.size()=torch.Size([3063744]), new_param_packed_fragment.size()=torch.Size([3063744]), master_param_fragment.size()=torch.Size([3063744])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
NCCL version 2.7.5+cuda11.0
self._net_total_param_size=160671360, self._total_param_size=160677888, dwu_min_page_size=12288, self._block_size=53559296, self._chunk_size=26779648, self._shard_size=3347456
[0, 9, 34]
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([2169856]), new_param_packed_fragment.size()=torch.Size([2169856]), master_param_fragment.size()=torch.Size([2169856])
model_param_fragment.size()=torch.Size([1177600]), new_param_packed_fragment.size()=torch.Size([1177600]), master_param_fragment.size()=torch.Size([1177600])
model_param_fragment.size()=torch.Size([572416]), new_param_packed_fragment.size()=torch.Size([572416]), master_param_fragment.size()=torch.Size([572416])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([2766848]), new_param_packed_fragment.size()=torch.Size([2766848]), master_param_fragment.size()=torch.Size([2766848])
model_param_fragment.size()=torch.Size([1082432]), new_param_packed_fragment.size()=torch.Size([1082432]), master_param_fragment.size()=torch.Size([1082432])
model_param_fragment.size()=torch.Size([2265024]), new_param_packed_fragment.size()=torch.Size([2265024]), master_param_fragment.size()=torch.Size([2265024])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
NCCL version 2.7.5+cuda11.0
self._net_total_param_size=160671360, self._total_param_size=160677888, dwu_min_page_size=12288, self._block_size=53559296, self._chunk_size=26779648, self._shard_size=3347456
[0, 9, 34]
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([467968]), new_param_packed_fragment.size()=torch.Size([467968]), master_param_fragment.size()=torch.Size([467968])
model_param_fragment.size()=torch.Size([2879488]), new_param_packed_fragment.size()=torch.Size([2879488]), master_param_fragment.size()=torch.Size([2879488])
model_param_fragment.size()=torch.Size([3073024]), new_param_packed_fragment.size()=torch.Size([3073024]), master_param_fragment.size()=torch.Size([3073024])
model_param_fragment.size()=torch.Size([274432]), new_param_packed_fragment.size()=torch.Size([274432]), master_param_fragment.size()=torch.Size([274432])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
NCCL version 2.7.5+cuda11.0
0: Using optimizer: DistributedFusedAdam (
Parameter Group 0
    betas: (0.9, 0.999)
    bias_correction: True
    eps: 1e-08
    lr: 0.002875
    max_grad_norm: 5.0
    weight_decay: 0.0
)
:::MLLOG {"namespace": "", "time_ms": 1593112690117, "event_type": "POINT_IN_TIME", "key": "opt_name", "value": "adam", "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/trainer.py", "lineno": 176}}
:::MLLOG {"namespace": "", "time_ms": 1593112690117, "event_type": "POINT_IN_TIME", "key": "opt_base_learning_rate", "value": 0.002875, "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/trainer.py", "lineno": 178}}
:::MLLOG {"namespace": "", "time_ms": 1593112690117, "event_type": "POINT_IN_TIME", "key": "opt_adam_beta_1", "value": 0.9, "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/trainer.py", "lineno": 180}}
:::MLLOG {"namespace": "", "time_ms": 1593112690117, "event_type": "POINT_IN_TIME", "key": "opt_adam_beta_2", "value": 0.999, "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/trainer.py", "lineno": 182}}
:::MLLOG {"namespace": "", "time_ms": 1593112690117, "event_type": "POINT_IN_TIME", "key": "opt_adam_epsilon", "value": 1e-08, "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/trainer.py", "lineno": 184}}
0: Executing preallocation
:::MLLOG {"namespace": "", "time_ms": 1593112691579, "event_type": "INTERVAL_END", "key": "init_stop", "value": null, "metadata": {"file": "train.py", "lineno": 429}}
:::MLLOG {"namespace": "", "time_ms": 1593112691580, "event_type": "INTERVAL_START", "key": "run_start", "value": null, "metadata": {"file": "train.py", "lineno": 430}}
:::MLLOG {"namespace": "", "time_ms": 1593112691580, "event_type": "POINT_IN_TIME", "key": "max_sequence_length", "value": 75, "metadata": {"file": "train.py", "lineno": 435, "method": "discard"}}
0: Reading preprocessed data file from /preproc_data/75/training.bin
0: Preprocessed data: length: 3975116 min length: 0 max length: 75
0: Opening preprocessed data /preproc_data/75/training.bin for reading
0: Processing data from /data/newstest2014.tok.bpe.32000.en
0: Filtering data, min len: 0, max len: 150
0: Pairs before: 3003, after: 3003
:::MLLOG {"namespace": "", "time_ms": 1593112691809, "event_type": "POINT_IN_TIME", "key": "global_batch_size", "value": 3072, "metadata": {"file": "train.py", "lineno": 480}}
:::MLLOG {"namespace": "", "time_ms": 1593112691810, "event_type": "POINT_IN_TIME", "key": "train_samples", "value": 3965952, "metadata": {"file": "train.py", "lineno": 488}}
:::MLLOG {"namespace": "", "time_ms": 1593112691810, "event_type": "POINT_IN_TIME", "key": "eval_samples", "value": 3003, "metadata": {"file": "train.py", "lineno": 490}}
0: Training LR schedule config: {'warmup_steps': 200, 'remain_steps': 4054, 'decay_interval': 506, 'decay_steps': 4, 'decay_factor': 0.5}
0: Scheduler warmup steps: 200
0: Scheduler remain steps: 4054
0: Scheduler decay interval: 506
0: Scheduler decay factor: 0.5
0: Scheduler max decay steps: 4
:::MLLOG {"namespace": "", "time_ms": 1593112691811, "event_type": "POINT_IN_TIME", "key": "opt_learning_rate_alt_decay_func", "value": true, "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/lr_scheduler.py", "lineno": 78}}
:::MLLOG {"namespace": "", "time_ms": 1593112691811, "event_type": "POINT_IN_TIME", "key": "opt_learning_rate_alt_warmup_func", "value": true, "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/lr_scheduler.py", "lineno": 80}}
:::MLLOG {"namespace": "", "time_ms": 1593112691811, "event_type": "POINT_IN_TIME", "key": "opt_learning_rate_decay_interval", "value": 506, "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/lr_scheduler.py", "lineno": 83}}
:::MLLOG {"namespace": "", "time_ms": 1593112691811, "event_type": "POINT_IN_TIME", "key": "opt_learning_rate_decay_factor", "value": 0.5, "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/lr_scheduler.py", "lineno": 85}}
:::MLLOG {"namespace": "", "time_ms": 1593112691811, "event_type": "POINT_IN_TIME", "key": "opt_learning_rate_decay_steps", "value": 4, "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/lr_scheduler.py", "lineno": 87}}
:::MLLOG {"namespace": "", "time_ms": 1593112691811, "event_type": "POINT_IN_TIME", "key": "opt_learning_rate_remain_steps", "value": 4054, "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/lr_scheduler.py", "lineno": 89}}
:::MLLOG {"namespace": "", "time_ms": 1593112691812, "event_type": "POINT_IN_TIME", "key": "opt_learning_rate_warmup_steps", "value": 200, "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/lr_scheduler.py", "lineno": 91}}
:::MLLOG {"namespace": "", "time_ms": 1593112691812, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "train.py", "lineno": 538, "first_epoch_num": 1, "epoch_count": 1}}
:::MLLOG {"namespace": "", "time_ms": 1593112691812, "event_type": "INTERVAL_START", "key": "epoch_start", "value": null, "metadata": {"file": "train.py", "lineno": 541, "epoch_num": 1}}
0: Starting epoch 0
0: Sampler for epoch 0 uses seed 4013050620
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
0: TRAIN [0][0/1291]	Time 0.387 (0.387)	Data 1.95e-01 (1.95e-01)	Tok/s 32863 (32863)	Loss/tok 10.6728 (10.6728)	LR 2.942e-05
0: TRAIN [0][10/1291]	Time 0.075 (0.087)	Data 9.49e-05 (1.78e-02)	Tok/s 232016 (195956)	Loss/tok 9.6103 (10.0141)	LR 3.704e-05
0: TRAIN [0][20/1291]	Time 0.058 (0.070)	Data 9.25e-05 (9.36e-03)	Tok/s 217898 (198665)	Loss/tok 9.2061 (9.6943)	LR 4.663e-05
0: TRAIN [0][30/1291]	Time 0.058 (0.060)	Data 9.04e-05 (6.37e-03)	Tok/s 215338 (193478)	Loss/tok 8.9472 (9.5050)	LR 5.870e-05
0: Gradient norm: inf
0: Skipped batch, new scale: 512.0
0: TRAIN [0][40/1291]	Time 0.058 (0.058)	Data 9.30e-05 (4.84e-03)	Tok/s 217408 (194905)	Loss/tok 8.6642 (9.3232)	LR 7.222e-05
0: TRAIN [0][50/1291]	Time 0.041 (0.057)	Data 8.63e-05 (3.91e-03)	Tok/s 182380 (195785)	Loss/tok 8.2690 (9.1449)	LR 9.092e-05
0: TRAIN [0][60/1291]	Time 0.041 (0.056)	Data 8.82e-05 (3.28e-03)	Tok/s 189241 (197202)	Loss/tok 8.0657 (8.9988)	LR 1.145e-04
0: Gradient norm: inf
0: Skipped batch, new scale: 256.0
0: Gradient norm: inf
0: Skipped batch, new scale: 128.0
0: TRAIN [0][70/1291]	Time 0.058 (0.055)	Data 8.77e-05 (2.83e-03)	Tok/s 219980 (197650)	Loss/tok 8.0851 (8.9041)	LR 1.376e-04
0: TRAIN [0][80/1291]	Time 0.058 (0.055)	Data 8.89e-05 (2.49e-03)	Tok/s 219330 (199130)	Loss/tok 7.9475 (8.7897)	LR 1.732e-04
0: TRAIN [0][90/1291]	Time 0.025 (0.056)	Data 9.11e-05 (2.23e-03)	Tok/s 158740 (200324)	Loss/tok 7.3435 (8.6847)	LR 2.181e-04
0: TRAIN [0][100/1291]	Time 0.041 (0.056)	Data 8.73e-05 (2.02e-03)	Tok/s 189147 (200822)	Loss/tok 7.7137 (8.6061)	LR 2.746e-04
0: TRAIN [0][110/1291]	Time 0.041 (0.055)	Data 9.06e-05 (1.84e-03)	Tok/s 188738 (200611)	Loss/tok 7.6951 (8.5446)	LR 3.457e-04
0: Gradient norm: inf
0: Skipped batch, new scale: 64.0
0: TRAIN [0][120/1291]	Time 0.042 (0.054)	Data 9.54e-05 (1.70e-03)	Tok/s 184129 (200141)	Loss/tok 7.6944 (8.4932)	LR 4.252e-04
0: TRAIN [0][130/1291]	Time 0.096 (0.054)	Data 8.56e-05 (1.58e-03)	Tok/s 234983 (200096)	Loss/tok 8.0360 (8.4415)	LR 5.354e-04
0: TRAIN [0][140/1291]	Time 0.058 (0.054)	Data 1.17e-04 (1.47e-03)	Tok/s 216083 (200778)	Loss/tok 7.6699 (8.3882)	LR 6.740e-04
0: TRAIN [0][150/1291]	Time 0.042 (0.054)	Data 1.45e-04 (1.38e-03)	Tok/s 190476 (200407)	Loss/tok 7.4339 (8.3402)	LR 8.485e-04
0: TRAIN [0][160/1291]	Time 0.042 (0.054)	Data 8.68e-05 (1.30e-03)	Tok/s 185770 (200396)	Loss/tok 7.2776 (8.2903)	LR 1.068e-03
0: TRAIN [0][170/1291]	Time 0.076 (0.053)	Data 8.18e-05 (1.23e-03)	Tok/s 231163 (200590)	Loss/tok 7.4905 (8.2338)	LR 1.345e-03
0: TRAIN [0][180/1291]	Time 0.096 (0.054)	Data 1.39e-04 (1.17e-03)	Tok/s 234810 (201120)	Loss/tok 7.3280 (8.1658)	LR 1.693e-03
0: TRAIN [0][190/1291]	Time 0.058 (0.053)	Data 8.37e-05 (1.11e-03)	Tok/s 218301 (200723)	Loss/tok 7.0917 (8.1140)	LR 2.131e-03
0: TRAIN [0][200/1291]	Time 0.025 (0.053)	Data 8.58e-05 (1.06e-03)	Tok/s 162623 (200784)	Loss/tok 5.9004 (8.0511)	LR 2.683e-03
0: TRAIN [0][210/1291]	Time 0.058 (0.053)	Data 8.23e-05 (1.01e-03)	Tok/s 216129 (201049)	Loss/tok 6.7516 (7.9888)	LR 2.875e-03
0: TRAIN [0][220/1291]	Time 0.024 (0.053)	Data 8.49e-05 (9.73e-04)	Tok/s 156475 (200996)	Loss/tok 5.5614 (7.9268)	LR 2.875e-03
0: TRAIN [0][230/1291]	Time 0.058 (0.053)	Data 8.32e-05 (9.35e-04)	Tok/s 217521 (200776)	Loss/tok 6.4749 (7.8718)	LR 2.875e-03
0: TRAIN [0][240/1291]	Time 0.041 (0.052)	Data 8.75e-05 (9.00e-04)	Tok/s 187520 (200391)	Loss/tok 5.9182 (7.8142)	LR 2.875e-03
0: Upscaling, new scale: 128.0
0: TRAIN [0][250/1291]	Time 0.058 (0.052)	Data 8.25e-05 (8.68e-04)	Tok/s 216923 (200050)	Loss/tok 6.3005 (7.7551)	LR 2.875e-03
0: TRAIN [0][260/1291]	Time 0.057 (0.052)	Data 1.44e-04 (8.39e-04)	Tok/s 219428 (200358)	Loss/tok 5.9945 (7.6774)	LR 2.875e-03
0: TRAIN [0][270/1291]	Time 0.041 (0.052)	Data 8.56e-05 (8.11e-04)	Tok/s 190469 (200344)	Loss/tok 5.5723 (7.6149)	LR 2.875e-03
0: TRAIN [0][280/1291]	Time 0.025 (0.052)	Data 8.30e-05 (7.85e-04)	Tok/s 160333 (199742)	Loss/tok 4.6430 (7.5617)	LR 2.875e-03
0: TRAIN [0][290/1291]	Time 0.076 (0.052)	Data 8.70e-05 (7.61e-04)	Tok/s 229729 (199980)	Loss/tok 5.8844 (7.4948)	LR 2.875e-03
0: TRAIN [0][300/1291]	Time 0.076 (0.052)	Data 8.49e-05 (7.39e-04)	Tok/s 232031 (200244)	Loss/tok 5.7875 (7.4240)	LR 2.875e-03
0: TRAIN [0][310/1291]	Time 0.076 (0.052)	Data 8.49e-05 (7.18e-04)	Tok/s 233236 (200072)	Loss/tok 5.7734 (7.3651)	LR 2.875e-03
0: TRAIN [0][320/1291]	Time 0.058 (0.052)	Data 1.48e-04 (6.98e-04)	Tok/s 217912 (200017)	Loss/tok 5.1456 (7.3008)	LR 2.875e-03
0: TRAIN [0][330/1291]	Time 0.059 (0.052)	Data 1.01e-04 (6.80e-04)	Tok/s 212902 (200052)	Loss/tok 5.2010 (7.2382)	LR 2.875e-03
0: TRAIN [0][340/1291]	Time 0.096 (0.052)	Data 1.39e-04 (6.63e-04)	Tok/s 233049 (200416)	Loss/tok 5.4782 (7.1642)	LR 2.875e-03
0: TRAIN [0][350/1291]	Time 0.024 (0.052)	Data 9.11e-05 (6.47e-04)	Tok/s 158917 (200631)	Loss/tok 3.9360 (7.0961)	LR 2.875e-03
0: TRAIN [0][360/1291]	Time 0.043 (0.052)	Data 8.44e-05 (6.32e-04)	Tok/s 188541 (200908)	Loss/tok 4.4408 (7.0272)	LR 2.875e-03
0: TRAIN [0][370/1291]	Time 0.025 (0.052)	Data 8.65e-05 (6.17e-04)	Tok/s 157416 (200578)	Loss/tok 3.6315 (6.9743)	LR 2.875e-03
0: Upscaling, new scale: 256.0
0: TRAIN [0][380/1291]	Time 0.041 (0.052)	Data 8.30e-05 (6.03e-04)	Tok/s 190829 (200576)	Loss/tok 4.4378 (6.9144)	LR 2.875e-03
0: TRAIN [0][390/1291]	Time 0.042 (0.052)	Data 8.25e-05 (5.91e-04)	Tok/s 180825 (200556)	Loss/tok 4.1684 (6.8544)	LR 2.875e-03
0: TRAIN [0][400/1291]	Time 0.041 (0.052)	Data 1.52e-04 (5.78e-04)	Tok/s 191171 (200509)	Loss/tok 4.2165 (6.8003)	LR 2.875e-03
0: TRAIN [0][410/1291]	Time 0.042 (0.052)	Data 8.46e-05 (5.66e-04)	Tok/s 181989 (200440)	Loss/tok 4.2159 (6.7474)	LR 2.875e-03
0: TRAIN [0][420/1291]	Time 0.025 (0.052)	Data 1.40e-04 (5.56e-04)	Tok/s 157661 (200352)	Loss/tok 3.6444 (6.6975)	LR 2.875e-03
0: TRAIN [0][430/1291]	Time 0.076 (0.052)	Data 8.54e-05 (5.45e-04)	Tok/s 230181 (200533)	Loss/tok 4.7407 (6.6392)	LR 2.875e-03
0: TRAIN [0][440/1291]	Time 0.041 (0.052)	Data 8.63e-05 (5.35e-04)	Tok/s 188122 (200864)	Loss/tok 4.1756 (6.5788)	LR 2.875e-03
0: TRAIN [0][450/1291]	Time 0.096 (0.052)	Data 8.39e-05 (5.25e-04)	Tok/s 235265 (200963)	Loss/tok 4.7300 (6.5239)	LR 2.875e-03
0: TRAIN [0][460/1291]	Time 0.041 (0.052)	Data 8.61e-05 (5.15e-04)	Tok/s 188602 (200855)	Loss/tok 4.0985 (6.4795)	LR 2.875e-03
0: TRAIN [0][470/1291]	Time 0.025 (0.052)	Data 1.46e-04 (5.07e-04)	Tok/s 162477 (200489)	Loss/tok 3.4517 (6.4437)	LR 2.875e-03
0: TRAIN [0][480/1291]	Time 0.042 (0.052)	Data 8.30e-05 (4.98e-04)	Tok/s 186529 (200507)	Loss/tok 4.0207 (6.3998)	LR 2.875e-03
0: TRAIN [0][490/1291]	Time 0.042 (0.052)	Data 7.94e-05 (4.89e-04)	Tok/s 182793 (200537)	Loss/tok 3.8966 (6.3540)	LR 2.875e-03
0: TRAIN [0][500/1291]	Time 0.041 (0.052)	Data 8.80e-05 (4.82e-04)	Tok/s 185380 (200441)	Loss/tok 3.8383 (6.3128)	LR 2.875e-03
0: Upscaling, new scale: 512.0
0: Gradient norm: inf
0: Skipped batch, new scale: 256.0
0: TRAIN [0][510/1291]	Time 0.058 (0.052)	Data 8.32e-05 (4.74e-04)	Tok/s 217774 (200494)	Loss/tok 4.1839 (6.2690)	LR 2.875e-03
0: TRAIN [0][520/1291]	Time 0.024 (0.052)	Data 8.18e-05 (4.67e-04)	Tok/s 159904 (200456)	Loss/tok 3.2478 (6.2298)	LR 2.875e-03
0: TRAIN [0][530/1291]	Time 0.058 (0.052)	Data 8.23e-05 (4.59e-04)	Tok/s 212971 (200322)	Loss/tok 4.0727 (6.1930)	LR 2.875e-03
0: TRAIN [0][540/1291]	Time 0.076 (0.052)	Data 8.27e-05 (4.53e-04)	Tok/s 229232 (200325)	Loss/tok 4.4812 (6.1545)	LR 2.875e-03
0: TRAIN [0][550/1291]	Time 0.096 (0.052)	Data 8.39e-05 (4.46e-04)	Tok/s 231551 (200440)	Loss/tok 4.4802 (6.1140)	LR 2.875e-03
0: TRAIN [0][560/1291]	Time 0.059 (0.052)	Data 1.43e-04 (4.40e-04)	Tok/s 214760 (200540)	Loss/tok 4.0507 (6.0748)	LR 2.875e-03
0: TRAIN [0][570/1291]	Time 0.042 (0.052)	Data 8.34e-05 (4.34e-04)	Tok/s 185282 (200583)	Loss/tok 3.7006 (6.0389)	LR 2.875e-03
0: TRAIN [0][580/1291]	Time 0.041 (0.052)	Data 8.25e-05 (4.28e-04)	Tok/s 187784 (200858)	Loss/tok 3.8265 (5.9955)	LR 2.875e-03
0: TRAIN [0][590/1291]	Time 0.041 (0.052)	Data 1.40e-04 (4.23e-04)	Tok/s 186377 (200672)	Loss/tok 3.7861 (5.9675)	LR 2.875e-03
0: TRAIN [0][600/1291]	Time 0.041 (0.052)	Data 8.82e-05 (4.17e-04)	Tok/s 183503 (200764)	Loss/tok 3.7207 (5.9301)	LR 2.875e-03
0: TRAIN [0][610/1291]	Time 0.058 (0.052)	Data 8.82e-05 (4.12e-04)	Tok/s 215460 (200708)	Loss/tok 3.9429 (5.8999)	LR 2.875e-03
0: TRAIN [0][620/1291]	Time 0.058 (0.052)	Data 8.51e-05 (4.07e-04)	Tok/s 216477 (200825)	Loss/tok 3.9310 (5.8659)	LR 2.875e-03
0: TRAIN [0][630/1291]	Time 0.097 (0.052)	Data 1.02e-04 (4.02e-04)	Tok/s 232969 (200987)	Loss/tok 4.2798 (5.8305)	LR 2.875e-03
0: Upscaling, new scale: 512.0
0: TRAIN [0][640/1291]	Time 0.076 (0.052)	Data 8.18e-05 (3.97e-04)	Tok/s 231238 (201072)	Loss/tok 4.1546 (5.7987)	LR 2.875e-03
0: TRAIN [0][650/1291]	Time 0.041 (0.052)	Data 8.46e-05 (3.93e-04)	Tok/s 188237 (201031)	Loss/tok 3.7937 (5.7714)	LR 2.875e-03
0: TRAIN [0][660/1291]	Time 0.058 (0.052)	Data 8.18e-05 (3.88e-04)	Tok/s 212459 (200982)	Loss/tok 4.0794 (5.7450)	LR 2.875e-03
0: TRAIN [0][670/1291]	Time 0.041 (0.052)	Data 8.18e-05 (3.83e-04)	Tok/s 187342 (200913)	Loss/tok 3.7348 (5.7198)	LR 2.875e-03
0: TRAIN [0][680/1291]	Time 0.041 (0.052)	Data 1.41e-04 (3.79e-04)	Tok/s 190726 (200997)	Loss/tok 3.5636 (5.6918)	LR 2.875e-03
0: TRAIN [0][690/1291]	Time 0.042 (0.052)	Data 8.85e-05 (3.75e-04)	Tok/s 185942 (201125)	Loss/tok 3.6541 (5.6625)	LR 2.875e-03
0: TRAIN [0][700/1291]	Time 0.025 (0.052)	Data 8.49e-05 (3.71e-04)	Tok/s 158292 (201202)	Loss/tok 3.0575 (5.6341)	LR 2.875e-03
0: TRAIN [0][710/1291]	Time 0.041 (0.052)	Data 1.43e-04 (3.68e-04)	Tok/s 182357 (201175)	Loss/tok 3.6263 (5.6099)	LR 2.875e-03
0: TRAIN [0][720/1291]	Time 0.076 (0.052)	Data 8.20e-05 (3.64e-04)	Tok/s 232814 (201310)	Loss/tok 4.0313 (5.5818)	LR 2.875e-03
0: TRAIN [0][730/1291]	Time 0.058 (0.052)	Data 8.85e-05 (3.60e-04)	Tok/s 218344 (201284)	Loss/tok 3.9563 (5.5586)	LR 2.875e-03
0: TRAIN [0][740/1291]	Time 0.076 (0.052)	Data 8.46e-05 (3.57e-04)	Tok/s 231107 (201382)	Loss/tok 4.0768 (5.5340)	LR 2.875e-03
0: TRAIN [0][750/1291]	Time 0.042 (0.053)	Data 8.34e-05 (3.53e-04)	Tok/s 184282 (201498)	Loss/tok 3.3932 (5.5096)	LR 2.875e-03
0: TRAIN [0][760/1291]	Time 0.041 (0.052)	Data 8.25e-05 (3.50e-04)	Tok/s 187889 (201360)	Loss/tok 3.4241 (5.4908)	LR 2.875e-03
0: Upscaling, new scale: 1024.0
0: TRAIN [0][770/1291]	Time 0.041 (0.053)	Data 8.32e-05 (3.46e-04)	Tok/s 184623 (201499)	Loss/tok 3.5819 (5.4665)	LR 2.875e-03
0: TRAIN [0][780/1291]	Time 0.041 (0.053)	Data 1.40e-04 (3.43e-04)	Tok/s 192104 (201511)	Loss/tok 3.4823 (5.4445)	LR 2.875e-03
0: TRAIN [0][790/1291]	Time 0.058 (0.053)	Data 8.54e-05 (3.40e-04)	Tok/s 213885 (201644)	Loss/tok 3.8089 (5.4213)	LR 2.875e-03
0: TRAIN [0][800/1291]	Time 0.041 (0.053)	Data 8.37e-05 (3.37e-04)	Tok/s 189481 (201626)	Loss/tok 3.7338 (5.4010)	LR 2.875e-03
0: TRAIN [0][810/1291]	Time 0.041 (0.052)	Data 7.92e-05 (3.34e-04)	Tok/s 194613 (201524)	Loss/tok 3.4871 (5.3831)	LR 2.875e-03
0: TRAIN [0][820/1291]	Time 0.076 (0.052)	Data 1.40e-04 (3.31e-04)	Tok/s 227613 (201452)	Loss/tok 3.9695 (5.3646)	LR 2.875e-03
0: TRAIN [0][830/1291]	Time 0.041 (0.052)	Data 7.99e-05 (3.28e-04)	Tok/s 191895 (201480)	Loss/tok 3.5573 (5.3440)	LR 2.875e-03
0: TRAIN [0][840/1291]	Time 0.096 (0.053)	Data 8.46e-05 (3.26e-04)	Tok/s 231531 (201485)	Loss/tok 4.1726 (5.3251)	LR 2.875e-03
0: TRAIN [0][850/1291]	Time 0.076 (0.053)	Data 8.34e-05 (3.23e-04)	Tok/s 230053 (201503)	Loss/tok 3.8850 (5.3063)	LR 2.875e-03
0: TRAIN [0][860/1291]	Time 0.058 (0.052)	Data 8.70e-05 (3.20e-04)	Tok/s 217595 (201446)	Loss/tok 3.6697 (5.2892)	LR 2.875e-03
0: TRAIN [0][870/1291]	Time 0.041 (0.052)	Data 1.44e-04 (3.18e-04)	Tok/s 184662 (201396)	Loss/tok 3.5865 (5.2726)	LR 2.875e-03
0: TRAIN [0][880/1291]	Time 0.058 (0.052)	Data 1.41e-04 (3.15e-04)	Tok/s 213416 (201449)	Loss/tok 3.8086 (5.2538)	LR 2.875e-03
0: TRAIN [0][890/1291]	Time 0.076 (0.053)	Data 8.61e-05 (3.13e-04)	Tok/s 232576 (201484)	Loss/tok 3.7441 (5.2355)	LR 2.875e-03
0: Upscaling, new scale: 2048.0
0: TRAIN [0][900/1291]	Time 0.042 (0.052)	Data 1.03e-04 (3.11e-04)	Tok/s 187524 (201455)	Loss/tok 3.6161 (5.2195)	LR 2.875e-03
0: TRAIN [0][910/1291]	Time 0.059 (0.053)	Data 8.39e-05 (3.08e-04)	Tok/s 214099 (201562)	Loss/tok 3.6378 (5.2008)	LR 2.875e-03
0: TRAIN [0][920/1291]	Time 0.058 (0.053)	Data 8.32e-05 (3.06e-04)	Tok/s 212351 (201599)	Loss/tok 3.7424 (5.1842)	LR 2.875e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 1024.0
0: Gradient norm: inf
0: Skipped batch, new scale: 512.0
0: TRAIN [0][930/1291]	Time 0.025 (0.053)	Data 8.87e-05 (3.04e-04)	Tok/s 156524 (201690)	Loss/tok 3.1450 (5.1666)	LR 2.875e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 256.0
0: TRAIN [0][940/1291]	Time 0.058 (0.053)	Data 8.15e-05 (3.01e-04)	Tok/s 215816 (201590)	Loss/tok 3.6990 (5.1528)	LR 2.875e-03
0: TRAIN [0][950/1291]	Time 0.024 (0.053)	Data 1.41e-04 (2.99e-04)	Tok/s 161820 (201482)	Loss/tok 2.9540 (5.1393)	LR 2.875e-03
0: TRAIN [0][960/1291]	Time 0.058 (0.053)	Data 8.32e-05 (2.97e-04)	Tok/s 218838 (201553)	Loss/tok 3.6425 (5.1233)	LR 2.875e-03
0: TRAIN [0][970/1291]	Time 0.060 (0.053)	Data 1.38e-04 (2.95e-04)	Tok/s 207998 (201468)	Loss/tok 3.7150 (5.1102)	LR 2.875e-03
0: TRAIN [0][980/1291]	Time 0.058 (0.053)	Data 8.34e-05 (2.93e-04)	Tok/s 216472 (201484)	Loss/tok 3.7228 (5.0951)	LR 2.875e-03
0: TRAIN [0][990/1291]	Time 0.025 (0.052)	Data 1.14e-04 (2.91e-04)	Tok/s 157230 (201436)	Loss/tok 2.9003 (5.0817)	LR 2.875e-03
0: TRAIN [0][1000/1291]	Time 0.058 (0.052)	Data 8.23e-05 (2.89e-04)	Tok/s 216519 (201385)	Loss/tok 3.6017 (5.0685)	LR 2.875e-03
0: TRAIN [0][1010/1291]	Time 0.041 (0.052)	Data 1.07e-04 (2.87e-04)	Tok/s 187475 (201424)	Loss/tok 3.3134 (5.0538)	LR 2.875e-03
0: TRAIN [0][1020/1291]	Time 0.058 (0.052)	Data 9.49e-05 (2.85e-04)	Tok/s 215468 (201423)	Loss/tok 3.5887 (5.0400)	LR 2.875e-03
0: TRAIN [0][1030/1291]	Time 0.076 (0.052)	Data 8.87e-05 (2.83e-04)	Tok/s 230543 (201457)	Loss/tok 3.8199 (5.0263)	LR 2.875e-03
0: TRAIN [0][1040/1291]	Time 0.058 (0.053)	Data 1.28e-04 (2.81e-04)	Tok/s 217828 (201525)	Loss/tok 3.6832 (5.0121)	LR 2.875e-03
0: TRAIN [0][1050/1291]	Time 0.041 (0.053)	Data 8.27e-05 (2.79e-04)	Tok/s 184890 (201520)	Loss/tok 3.4752 (4.9999)	LR 2.875e-03
0: Upscaling, new scale: 512.0
0: TRAIN [0][1060/1291]	Time 0.058 (0.053)	Data 8.37e-05 (2.78e-04)	Tok/s 214975 (201609)	Loss/tok 3.5055 (4.9856)	LR 2.875e-03
0: TRAIN [0][1070/1291]	Time 0.042 (0.053)	Data 8.92e-05 (2.76e-04)	Tok/s 184143 (201521)	Loss/tok 3.3315 (4.9745)	LR 2.875e-03
0: TRAIN [0][1080/1291]	Time 0.025 (0.053)	Data 8.80e-05 (2.74e-04)	Tok/s 163521 (201585)	Loss/tok 2.9790 (4.9603)	LR 2.875e-03
0: TRAIN [0][1090/1291]	Time 0.058 (0.053)	Data 8.68e-05 (2.73e-04)	Tok/s 217269 (201507)	Loss/tok 3.6571 (4.9493)	LR 2.875e-03
0: TRAIN [0][1100/1291]	Time 0.096 (0.053)	Data 8.32e-05 (2.71e-04)	Tok/s 228741 (201542)	Loss/tok 3.9715 (4.9364)	LR 2.875e-03
0: TRAIN [0][1110/1291]	Time 0.058 (0.053)	Data 8.42e-05 (2.69e-04)	Tok/s 212607 (201494)	Loss/tok 3.5936 (4.9255)	LR 2.875e-03
0: TRAIN [0][1120/1291]	Time 0.097 (0.053)	Data 8.70e-05 (2.68e-04)	Tok/s 232383 (201503)	Loss/tok 3.9543 (4.9133)	LR 2.875e-03
0: TRAIN [0][1130/1291]	Time 0.041 (0.053)	Data 8.18e-05 (2.66e-04)	Tok/s 186973 (201490)	Loss/tok 3.3196 (4.9020)	LR 2.875e-03
0: TRAIN [0][1140/1291]	Time 0.097 (0.053)	Data 8.11e-05 (2.65e-04)	Tok/s 231388 (201580)	Loss/tok 3.9784 (4.8891)	LR 2.875e-03
0: TRAIN [0][1150/1291]	Time 0.042 (0.053)	Data 8.23e-05 (2.63e-04)	Tok/s 190648 (201545)	Loss/tok 3.4061 (4.8787)	LR 2.875e-03
0: TRAIN [0][1160/1291]	Time 0.097 (0.053)	Data 8.80e-05 (2.62e-04)	Tok/s 230740 (201549)	Loss/tok 3.9195 (4.8671)	LR 2.875e-03
0: TRAIN [0][1170/1291]	Time 0.058 (0.053)	Data 8.54e-05 (2.60e-04)	Tok/s 213996 (201452)	Loss/tok 3.6703 (4.8579)	LR 2.875e-03
0: TRAIN [0][1180/1291]	Time 0.042 (0.053)	Data 8.37e-05 (2.59e-04)	Tok/s 184757 (201517)	Loss/tok 3.3532 (4.8464)	LR 2.875e-03
0: Upscaling, new scale: 1024.0
0: TRAIN [0][1190/1291]	Time 0.059 (0.053)	Data 8.51e-05 (2.57e-04)	Tok/s 212899 (201581)	Loss/tok 3.6940 (4.8355)	LR 2.875e-03
0: TRAIN [0][1200/1291]	Time 0.058 (0.053)	Data 8.32e-05 (2.56e-04)	Tok/s 215085 (201576)	Loss/tok 3.6570 (4.8251)	LR 2.875e-03
0: TRAIN [0][1210/1291]	Time 0.058 (0.053)	Data 8.08e-05 (2.55e-04)	Tok/s 213386 (201588)	Loss/tok 3.6667 (4.8147)	LR 2.875e-03
0: TRAIN [0][1220/1291]	Time 0.058 (0.053)	Data 8.58e-05 (2.54e-04)	Tok/s 217406 (201600)	Loss/tok 3.5331 (4.8045)	LR 2.875e-03
0: TRAIN [0][1230/1291]	Time 0.025 (0.053)	Data 8.25e-05 (2.52e-04)	Tok/s 164389 (201643)	Loss/tok 2.8354 (4.7942)	LR 2.875e-03
0: TRAIN [0][1240/1291]	Time 0.025 (0.053)	Data 1.06e-04 (2.51e-04)	Tok/s 162071 (201601)	Loss/tok 2.7921 (4.7847)	LR 2.875e-03
0: TRAIN [0][1250/1291]	Time 0.042 (0.053)	Data 8.54e-05 (2.50e-04)	Tok/s 192129 (201660)	Loss/tok 3.3224 (4.7743)	LR 2.875e-03
0: TRAIN [0][1260/1291]	Time 0.042 (0.053)	Data 8.65e-05 (2.48e-04)	Tok/s 187410 (201661)	Loss/tok 3.3286 (4.7648)	LR 2.875e-03
0: TRAIN [0][1270/1291]	Time 0.041 (0.053)	Data 7.84e-05 (2.47e-04)	Tok/s 193290 (201719)	Loss/tok 3.3313 (4.7542)	LR 2.875e-03
0: TRAIN [0][1280/1291]	Time 0.076 (0.053)	Data 8.94e-05 (2.46e-04)	Tok/s 227522 (201702)	Loss/tok 3.9423 (4.7453)	LR 2.875e-03
0: TRAIN [0][1290/1291]	Time 0.024 (0.053)	Data 4.53e-05 (2.46e-04)	Tok/s 159372 (201666)	Loss/tok 2.9074 (4.7368)	LR 2.875e-03
:::MLLOG {"namespace": "", "time_ms": 1593112759986, "event_type": "INTERVAL_END", "key": "epoch_stop", "value": null, "metadata": {"file": "train.py", "lineno": 551, "epoch_num": 1}}
:::MLLOG {"namespace": "", "time_ms": 1593112759986, "event_type": "INTERVAL_START", "key": "eval_start", "value": null, "metadata": {"file": "train.py", "lineno": 556, "epoch_num": 1}}
0: Running evaluation on test set
0: TEST [0][0/3]	Time 0.401 (0.401)	Decoder iters 149.0 (149.0)	Tok/s 22612 (22612)
0: Running moses detokenizer
0: BLEU(score=19.487220382357293, counts=[34432, 15644, 8275, 4513], totals=[65709, 62706, 59703, 56704], precisions=[52.400736580985864, 24.948170828947788, 13.860275028055542, 7.958874153498871], bp=1.0, sys_len=65709, ref_len=64676)
0: Finished evaluation on test set
:::MLLOG {"namespace": "", "time_ms": 1593112761379, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.1949, "metadata": {"file": "train.py", "lineno": 562, "epoch_num": 1}}
:::MLLOG {"namespace": "", "time_ms": 1593112761379, "event_type": "INTERVAL_END", "key": "eval_stop", "value": null, "metadata": {"file": "train.py", "lineno": 565, "epoch_num": 1}}
0: Summary: Epoch: 0	Training Loss: 4.7362	Test BLEU: 19.49
0: Performance: Epoch: 0	Training: 3227559 Tok/s
0: Finished epoch 0
:::MLLOG {"namespace": "", "time_ms": 1593112761379, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "train.py", "lineno": 584, "first_epoch_num": 1}}
:::MLLOG {"namespace": "", "time_ms": 1593112761379, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "train.py", "lineno": 538, "first_epoch_num": 2, "epoch_count": 1}}
:::MLLOG {"namespace": "", "time_ms": 1593112761379, "event_type": "INTERVAL_START", "key": "epoch_start", "value": null, "metadata": {"file": "train.py", "lineno": 541, "epoch_num": 2}}
0: Starting epoch 1
0: Sampler for epoch 1 uses seed 1447839006
0: TRAIN [1][0/1291]	Time 0.317 (0.317)	Data 1.70e-01 (1.70e-01)	Tok/s 56734 (56734)	Loss/tok 3.6257 (3.6257)	LR 2.875e-03
0: TRAIN [1][10/1291]	Time 0.041 (0.071)	Data 8.63e-05 (1.55e-02)	Tok/s 186756 (183631)	Loss/tok 3.3323 (3.4210)	LR 2.875e-03
0: TRAIN [1][20/1291]	Time 0.041 (0.063)	Data 9.08e-05 (8.19e-03)	Tok/s 184566 (196389)	Loss/tok 3.1820 (3.4279)	LR 2.875e-03
0: Upscaling, new scale: 2048.0
0: TRAIN [1][30/1291]	Time 0.041 (0.059)	Data 1.51e-04 (5.58e-03)	Tok/s 189306 (195858)	Loss/tok 3.1604 (3.4319)	LR 2.875e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 1024.0
0: TRAIN [1][40/1291]	Time 0.059 (0.057)	Data 8.23e-05 (4.24e-03)	Tok/s 212159 (197459)	Loss/tok 3.4663 (3.4483)	LR 2.875e-03
0: TRAIN [1][50/1291]	Time 0.042 (0.056)	Data 8.32e-05 (3.43e-03)	Tok/s 184904 (197185)	Loss/tok 3.2227 (3.4588)	LR 2.875e-03
0: TRAIN [1][60/1291]	Time 0.042 (0.056)	Data 8.42e-05 (2.88e-03)	Tok/s 187320 (199363)	Loss/tok 3.2061 (3.4707)	LR 2.875e-03
0: TRAIN [1][70/1291]	Time 0.058 (0.056)	Data 8.51e-05 (2.49e-03)	Tok/s 218339 (199947)	Loss/tok 3.5793 (3.4811)	LR 2.875e-03
0: TRAIN [1][80/1291]	Time 0.042 (0.056)	Data 8.30e-05 (2.19e-03)	Tok/s 182025 (199493)	Loss/tok 3.2430 (3.4743)	LR 2.875e-03
0: TRAIN [1][90/1291]	Time 0.042 (0.055)	Data 1.39e-04 (1.96e-03)	Tok/s 190262 (200051)	Loss/tok 3.2301 (3.4740)	LR 2.875e-03
0: TRAIN [1][100/1291]	Time 0.076 (0.056)	Data 8.65e-05 (1.78e-03)	Tok/s 229262 (201377)	Loss/tok 3.6873 (3.4865)	LR 2.875e-03
0: TRAIN [1][110/1291]	Time 0.076 (0.056)	Data 1.50e-04 (1.63e-03)	Tok/s 234053 (201540)	Loss/tok 3.5432 (3.4837)	LR 2.875e-03
0: TRAIN [1][120/1291]	Time 0.059 (0.056)	Data 8.34e-05 (1.50e-03)	Tok/s 216474 (201981)	Loss/tok 3.4784 (3.4874)	LR 2.875e-03
0: TRAIN [1][130/1291]	Time 0.041 (0.055)	Data 8.27e-05 (1.39e-03)	Tok/s 189027 (201724)	Loss/tok 3.2436 (3.4830)	LR 2.875e-03
0: TRAIN [1][140/1291]	Time 0.041 (0.055)	Data 8.58e-05 (1.30e-03)	Tok/s 190006 (201952)	Loss/tok 3.1390 (3.4865)	LR 2.875e-03
0: TRAIN [1][150/1291]	Time 0.042 (0.055)	Data 8.15e-05 (1.22e-03)	Tok/s 189811 (201901)	Loss/tok 3.2103 (3.4840)	LR 2.875e-03
0: TRAIN [1][160/1291]	Time 0.025 (0.055)	Data 8.13e-05 (1.15e-03)	Tok/s 160239 (201883)	Loss/tok 2.8195 (3.4874)	LR 2.875e-03
0: Upscaling, new scale: 2048.0
0: TRAIN [1][170/1291]	Time 0.058 (0.055)	Data 1.41e-04 (1.09e-03)	Tok/s 211625 (201941)	Loss/tok 3.6470 (3.4862)	LR 2.875e-03
0: TRAIN [1][180/1291]	Time 0.042 (0.055)	Data 8.77e-05 (1.03e-03)	Tok/s 186200 (202234)	Loss/tok 3.2059 (3.4818)	LR 2.875e-03
0: TRAIN [1][190/1291]	Time 0.058 (0.054)	Data 8.08e-05 (9.85e-04)	Tok/s 212064 (201976)	Loss/tok 3.6450 (3.4793)	LR 2.875e-03
0: TRAIN [1][200/1291]	Time 0.042 (0.054)	Data 8.23e-05 (9.41e-04)	Tok/s 186526 (201275)	Loss/tok 3.3653 (3.4736)	LR 2.875e-03
0: TRAIN [1][210/1291]	Time 0.076 (0.054)	Data 8.13e-05 (9.00e-04)	Tok/s 229713 (201704)	Loss/tok 3.5941 (3.4764)	LR 2.875e-03
0: TRAIN [1][220/1291]	Time 0.041 (0.054)	Data 8.54e-05 (8.64e-04)	Tok/s 187826 (202128)	Loss/tok 3.1007 (3.4763)	LR 2.875e-03
0: TRAIN [1][230/1291]	Time 0.058 (0.054)	Data 8.37e-05 (8.31e-04)	Tok/s 214535 (202019)	Loss/tok 3.5538 (3.4724)	LR 2.875e-03
0: TRAIN [1][240/1291]	Time 0.097 (0.054)	Data 7.92e-05 (8.00e-04)	Tok/s 232634 (202069)	Loss/tok 3.8700 (3.4749)	LR 2.875e-03
0: TRAIN [1][250/1291]	Time 0.025 (0.054)	Data 8.15e-05 (7.72e-04)	Tok/s 158180 (201752)	Loss/tok 2.7325 (3.4722)	LR 2.875e-03
0: TRAIN [1][260/1291]	Time 0.058 (0.054)	Data 1.20e-04 (7.46e-04)	Tok/s 217240 (202122)	Loss/tok 3.4282 (3.4784)	LR 2.875e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 1024.0
0: TRAIN [1][270/1291]	Time 0.058 (0.054)	Data 8.30e-05 (7.22e-04)	Tok/s 219579 (202228)	Loss/tok 3.6243 (3.4820)	LR 2.875e-03
0: TRAIN [1][280/1291]	Time 0.076 (0.054)	Data 8.15e-05 (7.00e-04)	Tok/s 230523 (202201)	Loss/tok 3.7259 (3.4816)	LR 2.875e-03
0: TRAIN [1][290/1291]	Time 0.058 (0.054)	Data 8.15e-05 (6.79e-04)	Tok/s 215205 (201824)	Loss/tok 3.5936 (3.4802)	LR 2.875e-03
0: TRAIN [1][300/1291]	Time 0.076 (0.054)	Data 8.39e-05 (6.60e-04)	Tok/s 228493 (201825)	Loss/tok 3.6566 (3.4807)	LR 2.875e-03
0: TRAIN [1][310/1291]	Time 0.058 (0.054)	Data 8.39e-05 (6.42e-04)	Tok/s 218456 (201873)	Loss/tok 3.4691 (3.4784)	LR 2.875e-03
0: TRAIN [1][320/1291]	Time 0.058 (0.054)	Data 7.84e-05 (6.25e-04)	Tok/s 218036 (201898)	Loss/tok 3.4472 (3.4753)	LR 2.875e-03
0: TRAIN [1][330/1291]	Time 0.024 (0.054)	Data 1.37e-04 (6.09e-04)	Tok/s 164064 (201764)	Loss/tok 2.7577 (3.4772)	LR 2.875e-03
0: TRAIN [1][340/1291]	Time 0.041 (0.053)	Data 1.44e-04 (5.94e-04)	Tok/s 188836 (201670)	Loss/tok 3.2510 (3.4748)	LR 2.875e-03
0: TRAIN [1][350/1291]	Time 0.041 (0.053)	Data 8.49e-05 (5.80e-04)	Tok/s 186329 (201445)	Loss/tok 3.2747 (3.4719)	LR 2.875e-03
0: TRAIN [1][360/1291]	Time 0.041 (0.053)	Data 8.11e-05 (5.67e-04)	Tok/s 190500 (201203)	Loss/tok 3.1800 (3.4688)	LR 2.875e-03
0: TRAIN [1][370/1291]	Time 0.058 (0.053)	Data 9.27e-05 (5.54e-04)	Tok/s 214306 (200990)	Loss/tok 3.5383 (3.4677)	LR 2.875e-03
0: TRAIN [1][380/1291]	Time 0.076 (0.053)	Data 8.11e-05 (5.42e-04)	Tok/s 230320 (200895)	Loss/tok 3.5942 (3.4669)	LR 2.875e-03
0: TRAIN [1][390/1291]	Time 0.041 (0.053)	Data 8.82e-05 (5.30e-04)	Tok/s 181644 (200892)	Loss/tok 3.2384 (3.4674)	LR 2.875e-03
0: Upscaling, new scale: 2048.0
0: TRAIN [1][400/1291]	Time 0.025 (0.053)	Data 8.30e-05 (5.19e-04)	Tok/s 157346 (200802)	Loss/tok 2.7352 (3.4658)	LR 2.875e-03
0: TRAIN [1][410/1291]	Time 0.025 (0.053)	Data 8.61e-05 (5.09e-04)	Tok/s 160327 (200794)	Loss/tok 2.7425 (3.4660)	LR 2.875e-03
0: TRAIN [1][420/1291]	Time 0.096 (0.053)	Data 1.39e-04 (4.99e-04)	Tok/s 232480 (200812)	Loss/tok 3.8288 (3.4658)	LR 2.875e-03
0: TRAIN [1][430/1291]	Time 0.041 (0.053)	Data 1.39e-04 (4.90e-04)	Tok/s 189471 (200851)	Loss/tok 3.2053 (3.4657)	LR 2.875e-03
0: TRAIN [1][440/1291]	Time 0.042 (0.053)	Data 8.34e-05 (4.81e-04)	Tok/s 184846 (200753)	Loss/tok 3.3038 (3.4640)	LR 2.875e-03
0: TRAIN [1][450/1291]	Time 0.042 (0.052)	Data 8.49e-05 (4.72e-04)	Tok/s 184150 (200498)	Loss/tok 3.3390 (3.4628)	LR 2.875e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 1024.0
0: TRAIN [1][460/1291]	Time 0.098 (0.053)	Data 1.70e-04 (4.64e-04)	Tok/s 228130 (200766)	Loss/tok 3.8093 (3.4667)	LR 2.875e-03
0: TRAIN [1][470/1291]	Time 0.041 (0.053)	Data 8.08e-05 (4.56e-04)	Tok/s 188190 (200575)	Loss/tok 3.1964 (3.4647)	LR 2.875e-03
0: TRAIN [1][480/1291]	Time 0.041 (0.053)	Data 8.20e-05 (4.49e-04)	Tok/s 191111 (200740)	Loss/tok 3.2212 (3.4657)	LR 2.875e-03
0: TRAIN [1][490/1291]	Time 0.058 (0.053)	Data 8.70e-05 (4.41e-04)	Tok/s 213481 (200686)	Loss/tok 3.5059 (3.4674)	LR 2.875e-03
0: TRAIN [1][500/1291]	Time 0.041 (0.053)	Data 1.05e-04 (4.34e-04)	Tok/s 190130 (200664)	Loss/tok 3.2263 (3.4658)	LR 2.875e-03
0: TRAIN [1][510/1291]	Time 0.058 (0.053)	Data 8.63e-05 (4.28e-04)	Tok/s 219758 (200672)	Loss/tok 3.5915 (3.4662)	LR 2.875e-03
0: TRAIN [1][520/1291]	Time 0.058 (0.053)	Data 8.20e-05 (4.21e-04)	Tok/s 221780 (200678)	Loss/tok 3.4481 (3.4657)	LR 2.875e-03
0: TRAIN [1][530/1291]	Time 0.041 (0.052)	Data 8.68e-05 (4.15e-04)	Tok/s 184678 (200504)	Loss/tok 3.1146 (3.4635)	LR 2.875e-03
0: TRAIN [1][540/1291]	Time 0.076 (0.053)	Data 8.65e-05 (4.09e-04)	Tok/s 229034 (200687)	Loss/tok 3.6343 (3.4653)	LR 2.875e-03
0: TRAIN [1][550/1291]	Time 0.042 (0.053)	Data 8.11e-05 (4.04e-04)	Tok/s 186748 (200546)	Loss/tok 3.2011 (3.4636)	LR 2.875e-03
0: TRAIN [1][560/1291]	Time 0.058 (0.052)	Data 8.30e-05 (3.98e-04)	Tok/s 211977 (200319)	Loss/tok 3.3852 (3.4613)	LR 2.875e-03
0: TRAIN [1][570/1291]	Time 0.076 (0.052)	Data 8.46e-05 (3.93e-04)	Tok/s 229423 (200472)	Loss/tok 3.5539 (3.4604)	LR 2.875e-03
0: TRAIN [1][580/1291]	Time 0.041 (0.052)	Data 8.39e-05 (3.88e-04)	Tok/s 188510 (200451)	Loss/tok 3.0959 (3.4592)	LR 2.875e-03
0: Upscaling, new scale: 2048.0
0: TRAIN [1][590/1291]	Time 0.076 (0.052)	Data 8.37e-05 (3.83e-04)	Tok/s 227626 (200506)	Loss/tok 3.7207 (3.4590)	LR 2.875e-03
0: TRAIN [1][600/1291]	Time 0.058 (0.052)	Data 1.42e-04 (3.78e-04)	Tok/s 213893 (200641)	Loss/tok 3.5209 (3.4601)	LR 2.875e-03
0: TRAIN [1][610/1291]	Time 0.058 (0.053)	Data 8.39e-05 (3.73e-04)	Tok/s 217820 (200762)	Loss/tok 3.4395 (3.4603)	LR 2.875e-03
0: TRAIN [1][620/1291]	Time 0.059 (0.052)	Data 8.20e-05 (3.69e-04)	Tok/s 215360 (200536)	Loss/tok 3.3798 (3.4581)	LR 2.875e-03
0: TRAIN [1][630/1291]	Time 0.042 (0.052)	Data 8.15e-05 (3.65e-04)	Tok/s 183719 (200599)	Loss/tok 3.1640 (3.4579)	LR 2.875e-03
0: TRAIN [1][640/1291]	Time 0.042 (0.053)	Data 8.15e-05 (3.60e-04)	Tok/s 188831 (200628)	Loss/tok 3.1730 (3.4573)	LR 2.875e-03
0: TRAIN [1][650/1291]	Time 0.096 (0.052)	Data 8.08e-05 (3.56e-04)	Tok/s 234088 (200460)	Loss/tok 3.8372 (3.4564)	LR 2.875e-03
0: TRAIN [1][660/1291]	Time 0.058 (0.052)	Data 8.06e-05 (3.52e-04)	Tok/s 214142 (200429)	Loss/tok 3.4569 (3.4554)	LR 2.875e-03
0: TRAIN [1][670/1291]	Time 0.041 (0.052)	Data 1.36e-04 (3.48e-04)	Tok/s 187704 (200386)	Loss/tok 3.2242 (3.4547)	LR 2.875e-03
0: TRAIN [1][680/1291]	Time 0.058 (0.052)	Data 1.41e-04 (3.45e-04)	Tok/s 214850 (200397)	Loss/tok 3.3169 (3.4535)	LR 2.875e-03
0: TRAIN [1][690/1291]	Time 0.041 (0.052)	Data 8.34e-05 (3.41e-04)	Tok/s 186627 (200434)	Loss/tok 3.3533 (3.4527)	LR 2.875e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 1024.0
0: TRAIN [1][700/1291]	Time 0.058 (0.052)	Data 8.54e-05 (3.38e-04)	Tok/s 214868 (200419)	Loss/tok 3.4225 (3.4520)	LR 2.875e-03
0: TRAIN [1][710/1291]	Time 0.041 (0.052)	Data 1.13e-04 (3.34e-04)	Tok/s 192786 (200401)	Loss/tok 3.3889 (3.4513)	LR 2.875e-03
0: TRAIN [1][720/1291]	Time 0.058 (0.052)	Data 8.11e-05 (3.31e-04)	Tok/s 213512 (200555)	Loss/tok 3.4183 (3.4523)	LR 2.875e-03
0: TRAIN [1][730/1291]	Time 0.042 (0.052)	Data 1.38e-04 (3.28e-04)	Tok/s 186131 (200659)	Loss/tok 3.1420 (3.4516)	LR 2.875e-03
0: TRAIN [1][740/1291]	Time 0.059 (0.052)	Data 8.13e-05 (3.25e-04)	Tok/s 214390 (200607)	Loss/tok 3.5144 (3.4499)	LR 2.875e-03
0: TRAIN [1][750/1291]	Time 0.041 (0.052)	Data 8.51e-05 (3.22e-04)	Tok/s 185285 (200615)	Loss/tok 3.2551 (3.4497)	LR 2.875e-03
0: TRAIN [1][760/1291]	Time 0.041 (0.052)	Data 8.18e-05 (3.19e-04)	Tok/s 184877 (200637)	Loss/tok 3.2792 (3.4490)	LR 2.875e-03
0: TRAIN [1][770/1291]	Time 0.042 (0.052)	Data 8.01e-05 (3.16e-04)	Tok/s 188056 (200786)	Loss/tok 3.1383 (3.4495)	LR 2.875e-03
0: TRAIN [1][780/1291]	Time 0.041 (0.052)	Data 8.37e-05 (3.13e-04)	Tok/s 188383 (200816)	Loss/tok 3.2258 (3.4485)	LR 2.875e-03
0: TRAIN [1][790/1291]	Time 0.058 (0.053)	Data 1.37e-04 (3.10e-04)	Tok/s 217051 (200872)	Loss/tok 3.4029 (3.4499)	LR 2.875e-03
0: TRAIN [1][800/1291]	Time 0.041 (0.052)	Data 1.44e-04 (3.08e-04)	Tok/s 188630 (200784)	Loss/tok 3.3460 (3.4485)	LR 2.875e-03
0: TRAIN [1][810/1291]	Time 0.041 (0.052)	Data 1.40e-04 (3.05e-04)	Tok/s 189633 (200713)	Loss/tok 3.1258 (3.4468)	LR 2.875e-03
0: TRAIN [1][820/1291]	Time 0.041 (0.052)	Data 8.37e-05 (3.03e-04)	Tok/s 188148 (200673)	Loss/tok 3.1483 (3.4453)	LR 2.875e-03
0: Upscaling, new scale: 2048.0
0: TRAIN [1][830/1291]	Time 0.097 (0.052)	Data 8.56e-05 (3.00e-04)	Tok/s 227701 (200831)	Loss/tok 3.7370 (3.4468)	LR 2.875e-03
0: TRAIN [1][840/1291]	Time 0.059 (0.052)	Data 7.94e-05 (2.98e-04)	Tok/s 215607 (200794)	Loss/tok 3.4614 (3.4468)	LR 2.875e-03
0: TRAIN [1][850/1291]	Time 0.041 (0.052)	Data 1.37e-04 (2.95e-04)	Tok/s 188239 (200711)	Loss/tok 3.2528 (3.4459)	LR 2.875e-03
0: TRAIN [1][860/1291]	Time 0.041 (0.052)	Data 8.32e-05 (2.93e-04)	Tok/s 184144 (200736)	Loss/tok 3.1996 (3.4458)	LR 2.875e-03
0: TRAIN [1][870/1291]	Time 0.042 (0.052)	Data 8.82e-05 (2.90e-04)	Tok/s 184475 (200591)	Loss/tok 3.1174 (3.4449)	LR 2.875e-03
0: TRAIN [1][880/1291]	Time 0.042 (0.052)	Data 8.49e-05 (2.88e-04)	Tok/s 188405 (200495)	Loss/tok 3.1854 (3.4433)	LR 2.875e-03
0: TRAIN [1][890/1291]	Time 0.041 (0.052)	Data 9.42e-05 (2.86e-04)	Tok/s 188164 (200616)	Loss/tok 3.0483 (3.4433)	LR 2.875e-03
0: TRAIN [1][900/1291]	Time 0.041 (0.052)	Data 8.68e-05 (2.84e-04)	Tok/s 189247 (200664)	Loss/tok 3.1897 (3.4436)	LR 2.875e-03
0: TRAIN [1][910/1291]	Time 0.076 (0.052)	Data 8.77e-05 (2.82e-04)	Tok/s 229446 (200759)	Loss/tok 3.5552 (3.4435)	LR 2.875e-03
0: TRAIN [1][920/1291]	Time 0.058 (0.052)	Data 9.08e-05 (2.80e-04)	Tok/s 219504 (200815)	Loss/tok 3.2465 (3.4425)	LR 2.875e-03
0: TRAIN [1][930/1291]	Time 0.058 (0.052)	Data 9.13e-05 (2.78e-04)	Tok/s 216607 (200852)	Loss/tok 3.4088 (3.4421)	LR 2.875e-03
0: TRAIN [1][940/1291]	Time 0.058 (0.052)	Data 9.13e-05 (2.75e-04)	Tok/s 214732 (200853)	Loss/tok 3.3630 (3.4412)	LR 2.875e-03
0: TRAIN [1][950/1291]	Time 0.042 (0.052)	Data 8.44e-05 (2.74e-04)	Tok/s 180926 (200862)	Loss/tok 3.1412 (3.4402)	LR 2.875e-03
0: Upscaling, new scale: 4096.0
0: TRAIN [1][960/1291]	Time 0.076 (0.052)	Data 8.61e-05 (2.72e-04)	Tok/s 225733 (200889)	Loss/tok 3.8222 (3.4396)	LR 2.875e-03
0: TRAIN [1][970/1291]	Time 0.041 (0.053)	Data 8.82e-05 (2.70e-04)	Tok/s 183800 (200979)	Loss/tok 3.1104 (3.4394)	LR 2.875e-03
0: TRAIN [1][980/1291]	Time 0.097 (0.053)	Data 8.39e-05 (2.68e-04)	Tok/s 233213 (201189)	Loss/tok 3.6620 (3.4400)	LR 2.875e-03
0: TRAIN [1][990/1291]	Time 0.041 (0.053)	Data 8.63e-05 (2.66e-04)	Tok/s 185407 (201101)	Loss/tok 3.1198 (3.4386)	LR 2.875e-03
0: TRAIN [1][1000/1291]	Time 0.041 (0.053)	Data 9.11e-05 (2.64e-04)	Tok/s 185132 (201049)	Loss/tok 3.1250 (3.4373)	LR 2.875e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 2048.0
0: TRAIN [1][1010/1291]	Time 0.076 (0.053)	Data 8.58e-05 (2.63e-04)	Tok/s 232395 (201120)	Loss/tok 3.6362 (3.4370)	LR 2.875e-03
0: TRAIN [1][1020/1291]	Time 0.058 (0.053)	Data 9.11e-05 (2.61e-04)	Tok/s 220083 (201197)	Loss/tok 3.3595 (3.4375)	LR 2.875e-03
0: TRAIN [1][1030/1291]	Time 0.059 (0.053)	Data 8.63e-05 (2.59e-04)	Tok/s 216299 (201334)	Loss/tok 3.2772 (3.4381)	LR 2.875e-03
0: TRAIN [1][1040/1291]	Time 0.042 (0.053)	Data 9.82e-05 (2.58e-04)	Tok/s 191332 (201369)	Loss/tok 3.1813 (3.4374)	LR 2.875e-03
0: TRAIN [1][1050/1291]	Time 0.041 (0.053)	Data 9.20e-05 (2.56e-04)	Tok/s 186002 (201287)	Loss/tok 3.2043 (3.4363)	LR 2.875e-03
0: TRAIN [1][1060/1291]	Time 0.058 (0.053)	Data 9.08e-05 (2.54e-04)	Tok/s 211418 (201344)	Loss/tok 3.4082 (3.4366)	LR 2.875e-03
0: TRAIN [1][1070/1291]	Time 0.058 (0.053)	Data 8.63e-05 (2.53e-04)	Tok/s 214403 (201370)	Loss/tok 3.4349 (3.4367)	LR 2.875e-03
0: TRAIN [1][1080/1291]	Time 0.041 (0.053)	Data 9.01e-05 (2.51e-04)	Tok/s 190801 (201336)	Loss/tok 3.1564 (3.4357)	LR 2.875e-03
0: TRAIN [1][1090/1291]	Time 0.041 (0.053)	Data 9.01e-05 (2.50e-04)	Tok/s 185646 (201333)	Loss/tok 3.1531 (3.4356)	LR 2.875e-03
0: TRAIN [1][1100/1291]	Time 0.059 (0.053)	Data 8.94e-05 (2.48e-04)	Tok/s 213174 (201320)	Loss/tok 3.3336 (3.4344)	LR 2.875e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 1024.0
0: TRAIN [1][1110/1291]	Time 0.025 (0.053)	Data 8.65e-05 (2.47e-04)	Tok/s 163451 (201284)	Loss/tok 2.7236 (3.4341)	LR 2.875e-03
0: TRAIN [1][1120/1291]	Time 0.041 (0.053)	Data 8.89e-05 (2.46e-04)	Tok/s 184534 (201242)	Loss/tok 3.1828 (3.4330)	LR 2.875e-03
0: TRAIN [1][1130/1291]	Time 0.058 (0.053)	Data 8.82e-05 (2.44e-04)	Tok/s 215923 (201264)	Loss/tok 3.3022 (3.4323)	LR 2.875e-03
0: TRAIN [1][1140/1291]	Time 0.076 (0.053)	Data 8.65e-05 (2.43e-04)	Tok/s 231083 (201348)	Loss/tok 3.5249 (3.4323)	LR 2.875e-03
0: TRAIN [1][1150/1291]	Time 0.041 (0.053)	Data 8.25e-05 (2.41e-04)	Tok/s 188101 (201465)	Loss/tok 3.0691 (3.4325)	LR 2.875e-03
0: TRAIN [1][1160/1291]	Time 0.041 (0.053)	Data 8.82e-05 (2.40e-04)	Tok/s 190266 (201386)	Loss/tok 3.1263 (3.4309)	LR 2.875e-03
0: TRAIN [1][1170/1291]	Time 0.041 (0.053)	Data 9.47e-05 (2.39e-04)	Tok/s 187835 (201340)	Loss/tok 3.1191 (3.4294)	LR 2.875e-03
0: TRAIN [1][1180/1291]	Time 0.041 (0.053)	Data 8.56e-05 (2.38e-04)	Tok/s 189164 (201304)	Loss/tok 3.1788 (3.4288)	LR 2.875e-03
0: TRAIN [1][1190/1291]	Time 0.042 (0.053)	Data 8.20e-05 (2.36e-04)	Tok/s 185619 (201388)	Loss/tok 3.1506 (3.4290)	LR 2.875e-03
0: TRAIN [1][1200/1291]	Time 0.025 (0.053)	Data 8.49e-05 (2.35e-04)	Tok/s 164564 (201409)	Loss/tok 2.5774 (3.4285)	LR 2.875e-03
0: TRAIN [1][1210/1291]	Time 0.041 (0.053)	Data 9.11e-05 (2.34e-04)	Tok/s 190649 (201325)	Loss/tok 3.1267 (3.4275)	LR 2.875e-03
0: TRAIN [1][1220/1291]	Time 0.058 (0.053)	Data 9.13e-05 (2.33e-04)	Tok/s 215370 (201384)	Loss/tok 3.3206 (3.4280)	LR 2.875e-03
0: Upscaling, new scale: 2048.0
0: TRAIN [1][1230/1291]	Time 0.025 (0.053)	Data 8.56e-05 (2.31e-04)	Tok/s 160341 (201453)	Loss/tok 2.7273 (3.4280)	LR 2.875e-03
0: TRAIN [1][1240/1291]	Time 0.042 (0.053)	Data 8.51e-05 (2.30e-04)	Tok/s 185318 (201417)	Loss/tok 3.2699 (3.4268)	LR 2.875e-03
0: TRAIN [1][1250/1291]	Time 0.059 (0.053)	Data 8.70e-05 (2.29e-04)	Tok/s 212949 (201412)	Loss/tok 3.3187 (3.4260)	LR 2.875e-03
0: TRAIN [1][1260/1291]	Time 0.041 (0.053)	Data 8.37e-05 (2.28e-04)	Tok/s 186563 (201417)	Loss/tok 3.1129 (3.4249)	LR 2.875e-03
0: TRAIN [1][1270/1291]	Time 0.058 (0.053)	Data 9.04e-05 (2.27e-04)	Tok/s 216212 (201442)	Loss/tok 3.3863 (3.4248)	LR 2.875e-03
0: TRAIN [1][1280/1291]	Time 0.076 (0.053)	Data 8.39e-05 (2.26e-04)	Tok/s 229139 (201452)	Loss/tok 3.5346 (3.4245)	LR 2.875e-03
0: TRAIN [1][1290/1291]	Time 0.041 (0.053)	Data 4.41e-05 (2.26e-04)	Tok/s 190530 (201379)	Loss/tok 3.1175 (3.4236)	LR 2.875e-03
:::MLLOG {"namespace": "", "time_ms": 1593112829601, "event_type": "INTERVAL_END", "key": "epoch_stop", "value": null, "metadata": {"file": "train.py", "lineno": 551, "epoch_num": 2}}
:::MLLOG {"namespace": "", "time_ms": 1593112829602, "event_type": "INTERVAL_START", "key": "eval_start", "value": null, "metadata": {"file": "train.py", "lineno": 556, "epoch_num": 2}}
0: Running evaluation on test set
0: TEST [1][0/3]	Time 0.365 (0.365)	Decoder iters 139.0 (139.0)	Tok/s 24292 (24292)
0: Running moses detokenizer
0: BLEU(score=21.88912360435989, counts=[35442, 16951, 9335, 5327], totals=[64436, 61433, 58430, 55432], precisions=[55.003414240486684, 27.592661924372894, 15.97638199555023, 9.60997257901573], bp=0.996282301830049, sys_len=64436, ref_len=64676)
0: Finished evaluation on test set
:::MLLOG {"namespace": "", "time_ms": 1593112830879, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.2189, "metadata": {"file": "train.py", "lineno": 562, "epoch_num": 2}}
:::MLLOG {"namespace": "", "time_ms": 1593112830880, "event_type": "INTERVAL_END", "key": "eval_stop", "value": null, "metadata": {"file": "train.py", "lineno": 565, "epoch_num": 2}}
0: Summary: Epoch: 1	Training Loss: 3.4242	Test BLEU: 21.89
0: Performance: Epoch: 1	Training: 3221796 Tok/s
0: Finished epoch 1
:::MLLOG {"namespace": "", "time_ms": 1593112830880, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "train.py", "lineno": 584, "first_epoch_num": 2}}
:::MLLOG {"namespace": "", "time_ms": 1593112830880, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "train.py", "lineno": 538, "first_epoch_num": 3, "epoch_count": 1}}
:::MLLOG {"namespace": "", "time_ms": 1593112830880, "event_type": "INTERVAL_START", "key": "epoch_start", "value": null, "metadata": {"file": "train.py", "lineno": 541, "epoch_num": 3}}
0: Starting epoch 2
0: Sampler for epoch 2 uses seed 3536758313
0: TRAIN [2][0/1291]	Time 0.309 (0.309)	Data 1.74e-01 (1.74e-01)	Tok/s 25017 (25017)	Loss/tok 3.0662 (3.0662)	LR 2.875e-03
0: TRAIN [2][10/1291]	Time 0.041 (0.074)	Data 8.34e-05 (1.59e-02)	Tok/s 188470 (180924)	Loss/tok 3.1820 (3.2429)	LR 2.875e-03
0: TRAIN [2][20/1291]	Time 0.042 (0.066)	Data 8.18e-05 (8.35e-03)	Tok/s 175912 (192858)	Loss/tok 2.9554 (3.2802)	LR 2.875e-03
0: TRAIN [2][30/1291]	Time 0.058 (0.061)	Data 8.68e-05 (5.69e-03)	Tok/s 216702 (194450)	Loss/tok 3.3084 (3.2946)	LR 2.875e-03
0: TRAIN [2][40/1291]	Time 0.042 (0.058)	Data 9.11e-05 (4.32e-03)	Tok/s 182701 (195055)	Loss/tok 3.0995 (3.2845)	LR 2.875e-03
0: TRAIN [2][50/1291]	Time 0.076 (0.060)	Data 8.87e-05 (3.49e-03)	Tok/s 229881 (199136)	Loss/tok 3.3949 (3.3104)	LR 2.875e-03
0: TRAIN [2][60/1291]	Time 0.039 (0.059)	Data 9.16e-05 (2.93e-03)	Tok/s 100643 (198148)	Loss/tok 2.6141 (3.3057)	LR 2.875e-03
0: Upscaling, new scale: 4096.0
0: TRAIN [2][70/1291]	Time 0.058 (0.058)	Data 8.92e-05 (2.53e-03)	Tok/s 214968 (199786)	Loss/tok 3.3261 (3.2969)	LR 2.875e-03
0: TRAIN [2][80/1291]	Time 0.059 (0.058)	Data 8.68e-05 (2.23e-03)	Tok/s 216826 (200800)	Loss/tok 3.2909 (3.2923)	LR 2.875e-03
0: TRAIN [2][90/1291]	Time 0.042 (0.058)	Data 8.58e-05 (1.99e-03)	Tok/s 184474 (201692)	Loss/tok 3.0127 (3.3070)	LR 2.875e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 2048.0
0: TRAIN [2][100/1291]	Time 0.042 (0.058)	Data 8.96e-05 (1.80e-03)	Tok/s 186611 (201751)	Loss/tok 3.1329 (3.3078)	LR 2.875e-03
0: TRAIN [2][110/1291]	Time 0.041 (0.057)	Data 1.04e-04 (1.65e-03)	Tok/s 190978 (201991)	Loss/tok 2.9961 (3.3051)	LR 2.875e-03
0: TRAIN [2][120/1291]	Time 0.059 (0.057)	Data 8.87e-05 (1.52e-03)	Tok/s 211573 (202044)	Loss/tok 3.3383 (3.3040)	LR 2.875e-03
0: TRAIN [2][130/1291]	Time 0.024 (0.056)	Data 9.85e-05 (1.41e-03)	Tok/s 164699 (201804)	Loss/tok 2.7009 (3.2975)	LR 2.875e-03
0: TRAIN [2][140/1291]	Time 0.025 (0.055)	Data 8.08e-05 (1.32e-03)	Tok/s 163322 (200787)	Loss/tok 2.6836 (3.2923)	LR 2.875e-03
0: TRAIN [2][150/1291]	Time 0.042 (0.055)	Data 8.63e-05 (1.24e-03)	Tok/s 183567 (200535)	Loss/tok 3.1219 (3.2831)	LR 2.875e-03
0: TRAIN [2][160/1291]	Time 0.042 (0.055)	Data 8.49e-05 (1.16e-03)	Tok/s 185286 (200761)	Loss/tok 2.9112 (3.2903)	LR 2.875e-03
0: TRAIN [2][170/1291]	Time 0.058 (0.055)	Data 8.39e-05 (1.10e-03)	Tok/s 217142 (200794)	Loss/tok 3.1665 (3.2931)	LR 2.875e-03
0: TRAIN [2][180/1291]	Time 0.042 (0.055)	Data 8.30e-05 (1.05e-03)	Tok/s 185286 (201367)	Loss/tok 3.1336 (3.2968)	LR 2.875e-03
0: TRAIN [2][190/1291]	Time 0.025 (0.055)	Data 8.11e-05 (9.95e-04)	Tok/s 154480 (201191)	Loss/tok 2.4830 (3.2987)	LR 2.875e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 1024.0
0: TRAIN [2][200/1291]	Time 0.041 (0.055)	Data 8.30e-05 (9.50e-04)	Tok/s 188542 (201378)	Loss/tok 3.0525 (3.3000)	LR 2.875e-03
0: TRAIN [2][210/1291]	Time 0.076 (0.055)	Data 8.85e-05 (9.09e-04)	Tok/s 230810 (201541)	Loss/tok 3.2863 (3.2993)	LR 2.875e-03
0: TRAIN [2][220/1291]	Time 0.042 (0.055)	Data 8.80e-05 (8.72e-04)	Tok/s 185932 (201542)	Loss/tok 3.0978 (3.3010)	LR 2.875e-03
0: TRAIN [2][230/1291]	Time 0.058 (0.055)	Data 8.13e-05 (8.38e-04)	Tok/s 216978 (201678)	Loss/tok 3.4231 (3.3005)	LR 2.875e-03
0: TRAIN [2][240/1291]	Time 0.042 (0.055)	Data 8.63e-05 (8.06e-04)	Tok/s 190603 (201551)	Loss/tok 3.1160 (3.2983)	LR 2.875e-03
0: TRAIN [2][250/1291]	Time 0.058 (0.055)	Data 8.34e-05 (7.78e-04)	Tok/s 216226 (201576)	Loss/tok 3.3253 (3.3001)	LR 2.875e-03
0: TRAIN [2][260/1291]	Time 0.041 (0.055)	Data 7.99e-05 (7.51e-04)	Tok/s 191273 (201429)	Loss/tok 3.0836 (3.2982)	LR 2.875e-03
0: TRAIN [2][270/1291]	Time 0.042 (0.054)	Data 8.54e-05 (7.26e-04)	Tok/s 184039 (201003)	Loss/tok 3.2039 (3.2941)	LR 2.875e-03
0: TRAIN [2][280/1291]	Time 0.042 (0.054)	Data 8.51e-05 (7.03e-04)	Tok/s 183009 (201053)	Loss/tok 3.0971 (3.2925)	LR 2.875e-03
0: TRAIN [2][290/1291]	Time 0.024 (0.054)	Data 8.23e-05 (6.82e-04)	Tok/s 162374 (200840)	Loss/tok 2.6082 (3.2892)	LR 2.875e-03
0: TRAIN [2][300/1291]	Time 0.058 (0.054)	Data 8.13e-05 (6.62e-04)	Tok/s 213383 (201181)	Loss/tok 3.3332 (3.2920)	LR 2.875e-03
0: TRAIN [2][310/1291]	Time 0.076 (0.054)	Data 8.06e-05 (6.44e-04)	Tok/s 230788 (201306)	Loss/tok 3.4873 (3.2907)	LR 2.875e-03
0: Upscaling, new scale: 2048.0
0: TRAIN [2][320/1291]	Time 0.025 (0.054)	Data 8.80e-05 (6.26e-04)	Tok/s 155443 (201094)	Loss/tok 2.6561 (3.2877)	LR 2.875e-03
0: TRAIN [2][330/1291]	Time 0.058 (0.054)	Data 8.68e-05 (6.10e-04)	Tok/s 217419 (201155)	Loss/tok 3.4026 (3.2862)	LR 2.875e-03
0: TRAIN [2][340/1291]	Time 0.041 (0.054)	Data 7.92e-05 (5.95e-04)	Tok/s 185103 (200938)	Loss/tok 2.9380 (3.2858)	LR 2.875e-03
0: TRAIN [2][350/1291]	Time 0.058 (0.054)	Data 8.20e-05 (5.80e-04)	Tok/s 217189 (200956)	Loss/tok 3.2979 (3.2866)	LR 2.875e-03
0: TRAIN [2][360/1291]	Time 0.076 (0.054)	Data 8.06e-05 (5.66e-04)	Tok/s 228460 (200974)	Loss/tok 3.3164 (3.2866)	LR 2.875e-03
0: TRAIN [2][370/1291]	Time 0.041 (0.054)	Data 8.49e-05 (5.53e-04)	Tok/s 188358 (201084)	Loss/tok 3.1319 (3.2856)	LR 2.875e-03
0: TRAIN [2][380/1291]	Time 0.041 (0.054)	Data 8.18e-05 (5.41e-04)	Tok/s 191649 (201138)	Loss/tok 3.0704 (3.2860)	LR 2.875e-03
0: TRAIN [2][390/1291]	Time 0.076 (0.054)	Data 8.54e-05 (5.29e-04)	Tok/s 225573 (201134)	Loss/tok 3.6457 (3.2860)	LR 2.875e-03
0: TRAIN [2][400/1291]	Time 0.042 (0.053)	Data 8.13e-05 (5.18e-04)	Tok/s 185836 (201012)	Loss/tok 3.1117 (3.2856)	LR 2.875e-03
0: TRAIN [2][410/1291]	Time 0.058 (0.054)	Data 8.37e-05 (5.08e-04)	Tok/s 219950 (201184)	Loss/tok 3.2460 (3.2861)	LR 2.875e-03
0: TRAIN [2][420/1291]	Time 0.058 (0.054)	Data 8.61e-05 (4.98e-04)	Tok/s 218293 (201339)	Loss/tok 3.2729 (3.2877)	LR 2.875e-03
0: TRAIN [2][430/1291]	Time 0.058 (0.053)	Data 8.08e-05 (4.88e-04)	Tok/s 215620 (200985)	Loss/tok 3.2525 (3.2871)	LR 2.875e-03
0: TRAIN [2][440/1291]	Time 0.042 (0.053)	Data 8.11e-05 (4.79e-04)	Tok/s 183675 (200820)	Loss/tok 2.9092 (3.2853)	LR 2.875e-03
0: Upscaling, new scale: 4096.0
0: TRAIN [2][450/1291]	Time 0.058 (0.053)	Data 8.63e-05 (4.70e-04)	Tok/s 215537 (200886)	Loss/tok 3.3470 (3.2866)	LR 2.875e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 2048.0
0: TRAIN [2][460/1291]	Time 0.024 (0.053)	Data 8.49e-05 (4.62e-04)	Tok/s 158315 (200745)	Loss/tok 2.6646 (3.2864)	LR 2.875e-03
0: TRAIN [2][470/1291]	Time 0.058 (0.053)	Data 8.30e-05 (4.54e-04)	Tok/s 215976 (200660)	Loss/tok 3.3990 (3.2874)	LR 2.875e-03
0: TRAIN [2][480/1291]	Time 0.042 (0.053)	Data 8.08e-05 (4.46e-04)	Tok/s 186782 (200519)	Loss/tok 3.0869 (3.2865)	LR 2.875e-03
0: TRAIN [2][490/1291]	Time 0.041 (0.053)	Data 8.34e-05 (4.39e-04)	Tok/s 192304 (200427)	Loss/tok 3.0966 (3.2847)	LR 2.875e-03
0: TRAIN [2][500/1291]	Time 0.058 (0.053)	Data 8.27e-05 (4.32e-04)	Tok/s 223715 (200238)	Loss/tok 3.2312 (3.2823)	LR 2.875e-03
0: TRAIN [2][510/1291]	Time 0.024 (0.052)	Data 8.11e-05 (4.25e-04)	Tok/s 160176 (200042)	Loss/tok 2.8641 (3.2800)	LR 2.875e-03
0: TRAIN [2][520/1291]	Time 0.076 (0.052)	Data 8.13e-05 (4.18e-04)	Tok/s 232666 (199987)	Loss/tok 3.3840 (3.2793)	LR 2.875e-03
0: TRAIN [2][530/1291]	Time 0.058 (0.053)	Data 8.03e-05 (4.12e-04)	Tok/s 219349 (200178)	Loss/tok 3.2319 (3.2810)	LR 2.875e-03
0: TRAIN [2][540/1291]	Time 0.058 (0.052)	Data 8.44e-05 (4.06e-04)	Tok/s 216062 (200017)	Loss/tok 3.2911 (3.2789)	LR 2.875e-03
0: TRAIN [2][550/1291]	Time 0.042 (0.052)	Data 8.65e-05 (4.00e-04)	Tok/s 184050 (200145)	Loss/tok 3.1181 (3.2809)	LR 2.875e-03
0: TRAIN [2][560/1291]	Time 0.076 (0.052)	Data 8.37e-05 (3.94e-04)	Tok/s 231955 (200005)	Loss/tok 3.3210 (3.2797)	LR 2.875e-03
0: TRAIN [2][570/1291]	Time 0.041 (0.052)	Data 8.99e-05 (3.89e-04)	Tok/s 187061 (199889)	Loss/tok 3.1972 (3.2778)	LR 2.875e-03
0: TRAIN [2][580/1291]	Time 0.076 (0.052)	Data 8.32e-05 (3.84e-04)	Tok/s 226044 (199821)	Loss/tok 3.4865 (3.2774)	LR 2.875e-03
0: Upscaling, new scale: 4096.0
0: Gradient norm: inf
0: Skipped batch, new scale: 2048.0
0: TRAIN [2][590/1291]	Time 0.097 (0.052)	Data 8.77e-05 (3.79e-04)	Tok/s 231978 (199909)	Loss/tok 3.6247 (3.2793)	LR 2.875e-03
0: TRAIN [2][600/1291]	Time 0.042 (0.052)	Data 7.99e-05 (3.74e-04)	Tok/s 183610 (199859)	Loss/tok 2.9732 (3.2783)	LR 2.875e-03
0: TRAIN [2][610/1291]	Time 0.042 (0.052)	Data 8.03e-05 (3.69e-04)	Tok/s 183581 (199876)	Loss/tok 3.0247 (3.2792)	LR 2.875e-03
0: TRAIN [2][620/1291]	Time 0.058 (0.052)	Data 8.15e-05 (3.64e-04)	Tok/s 215063 (200007)	Loss/tok 3.3420 (3.2807)	LR 2.875e-03
0: TRAIN [2][630/1291]	Time 0.041 (0.052)	Data 8.42e-05 (3.60e-04)	Tok/s 187267 (200036)	Loss/tok 3.0885 (3.2802)	LR 2.875e-03
0: TRAIN [2][640/1291]	Time 0.059 (0.052)	Data 8.32e-05 (3.56e-04)	Tok/s 210712 (200105)	Loss/tok 3.3808 (3.2807)	LR 2.875e-03
0: TRAIN [2][650/1291]	Time 0.076 (0.052)	Data 8.65e-05 (3.51e-04)	Tok/s 231822 (200286)	Loss/tok 3.4072 (3.2816)	LR 2.875e-03
0: TRAIN [2][660/1291]	Time 0.059 (0.053)	Data 8.61e-05 (3.47e-04)	Tok/s 213352 (200441)	Loss/tok 3.4250 (3.2832)	LR 2.875e-03
0: TRAIN [2][670/1291]	Time 0.041 (0.052)	Data 8.61e-05 (3.43e-04)	Tok/s 186274 (200354)	Loss/tok 3.1328 (3.2816)	LR 2.875e-03
0: TRAIN [2][680/1291]	Time 0.042 (0.053)	Data 8.32e-05 (3.40e-04)	Tok/s 183450 (200510)	Loss/tok 3.1519 (3.2820)	LR 2.875e-03
0: TRAIN [2][690/1291]	Time 0.059 (0.053)	Data 8.37e-05 (3.36e-04)	Tok/s 211946 (200581)	Loss/tok 3.3049 (3.2819)	LR 2.875e-03
0: TRAIN [2][700/1291]	Time 0.042 (0.053)	Data 8.11e-05 (3.32e-04)	Tok/s 184118 (200586)	Loss/tok 2.9549 (3.2808)	LR 2.875e-03
0: TRAIN [2][710/1291]	Time 0.025 (0.053)	Data 8.25e-05 (3.29e-04)	Tok/s 160621 (200550)	Loss/tok 2.6888 (3.2815)	LR 2.875e-03
0: Upscaling, new scale: 4096.0
0: TRAIN [2][720/1291]	Time 0.041 (0.052)	Data 8.25e-05 (3.25e-04)	Tok/s 184563 (200485)	Loss/tok 3.2047 (3.2808)	LR 2.875e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 2048.0
0: TRAIN [2][730/1291]	Time 0.041 (0.053)	Data 8.18e-05 (3.22e-04)	Tok/s 190263 (200583)	Loss/tok 3.0807 (3.2840)	LR 2.875e-03
0: TRAIN [2][740/1291]	Time 0.058 (0.053)	Data 9.70e-05 (3.19e-04)	Tok/s 217367 (200633)	Loss/tok 3.3079 (3.2843)	LR 2.875e-03
0: TRAIN [2][750/1291]	Time 0.097 (0.053)	Data 8.58e-05 (3.16e-04)	Tok/s 227764 (200566)	Loss/tok 3.6891 (3.2843)	LR 2.875e-03
0: TRAIN [2][760/1291]	Time 0.058 (0.053)	Data 8.42e-05 (3.13e-04)	Tok/s 214122 (200725)	Loss/tok 3.3910 (3.2856)	LR 2.875e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 1024.0
0: TRAIN [2][770/1291]	Time 0.042 (0.053)	Data 8.37e-05 (3.10e-04)	Tok/s 187165 (200623)	Loss/tok 3.0361 (3.2867)	LR 2.875e-03
0: TRAIN [2][780/1291]	Time 0.059 (0.053)	Data 8.92e-05 (3.07e-04)	Tok/s 216589 (200492)	Loss/tok 3.2783 (3.2854)	LR 2.875e-03
0: TRAIN [2][790/1291]	Time 0.043 (0.053)	Data 8.92e-05 (3.04e-04)	Tok/s 183735 (200368)	Loss/tok 3.0576 (3.2845)	LR 2.875e-03
0: TRAIN [2][800/1291]	Time 0.097 (0.053)	Data 8.75e-05 (3.01e-04)	Tok/s 228997 (200507)	Loss/tok 3.6017 (3.2866)	LR 2.875e-03
0: TRAIN [2][810/1291]	Time 0.059 (0.053)	Data 8.68e-05 (2.99e-04)	Tok/s 210945 (200479)	Loss/tok 3.2812 (3.2868)	LR 2.875e-03
0: TRAIN [2][820/1291]	Time 0.059 (0.053)	Data 8.96e-05 (2.96e-04)	Tok/s 210451 (200395)	Loss/tok 3.2983 (3.2859)	LR 2.875e-03
0: TRAIN [2][830/1291]	Time 0.042 (0.053)	Data 8.85e-05 (2.94e-04)	Tok/s 183847 (200258)	Loss/tok 3.0667 (3.2849)	LR 2.875e-03
0: TRAIN [2][840/1291]	Time 0.078 (0.053)	Data 8.54e-05 (2.91e-04)	Tok/s 224145 (200391)	Loss/tok 3.5063 (3.2868)	LR 2.875e-03
0: TRAIN [2][850/1291]	Time 0.043 (0.053)	Data 8.92e-05 (2.89e-04)	Tok/s 180992 (200262)	Loss/tok 2.9593 (3.2858)	LR 2.875e-03
0: TRAIN [2][860/1291]	Time 0.077 (0.053)	Data 9.30e-05 (2.87e-04)	Tok/s 228088 (200366)	Loss/tok 3.5028 (3.2856)	LR 2.875e-03
0: TRAIN [2][870/1291]	Time 0.059 (0.053)	Data 8.54e-05 (2.84e-04)	Tok/s 209624 (200504)	Loss/tok 3.2526 (3.2881)	LR 2.875e-03
0: TRAIN [2][880/1291]	Time 0.042 (0.053)	Data 1.13e-04 (2.82e-04)	Tok/s 186954 (200346)	Loss/tok 2.9577 (3.2874)	LR 2.875e-03
0: TRAIN [2][890/1291]	Time 0.043 (0.053)	Data 8.03e-05 (2.80e-04)	Tok/s 181967 (200245)	Loss/tok 3.0247 (3.2879)	LR 2.875e-03
0: Upscaling, new scale: 2048.0
0: TRAIN [2][900/1291]	Time 0.042 (0.053)	Data 8.68e-05 (2.78e-04)	Tok/s 182345 (200230)	Loss/tok 3.3689 (3.2882)	LR 2.875e-03
0: TRAIN [2][910/1291]	Time 0.078 (0.053)	Data 8.27e-05 (2.76e-04)	Tok/s 228230 (200305)	Loss/tok 3.3450 (3.2879)	LR 2.875e-03
0: TRAIN [2][920/1291]	Time 0.058 (0.053)	Data 8.27e-05 (2.74e-04)	Tok/s 213602 (200390)	Loss/tok 3.2830 (3.2885)	LR 2.875e-03
0: TRAIN [2][930/1291]	Time 0.058 (0.053)	Data 9.56e-05 (2.72e-04)	Tok/s 215635 (200328)	Loss/tok 3.2272 (3.2882)	LR 2.875e-03
0: TRAIN [2][940/1291]	Time 0.041 (0.053)	Data 8.06e-05 (2.69e-04)	Tok/s 191277 (200302)	Loss/tok 2.9500 (3.2868)	LR 2.875e-03
0: TRAIN [2][950/1291]	Time 0.041 (0.053)	Data 8.82e-05 (2.68e-04)	Tok/s 186617 (200318)	Loss/tok 3.1840 (3.2865)	LR 2.875e-03
0: TRAIN [2][960/1291]	Time 0.059 (0.053)	Data 8.63e-05 (2.66e-04)	Tok/s 216119 (200365)	Loss/tok 3.2467 (3.2870)	LR 2.875e-03
0: TRAIN [2][970/1291]	Time 0.041 (0.053)	Data 9.73e-05 (2.64e-04)	Tok/s 187295 (200444)	Loss/tok 3.0335 (3.2867)	LR 2.875e-03
0: TRAIN [2][980/1291]	Time 0.042 (0.053)	Data 8.87e-05 (2.62e-04)	Tok/s 183787 (200484)	Loss/tok 3.1182 (3.2864)	LR 2.875e-03
0: TRAIN [2][990/1291]	Time 0.058 (0.053)	Data 8.80e-05 (2.60e-04)	Tok/s 214796 (200428)	Loss/tok 3.2708 (3.2856)	LR 2.875e-03
0: TRAIN [2][1000/1291]	Time 0.058 (0.053)	Data 8.44e-05 (2.59e-04)	Tok/s 216073 (200381)	Loss/tok 3.2845 (3.2854)	LR 2.875e-03
0: TRAIN [2][1010/1291]	Time 0.041 (0.053)	Data 8.32e-05 (2.57e-04)	Tok/s 187553 (200224)	Loss/tok 3.1992 (3.2838)	LR 2.875e-03
0: TRAIN [2][1020/1291]	Time 0.076 (0.053)	Data 8.49e-05 (2.55e-04)	Tok/s 231869 (200243)	Loss/tok 3.4099 (3.2835)	LR 2.875e-03
0: Upscaling, new scale: 4096.0
0: TRAIN [2][1030/1291]	Time 0.058 (0.053)	Data 8.85e-05 (2.54e-04)	Tok/s 217640 (200309)	Loss/tok 3.1771 (3.2831)	LR 2.875e-03
0: TRAIN [2][1040/1291]	Time 0.058 (0.053)	Data 8.32e-05 (2.52e-04)	Tok/s 212663 (200437)	Loss/tok 3.4104 (3.2828)	LR 2.875e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 2048.0
0: TRAIN [2][1050/1291]	Time 0.025 (0.053)	Data 9.01e-05 (2.50e-04)	Tok/s 163970 (200481)	Loss/tok 2.6156 (3.2828)	LR 2.875e-03
0: TRAIN [2][1060/1291]	Time 0.025 (0.053)	Data 8.27e-05 (2.49e-04)	Tok/s 162167 (200500)	Loss/tok 2.7282 (3.2827)	LR 2.875e-03
0: TRAIN [2][1070/1291]	Time 0.041 (0.053)	Data 8.08e-05 (2.47e-04)	Tok/s 190679 (200475)	Loss/tok 3.0444 (3.2818)	LR 2.875e-03
0: TRAIN [2][1080/1291]	Time 0.042 (0.053)	Data 8.75e-05 (2.46e-04)	Tok/s 182402 (200475)	Loss/tok 3.0459 (3.2816)	LR 2.875e-03
0: TRAIN [2][1090/1291]	Time 0.076 (0.053)	Data 8.75e-05 (2.44e-04)	Tok/s 229067 (200503)	Loss/tok 3.2937 (3.2812)	LR 2.875e-03
0: TRAIN [2][1100/1291]	Time 0.041 (0.053)	Data 8.18e-05 (2.43e-04)	Tok/s 188458 (200428)	Loss/tok 3.0875 (3.2802)	LR 2.875e-03
0: TRAIN [2][1110/1291]	Time 0.059 (0.053)	Data 8.42e-05 (2.41e-04)	Tok/s 212038 (200505)	Loss/tok 3.2811 (3.2807)	LR 2.875e-03
0: TRAIN [2][1120/1291]	Time 0.077 (0.053)	Data 8.39e-05 (2.40e-04)	Tok/s 230118 (200513)	Loss/tok 3.3961 (3.2812)	LR 2.875e-03
0: TRAIN [2][1130/1291]	Time 0.042 (0.053)	Data 8.30e-05 (2.39e-04)	Tok/s 182318 (200450)	Loss/tok 3.1070 (3.2807)	LR 2.875e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 1024.0
0: TRAIN [2][1140/1291]	Time 0.025 (0.053)	Data 9.49e-05 (2.37e-04)	Tok/s 161128 (200420)	Loss/tok 2.7170 (3.2807)	LR 2.875e-03
0: TRAIN [2][1150/1291]	Time 0.058 (0.053)	Data 8.25e-05 (2.36e-04)	Tok/s 217083 (200459)	Loss/tok 3.2493 (3.2809)	LR 2.875e-03
0: TRAIN [2][1160/1291]	Time 0.041 (0.053)	Data 8.34e-05 (2.35e-04)	Tok/s 185726 (200356)	Loss/tok 2.9668 (3.2798)	LR 2.875e-03
0: TRAIN [2][1170/1291]	Time 0.097 (0.053)	Data 8.70e-05 (2.33e-04)	Tok/s 228766 (200474)	Loss/tok 3.6948 (3.2815)	LR 2.875e-03
0: TRAIN [2][1180/1291]	Time 0.058 (0.053)	Data 8.25e-05 (2.32e-04)	Tok/s 216920 (200565)	Loss/tok 3.3546 (3.2823)	LR 2.875e-03
0: TRAIN [2][1190/1291]	Time 0.096 (0.053)	Data 8.23e-05 (2.31e-04)	Tok/s 232585 (200495)	Loss/tok 3.6115 (3.2817)	LR 2.875e-03
0: TRAIN [2][1200/1291]	Time 0.041 (0.053)	Data 8.08e-05 (2.30e-04)	Tok/s 190443 (200497)	Loss/tok 3.0932 (3.2813)	LR 2.875e-03
0: TRAIN [2][1210/1291]	Time 0.042 (0.053)	Data 8.49e-05 (2.28e-04)	Tok/s 183766 (200537)	Loss/tok 3.0656 (3.2817)	LR 2.875e-03
0: TRAIN [2][1220/1291]	Time 0.042 (0.053)	Data 8.58e-05 (2.27e-04)	Tok/s 186213 (200536)	Loss/tok 3.0382 (3.2811)	LR 2.875e-03
0: TRAIN [2][1230/1291]	Time 0.059 (0.053)	Data 8.54e-05 (2.26e-04)	Tok/s 215042 (200628)	Loss/tok 3.1305 (3.2815)	LR 2.875e-03
0: TRAIN [2][1240/1291]	Time 0.097 (0.053)	Data 8.06e-05 (2.25e-04)	Tok/s 229760 (200645)	Loss/tok 3.7441 (3.2830)	LR 2.875e-03
0: TRAIN [2][1250/1291]	Time 0.059 (0.053)	Data 8.34e-05 (2.24e-04)	Tok/s 216740 (200744)	Loss/tok 3.2824 (3.2834)	LR 2.875e-03
0: TRAIN [2][1260/1291]	Time 0.042 (0.053)	Data 8.37e-05 (2.23e-04)	Tok/s 182424 (200816)	Loss/tok 3.0108 (3.2844)	LR 2.875e-03
0: Upscaling, new scale: 2048.0
0: TRAIN [2][1270/1291]	Time 0.041 (0.053)	Data 8.11e-05 (2.21e-04)	Tok/s 192600 (200769)	Loss/tok 2.9367 (3.2833)	LR 2.875e-03
0: TRAIN [2][1280/1291]	Time 0.058 (0.053)	Data 8.37e-05 (2.20e-04)	Tok/s 214475 (200786)	Loss/tok 3.2394 (3.2828)	LR 2.875e-03
0: TRAIN [2][1290/1291]	Time 0.058 (0.053)	Data 4.43e-05 (2.21e-04)	Tok/s 219660 (200764)	Loss/tok 3.2740 (3.2822)	LR 2.875e-03
:::MLLOG {"namespace": "", "time_ms": 1593112899333, "event_type": "INTERVAL_END", "key": "epoch_stop", "value": null, "metadata": {"file": "train.py", "lineno": 551, "epoch_num": 3}}
:::MLLOG {"namespace": "", "time_ms": 1593112899334, "event_type": "INTERVAL_START", "key": "eval_start", "value": null, "metadata": {"file": "train.py", "lineno": 556, "epoch_num": 3}}
0: Running evaluation on test set
0: TEST [2][0/3]	Time 0.383 (0.383)	Decoder iters 149.0 (149.0)	Tok/s 23721 (23721)
0: Running moses detokenizer
0: BLEU(score=22.49788743900352, counts=[36440, 17739, 9906, 5800], totals=[66299, 63296, 60293, 57295], precisions=[54.96312161571064, 28.025467644084934, 16.429767966430596, 10.123047386333885], bp=1.0, sys_len=66299, ref_len=64676)
0: Finished evaluation on test set
:::MLLOG {"namespace": "", "time_ms": 1593112900610, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.225, "metadata": {"file": "train.py", "lineno": 562, "epoch_num": 3}}
:::MLLOG {"namespace": "", "time_ms": 1593112900610, "event_type": "INTERVAL_END", "key": "eval_stop", "value": null, "metadata": {"file": "train.py", "lineno": 565, "epoch_num": 3}}
0: Summary: Epoch: 2	Training Loss: 3.2809	Test BLEU: 22.50
0: Performance: Epoch: 2	Training: 3211900 Tok/s
0: Finished epoch 2
:::MLLOG {"namespace": "", "time_ms": 1593112900610, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "train.py", "lineno": 584, "first_epoch_num": 3}}
:::MLLOG {"namespace": "", "time_ms": 1593112900611, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "train.py", "lineno": 538, "first_epoch_num": 4, "epoch_count": 1}}
:::MLLOG {"namespace": "", "time_ms": 1593112900611, "event_type": "INTERVAL_START", "key": "epoch_start", "value": null, "metadata": {"file": "train.py", "lineno": 541, "epoch_num": 4}}
0: Starting epoch 3
0: Sampler for epoch 3 uses seed 2588737095
0: Gradient norm: inf
0: Skipped batch, new scale: 1024.0
0: TRAIN [3][0/1291]	Time 0.346 (0.346)	Data 1.80e-01 (1.80e-01)	Tok/s 50315 (50315)	Loss/tok 3.2742 (3.2742)	LR 2.875e-03
0: TRAIN [3][10/1291]	Time 0.042 (0.077)	Data 9.87e-05 (1.65e-02)	Tok/s 186427 (183934)	Loss/tok 3.0181 (3.1934)	LR 2.875e-03
0: TRAIN [3][20/1291]	Time 0.041 (0.063)	Data 1.22e-04 (8.68e-03)	Tok/s 187665 (189206)	Loss/tok 3.0048 (3.1327)	LR 2.875e-03
0: TRAIN [3][30/1291]	Time 0.058 (0.060)	Data 1.03e-04 (5.91e-03)	Tok/s 219733 (195676)	Loss/tok 3.1197 (3.1520)	LR 2.875e-03
0: TRAIN [3][40/1291]	Time 0.058 (0.059)	Data 9.23e-05 (4.49e-03)	Tok/s 216270 (197595)	Loss/tok 3.1370 (3.1592)	LR 2.875e-03
0: TRAIN [3][50/1291]	Time 0.041 (0.056)	Data 1.03e-04 (3.63e-03)	Tok/s 186998 (196423)	Loss/tok 2.9612 (3.1539)	LR 2.875e-03
0: TRAIN [3][60/1291]	Time 0.058 (0.055)	Data 1.03e-04 (3.06e-03)	Tok/s 215246 (196843)	Loss/tok 3.1516 (3.1480)	LR 2.875e-03
0: TRAIN [3][70/1291]	Time 0.058 (0.056)	Data 9.13e-05 (2.64e-03)	Tok/s 214133 (198499)	Loss/tok 3.3143 (3.1661)	LR 2.875e-03
0: TRAIN [3][80/1291]	Time 0.042 (0.056)	Data 8.39e-05 (2.32e-03)	Tok/s 182897 (199821)	Loss/tok 2.8835 (3.1737)	LR 2.875e-03
0: TRAIN [3][90/1291]	Time 0.041 (0.055)	Data 9.04e-05 (2.08e-03)	Tok/s 188995 (199094)	Loss/tok 2.9519 (3.1688)	LR 2.875e-03
0: TRAIN [3][100/1291]	Time 0.058 (0.055)	Data 8.06e-05 (1.88e-03)	Tok/s 215684 (200006)	Loss/tok 3.2221 (3.1741)	LR 2.875e-03
0: TRAIN [3][110/1291]	Time 0.076 (0.055)	Data 8.39e-05 (1.72e-03)	Tok/s 230077 (200404)	Loss/tok 3.3585 (3.1781)	LR 2.875e-03
0: TRAIN [3][120/1291]	Time 0.042 (0.055)	Data 8.51e-05 (1.58e-03)	Tok/s 180975 (200408)	Loss/tok 3.0749 (3.1817)	LR 2.875e-03
0: Upscaling, new scale: 2048.0
0: TRAIN [3][130/1291]	Time 0.058 (0.056)	Data 8.39e-05 (1.47e-03)	Tok/s 212975 (201219)	Loss/tok 3.1686 (3.1884)	LR 2.875e-03
0: TRAIN [3][140/1291]	Time 0.041 (0.056)	Data 8.54e-05 (1.37e-03)	Tok/s 188555 (201436)	Loss/tok 2.9817 (3.1870)	LR 2.875e-03
0: TRAIN [3][150/1291]	Time 0.058 (0.055)	Data 8.08e-05 (1.29e-03)	Tok/s 215119 (201649)	Loss/tok 3.2249 (3.1835)	LR 2.875e-03
0: TRAIN [3][160/1291]	Time 0.076 (0.055)	Data 7.99e-05 (1.21e-03)	Tok/s 234939 (201936)	Loss/tok 3.3353 (3.1852)	LR 2.875e-03
0: TRAIN [3][170/1291]	Time 0.058 (0.055)	Data 8.44e-05 (1.14e-03)	Tok/s 219592 (201774)	Loss/tok 3.2032 (3.1850)	LR 2.875e-03
0: TRAIN [3][180/1291]	Time 0.042 (0.055)	Data 8.15e-05 (1.09e-03)	Tok/s 185952 (201265)	Loss/tok 2.9973 (3.1850)	LR 2.875e-03
0: TRAIN [3][190/1291]	Time 0.041 (0.055)	Data 8.01e-05 (1.03e-03)	Tok/s 188332 (201556)	Loss/tok 2.9424 (3.1920)	LR 2.875e-03
0: TRAIN [3][200/1291]	Time 0.077 (0.055)	Data 8.63e-05 (9.86e-04)	Tok/s 229560 (201337)	Loss/tok 3.4412 (3.1895)	LR 2.875e-03
0: TRAIN [3][210/1291]	Time 0.041 (0.054)	Data 7.87e-05 (9.43e-04)	Tok/s 184085 (200831)	Loss/tok 2.9413 (3.1868)	LR 1.437e-03
0: TRAIN [3][220/1291]	Time 0.025 (0.054)	Data 8.54e-05 (9.04e-04)	Tok/s 159516 (200425)	Loss/tok 2.7100 (3.1850)	LR 1.437e-03
0: TRAIN [3][230/1291]	Time 0.041 (0.053)	Data 8.25e-05 (8.68e-04)	Tok/s 184038 (200122)	Loss/tok 2.9841 (3.1843)	LR 1.437e-03
0: TRAIN [3][240/1291]	Time 0.025 (0.054)	Data 7.84e-05 (8.36e-04)	Tok/s 161639 (200220)	Loss/tok 2.5933 (3.1867)	LR 1.437e-03
0: TRAIN [3][250/1291]	Time 0.077 (0.054)	Data 7.99e-05 (8.06e-04)	Tok/s 228281 (200652)	Loss/tok 3.2182 (3.1872)	LR 1.437e-03
0: Upscaling, new scale: 4096.0
0: TRAIN [3][260/1291]	Time 0.025 (0.054)	Data 8.08e-05 (7.78e-04)	Tok/s 159141 (201111)	Loss/tok 2.4505 (3.1870)	LR 1.437e-03
0: TRAIN [3][270/1291]	Time 0.042 (0.054)	Data 7.99e-05 (7.52e-04)	Tok/s 181003 (201418)	Loss/tok 3.0054 (3.1927)	LR 1.437e-03
0: TRAIN [3][280/1291]	Time 0.042 (0.055)	Data 7.84e-05 (7.28e-04)	Tok/s 182539 (201487)	Loss/tok 2.9684 (3.1957)	LR 1.437e-03
0: TRAIN [3][290/1291]	Time 0.041 (0.054)	Data 8.49e-05 (7.06e-04)	Tok/s 187101 (201062)	Loss/tok 2.9443 (3.1928)	LR 1.437e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 2048.0
0: TRAIN [3][300/1291]	Time 0.078 (0.054)	Data 8.34e-05 (6.85e-04)	Tok/s 224783 (201023)	Loss/tok 3.2884 (3.1902)	LR 1.437e-03
0: TRAIN [3][310/1291]	Time 0.024 (0.054)	Data 8.13e-05 (6.66e-04)	Tok/s 158127 (200601)	Loss/tok 2.5827 (3.1863)	LR 1.437e-03
0: TRAIN [3][320/1291]	Time 0.058 (0.054)	Data 8.25e-05 (6.48e-04)	Tok/s 217767 (200538)	Loss/tok 3.2477 (3.1862)	LR 1.437e-03
0: TRAIN [3][330/1291]	Time 0.042 (0.054)	Data 8.15e-05 (6.31e-04)	Tok/s 187822 (200369)	Loss/tok 2.9276 (3.1862)	LR 1.437e-03
0: TRAIN [3][340/1291]	Time 0.076 (0.054)	Data 8.06e-05 (6.15e-04)	Tok/s 231874 (200458)	Loss/tok 3.2111 (3.1853)	LR 1.437e-03
0: TRAIN [3][350/1291]	Time 0.041 (0.054)	Data 7.77e-05 (6.00e-04)	Tok/s 184802 (200584)	Loss/tok 2.9466 (3.1842)	LR 1.437e-03
0: TRAIN [3][360/1291]	Time 0.058 (0.054)	Data 7.80e-05 (5.85e-04)	Tok/s 214729 (200730)	Loss/tok 3.2557 (3.1836)	LR 1.437e-03
0: TRAIN [3][370/1291]	Time 0.058 (0.053)	Data 8.15e-05 (5.71e-04)	Tok/s 215833 (200636)	Loss/tok 3.1523 (3.1820)	LR 1.437e-03
0: TRAIN [3][380/1291]	Time 0.042 (0.053)	Data 7.89e-05 (5.59e-04)	Tok/s 191437 (200500)	Loss/tok 2.8849 (3.1798)	LR 1.437e-03
0: TRAIN [3][390/1291]	Time 0.042 (0.053)	Data 7.84e-05 (5.46e-04)	Tok/s 183302 (200416)	Loss/tok 3.0034 (3.1775)	LR 1.437e-03
0: TRAIN [3][400/1291]	Time 0.041 (0.053)	Data 7.80e-05 (5.35e-04)	Tok/s 189516 (200311)	Loss/tok 3.0250 (3.1754)	LR 1.437e-03
0: TRAIN [3][410/1291]	Time 0.042 (0.053)	Data 8.37e-05 (5.24e-04)	Tok/s 184847 (200190)	Loss/tok 2.9424 (3.1723)	LR 1.437e-03
0: TRAIN [3][420/1291]	Time 0.058 (0.053)	Data 8.13e-05 (5.13e-04)	Tok/s 215318 (200175)	Loss/tok 3.2016 (3.1714)	LR 1.437e-03
0: Upscaling, new scale: 4096.0
0: TRAIN [3][430/1291]	Time 0.042 (0.053)	Data 7.94e-05 (5.03e-04)	Tok/s 185734 (200282)	Loss/tok 2.9508 (3.1703)	LR 1.437e-03
0: TRAIN [3][440/1291]	Time 0.041 (0.053)	Data 7.77e-05 (4.94e-04)	Tok/s 187217 (200284)	Loss/tok 2.9958 (3.1694)	LR 1.437e-03
0: TRAIN [3][450/1291]	Time 0.042 (0.053)	Data 8.23e-05 (4.84e-04)	Tok/s 184407 (200413)	Loss/tok 2.9673 (3.1727)	LR 1.437e-03
0: TRAIN [3][460/1291]	Time 0.097 (0.053)	Data 8.25e-05 (4.76e-04)	Tok/s 225174 (200670)	Loss/tok 3.5550 (3.1745)	LR 1.437e-03
0: TRAIN [3][470/1291]	Time 0.041 (0.053)	Data 9.32e-05 (4.68e-04)	Tok/s 190579 (200693)	Loss/tok 2.9526 (3.1735)	LR 1.437e-03
0: TRAIN [3][480/1291]	Time 0.041 (0.053)	Data 8.85e-05 (4.60e-04)	Tok/s 192759 (200558)	Loss/tok 2.9913 (3.1727)	LR 1.437e-03
0: TRAIN [3][490/1291]	Time 0.041 (0.053)	Data 9.70e-05 (4.53e-04)	Tok/s 185863 (200394)	Loss/tok 2.9865 (3.1711)	LR 1.437e-03
0: TRAIN [3][500/1291]	Time 0.076 (0.053)	Data 9.49e-05 (4.46e-04)	Tok/s 235645 (200417)	Loss/tok 3.1911 (3.1698)	LR 1.437e-03
0: TRAIN [3][510/1291]	Time 0.058 (0.053)	Data 8.49e-05 (4.39e-04)	Tok/s 215522 (200362)	Loss/tok 3.1110 (3.1679)	LR 1.437e-03
0: TRAIN [3][520/1291]	Time 0.058 (0.053)	Data 8.20e-05 (4.32e-04)	Tok/s 215618 (200552)	Loss/tok 3.0646 (3.1680)	LR 1.437e-03
0: TRAIN [3][530/1291]	Time 0.041 (0.053)	Data 8.27e-05 (4.26e-04)	Tok/s 189807 (200543)	Loss/tok 2.9171 (3.1667)	LR 1.437e-03
0: TRAIN [3][540/1291]	Time 0.059 (0.053)	Data 8.15e-05 (4.20e-04)	Tok/s 215762 (200502)	Loss/tok 3.1546 (3.1647)	LR 1.437e-03
0: TRAIN [3][550/1291]	Time 0.076 (0.053)	Data 8.44e-05 (4.14e-04)	Tok/s 227858 (200581)	Loss/tok 3.3335 (3.1676)	LR 1.437e-03
0: Upscaling, new scale: 8192.0
0: TRAIN [3][560/1291]	Time 0.024 (0.053)	Data 8.58e-05 (4.08e-04)	Tok/s 162973 (200438)	Loss/tok 2.5711 (3.1652)	LR 1.437e-03
0: TRAIN [3][570/1291]	Time 0.058 (0.053)	Data 7.96e-05 (4.03e-04)	Tok/s 220882 (200459)	Loss/tok 3.0670 (3.1640)	LR 1.437e-03
0: TRAIN [3][580/1291]	Time 0.042 (0.053)	Data 8.63e-05 (3.98e-04)	Tok/s 181520 (200399)	Loss/tok 2.9790 (3.1638)	LR 1.437e-03
0: TRAIN [3][590/1291]	Time 0.042 (0.053)	Data 7.92e-05 (3.92e-04)	Tok/s 180433 (200299)	Loss/tok 2.9447 (3.1628)	LR 1.437e-03
0: TRAIN [3][600/1291]	Time 0.058 (0.053)	Data 9.49e-05 (3.87e-04)	Tok/s 214657 (200373)	Loss/tok 3.1817 (3.1627)	LR 1.437e-03
0: TRAIN [3][610/1291]	Time 0.041 (0.053)	Data 8.23e-05 (3.83e-04)	Tok/s 185458 (200274)	Loss/tok 2.9035 (3.1606)	LR 1.437e-03
0: TRAIN [3][620/1291]	Time 0.058 (0.053)	Data 8.77e-05 (3.78e-04)	Tok/s 217291 (200221)	Loss/tok 3.1490 (3.1613)	LR 1.437e-03
0: TRAIN [3][630/1291]	Time 0.058 (0.053)	Data 1.10e-04 (3.73e-04)	Tok/s 216347 (200161)	Loss/tok 3.0844 (3.1600)	LR 1.437e-03
0: TRAIN [3][640/1291]	Time 0.025 (0.053)	Data 1.44e-04 (3.69e-04)	Tok/s 158954 (200218)	Loss/tok 2.6035 (3.1589)	LR 1.437e-03
0: TRAIN [3][650/1291]	Time 0.097 (0.053)	Data 1.41e-04 (3.65e-04)	Tok/s 231656 (200286)	Loss/tok 3.3451 (3.1593)	LR 1.437e-03
0: TRAIN [3][660/1291]	Time 0.058 (0.053)	Data 8.23e-05 (3.61e-04)	Tok/s 215904 (200375)	Loss/tok 3.2367 (3.1585)	LR 1.437e-03
0: TRAIN [3][670/1291]	Time 0.042 (0.053)	Data 8.34e-05 (3.57e-04)	Tok/s 185122 (200302)	Loss/tok 2.9402 (3.1573)	LR 1.437e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 4096.0
0: TRAIN [3][680/1291]	Time 0.042 (0.053)	Data 7.99e-05 (3.53e-04)	Tok/s 187171 (200392)	Loss/tok 3.0017 (3.1585)	LR 1.437e-03
0: TRAIN [3][690/1291]	Time 0.024 (0.053)	Data 1.39e-04 (3.50e-04)	Tok/s 168796 (200338)	Loss/tok 2.5730 (3.1576)	LR 1.437e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 2048.0
0: TRAIN [3][700/1291]	Time 0.059 (0.053)	Data 8.61e-05 (3.46e-04)	Tok/s 210840 (200488)	Loss/tok 3.1752 (3.1597)	LR 1.437e-03
0: TRAIN [3][710/1291]	Time 0.041 (0.053)	Data 8.03e-05 (3.42e-04)	Tok/s 185040 (200266)	Loss/tok 2.9230 (3.1583)	LR 1.437e-03
0: TRAIN [3][720/1291]	Time 0.041 (0.052)	Data 8.54e-05 (3.39e-04)	Tok/s 185494 (200143)	Loss/tok 2.8979 (3.1571)	LR 7.187e-04
0: TRAIN [3][730/1291]	Time 0.024 (0.052)	Data 8.51e-05 (3.35e-04)	Tok/s 161540 (200059)	Loss/tok 2.5710 (3.1568)	LR 7.187e-04
0: TRAIN [3][740/1291]	Time 0.058 (0.052)	Data 8.46e-05 (3.32e-04)	Tok/s 214855 (200095)	Loss/tok 3.2006 (3.1563)	LR 7.187e-04
0: TRAIN [3][750/1291]	Time 0.042 (0.052)	Data 8.61e-05 (3.29e-04)	Tok/s 178657 (200048)	Loss/tok 2.9626 (3.1555)	LR 7.187e-04
0: TRAIN [3][760/1291]	Time 0.058 (0.052)	Data 8.58e-05 (3.26e-04)	Tok/s 211991 (200011)	Loss/tok 3.1334 (3.1544)	LR 7.187e-04
0: TRAIN [3][770/1291]	Time 0.024 (0.052)	Data 8.44e-05 (3.23e-04)	Tok/s 159909 (199977)	Loss/tok 2.4567 (3.1532)	LR 7.187e-04
0: TRAIN [3][780/1291]	Time 0.025 (0.052)	Data 1.05e-04 (3.20e-04)	Tok/s 161369 (199973)	Loss/tok 2.4959 (3.1528)	LR 7.187e-04
0: TRAIN [3][790/1291]	Time 0.058 (0.052)	Data 7.92e-05 (3.17e-04)	Tok/s 215551 (200072)	Loss/tok 3.0816 (3.1529)	LR 7.187e-04
0: TRAIN [3][800/1291]	Time 0.058 (0.052)	Data 8.61e-05 (3.15e-04)	Tok/s 221927 (200174)	Loss/tok 3.1149 (3.1534)	LR 7.187e-04
0: TRAIN [3][810/1291]	Time 0.041 (0.053)	Data 9.47e-05 (3.12e-04)	Tok/s 184942 (200339)	Loss/tok 2.9421 (3.1542)	LR 7.187e-04
0: TRAIN [3][820/1291]	Time 0.058 (0.053)	Data 8.30e-05 (3.09e-04)	Tok/s 215921 (200424)	Loss/tok 3.1741 (3.1530)	LR 7.187e-04
0: Upscaling, new scale: 4096.0
0: TRAIN [3][830/1291]	Time 0.042 (0.053)	Data 8.39e-05 (3.06e-04)	Tok/s 186295 (200374)	Loss/tok 2.8616 (3.1530)	LR 7.187e-04
0: TRAIN [3][840/1291]	Time 0.025 (0.052)	Data 8.56e-05 (3.04e-04)	Tok/s 163592 (200350)	Loss/tok 2.5179 (3.1525)	LR 7.187e-04
0: TRAIN [3][850/1291]	Time 0.042 (0.053)	Data 1.41e-04 (3.02e-04)	Tok/s 187304 (200398)	Loss/tok 2.9210 (3.1530)	LR 7.187e-04
0: TRAIN [3][860/1291]	Time 0.058 (0.053)	Data 8.30e-05 (2.99e-04)	Tok/s 219435 (200394)	Loss/tok 3.0982 (3.1526)	LR 7.187e-04
0: TRAIN [3][870/1291]	Time 0.041 (0.052)	Data 1.40e-04 (2.97e-04)	Tok/s 181795 (200348)	Loss/tok 3.0309 (3.1519)	LR 7.187e-04
0: TRAIN [3][880/1291]	Time 0.041 (0.052)	Data 8.87e-05 (2.94e-04)	Tok/s 184048 (200239)	Loss/tok 2.8764 (3.1513)	LR 7.187e-04
0: TRAIN [3][890/1291]	Time 0.041 (0.052)	Data 8.46e-05 (2.92e-04)	Tok/s 186736 (200264)	Loss/tok 3.0976 (3.1510)	LR 7.187e-04
0: TRAIN [3][900/1291]	Time 0.042 (0.052)	Data 8.51e-05 (2.90e-04)	Tok/s 185792 (200294)	Loss/tok 2.9301 (3.1507)	LR 7.187e-04
0: TRAIN [3][910/1291]	Time 0.041 (0.052)	Data 1.29e-04 (2.88e-04)	Tok/s 187509 (200167)	Loss/tok 2.9838 (3.1494)	LR 7.187e-04
0: TRAIN [3][920/1291]	Time 0.042 (0.052)	Data 8.75e-05 (2.86e-04)	Tok/s 184544 (200134)	Loss/tok 2.8855 (3.1491)	LR 7.187e-04
0: TRAIN [3][930/1291]	Time 0.077 (0.052)	Data 8.39e-05 (2.84e-04)	Tok/s 228514 (200280)	Loss/tok 3.3751 (3.1502)	LR 7.187e-04
0: TRAIN [3][940/1291]	Time 0.042 (0.052)	Data 8.46e-05 (2.82e-04)	Tok/s 186361 (200367)	Loss/tok 2.8981 (3.1503)	LR 7.187e-04
0: TRAIN [3][950/1291]	Time 0.058 (0.052)	Data 1.18e-04 (2.80e-04)	Tok/s 219449 (200441)	Loss/tok 3.1952 (3.1501)	LR 7.187e-04
0: Upscaling, new scale: 8192.0
0: Gradient norm: inf
0: Skipped batch, new scale: 4096.0
0: TRAIN [3][960/1291]	Time 0.041 (0.052)	Data 8.37e-05 (2.78e-04)	Tok/s 186540 (200321)	Loss/tok 2.9882 (3.1490)	LR 7.187e-04
0: TRAIN [3][970/1291]	Time 0.041 (0.052)	Data 8.42e-05 (2.76e-04)	Tok/s 190411 (200407)	Loss/tok 2.8595 (3.1486)	LR 7.187e-04
0: TRAIN [3][980/1291]	Time 0.041 (0.052)	Data 8.87e-05 (2.74e-04)	Tok/s 186702 (200428)	Loss/tok 2.8683 (3.1487)	LR 7.187e-04
0: TRAIN [3][990/1291]	Time 0.076 (0.053)	Data 1.41e-04 (2.72e-04)	Tok/s 227790 (200553)	Loss/tok 3.3405 (3.1485)	LR 7.187e-04
0: TRAIN [3][1000/1291]	Time 0.042 (0.053)	Data 8.96e-05 (2.71e-04)	Tok/s 185832 (200669)	Loss/tok 2.7660 (3.1501)	LR 7.187e-04
0: TRAIN [3][1010/1291]	Time 0.041 (0.053)	Data 8.65e-05 (2.69e-04)	Tok/s 188056 (200735)	Loss/tok 3.0810 (3.1505)	LR 7.187e-04
0: TRAIN [3][1020/1291]	Time 0.077 (0.053)	Data 8.42e-05 (2.67e-04)	Tok/s 223605 (200838)	Loss/tok 3.3405 (3.1508)	LR 7.187e-04
0: TRAIN [3][1030/1291]	Time 0.058 (0.053)	Data 8.70e-05 (2.66e-04)	Tok/s 213624 (200912)	Loss/tok 3.2272 (3.1510)	LR 7.187e-04
0: TRAIN [3][1040/1291]	Time 0.077 (0.053)	Data 8.63e-05 (2.64e-04)	Tok/s 227587 (200909)	Loss/tok 3.3655 (3.1509)	LR 7.187e-04
0: TRAIN [3][1050/1291]	Time 0.058 (0.053)	Data 1.17e-04 (2.62e-04)	Tok/s 219324 (200874)	Loss/tok 3.0969 (3.1503)	LR 7.187e-04
0: TRAIN [3][1060/1291]	Time 0.059 (0.053)	Data 8.73e-05 (2.61e-04)	Tok/s 213517 (200870)	Loss/tok 3.1077 (3.1498)	LR 7.187e-04
0: TRAIN [3][1070/1291]	Time 0.025 (0.053)	Data 8.01e-05 (2.59e-04)	Tok/s 158245 (200871)	Loss/tok 2.6506 (3.1498)	LR 7.187e-04
0: TRAIN [3][1080/1291]	Time 0.058 (0.053)	Data 8.73e-05 (2.58e-04)	Tok/s 215804 (200845)	Loss/tok 3.2119 (3.1490)	LR 7.187e-04
0: Upscaling, new scale: 8192.0
0: Gradient norm: inf
0: Skipped batch, new scale: 4096.0
0: TRAIN [3][1090/1291]	Time 0.097 (0.053)	Data 8.85e-05 (2.56e-04)	Tok/s 231583 (200799)	Loss/tok 3.4715 (3.1488)	LR 7.187e-04
0: TRAIN [3][1100/1291]	Time 0.042 (0.053)	Data 1.39e-04 (2.55e-04)	Tok/s 186574 (200693)	Loss/tok 2.9095 (3.1477)	LR 7.187e-04
0: TRAIN [3][1110/1291]	Time 0.042 (0.053)	Data 8.68e-05 (2.54e-04)	Tok/s 190196 (200745)	Loss/tok 2.9056 (3.1485)	LR 7.187e-04
0: TRAIN [3][1120/1291]	Time 0.041 (0.053)	Data 8.80e-05 (2.52e-04)	Tok/s 187175 (200722)	Loss/tok 2.9564 (3.1485)	LR 7.187e-04
0: TRAIN [3][1130/1291]	Time 0.058 (0.053)	Data 8.73e-05 (2.51e-04)	Tok/s 218368 (200728)	Loss/tok 3.0572 (3.1487)	LR 7.187e-04
0: TRAIN [3][1140/1291]	Time 0.059 (0.053)	Data 1.60e-04 (2.49e-04)	Tok/s 219245 (200847)	Loss/tok 3.0891 (3.1493)	LR 7.187e-04
0: TRAIN [3][1150/1291]	Time 0.024 (0.053)	Data 1.47e-04 (2.48e-04)	Tok/s 163057 (200803)	Loss/tok 2.5144 (3.1482)	LR 7.187e-04
0: TRAIN [3][1160/1291]	Time 0.058 (0.053)	Data 9.63e-05 (2.47e-04)	Tok/s 216501 (200903)	Loss/tok 2.9821 (3.1474)	LR 7.187e-04
0: TRAIN [3][1170/1291]	Time 0.041 (0.053)	Data 1.41e-04 (2.46e-04)	Tok/s 186248 (200882)	Loss/tok 2.8745 (3.1470)	LR 7.187e-04
0: TRAIN [3][1180/1291]	Time 0.076 (0.053)	Data 8.51e-05 (2.44e-04)	Tok/s 229132 (200913)	Loss/tok 3.3390 (3.1469)	LR 7.187e-04
0: TRAIN [3][1190/1291]	Time 0.058 (0.053)	Data 8.73e-05 (2.43e-04)	Tok/s 217574 (200924)	Loss/tok 3.0213 (3.1462)	LR 7.187e-04
0: TRAIN [3][1200/1291]	Time 0.097 (0.053)	Data 9.04e-05 (2.42e-04)	Tok/s 231641 (200945)	Loss/tok 3.4311 (3.1463)	LR 7.187e-04
0: TRAIN [3][1210/1291]	Time 0.058 (0.053)	Data 8.61e-05 (2.41e-04)	Tok/s 214035 (200927)	Loss/tok 3.2000 (3.1463)	LR 7.187e-04
0: Upscaling, new scale: 8192.0
0: TRAIN [3][1220/1291]	Time 0.041 (0.053)	Data 8.30e-05 (2.40e-04)	Tok/s 187842 (200970)	Loss/tok 2.9083 (3.1460)	LR 3.594e-04
0: TRAIN [3][1230/1291]	Time 0.059 (0.053)	Data 8.25e-05 (2.38e-04)	Tok/s 212289 (201024)	Loss/tok 3.1562 (3.1462)	LR 3.594e-04
0: TRAIN [3][1240/1291]	Time 0.058 (0.053)	Data 8.58e-05 (2.37e-04)	Tok/s 221609 (201013)	Loss/tok 3.0774 (3.1458)	LR 3.594e-04
0: TRAIN [3][1250/1291]	Time 0.058 (0.053)	Data 8.99e-05 (2.36e-04)	Tok/s 217351 (201121)	Loss/tok 3.1243 (3.1455)	LR 3.594e-04
0: TRAIN [3][1260/1291]	Time 0.058 (0.053)	Data 1.49e-04 (2.35e-04)	Tok/s 218904 (201159)	Loss/tok 2.9283 (3.1452)	LR 3.594e-04
0: TRAIN [3][1270/1291]	Time 0.058 (0.053)	Data 8.80e-05 (2.34e-04)	Tok/s 214649 (201119)	Loss/tok 3.1747 (3.1449)	LR 3.594e-04
0: TRAIN [3][1280/1291]	Time 0.059 (0.053)	Data 8.85e-05 (2.33e-04)	Tok/s 212870 (201114)	Loss/tok 3.2286 (3.1444)	LR 3.594e-04
0: TRAIN [3][1290/1291]	Time 0.041 (0.053)	Data 4.58e-05 (2.33e-04)	Tok/s 189008 (201155)	Loss/tok 2.9298 (3.1441)	LR 3.594e-04
:::MLLOG {"namespace": "", "time_ms": 1593112968955, "event_type": "INTERVAL_END", "key": "epoch_stop", "value": null, "metadata": {"file": "train.py", "lineno": 551, "epoch_num": 4}}
:::MLLOG {"namespace": "", "time_ms": 1593112968955, "event_type": "INTERVAL_START", "key": "eval_start", "value": null, "metadata": {"file": "train.py", "lineno": 556, "epoch_num": 4}}
0: Running evaluation on test set
0: TEST [3][0/3]	Time 0.314 (0.314)	Decoder iters 111.0 (111.0)	Tok/s 28457 (28457)
0: Running moses detokenizer
0: BLEU(score=23.990247176678146, counts=[36965, 18502, 10535, 6263], totals=[65349, 62346, 59344, 56346], precisions=[56.56551745244763, 29.676322458537836, 17.75242653006201, 11.115252191814859], bp=1.0, sys_len=65349, ref_len=64676)
0: Finished evaluation on test set
:::MLLOG {"namespace": "", "time_ms": 1593112970175, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.23989999999999997, "metadata": {"file": "train.py", "lineno": 562, "epoch_num": 4}}
:::MLLOG {"namespace": "", "time_ms": 1593112970176, "event_type": "INTERVAL_END", "key": "eval_stop", "value": null, "metadata": {"file": "train.py", "lineno": 565, "epoch_num": 4}}
0: Summary: Epoch: 3	Training Loss: 3.1479	Test BLEU: 23.99
0: Performance: Epoch: 3	Training: 3217610 Tok/s
0: Finished epoch 3
:::MLLOG {"namespace": "", "time_ms": 1593112970176, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "train.py", "lineno": 584, "first_epoch_num": 4}}
:::MLLOG {"namespace": "", "time_ms": 1593112970176, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "train.py", "lineno": 538, "first_epoch_num": 5, "epoch_count": 1}}
:::MLLOG {"namespace": "", "time_ms": 1593112970176, "event_type": "INTERVAL_START", "key": "epoch_start", "value": null, "metadata": {"file": "train.py", "lineno": 541, "epoch_num": 5}}
0: Starting epoch 4
0: Sampler for epoch 4 uses seed 3575125142
0: TRAIN [4][0/1291]	Time 0.302 (0.302)	Data 1.79e-01 (1.79e-01)	Tok/s 25582 (25582)	Loss/tok 3.0451 (3.0451)	LR 3.594e-04
0: TRAIN [4][10/1291]	Time 0.058 (0.074)	Data 8.92e-05 (1.63e-02)	Tok/s 217334 (187175)	Loss/tok 3.0556 (3.0553)	LR 3.594e-04
0: TRAIN [4][20/1291]	Time 0.041 (0.065)	Data 1.02e-04 (8.61e-03)	Tok/s 189345 (197205)	Loss/tok 2.8054 (3.0559)	LR 3.594e-04
0: TRAIN [4][30/1291]	Time 0.041 (0.059)	Data 1.54e-04 (5.87e-03)	Tok/s 186632 (196859)	Loss/tok 2.8750 (3.0186)	LR 3.594e-04
0: TRAIN [4][40/1291]	Time 0.059 (0.058)	Data 1.53e-04 (4.46e-03)	Tok/s 216972 (199264)	Loss/tok 3.0460 (3.0262)	LR 3.594e-04
0: TRAIN [4][50/1291]	Time 0.042 (0.058)	Data 8.92e-05 (3.61e-03)	Tok/s 178828 (200391)	Loss/tok 2.9856 (3.0391)	LR 3.594e-04
0: Upscaling, new scale: 8192.0
0: TRAIN [4][60/1291]	Time 0.058 (0.057)	Data 8.49e-05 (3.03e-03)	Tok/s 215038 (200444)	Loss/tok 3.0182 (3.0459)	LR 3.594e-04
0: TRAIN [4][70/1291]	Time 0.041 (0.057)	Data 9.18e-05 (2.62e-03)	Tok/s 185590 (201321)	Loss/tok 2.6944 (3.0672)	LR 3.594e-04
0: TRAIN [4][80/1291]	Time 0.041 (0.056)	Data 1.45e-04 (2.31e-03)	Tok/s 187257 (200699)	Loss/tok 2.8713 (3.0619)	LR 3.594e-04
0: TRAIN [4][90/1291]	Time 0.041 (0.056)	Data 8.77e-05 (2.07e-03)	Tok/s 192649 (200550)	Loss/tok 2.9554 (3.0551)	LR 3.594e-04
0: TRAIN [4][100/1291]	Time 0.076 (0.056)	Data 8.42e-05 (1.87e-03)	Tok/s 227474 (201084)	Loss/tok 3.1506 (3.0513)	LR 3.594e-04
0: TRAIN [4][110/1291]	Time 0.058 (0.056)	Data 8.65e-05 (1.71e-03)	Tok/s 216062 (201685)	Loss/tok 3.0284 (3.0571)	LR 3.594e-04
0: TRAIN [4][120/1291]	Time 0.059 (0.056)	Data 8.51e-05 (1.58e-03)	Tok/s 214061 (202364)	Loss/tok 3.0482 (3.0563)	LR 3.594e-04
0: TRAIN [4][130/1291]	Time 0.041 (0.056)	Data 8.68e-05 (1.47e-03)	Tok/s 185271 (201992)	Loss/tok 2.8374 (3.0554)	LR 3.594e-04
0: TRAIN [4][140/1291]	Time 0.025 (0.055)	Data 1.55e-04 (1.37e-03)	Tok/s 158608 (202195)	Loss/tok 2.5305 (3.0509)	LR 3.594e-04
0: TRAIN [4][150/1291]	Time 0.059 (0.055)	Data 8.54e-05 (1.29e-03)	Tok/s 218015 (202379)	Loss/tok 2.9738 (3.0498)	LR 3.594e-04
0: TRAIN [4][160/1291]	Time 0.058 (0.055)	Data 8.39e-05 (1.21e-03)	Tok/s 216813 (202536)	Loss/tok 2.9855 (3.0510)	LR 3.594e-04
0: TRAIN [4][170/1291]	Time 0.058 (0.055)	Data 8.89e-05 (1.15e-03)	Tok/s 214556 (201844)	Loss/tok 3.1419 (3.0492)	LR 3.594e-04
0: TRAIN [4][180/1291]	Time 0.041 (0.054)	Data 8.94e-05 (1.09e-03)	Tok/s 184607 (201782)	Loss/tok 2.8531 (3.0451)	LR 3.594e-04
0: Upscaling, new scale: 8192.0
0: TRAIN [4][190/1291]	Time 0.075 (0.054)	Data 1.12e-04 (1.04e-03)	Tok/s 233199 (201514)	Loss/tok 3.0690 (3.0402)	LR 3.594e-04
0: TRAIN [4][200/1291]	Time 0.058 (0.054)	Data 9.23e-05 (9.90e-04)	Tok/s 219133 (201013)	Loss/tok 3.0986 (3.0347)	LR 3.594e-04
0: TRAIN [4][210/1291]	Time 0.059 (0.053)	Data 1.43e-04 (9.47e-04)	Tok/s 215169 (200814)	Loss/tok 3.1348 (3.0357)	LR 3.594e-04
0: TRAIN [4][220/1291]	Time 0.077 (0.054)	Data 8.56e-05 (9.09e-04)	Tok/s 224716 (201259)	Loss/tok 3.2849 (3.0363)	LR 3.594e-04
0: TRAIN [4][230/1291]	Time 0.058 (0.054)	Data 8.68e-05 (8.73e-04)	Tok/s 214470 (201458)	Loss/tok 3.1193 (3.0355)	LR 3.594e-04
0: TRAIN [4][240/1291]	Time 0.041 (0.053)	Data 1.46e-04 (8.41e-04)	Tok/s 187504 (201246)	Loss/tok 2.8828 (3.0325)	LR 3.594e-04
0: Gradient norm: inf
0: Skipped batch, new scale: 4096.0
0: TRAIN [4][250/1291]	Time 0.042 (0.054)	Data 1.52e-04 (8.12e-04)	Tok/s 192133 (201850)	Loss/tok 2.7582 (3.0391)	LR 3.594e-04
0: TRAIN [4][260/1291]	Time 0.058 (0.054)	Data 8.56e-05 (7.84e-04)	Tok/s 218791 (202067)	Loss/tok 3.0488 (3.0384)	LR 3.594e-04
0: TRAIN [4][270/1291]	Time 0.042 (0.054)	Data 8.56e-05 (7.59e-04)	Tok/s 188461 (202097)	Loss/tok 2.8692 (3.0415)	LR 3.594e-04
0: TRAIN [4][280/1291]	Time 0.025 (0.054)	Data 8.58e-05 (7.36e-04)	Tok/s 157812 (201508)	Loss/tok 2.5429 (3.0381)	LR 3.594e-04
0: TRAIN [4][290/1291]	Time 0.076 (0.054)	Data 8.65e-05 (7.13e-04)	Tok/s 230447 (201600)	Loss/tok 3.1404 (3.0398)	LR 3.594e-04
0: TRAIN [4][300/1291]	Time 0.025 (0.053)	Data 8.25e-05 (6.93e-04)	Tok/s 162159 (201484)	Loss/tok 2.5161 (3.0378)	LR 3.594e-04
0: TRAIN [4][310/1291]	Time 0.042 (0.053)	Data 8.46e-05 (6.73e-04)	Tok/s 188346 (201598)	Loss/tok 2.8284 (3.0371)	LR 3.594e-04
0: TRAIN [4][320/1291]	Time 0.042 (0.053)	Data 8.96e-05 (6.55e-04)	Tok/s 183387 (201279)	Loss/tok 2.8598 (3.0350)	LR 3.594e-04
0: TRAIN [4][330/1291]	Time 0.076 (0.053)	Data 1.22e-04 (6.38e-04)	Tok/s 229432 (201675)	Loss/tok 3.1735 (3.0384)	LR 3.594e-04
0: TRAIN [4][340/1291]	Time 0.024 (0.053)	Data 9.04e-05 (6.23e-04)	Tok/s 160250 (201643)	Loss/tok 2.4320 (3.0379)	LR 3.594e-04
0: TRAIN [4][350/1291]	Time 0.059 (0.053)	Data 8.58e-05 (6.08e-04)	Tok/s 213024 (201278)	Loss/tok 3.0726 (3.0347)	LR 3.594e-04
0: TRAIN [4][360/1291]	Time 0.077 (0.053)	Data 8.61e-05 (5.94e-04)	Tok/s 220828 (201200)	Loss/tok 3.2748 (3.0340)	LR 3.594e-04
0: TRAIN [4][370/1291]	Time 0.059 (0.053)	Data 8.54e-05 (5.80e-04)	Tok/s 209016 (201038)	Loss/tok 3.0782 (3.0342)	LR 3.594e-04
0: Upscaling, new scale: 8192.0
0: TRAIN [4][380/1291]	Time 0.042 (0.053)	Data 8.68e-05 (5.67e-04)	Tok/s 179866 (201021)	Loss/tok 2.8609 (3.0336)	LR 3.594e-04
0: TRAIN [4][390/1291]	Time 0.042 (0.053)	Data 1.41e-04 (5.55e-04)	Tok/s 185575 (201025)	Loss/tok 2.8579 (3.0352)	LR 3.594e-04
0: TRAIN [4][400/1291]	Time 0.042 (0.053)	Data 8.87e-05 (5.44e-04)	Tok/s 185431 (201037)	Loss/tok 2.7725 (3.0370)	LR 3.594e-04
0: TRAIN [4][410/1291]	Time 0.060 (0.054)	Data 8.49e-05 (5.33e-04)	Tok/s 211073 (201057)	Loss/tok 3.0227 (3.0390)	LR 3.594e-04
0: TRAIN [4][420/1291]	Time 0.099 (0.054)	Data 8.89e-05 (5.23e-04)	Tok/s 222562 (201113)	Loss/tok 3.5346 (3.0413)	LR 3.594e-04
0: TRAIN [4][430/1291]	Time 0.058 (0.054)	Data 8.87e-05 (5.13e-04)	Tok/s 218494 (201068)	Loss/tok 2.9675 (3.0414)	LR 3.594e-04
0: TRAIN [4][440/1291]	Time 0.058 (0.053)	Data 1.45e-04 (5.04e-04)	Tok/s 218015 (200943)	Loss/tok 3.0275 (3.0408)	LR 1.797e-04
0: TRAIN [4][450/1291]	Time 0.076 (0.053)	Data 9.18e-05 (4.95e-04)	Tok/s 231714 (200931)	Loss/tok 3.0965 (3.0405)	LR 1.797e-04
0: TRAIN [4][460/1291]	Time 0.059 (0.054)	Data 8.56e-05 (4.86e-04)	Tok/s 217770 (201192)	Loss/tok 3.1086 (3.0424)	LR 1.797e-04
0: TRAIN [4][470/1291]	Time 0.041 (0.054)	Data 1.42e-04 (4.78e-04)	Tok/s 189284 (201195)	Loss/tok 2.7837 (3.0431)	LR 1.797e-04
0: TRAIN [4][480/1291]	Time 0.041 (0.053)	Data 8.39e-05 (4.70e-04)	Tok/s 185002 (201052)	Loss/tok 2.8202 (3.0422)	LR 1.797e-04
0: TRAIN [4][490/1291]	Time 0.024 (0.054)	Data 1.04e-04 (4.62e-04)	Tok/s 165696 (201124)	Loss/tok 2.5623 (3.0442)	LR 1.797e-04
0: TRAIN [4][500/1291]	Time 0.042 (0.053)	Data 8.46e-05 (4.55e-04)	Tok/s 185690 (201110)	Loss/tok 2.7577 (3.0439)	LR 1.797e-04
0: Upscaling, new scale: 8192.0
0: TRAIN [4][510/1291]	Time 0.076 (0.054)	Data 8.77e-05 (4.48e-04)	Tok/s 228187 (201253)	Loss/tok 3.0968 (3.0458)	LR 1.797e-04
0: TRAIN [4][520/1291]	Time 0.042 (0.054)	Data 8.68e-05 (4.41e-04)	Tok/s 183606 (201125)	Loss/tok 2.8732 (3.0446)	LR 1.797e-04
0: TRAIN [4][530/1291]	Time 0.041 (0.053)	Data 8.37e-05 (4.34e-04)	Tok/s 186496 (201139)	Loss/tok 2.8357 (3.0445)	LR 1.797e-04
0: TRAIN [4][540/1291]	Time 0.042 (0.053)	Data 8.68e-05 (4.28e-04)	Tok/s 187900 (201121)	Loss/tok 2.8320 (3.0427)	LR 1.797e-04
0: TRAIN [4][550/1291]	Time 0.025 (0.053)	Data 8.68e-05 (4.22e-04)	Tok/s 158502 (201053)	Loss/tok 2.5649 (3.0427)	LR 1.797e-04
0: TRAIN [4][560/1291]	Time 0.059 (0.053)	Data 8.65e-05 (4.17e-04)	Tok/s 212925 (201078)	Loss/tok 3.0269 (3.0427)	LR 1.797e-04
0: TRAIN [4][570/1291]	Time 0.058 (0.053)	Data 8.73e-05 (4.11e-04)	Tok/s 219136 (201123)	Loss/tok 3.0059 (3.0418)	LR 1.797e-04
0: TRAIN [4][580/1291]	Time 0.042 (0.053)	Data 8.56e-05 (4.05e-04)	Tok/s 179436 (201066)	Loss/tok 2.7934 (3.0409)	LR 1.797e-04
0: TRAIN [4][590/1291]	Time 0.076 (0.053)	Data 8.20e-05 (4.00e-04)	Tok/s 229917 (200988)	Loss/tok 3.2134 (3.0397)	LR 1.797e-04
0: TRAIN [4][600/1291]	Time 0.042 (0.053)	Data 8.73e-05 (3.95e-04)	Tok/s 187630 (200943)	Loss/tok 2.8886 (3.0394)	LR 1.797e-04
0: TRAIN [4][610/1291]	Time 0.058 (0.053)	Data 8.51e-05 (3.91e-04)	Tok/s 217970 (200924)	Loss/tok 3.0686 (3.0387)	LR 1.797e-04
0: TRAIN [4][620/1291]	Time 0.041 (0.053)	Data 8.73e-05 (3.86e-04)	Tok/s 184016 (200895)	Loss/tok 2.8275 (3.0388)	LR 1.797e-04
0: TRAIN [4][630/1291]	Time 0.097 (0.053)	Data 1.47e-04 (3.81e-04)	Tok/s 233214 (200923)	Loss/tok 3.2917 (3.0394)	LR 1.797e-04
0: Upscaling, new scale: 8192.0
0: TRAIN [4][640/1291]	Time 0.076 (0.053)	Data 1.43e-04 (3.77e-04)	Tok/s 231791 (200976)	Loss/tok 3.1455 (3.0409)	LR 1.797e-04
0: TRAIN [4][650/1291]	Time 0.041 (0.053)	Data 8.58e-05 (3.73e-04)	Tok/s 191941 (200869)	Loss/tok 2.7613 (3.0400)	LR 1.797e-04
0: TRAIN [4][660/1291]	Time 0.024 (0.053)	Data 1.01e-04 (3.69e-04)	Tok/s 165193 (200689)	Loss/tok 2.4060 (3.0381)	LR 1.797e-04
0: TRAIN [4][670/1291]	Time 0.041 (0.053)	Data 8.49e-05 (3.65e-04)	Tok/s 191984 (200595)	Loss/tok 2.9568 (3.0366)	LR 1.797e-04
0: TRAIN [4][680/1291]	Time 0.042 (0.052)	Data 8.25e-05 (3.60e-04)	Tok/s 184960 (200472)	Loss/tok 2.8887 (3.0351)	LR 1.797e-04
0: TRAIN [4][690/1291]	Time 0.076 (0.052)	Data 8.49e-05 (3.57e-04)	Tok/s 231915 (200321)	Loss/tok 3.1768 (3.0353)	LR 1.797e-04
0: TRAIN [4][700/1291]	Time 0.076 (0.052)	Data 8.63e-05 (3.53e-04)	Tok/s 226862 (200375)	Loss/tok 3.2793 (3.0352)	LR 1.797e-04
0: TRAIN [4][710/1291]	Time 0.058 (0.052)	Data 8.82e-05 (3.50e-04)	Tok/s 216098 (200374)	Loss/tok 3.0338 (3.0349)	LR 1.797e-04
0: TRAIN [4][720/1291]	Time 0.024 (0.052)	Data 1.03e-04 (3.46e-04)	Tok/s 162906 (200414)	Loss/tok 2.5164 (3.0348)	LR 1.797e-04
0: TRAIN [4][730/1291]	Time 0.076 (0.052)	Data 8.18e-05 (3.43e-04)	Tok/s 227232 (200405)	Loss/tok 3.3339 (3.0350)	LR 1.797e-04
0: TRAIN [4][740/1291]	Time 0.096 (0.053)	Data 1.21e-04 (3.40e-04)	Tok/s 229895 (200461)	Loss/tok 3.4882 (3.0378)	LR 1.797e-04
0: TRAIN [4][750/1291]	Time 0.058 (0.052)	Data 8.51e-05 (3.36e-04)	Tok/s 213219 (200222)	Loss/tok 2.9573 (3.0359)	LR 1.797e-04
0: Upscaling, new scale: 8192.0
0: TRAIN [4][760/1291]	Time 0.097 (0.052)	Data 1.50e-04 (3.33e-04)	Tok/s 230685 (200344)	Loss/tok 3.4659 (3.0374)	LR 1.797e-04
0: TRAIN [4][770/1291]	Time 0.041 (0.052)	Data 8.80e-05 (3.30e-04)	Tok/s 191148 (200370)	Loss/tok 3.1318 (3.0371)	LR 1.797e-04
0: TRAIN [4][780/1291]	Time 0.058 (0.052)	Data 8.11e-05 (3.27e-04)	Tok/s 215982 (200520)	Loss/tok 3.0901 (3.0385)	LR 1.797e-04
0: TRAIN [4][790/1291]	Time 0.025 (0.052)	Data 1.40e-04 (3.24e-04)	Tok/s 158822 (200560)	Loss/tok 2.5177 (3.0380)	LR 1.797e-04
0: TRAIN [4][800/1291]	Time 0.042 (0.053)	Data 7.89e-05 (3.22e-04)	Tok/s 185827 (200694)	Loss/tok 2.8975 (3.0387)	LR 1.797e-04
0: TRAIN [4][810/1291]	Time 0.058 (0.053)	Data 1.30e-04 (3.19e-04)	Tok/s 216627 (200813)	Loss/tok 3.1090 (3.0406)	LR 1.797e-04
0: TRAIN [4][820/1291]	Time 0.076 (0.053)	Data 1.28e-04 (3.16e-04)	Tok/s 226333 (200903)	Loss/tok 3.2488 (3.0433)	LR 1.797e-04
0: TRAIN [4][830/1291]	Time 0.058 (0.053)	Data 1.35e-04 (3.13e-04)	Tok/s 211819 (200959)	Loss/tok 3.0203 (3.0431)	LR 1.797e-04
0: TRAIN [4][840/1291]	Time 0.041 (0.053)	Data 8.51e-05 (3.11e-04)	Tok/s 191468 (201016)	Loss/tok 2.7962 (3.0425)	LR 1.797e-04
0: TRAIN [4][850/1291]	Time 0.041 (0.053)	Data 8.58e-05 (3.08e-04)	Tok/s 190410 (201092)	Loss/tok 2.7674 (3.0430)	LR 1.797e-04
0: TRAIN [4][860/1291]	Time 0.025 (0.053)	Data 8.63e-05 (3.06e-04)	Tok/s 159922 (201006)	Loss/tok 2.5185 (3.0433)	LR 1.797e-04
0: TRAIN [4][870/1291]	Time 0.042 (0.053)	Data 8.96e-05 (3.03e-04)	Tok/s 185737 (201033)	Loss/tok 2.8299 (3.0437)	LR 1.797e-04
0: TRAIN [4][880/1291]	Time 0.042 (0.053)	Data 9.01e-05 (3.01e-04)	Tok/s 182322 (200893)	Loss/tok 2.9554 (3.0426)	LR 1.797e-04
0: Upscaling, new scale: 8192.0
0: TRAIN [4][890/1291]	Time 0.058 (0.053)	Data 1.44e-04 (2.99e-04)	Tok/s 217937 (201068)	Loss/tok 3.1037 (3.0444)	LR 1.797e-04
0: TRAIN [4][900/1291]	Time 0.076 (0.053)	Data 1.28e-04 (2.97e-04)	Tok/s 228837 (201109)	Loss/tok 3.2261 (3.0451)	LR 1.797e-04
0: TRAIN [4][910/1291]	Time 0.041 (0.053)	Data 8.99e-05 (2.94e-04)	Tok/s 193410 (201099)	Loss/tok 2.9354 (3.0441)	LR 1.797e-04
0: TRAIN [4][920/1291]	Time 0.077 (0.053)	Data 8.96e-05 (2.92e-04)	Tok/s 231251 (201080)	Loss/tok 3.1710 (3.0441)	LR 1.797e-04
0: TRAIN [4][930/1291]	Time 0.025 (0.053)	Data 8.75e-05 (2.90e-04)	Tok/s 158678 (201110)	Loss/tok 2.4512 (3.0447)	LR 1.797e-04
0: TRAIN [4][940/1291]	Time 0.077 (0.053)	Data 9.27e-05 (2.88e-04)	Tok/s 231213 (201152)	Loss/tok 3.2411 (3.0449)	LR 1.797e-04
0: TRAIN [4][950/1291]	Time 0.097 (0.053)	Data 9.13e-05 (2.86e-04)	Tok/s 230417 (201130)	Loss/tok 3.2035 (3.0446)	LR 1.797e-04
0: TRAIN [4][960/1291]	Time 0.041 (0.053)	Data 8.27e-05 (2.85e-04)	Tok/s 187554 (201180)	Loss/tok 2.8154 (3.0457)	LR 1.797e-04
0: TRAIN [4][970/1291]	Time 0.059 (0.053)	Data 8.42e-05 (2.82e-04)	Tok/s 214142 (201031)	Loss/tok 3.0511 (3.0445)	LR 1.797e-04
0: TRAIN [4][980/1291]	Time 0.077 (0.053)	Data 9.01e-05 (2.81e-04)	Tok/s 229052 (201036)	Loss/tok 3.0541 (3.0442)	LR 1.797e-04
0: TRAIN [4][990/1291]	Time 0.059 (0.053)	Data 8.54e-05 (2.79e-04)	Tok/s 216547 (200989)	Loss/tok 3.0115 (3.0440)	LR 1.797e-04
0: TRAIN [4][1000/1291]	Time 0.042 (0.053)	Data 9.04e-05 (2.77e-04)	Tok/s 187117 (200932)	Loss/tok 2.8265 (3.0432)	LR 1.797e-04
0: TRAIN [4][1010/1291]	Time 0.042 (0.053)	Data 8.94e-05 (2.75e-04)	Tok/s 181058 (200985)	Loss/tok 2.8360 (3.0435)	LR 1.797e-04
0: Upscaling, new scale: 8192.0
0: TRAIN [4][1020/1291]	Time 0.058 (0.053)	Data 8.42e-05 (2.74e-04)	Tok/s 211315 (200933)	Loss/tok 3.0383 (3.0426)	LR 1.797e-04
0: TRAIN [4][1030/1291]	Time 0.025 (0.053)	Data 9.13e-05 (2.72e-04)	Tok/s 160443 (200965)	Loss/tok 2.4840 (3.0432)	LR 1.797e-04
0: TRAIN [4][1040/1291]	Time 0.058 (0.053)	Data 1.51e-04 (2.70e-04)	Tok/s 214514 (200852)	Loss/tok 2.9695 (3.0420)	LR 1.797e-04
0: TRAIN [4][1050/1291]	Time 0.058 (0.053)	Data 8.82e-05 (2.69e-04)	Tok/s 217478 (200919)	Loss/tok 3.0633 (3.0425)	LR 1.797e-04
0: TRAIN [4][1060/1291]	Time 0.041 (0.053)	Data 1.45e-04 (2.67e-04)	Tok/s 186364 (200957)	Loss/tok 2.7865 (3.0434)	LR 1.797e-04
0: TRAIN [4][1070/1291]	Time 0.077 (0.053)	Data 8.82e-05 (2.65e-04)	Tok/s 227546 (201085)	Loss/tok 3.1659 (3.0438)	LR 1.797e-04
0: TRAIN [4][1080/1291]	Time 0.041 (0.053)	Data 8.94e-05 (2.64e-04)	Tok/s 189854 (201020)	Loss/tok 2.8664 (3.0428)	LR 1.797e-04
0: TRAIN [4][1090/1291]	Time 0.076 (0.053)	Data 1.43e-04 (2.62e-04)	Tok/s 232401 (200923)	Loss/tok 3.1569 (3.0422)	LR 1.797e-04
0: TRAIN [4][1100/1291]	Time 0.042 (0.053)	Data 8.61e-05 (2.61e-04)	Tok/s 187973 (200914)	Loss/tok 2.7134 (3.0423)	LR 1.797e-04
0: TRAIN [4][1110/1291]	Time 0.098 (0.053)	Data 9.04e-05 (2.59e-04)	Tok/s 229504 (200920)	Loss/tok 3.3898 (3.0425)	LR 1.797e-04
0: TRAIN [4][1120/1291]	Time 0.041 (0.053)	Data 8.92e-05 (2.58e-04)	Tok/s 189237 (200840)	Loss/tok 2.9234 (3.0416)	LR 1.797e-04
0: TRAIN [4][1130/1291]	Time 0.076 (0.053)	Data 8.75e-05 (2.57e-04)	Tok/s 232309 (200946)	Loss/tok 3.1429 (3.0417)	LR 1.797e-04
0: TRAIN [4][1140/1291]	Time 0.025 (0.053)	Data 1.54e-04 (2.55e-04)	Tok/s 161618 (200970)	Loss/tok 2.4196 (3.0422)	LR 1.797e-04
0: Upscaling, new scale: 8192.0
0: TRAIN [4][1150/1291]	Time 0.058 (0.053)	Data 8.87e-05 (2.54e-04)	Tok/s 221479 (200923)	Loss/tok 3.1728 (3.0416)	LR 1.797e-04
0: TRAIN [4][1160/1291]	Time 0.076 (0.053)	Data 8.89e-05 (2.53e-04)	Tok/s 235729 (200851)	Loss/tok 3.0106 (3.0407)	LR 1.797e-04
0: TRAIN [4][1170/1291]	Time 0.058 (0.053)	Data 9.54e-05 (2.51e-04)	Tok/s 215196 (200912)	Loss/tok 3.1186 (3.0415)	LR 1.797e-04
0: TRAIN [4][1180/1291]	Time 0.076 (0.053)	Data 9.04e-05 (2.50e-04)	Tok/s 227797 (200889)	Loss/tok 3.2288 (3.0411)	LR 1.797e-04
0: TRAIN [4][1190/1291]	Time 0.041 (0.053)	Data 1.43e-04 (2.49e-04)	Tok/s 188363 (200875)	Loss/tok 2.9044 (3.0408)	LR 1.797e-04
0: TRAIN [4][1200/1291]	Time 0.041 (0.053)	Data 8.70e-05 (2.48e-04)	Tok/s 187988 (200865)	Loss/tok 2.8982 (3.0417)	LR 1.797e-04
0: TRAIN [4][1210/1291]	Time 0.042 (0.053)	Data 8.99e-05 (2.46e-04)	Tok/s 185472 (201007)	Loss/tok 2.8372 (3.0427)	LR 1.797e-04
0: TRAIN [4][1220/1291]	Time 0.058 (0.053)	Data 8.82e-05 (2.45e-04)	Tok/s 218927 (201011)	Loss/tok 3.0210 (3.0421)	LR 1.797e-04
0: TRAIN [4][1230/1291]	Time 0.076 (0.053)	Data 8.39e-05 (2.44e-04)	Tok/s 227417 (201029)	Loss/tok 3.3447 (3.0423)	LR 1.797e-04
0: TRAIN [4][1240/1291]	Time 0.042 (0.053)	Data 8.73e-05 (2.43e-04)	Tok/s 186521 (201067)	Loss/tok 2.8475 (3.0426)	LR 1.797e-04
0: TRAIN [4][1250/1291]	Time 0.042 (0.053)	Data 8.80e-05 (2.42e-04)	Tok/s 186911 (201054)	Loss/tok 2.7282 (3.0423)	LR 1.797e-04
0: TRAIN [4][1260/1291]	Time 0.058 (0.053)	Data 8.77e-05 (2.41e-04)	Tok/s 216871 (201119)	Loss/tok 3.0571 (3.0429)	LR 1.797e-04
0: TRAIN [4][1270/1291]	Time 0.041 (0.053)	Data 8.80e-05 (2.40e-04)	Tok/s 187996 (201185)	Loss/tok 2.8309 (3.0435)	LR 1.797e-04
0: Upscaling, new scale: 8192.0
0: TRAIN [4][1280/1291]	Time 0.097 (0.053)	Data 8.65e-05 (2.39e-04)	Tok/s 227582 (201201)	Loss/tok 3.3887 (3.0444)	LR 1.797e-04
0: TRAIN [4][1290/1291]	Time 0.041 (0.053)	Data 4.41e-05 (2.39e-04)	Tok/s 190101 (201215)	Loss/tok 2.7393 (3.0445)	LR 1.797e-04
:::MLLOG {"namespace": "", "time_ms": 1593113038551, "event_type": "INTERVAL_END", "key": "epoch_stop", "value": null, "metadata": {"file": "train.py", "lineno": 551, "epoch_num": 5}}
:::MLLOG {"namespace": "", "time_ms": 1593113038551, "event_type": "INTERVAL_START", "key": "eval_start", "value": null, "metadata": {"file": "train.py", "lineno": 556, "epoch_num": 5}}
0: Running evaluation on test set
0: TEST [4][0/3]	Time 0.304 (0.304)	Decoder iters 111.0 (111.0)	Tok/s 29239 (29239)
0: Running moses detokenizer
0: BLEU(score=24.27843865763854, counts=[37143, 18702, 10713, 6389], totals=[65417, 62414, 59412, 56413], precisions=[56.778818961432044, 29.964431057134618, 18.03171076550192, 11.32540371900094], bp=1.0, sys_len=65417, ref_len=64676)
0: Target accuracy reached
0: Finished evaluation on test set
:::MLLOG {"namespace": "", "time_ms": 1593113039763, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.24280000000000002, "metadata": {"file": "train.py", "lineno": 562, "epoch_num": 5}}
:::MLLOG {"namespace": "", "time_ms": 1593113039764, "event_type": "INTERVAL_END", "key": "eval_stop", "value": null, "metadata": {"file": "train.py", "lineno": 565, "epoch_num": 5}}
0: Summary: Epoch: 4	Training Loss: 3.0429	Test BLEU: 24.28
0: Performance: Epoch: 4	Training: 3215885 Tok/s
0: Finished epoch 4
:::MLLOG {"namespace": "", "time_ms": 1593113039764, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "train.py", "lineno": 584, "first_epoch_num": 5}}
0: Closing preprocessed data file
:::MLLOG {"namespace": "", "time_ms": 1593113039764, "event_type": "INTERVAL_END", "key": "run_stop", "value": null, "metadata": {"file": "train.py", "lineno": 595, "status": "success"}}
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
ENDING TIMING RUN AT 2020-06-25 12:24:06 PM
RESULT,RNN_TRANSLATOR,,381,nvidia,2020-06-25 12:17:45 PM
ENDING TIMING RUN AT 2020-06-25 12:24:06 PM
RESULT,RNN_TRANSLATOR,,381,nvidia,2020-06-25 12:17:45 PM
ENDING TIMING RUN AT 2020-06-25 12:24:07 PM
RESULT,RNN_TRANSLATOR,,382,nvidia,2020-06-25 12:17:45 PM
ENDING TIMING RUN AT 2020-06-25 12:24:07 PM
RESULT,RNN_TRANSLATOR,,382,nvidia,2020-06-25 12:17:45 PM
ENDING TIMING RUN AT 2020-06-25 12:24:07 PM
RESULT,RNN_TRANSLATOR,,382,nvidia,2020-06-25 12:17:45 PM
ENDING TIMING RUN AT 2020-06-25 12:24:07 PM
RESULT,RNN_TRANSLATOR,,382,nvidia,2020-06-25 12:17:45 PM
ENDING TIMING RUN AT 2020-06-25 12:24:07 PM
RESULT,RNN_TRANSLATOR,,382,nvidia,2020-06-25 12:17:45 PM
ENDING TIMING RUN AT 2020-06-25 12:24:07 PM
RESULT,RNN_TRANSLATOR,,382,nvidia,2020-06-25 12:17:45 PM
ENDING TIMING RUN AT 2020-06-25 12:24:07 PM
RESULT,RNN_TRANSLATOR,,382,nvidia,2020-06-25 12:17:45 PM
ENDING TIMING RUN AT 2020-06-25 12:24:07 PM
ENDING TIMING RUN AT 2020-06-25 12:24:07 PM
ENDING TIMING RUN AT 2020-06-25 12:24:07 PM
RESULT,RNN_TRANSLATOR,,382,nvidia,2020-06-25 12:17:45 PM
RESULT,RNN_TRANSLATOR,,382,nvidia,2020-06-25 12:17:45 PM
RESULT,RNN_TRANSLATOR,,382,nvidia,2020-06-25 12:17:45 PM
ENDING TIMING RUN AT 2020-06-25 12:24:07 PM
RESULT,RNN_TRANSLATOR,,382,nvidia,2020-06-25 12:17:45 PM
ENDING TIMING RUN AT 2020-06-25 12:24:07 PM
RESULT,RNN_TRANSLATOR,,382,nvidia,2020-06-25 12:17:45 PM
ENDING TIMING RUN AT 2020-06-25 12:24:07 PM
RESULT,RNN_TRANSLATOR,,382,nvidia,2020-06-25 12:17:45 PM
ENDING TIMING RUN AT 2020-06-25 12:24:07 PM
RESULT,RNN_TRANSLATOR,,382,nvidia,2020-06-25 12:17:45 PM
