+ echo 'Beginning trial 4 of 5'
Beginning trial 4 of 5
+ srun --ntasks=2 --container-name=rnn_translator python -c '
import mlperf_log_utils
from mlperf_logging.mllog import constants
mlperf_log_utils.mlperf_submission_log(constants.GNMT)'
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
:::MLLOG {"namespace": "", "time_ms": 1593113907465, "event_type": "POINT_IN_TIME", "key": "submission_benchmark", "value": "gnmt", "metadata": {"file": "/workspace/rnn_translator/mlperf_log_utils.py", "lineno": 19}}
:::MLLOG {"namespace": "", "time_ms": 1593113907503, "event_type": "POINT_IN_TIME", "key": "submission_org", "value": "NVIDIA", "metadata": {"file": "/workspace/rnn_translator/mlperf_log_utils.py", "lineno": 24}}
:::MLLOG {"namespace": "", "time_ms": 1593113907503, "event_type": "POINT_IN_TIME", "key": "submission_division", "value": "closed", "metadata": {"file": "/workspace/rnn_translator/mlperf_log_utils.py", "lineno": 28}}
:::MLLOG {"namespace": "", "time_ms": 1593113907503, "event_type": "POINT_IN_TIME", "key": "submission_status", "value": "onprem", "metadata": {"file": "/workspace/rnn_translator/mlperf_log_utils.py", "lineno": 32}}
:::MLLOG {"namespace": "", "time_ms": 1593113907503, "event_type": "POINT_IN_TIME", "key": "submission_platform", "value": "2xNVIDIA DGX A100", "metadata": {"file": "/workspace/rnn_translator/mlperf_log_utils.py", "lineno": 36}}
+ '[' 1 -eq 1 ']'
+ srun --ntasks=2 bash -c 'echo -n '\''Clearing cache on '\'' && hostname && sync && sudo /sbin/sysctl vm.drop_caches=3'
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
Clearing cache on luna-0099
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
Clearing cache on luna-0100
vm.drop_caches = 3
vm.drop_caches = 3
+ srun --ntasks=2 --container-name=rnn_translator python -c '
from mlperf_logging.mllog import constants
from seq2seq.utils import log_event
log_event(key=constants.CACHE_CLEAR, value=True)'
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
:::MLLOG {"namespace": "", "time_ms": 1593113912989, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
:::MLLOG {"namespace": "", "time_ms": 1593113913104, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
+ srun --mpi=none --ntasks=16 --ntasks-per-node=8 --container-name=rnn_translator --container-mounts=/lustre/fsr/datasets/wmt16_de_en:/data,/raid/scratch/svcnvdlfw/gnmt:/preproc_data,/lustre/fsw/mlperf-ci/14251651/results:/results ./run_and_time.sh
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
STARTING TIMING RUN AT 2020-06-25 12:38:35 PM
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ RESULTS_DIR=gnmt_wmt16
+ LR=2.875e-3
+ TRAIN_BATCH_SIZE=192
+ TEST_BATCH_SIZE=64
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ WARMUP_STEPS=200
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ REMAIN_STEPS=4054
+ DECAY_INTERVAL=506
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ TARGET=24.0
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ MAX_SEQ_LEN=75
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ NUMEPOCHS=8
Using TCMalloc
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 3    --dwu-num-chunks 2    --dwu-num-rs-pg 1    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-overlap-reductions    --dwu-grad-norm    '
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    '
+ MATH=fp16
+ declare -a CMD
running benchmark
+ '[' -n 0 ']'
+ '[' 16 -gt 2 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGXA100_multi_2x8x192_dist == *\D\G\X\A\1\0\0* ]]
+ echo 'Using TCMalloc'
+ export LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 192 --test-batch-size 64 --optimizer FusedAdam --lr 2.875e-3 --warmup-steps 200 --remain-steps 4054 --decay-interval 506 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
STARTING TIMING RUN AT 2020-06-25 12:38:35 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=2.875e-3
+ TRAIN_BATCH_SIZE=192
+ TEST_BATCH_SIZE=64
+ WARMUP_STEPS=200
+ REMAIN_STEPS=4054
+ DECAY_INTERVAL=506
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=8
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 3    --dwu-num-chunks 2    --dwu-num-rs-pg 1    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-overlap-reductions    --dwu-grad-norm    '
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 6 ']'
+ '[' 16 -gt 2 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGXA100_multi_2x8x192_dist == *\D\G\X\A\1\0\0* ]]
Using TCMalloc
+ echo 'Using TCMalloc'
+ export LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
running benchmark
+ echo 'running benchmark'
STARTING TIMING RUN AT 2020-06-25 12:38:35 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=2.875e-3
+ TRAIN_BATCH_SIZE=192
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 192 --test-batch-size 64 --optimizer FusedAdam --lr 2.875e-3 --warmup-steps 200 --remain-steps 4054 --decay-interval 506 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
+ TEST_BATCH_SIZE=64
+ WARMUP_STEPS=200
+ REMAIN_STEPS=4054
+ DECAY_INTERVAL=506
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=8
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 3    --dwu-num-chunks 2    --dwu-num-rs-pg 1    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-overlap-reductions    --dwu-grad-norm    '
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 3 ']'
+ '[' 16 -gt 2 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGXA100_multi_2x8x192_dist == *\D\G\X\A\1\0\0* ]]
Using TCMalloc
+ echo 'Using TCMalloc'
+ export LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
running benchmark
+ echo 'running benchmark'
STARTING TIMING RUN AT 2020-06-25 12:38:35 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=2.875e-3
+ TRAIN_BATCH_SIZE=192
+ TEST_BATCH_SIZE=64
+ WARMUP_STEPS=200
+ REMAIN_STEPS=4054
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 192 --test-batch-size 64 --optimizer FusedAdam --lr 2.875e-3 --warmup-steps 200 --remain-steps 4054 --decay-interval 506 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
+ DECAY_INTERVAL=506
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=8
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 3    --dwu-num-chunks 2    --dwu-num-rs-pg 1    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-overlap-reductions    --dwu-grad-norm    '
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 5 ']'
+ '[' 16 -gt 2 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
STARTING TIMING RUN AT 2020-06-25 12:38:35 PM
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGXA100_multi_2x8x192_dist == *\D\G\X\A\1\0\0* ]]
+ echo 'Using TCMalloc'
+ export LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ echo 'running benchmark'
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=2.875e-3
Using TCMalloc
running benchmark
+ TRAIN_BATCH_SIZE=192
+ TEST_BATCH_SIZE=64
+ WARMUP_STEPS=200
+ REMAIN_STEPS=4054
+ DECAY_INTERVAL=506
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=8
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 3    --dwu-num-chunks 2    --dwu-num-rs-pg 1    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-overlap-reductions    --dwu-grad-norm    '
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    '
+ MATH=fp16
+ declare -a CMD
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 192 --test-batch-size 64 --optimizer FusedAdam --lr 2.875e-3 --warmup-steps 200 --remain-steps 4054 --decay-interval 506 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
+ '[' -n 4 ']'
+ '[' 16 -gt 2 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGXA100_multi_2x8x192_dist == *\D\G\X\A\1\0\0* ]]
+ echo 'Using TCMalloc'
Using TCMalloc
+ export LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
running benchmark
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 192 --test-batch-size 64 --optimizer FusedAdam --lr 2.875e-3 --warmup-steps 200 --remain-steps 4054 --decay-interval 506 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
STARTING TIMING RUN AT 2020-06-25 12:38:35 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=2.875e-3
+ TRAIN_BATCH_SIZE=192
+ TEST_BATCH_SIZE=64
+ WARMUP_STEPS=200
+ REMAIN_STEPS=4054
+ DECAY_INTERVAL=506
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=8
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 3    --dwu-num-chunks 2    --dwu-num-rs-pg 1    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-overlap-reductions    --dwu-grad-norm    '
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 2 ']'
+ '[' 16 -gt 2 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
STARTING TIMING RUN AT 2020-06-25 12:38:35 PM
+ [[ DGXA100_multi_2x8x192_dist == *\D\G\X\A\1\0\0* ]]
+ echo 'Using TCMalloc'
Using TCMalloc
+ export LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
running benchmark
+ echo 'running benchmark'
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 192 --test-batch-size 64 --optimizer FusedAdam --lr 2.875e-3 --warmup-steps 200 --remain-steps 4054 --decay-interval 506 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
+ LR=2.875e-3
+ TRAIN_BATCH_SIZE=192
+ TEST_BATCH_SIZE=64
+ WARMUP_STEPS=200
+ REMAIN_STEPS=4054
+ DECAY_INTERVAL=506
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=8
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 3    --dwu-num-chunks 2    --dwu-num-rs-pg 1    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-overlap-reductions    --dwu-grad-norm    '
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 7 ']'
STARTING TIMING RUN AT 2020-06-25 12:38:35 PM
+ '[' 16 -gt 2 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGXA100_multi_2x8x192_dist == *\D\G\X\A\1\0\0* ]]
+ echo 'Using TCMalloc'
+ export LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ LR=2.875e-3
+ TRAIN_BATCH_SIZE=192
+ TEST_BATCH_SIZE=64
+ WARMUP_STEPS=200
Using TCMalloc
running benchmark
+ echo 'running benchmark'
+ REMAIN_STEPS=4054
+ DECAY_INTERVAL=506
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=8
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 3    --dwu-num-chunks 2    --dwu-num-rs-pg 1    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-overlap-reductions    --dwu-grad-norm    '
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    '
+ MATH=fp16
+ declare -a CMD
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 192 --test-batch-size 64 --optimizer FusedAdam --lr 2.875e-3 --warmup-steps 200 --remain-steps 4054 --decay-interval 506 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
+ '[' -n 1 ']'
+ '[' 16 -gt 2 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGXA100_multi_2x8x192_dist == *\D\G\X\A\1\0\0* ]]
+ echo 'Using TCMalloc'
Using TCMalloc
+ export LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ echo 'running benchmark'
running benchmark
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 192 --test-batch-size 64 --optimizer FusedAdam --lr 2.875e-3 --warmup-steps 200 --remain-steps 4054 --decay-interval 506 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
STARTING TIMING RUN AT 2020-06-25 12:38:35 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=2.875e-3
+ TRAIN_BATCH_SIZE=192
+ TEST_BATCH_SIZE=64
+ WARMUP_STEPS=200
+ REMAIN_STEPS=4054
+ DECAY_INTERVAL=506
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=8
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 3    --dwu-num-chunks 2    --dwu-num-rs-pg 1    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-overlap-reductions    --dwu-grad-norm    '
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 5 ']'
+ '[' 16 -gt 2 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
Using TCMalloc
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGXA100_multi_2x8x192_dist == *\D\G\X\A\1\0\0* ]]
+ echo 'Using TCMalloc'
+ export LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
running benchmark
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 192 --test-batch-size 64 --optimizer FusedAdam --lr 2.875e-3 --warmup-steps 200 --remain-steps 4054 --decay-interval 506 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
STARTING TIMING RUN AT 2020-06-25 12:38:35 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
STARTING TIMING RUN AT 2020-06-25 12:38:35 PM
+ RESULTS_DIR=gnmt_wmt16
+ LR=2.875e-3
+ TRAIN_BATCH_SIZE=192
+ DATASET_DIR=/data
+ TEST_BATCH_SIZE=64
+ WARMUP_STEPS=200
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ REMAIN_STEPS=4054
+ LR=2.875e-3
+ DECAY_INTERVAL=506
+ TRAIN_BATCH_SIZE=192
+ TARGET=24.0
+ TEST_BATCH_SIZE=64
+ MAX_SEQ_LEN=75
+ WARMUP_STEPS=200
+ REMAIN_STEPS=4054
+ NUMEPOCHS=8
+ DECAY_INTERVAL=506
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 3    --dwu-num-chunks 2    --dwu-num-rs-pg 1    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-overlap-reductions    --dwu-grad-norm    '
+ TARGET=24.0
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    '
+ MAX_SEQ_LEN=75
+ MATH=fp16
+ NUMEPOCHS=8
+ declare -a CMD
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 3    --dwu-num-chunks 2    --dwu-num-rs-pg 1    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-overlap-reductions    --dwu-grad-norm    '
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 3 ']'
+ '[' -n 1 ']'
+ '[' 16 -gt 2 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' 16 -gt 2 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGXA100_multi_2x8x192_dist == *\D\G\X\A\1\0\0* ]]
+ [[ DGXA100_multi_2x8x192_dist == *\D\G\X\A\1\0\0* ]]
Using TCMalloc
+ echo 'Using TCMalloc'
Using TCMalloc
+ echo 'Using TCMalloc'
+ export LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ export LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ echo 'running benchmark'
running benchmark
+ echo 'running benchmark'
running benchmark
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 192 --test-batch-size 64 --optimizer FusedAdam --lr 2.875e-3 --warmup-steps 200 --remain-steps 4054 --decay-interval 506 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 192 --test-batch-size 64 --optimizer FusedAdam --lr 2.875e-3 --warmup-steps 200 --remain-steps 4054 --decay-interval 506 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
STARTING TIMING RUN AT 2020-06-25 12:38:35 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=2.875e-3
+ TRAIN_BATCH_SIZE=192
+ TEST_BATCH_SIZE=64
+ WARMUP_STEPS=200
+ REMAIN_STEPS=4054
+ DECAY_INTERVAL=506
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=8
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 3    --dwu-num-chunks 2    --dwu-num-rs-pg 1    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-overlap-reductions    --dwu-grad-norm    '
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 2 ']'
STARTING TIMING RUN AT 2020-06-25 12:38:35 PM
+ '[' 16 -gt 2 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
STARTING TIMING RUN AT 2020-06-25 12:38:35 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ [[ DGXA100_multi_2x8x192_dist == *\D\G\X\A\1\0\0* ]]
+ echo 'Using TCMalloc'
+ RESULTS_DIR=gnmt_wmt16
+ LR=2.875e-3
Using TCMalloc
+ export LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ TRAIN_BATCH_SIZE=192
+ TEST_BATCH_SIZE=64
running benchmark
+ echo 'running benchmark'
+ WARMUP_STEPS=200
+ REMAIN_STEPS=4054
+ DATASET_DIR=/data
+ DECAY_INTERVAL=506
+ TARGET=24.0
+ PREPROC_DATADIR=/preproc_data
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=8
+ RESULTS_DIR=gnmt_wmt16
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 3    --dwu-num-chunks 2    --dwu-num-rs-pg 1    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-overlap-reductions    --dwu-grad-norm    '
+ LR=2.875e-3
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    '
+ MATH=fp16
+ declare -a CMD
+ TRAIN_BATCH_SIZE=192
+ TEST_BATCH_SIZE=64
+ WARMUP_STEPS=200
+ REMAIN_STEPS=4054
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 192 --test-batch-size 64 --optimizer FusedAdam --lr 2.875e-3 --warmup-steps 200 --remain-steps 4054 --decay-interval 506 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
+ '[' -n 7 ']'
+ DECAY_INTERVAL=506
+ TARGET=24.0
+ '[' 16 -gt 2 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=8
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 3    --dwu-num-chunks 2    --dwu-num-rs-pg 1    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-overlap-reductions    --dwu-grad-norm    '
+ [[ DGXA100_multi_2x8x192_dist == *\D\G\X\A\1\0\0* ]]
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    '
+ MATH=fp16
Using TCMalloc
running benchmark
+ echo 'Using TCMalloc'
+ export LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ echo 'running benchmark'
+ declare -a CMD
+ '[' -n 6 ']'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 192 --test-batch-size 64 --optimizer FusedAdam --lr 2.875e-3 --warmup-steps 200 --remain-steps 4054 --decay-interval 506 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
+ '[' 16 -gt 2 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
STARTING TIMING RUN AT 2020-06-25 12:38:35 PM
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGXA100_multi_2x8x192_dist == *\D\G\X\A\1\0\0* ]]
Using TCMalloc
+ echo 'Using TCMalloc'
+ export LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
running benchmark
+ echo 'running benchmark'
+ LR=2.875e-3
+ TRAIN_BATCH_SIZE=192
+ TEST_BATCH_SIZE=64
+ WARMUP_STEPS=200
+ REMAIN_STEPS=4054
+ DECAY_INTERVAL=506
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=8
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 3    --dwu-num-chunks 2    --dwu-num-rs-pg 1    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-overlap-reductions    --dwu-grad-norm    '
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 0 ']'
+ '[' 16 -gt 2 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 192 --test-batch-size 64 --optimizer FusedAdam --lr 2.875e-3 --warmup-steps 200 --remain-steps 4054 --decay-interval 506 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGXA100_multi_2x8x192_dist == *\D\G\X\A\1\0\0* ]]
Using TCMalloc
+ echo 'Using TCMalloc'
+ export LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
running benchmark
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 192 --test-batch-size 64 --optimizer FusedAdam --lr 2.875e-3 --warmup-steps 200 --remain-steps 4054 --decay-interval 506 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
STARTING TIMING RUN AT 2020-06-25 12:38:35 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=2.875e-3
+ TRAIN_BATCH_SIZE=192
+ TEST_BATCH_SIZE=64
+ WARMUP_STEPS=200
+ REMAIN_STEPS=4054
+ DECAY_INTERVAL=506
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=8
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 3    --dwu-num-chunks 2    --dwu-num-rs-pg 1    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-overlap-reductions    --dwu-grad-norm    '
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 4 ']'
+ '[' 16 -gt 2 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGXA100_multi_2x8x192_dist == *\D\G\X\A\1\0\0* ]]
+ echo 'Using TCMalloc'
Using TCMalloc
+ export LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
running benchmark
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 192 --test-batch-size 64 --optimizer FusedAdam --lr 2.875e-3 --warmup-steps 200 --remain-steps 4054 --decay-interval 506 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
num_sockets = 2 num_nodes=8 cores_per_socket=64
num_sockets = 2 num_nodes=8 cores_per_socket=64
+ exec numactl --physcpubind=0-15,128-143 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 192 --test-batch-size 64 --optimizer FusedAdam --lr 2.875e-3 --warmup-steps 200 --remain-steps 4054 --decay-interval 506 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
+ exec numactl --physcpubind=48-63,176-191 --membind=3 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 192 --test-batch-size 64 --optimizer FusedAdam --lr 2.875e-3 --warmup-steps 200 --remain-steps 4054 --decay-interval 506 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
num_sockets = 2 num_nodes=8 cores_per_socket=64
num_sockets = 2 num_nodes=8 cores_per_socket=64
+ exec numactl --physcpubind=32-47,160-175 --membind=2 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 192 --test-batch-size 64 --optimizer FusedAdam --lr 2.875e-3 --warmup-steps 200 --remain-steps 4054 --decay-interval 506 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
+ exec numactl --physcpubind=48-63,176-191 --membind=3 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 192 --test-batch-size 64 --optimizer FusedAdam --lr 2.875e-3 --warmup-steps 200 --remain-steps 4054 --decay-interval 506 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
num_sockets = 2 num_nodes=8 cores_per_socket=64
+ exec numactl --physcpubind=16-31,144-159 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 192 --test-batch-size 64 --optimizer FusedAdam --lr 2.875e-3 --warmup-steps 200 --remain-steps 4054 --decay-interval 506 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
num_sockets = 2 num_nodes=8 cores_per_socket=64
+ exec numactl --physcpubind=80-95,208-223 --membind=5 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 192 --test-batch-size 64 --optimizer FusedAdam --lr 2.875e-3 --warmup-steps 200 --remain-steps 4054 --decay-interval 506 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
num_sockets = 2 num_nodes=8 cores_per_socket=64
num_sockets = 2 num_nodes=8 cores_per_socket=64
num_sockets = 2 num_nodes=8 cores_per_socket=64
num_sockets = 2 num_nodes=8 cores_per_socket=64
+ exec numactl --physcpubind=16-31,144-159 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 192 --test-batch-size 64 --optimizer FusedAdam --lr 2.875e-3 --warmup-steps 200 --remain-steps 4054 --decay-interval 506 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
+ exec numactl --physcpubind=112-127,240-255 --membind=7 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 192 --test-batch-size 64 --optimizer FusedAdam --lr 2.875e-3 --warmup-steps 200 --remain-steps 4054 --decay-interval 506 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
+ exec numactl --physcpubind=64-79,192-207 --membind=4 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 192 --test-batch-size 64 --optimizer FusedAdam --lr 2.875e-3 --warmup-steps 200 --remain-steps 4054 --decay-interval 506 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
+ exec numactl --physcpubind=96-111,224-239 --membind=6 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 192 --test-batch-size 64 --optimizer FusedAdam --lr 2.875e-3 --warmup-steps 200 --remain-steps 4054 --decay-interval 506 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
num_sockets = 2 num_nodes=8 cores_per_socket=64
+ exec numactl --physcpubind=0-15,128-143 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 192 --test-batch-size 64 --optimizer FusedAdam --lr 2.875e-3 --warmup-steps 200 --remain-steps 4054 --decay-interval 506 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
num_sockets = 2 num_nodes=8 cores_per_socket=64
num_sockets = 2 num_nodes=8 cores_per_socket=64
num_sockets = 2 num_nodes=8 cores_per_socket=64
+ exec numactl --physcpubind=64-79,192-207 --membind=4 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 192 --test-batch-size 64 --optimizer FusedAdam --lr 2.875e-3 --warmup-steps 200 --remain-steps 4054 --decay-interval 506 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
num_sockets = 2 num_nodes=8 cores_per_socket=64
num_sockets = 2 num_nodes=8 cores_per_socket=64
+ exec numactl --physcpubind=32-47,160-175 --membind=2 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 192 --test-batch-size 64 --optimizer FusedAdam --lr 2.875e-3 --warmup-steps 200 --remain-steps 4054 --decay-interval 506 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
+ exec numactl --physcpubind=80-95,208-223 --membind=5 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 192 --test-batch-size 64 --optimizer FusedAdam --lr 2.875e-3 --warmup-steps 200 --remain-steps 4054 --decay-interval 506 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
+ exec numactl --physcpubind=112-127,240-255 --membind=7 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 192 --test-batch-size 64 --optimizer FusedAdam --lr 2.875e-3 --warmup-steps 200 --remain-steps 4054 --decay-interval 506 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
+ exec numactl --physcpubind=96-111,224-239 --membind=6 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 192 --test-batch-size 64 --optimizer FusedAdam --lr 2.875e-3 --warmup-steps 200 --remain-steps 4054 --decay-interval 506 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
:::MLLOG {"namespace": "", "time_ms": 1593113917065, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593113917090, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593113917093, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593113917096, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593113917102, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593113917160, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593113917174, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593113917178, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593113917195, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593113917206, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593113917212, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593113917266, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593113917299, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593113917304, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593113917315, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593113917324, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
NCCL version 2.7.5+cuda11.0
0: Saving results to: results/gnmt_wmt16
0: Run arguments: Namespace(apex_message_size=10000000.0, apex_num_allreduce_streams=1, batching='bucketing', beam_size=5, cov_penalty_factor=0.1, cuda=True, cudnn=True, dataset_dir='/data', decay_factor=0.5, decay_interval=506, decay_steps=4, distributed_weight_update=2, dropout=0.2, dwu_e5m2_allgather=False, dwu_full_pipeline=False, dwu_grad_norm=True, dwu_group_size=0, dwu_num_ag_pg=0, dwu_num_ar_pg=2, dwu_num_blocks=3, dwu_num_chunks=2, dwu_num_rs_pg=1, dwu_overlap_reductions=True, enable_apex_allreduce_overlap=False, env=False, epochs=8, eval=True, fused_attention=True, fused_xentropy=True, grad_clip=5.0, hidden_size=1024, init_scale=1024, intra_epoch_eval=0, keep_checkpoints=0, len_norm_const=5.0, len_norm_factor=0.6, local_rank=0, log_all_ranks=False, lr=0.002875, math='fp16', max_length_test=150, max_length_train=75, max_size=None, min_length_test=0, min_length_train=0, num_buckets=5, num_layers=4, optimizer='FusedAdam', optimizer_extra='{}', prealloc_mode='once', preproc_data_dir='/preproc_data/75', print_freq=10, rank=0, remain_steps=4054, results_dir='results', resume=None, save='gnmt_wmt16', save_all=False, save_freq=5000, save_path='results/gnmt_wmt16', seed=None, shard_size=80, share_embedding=True, smoothing=0.1, start_epoch=0, target_bleu=24.0, test_batch_size=64, test_loader_workers=0, train_batch_size=192, train_global_batch_size=None, train_iter_size=1, train_loader_workers=1, upscale_interval=128, use_preproc_data=True, warmup_steps=200)
0: L2 promotion: 128B
0: Using random master seed: 636964056
:::MLLOG {"namespace": "", "time_ms": 1593113926662, "event_type": "POINT_IN_TIME", "key": "seed", "value": 636964056, "metadata": {"file": "/workspace/rnn_translator/seq2seq/utils.py", "lineno": 144}}
0: Worker 0 is using worker seed: 2759920372
0: Building vocabulary from /data/vocab.bpe.32000
0: Size of vocabulary: 32320
0: Fused attention flag set to True
0: GNMT(
  (embedder): Embedding(32320, 1024, padding_idx=0)
  (encoder): ResidualRecurrentEncoder(
    (rnn_layers): ModuleList(
      (0): EmuBidirLSTM(
        (bidir): LSTM(1024, 1024, bidirectional=True)
        (layer1): LSTM(1024, 1024)
        (layer2): LSTM(1024, 1024)
      )
      (1): LSTM(2048, 1024)
      (2): LSTM(1024, 1024)
      (3): LSTM(1024, 1024)
    )
    (dropout): Dropout(p=0.2, inplace=False)
    (embedder): Embedding(32320, 1024, padding_idx=0)
  )
  (decoder): ResidualRecurrentDecoder(
    (att_rnn): RecurrentAttention(
      (rnn): LSTM(1024, 1024)
      (attn): BahdanauAttention(
        (linear_q): Linear(in_features=1024, out_features=1024, bias=False)
        (linear_k): Linear(in_features=1024, out_features=1024, bias=False)
      )
      (dropout): Dropout(p=0.2, inplace=False)
    )
    (rnn_layers): ModuleList(
      (0): LSTM(2048, 1024)
      (1): LSTM(2048, 1024)
      (2): LSTM(2048, 1024)
    )
    (embedder): Embedding(32320, 1024, padding_idx=0)
    (classifier): Classifier(
      (classifier): Linear(in_features=1024, out_features=32320, bias=True)
    )
    (dropout): Dropout(p=0.2, inplace=False)
  )
)
0: Building LabelSmoothingLoss (smoothing: 0.1)
0: Fused xentropy flag set to True
0: Training optimizer config: {'optimizer': 'FusedAdam', 'lr': 0.002875}
0: Number of parameters: 160671297
0: Saving state of the tokenizer
self._net_total_param_size=160671360, self._total_param_size=160677888, dwu_min_page_size=12288, self._block_size=53559296, self._chunk_size=26779648, self._shard_size=3347456
[0, 9, 34]
self._net_total_param_size=160671360, self._total_param_size=160677888, dwu_min_page_size=12288, self._block_size=53559296, self._chunk_size=26779648, self._shard_size=3347456
[0, 9, 34]
0: Initializing dwu fp16 optimizer
self._net_total_param_size=160671360, self._total_param_size=160677888, dwu_min_page_size=12288, self._block_size=53559296, self._chunk_size=26779648, self._shard_size=3347456
[0, 9, 34]
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
self._net_total_param_size=160671360, self._total_param_size=160677888, dwu_min_page_size=12288, self._block_size=53559296, self._chunk_size=26779648, self._shard_size=3347456
[0, 9, 34]
model_param_fragment.size()=torch.Size([467968]), new_param_packed_fragment.size()=torch.Size([467968]), master_param_fragment.size()=torch.Size([467968])
model_param_fragment.size()=torch.Size([2879488]), new_param_packed_fragment.size()=torch.Size([2879488]), master_param_fragment.size()=torch.Size([2879488])
model_param_fragment.size()=torch.Size([3073024]), new_param_packed_fragment.size()=torch.Size([3073024]), master_param_fragment.size()=torch.Size([3073024])
model_param_fragment.size()=torch.Size([274432]), new_param_packed_fragment.size()=torch.Size([274432]), master_param_fragment.size()=torch.Size([274432])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
self._net_total_param_size=160671360, self._total_param_size=160677888, dwu_min_page_size=12288, self._block_size=53559296, self._chunk_size=26779648, self._shard_size=3347456
[0, 9, 34]
NCCL version 2.7.5+cuda11.0
self._net_total_param_size=160671360, self._total_param_size=160677888, dwu_min_page_size=12288, self._block_size=53559296, self._chunk_size=26779648, self._shard_size=3347456
[0, 9, 34]
self._net_total_param_size=160671360, self._total_param_size=160677888, dwu_min_page_size=12288, self._block_size=53559296, self._chunk_size=26779648, self._shard_size=3347456
[0, 9, 34]
self._net_total_param_size=160671360, self._total_param_size=160677888, dwu_min_page_size=12288, self._block_size=53559296, self._chunk_size=26779648, self._shard_size=3347456
[0, 9, 34]
self._net_total_param_size=160671360, self._total_param_size=160677888, dwu_min_page_size=12288, self._block_size=53559296, self._chunk_size=26779648, self._shard_size=3347456
[0, 9, 34]
self._net_total_param_size=160671360, self._total_param_size=160677888, dwu_min_page_size=12288, self._block_size=53559296, self._chunk_size=26779648, self._shard_size=3347456
[0, 9, 34]
self._net_total_param_size=160671360, self._total_param_size=160677888, dwu_min_page_size=12288, self._block_size=53559296, self._chunk_size=26779648, self._shard_size=3347456
[0, 9, 34]
self._net_total_param_size=160671360, self._total_param_size=160677888, dwu_min_page_size=12288, self._block_size=53559296, self._chunk_size=26779648, self._shard_size=3347456
[0, 9, 34]
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([2968576]), new_param_packed_fragment.size()=torch.Size([2968576]), master_param_fragment.size()=torch.Size([2968576])
model_param_fragment.size()=torch.Size([378880]), new_param_packed_fragment.size()=torch.Size([378880]), master_param_fragment.size()=torch.Size([378880])
model_param_fragment.size()=torch.Size([1371136]), new_param_packed_fragment.size()=torch.Size([1371136]), master_param_fragment.size()=torch.Size([1371136])
model_param_fragment.size()=torch.Size([1976320]), new_param_packed_fragment.size()=torch.Size([1976320]), master_param_fragment.size()=torch.Size([1976320])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([283712]), new_param_packed_fragment.size()=torch.Size([283712]), master_param_fragment.size()=torch.Size([283712])
model_param_fragment.size()=torch.Size([3063744]), new_param_packed_fragment.size()=torch.Size([3063744]), master_param_fragment.size()=torch.Size([3063744])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
NCCL version 2.7.5+cuda11.0
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([2217984]), new_param_packed_fragment.size()=torch.Size([2217984]), master_param_fragment.size()=torch.Size([2217984])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([1121280]), new_param_packed_fragment.size()=torch.Size([1121280]), master_param_fragment.size()=torch.Size([1121280])
model_param_fragment.size()=torch.Size([628736]), new_param_packed_fragment.size()=torch.Size([628736]), master_param_fragment.size()=torch.Size([628736])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([1024]), new_param_packed_fragment.size()=torch.Size([1024]), master_param_fragment.size()=torch.Size([1024])
model_param_fragment.size()=torch.Size([1]), new_param_packed_fragment.size()=torch.Size([1]), master_param_fragment.size()=torch.Size([1])
model_param_fragment.size()=torch.Size([1024]), new_param_packed_fragment.size()=torch.Size([1024]), master_param_fragment.size()=torch.Size([1024])
model_param_fragment.size()=torch.Size([1048576]), new_param_packed_fragment.size()=torch.Size([1048576]), master_param_fragment.size()=torch.Size([1048576])
model_param_fragment.size()=torch.Size([1048576]), new_param_packed_fragment.size()=torch.Size([1048576]), master_param_fragment.size()=torch.Size([1048576])
model_param_fragment.size()=torch.Size([611264]), new_param_packed_fragment.size()=torch.Size([611264]), master_param_fragment.size()=torch.Size([611264])
model_param_fragment.size()=torch.Size([1130560]), new_param_packed_fragment.size()=torch.Size([1130560]), master_param_fragment.size()=torch.Size([1130560])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([2208704]), new_param_packed_fragment.size()=torch.Size([2208704]), master_param_fragment.size()=torch.Size([2208704])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([2169856]), new_param_packed_fragment.size()=torch.Size([2169856]), master_param_fragment.size()=torch.Size([2169856])
model_param_fragment.size()=torch.Size([1177600]), new_param_packed_fragment.size()=torch.Size([1177600]), master_param_fragment.size()=torch.Size([1177600])
model_param_fragment.size()=torch.Size([572416]), new_param_packed_fragment.size()=torch.Size([572416]), master_param_fragment.size()=torch.Size([572416])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([2766848]), new_param_packed_fragment.size()=torch.Size([2766848]), master_param_fragment.size()=torch.Size([2766848])
model_param_fragment.size()=torch.Size([1082432]), new_param_packed_fragment.size()=torch.Size([1082432]), master_param_fragment.size()=torch.Size([1082432])
model_param_fragment.size()=torch.Size([2265024]), new_param_packed_fragment.size()=torch.Size([2265024]), master_param_fragment.size()=torch.Size([2265024])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
NCCL version 2.7.5+cuda11.0
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3129344]), new_param_packed_fragment.size()=torch.Size([3129344]), master_param_fragment.size()=torch.Size([3129344])
model_param_fragment.size()=torch.Size([218112]), new_param_packed_fragment.size()=torch.Size([218112]), master_param_fragment.size()=torch.Size([218112])
model_param_fragment.size()=torch.Size([3129344]), new_param_packed_fragment.size()=torch.Size([3129344]), master_param_fragment.size()=torch.Size([3129344])
model_param_fragment.size()=torch.Size([218112]), new_param_packed_fragment.size()=torch.Size([218112]), master_param_fragment.size()=torch.Size([218112])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([1314816]), new_param_packed_fragment.size()=torch.Size([1314816]), master_param_fragment.size()=torch.Size([1314816])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([2024448]), new_param_packed_fragment.size()=torch.Size([2024448]), master_param_fragment.size()=torch.Size([2024448])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([2832448]), new_param_packed_fragment.size()=torch.Size([2832448]), master_param_fragment.size()=torch.Size([2832448])
model_param_fragment.size()=torch.Size([515008]), new_param_packed_fragment.size()=torch.Size([515008]), master_param_fragment.size()=torch.Size([515008])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
NCCL version 2.7.5+cuda11.0
self._net_total_param_size=160671360, self._total_param_size=160677888, dwu_min_page_size=12288, self._block_size=53559296, self._chunk_size=26779648, self._shard_size=3347456
[0, 9, 34]
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3016704]), new_param_packed_fragment.size()=torch.Size([3016704]), master_param_fragment.size()=torch.Size([3016704])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([322560]), new_param_packed_fragment.size()=torch.Size([322560]), master_param_fragment.size()=torch.Size([322560])
model_param_fragment.size()=torch.Size([1427456]), new_param_packed_fragment.size()=torch.Size([1427456]), master_param_fragment.size()=torch.Size([1427456])
model_param_fragment.size()=torch.Size([1920000]), new_param_packed_fragment.size()=torch.Size([1920000]), master_param_fragment.size()=torch.Size([1920000])
model_param_fragment.size()=torch.Size([1929280]), new_param_packed_fragment.size()=torch.Size([1929280]), master_param_fragment.size()=torch.Size([1929280])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([1409984]), new_param_packed_fragment.size()=torch.Size([1409984]), master_param_fragment.size()=torch.Size([1409984])
model_param_fragment.size()=torch.Size([331840]), new_param_packed_fragment.size()=torch.Size([331840]), master_param_fragment.size()=torch.Size([331840])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([3007424]), new_param_packed_fragment.size()=torch.Size([3007424]), master_param_fragment.size()=torch.Size([3007424])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([2274304]), new_param_packed_fragment.size()=torch.Size([2274304]), master_param_fragment.size()=torch.Size([2274304])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([1064960]), new_param_packed_fragment.size()=torch.Size([1064960]), master_param_fragment.size()=torch.Size([1064960])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3308608]), new_param_packed_fragment.size()=torch.Size([3308608]), master_param_fragment.size()=torch.Size([3308608])
model_param_fragment.size()=torch.Size([32320]), new_param_packed_fragment.size()=torch.Size([32320]), master_param_fragment.size()=torch.Size([32320])
NCCL version 2.7.5+cuda11.0
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([2169856]), new_param_packed_fragment.size()=torch.Size([2169856]), master_param_fragment.size()=torch.Size([2169856])
model_param_fragment.size()=torch.Size([1177600]), new_param_packed_fragment.size()=torch.Size([1177600]), master_param_fragment.size()=torch.Size([1177600])
model_param_fragment.size()=torch.Size([572416]), new_param_packed_fragment.size()=torch.Size([572416]), master_param_fragment.size()=torch.Size([572416])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([2766848]), new_param_packed_fragment.size()=torch.Size([2766848]), master_param_fragment.size()=torch.Size([2766848])
model_param_fragment.size()=torch.Size([1082432]), new_param_packed_fragment.size()=torch.Size([1082432]), master_param_fragment.size()=torch.Size([1082432])
model_param_fragment.size()=torch.Size([2265024]), new_param_packed_fragment.size()=torch.Size([2265024]), master_param_fragment.size()=torch.Size([2265024])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([1314816]), new_param_packed_fragment.size()=torch.Size([1314816]), master_param_fragment.size()=torch.Size([1314816])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([2024448]), new_param_packed_fragment.size()=torch.Size([2024448]), master_param_fragment.size()=torch.Size([2024448])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
self._net_total_param_size=160671360, self._total_param_size=160677888, dwu_min_page_size=12288, self._block_size=53559296, self._chunk_size=26779648, self._shard_size=3347456
[0, 9, 34]
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([2832448]), new_param_packed_fragment.size()=torch.Size([2832448]), master_param_fragment.size()=torch.Size([2832448])
model_param_fragment.size()=torch.Size([515008]), new_param_packed_fragment.size()=torch.Size([515008]), master_param_fragment.size()=torch.Size([515008])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([467968]), new_param_packed_fragment.size()=torch.Size([467968]), master_param_fragment.size()=torch.Size([467968])
model_param_fragment.size()=torch.Size([2879488]), new_param_packed_fragment.size()=torch.Size([2879488]), master_param_fragment.size()=torch.Size([2879488])
model_param_fragment.size()=torch.Size([3073024]), new_param_packed_fragment.size()=torch.Size([3073024]), master_param_fragment.size()=torch.Size([3073024])
model_param_fragment.size()=torch.Size([274432]), new_param_packed_fragment.size()=torch.Size([274432]), master_param_fragment.size()=torch.Size([274432])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([2968576]), new_param_packed_fragment.size()=torch.Size([2968576]), master_param_fragment.size()=torch.Size([2968576])
model_param_fragment.size()=torch.Size([378880]), new_param_packed_fragment.size()=torch.Size([378880]), master_param_fragment.size()=torch.Size([378880])
model_param_fragment.size()=torch.Size([1371136]), new_param_packed_fragment.size()=torch.Size([1371136]), master_param_fragment.size()=torch.Size([1371136])
model_param_fragment.size()=torch.Size([1976320]), new_param_packed_fragment.size()=torch.Size([1976320]), master_param_fragment.size()=torch.Size([1976320])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([283712]), new_param_packed_fragment.size()=torch.Size([283712]), master_param_fragment.size()=torch.Size([283712])
model_param_fragment.size()=torch.Size([3063744]), new_param_packed_fragment.size()=torch.Size([3063744]), master_param_fragment.size()=torch.Size([3063744])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
self._net_total_param_size=160671360, self._total_param_size=160677888, dwu_min_page_size=12288, self._block_size=53559296, self._chunk_size=26779648, self._shard_size=3347456
[0, 9, 34]
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([2274304]), new_param_packed_fragment.size()=torch.Size([2274304]), master_param_fragment.size()=torch.Size([2274304])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([1064960]), new_param_packed_fragment.size()=torch.Size([1064960]), master_param_fragment.size()=torch.Size([1064960])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3308608]), new_param_packed_fragment.size()=torch.Size([3308608]), master_param_fragment.size()=torch.Size([3308608])
model_param_fragment.size()=torch.Size([32320]), new_param_packed_fragment.size()=torch.Size([32320]), master_param_fragment.size()=torch.Size([32320])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3016704]), new_param_packed_fragment.size()=torch.Size([3016704]), master_param_fragment.size()=torch.Size([3016704])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([322560]), new_param_packed_fragment.size()=torch.Size([322560]), master_param_fragment.size()=torch.Size([322560])
model_param_fragment.size()=torch.Size([1427456]), new_param_packed_fragment.size()=torch.Size([1427456]), master_param_fragment.size()=torch.Size([1427456])
model_param_fragment.size()=torch.Size([1920000]), new_param_packed_fragment.size()=torch.Size([1920000]), master_param_fragment.size()=torch.Size([1920000])
model_param_fragment.size()=torch.Size([1929280]), new_param_packed_fragment.size()=torch.Size([1929280]), master_param_fragment.size()=torch.Size([1929280])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([1409984]), new_param_packed_fragment.size()=torch.Size([1409984]), master_param_fragment.size()=torch.Size([1409984])
model_param_fragment.size()=torch.Size([331840]), new_param_packed_fragment.size()=torch.Size([331840]), master_param_fragment.size()=torch.Size([331840])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([3007424]), new_param_packed_fragment.size()=torch.Size([3007424]), master_param_fragment.size()=torch.Size([3007424])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
NCCL version 2.7.5+cuda11.0
self._net_total_param_size=160671360, self._total_param_size=160677888, dwu_min_page_size=12288, self._block_size=53559296, self._chunk_size=26779648, self._shard_size=3347456
[0, 9, 34]
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([2217984]), new_param_packed_fragment.size()=torch.Size([2217984]), master_param_fragment.size()=torch.Size([2217984])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([1121280]), new_param_packed_fragment.size()=torch.Size([1121280]), master_param_fragment.size()=torch.Size([1121280])
model_param_fragment.size()=torch.Size([628736]), new_param_packed_fragment.size()=torch.Size([628736]), master_param_fragment.size()=torch.Size([628736])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([1024]), new_param_packed_fragment.size()=torch.Size([1024]), master_param_fragment.size()=torch.Size([1024])
model_param_fragment.size()=torch.Size([1]), new_param_packed_fragment.size()=torch.Size([1]), master_param_fragment.size()=torch.Size([1])
model_param_fragment.size()=torch.Size([1024]), new_param_packed_fragment.size()=torch.Size([1024]), master_param_fragment.size()=torch.Size([1024])
model_param_fragment.size()=torch.Size([1048576]), new_param_packed_fragment.size()=torch.Size([1048576]), master_param_fragment.size()=torch.Size([1048576])
model_param_fragment.size()=torch.Size([1048576]), new_param_packed_fragment.size()=torch.Size([1048576]), master_param_fragment.size()=torch.Size([1048576])
model_param_fragment.size()=torch.Size([611264]), new_param_packed_fragment.size()=torch.Size([611264]), master_param_fragment.size()=torch.Size([611264])
model_param_fragment.size()=torch.Size([1130560]), new_param_packed_fragment.size()=torch.Size([1130560]), master_param_fragment.size()=torch.Size([1130560])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([2208704]), new_param_packed_fragment.size()=torch.Size([2208704]), master_param_fragment.size()=torch.Size([2208704])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
NCCL version 2.7.5+cuda11.0
NCCL version 2.7.5+cuda11.0
0: Using optimizer: DistributedFusedAdam (
Parameter Group 0
    betas: (0.9, 0.999)
    bias_correction: True
    eps: 1e-08
    lr: 0.002875
    max_grad_norm: 5.0
    weight_decay: 0.0
)
:::MLLOG {"namespace": "", "time_ms": 1593113938628, "event_type": "POINT_IN_TIME", "key": "opt_name", "value": "adam", "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/trainer.py", "lineno": 176}}
:::MLLOG {"namespace": "", "time_ms": 1593113938629, "event_type": "POINT_IN_TIME", "key": "opt_base_learning_rate", "value": 0.002875, "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/trainer.py", "lineno": 178}}
:::MLLOG {"namespace": "", "time_ms": 1593113938629, "event_type": "POINT_IN_TIME", "key": "opt_adam_beta_1", "value": 0.9, "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/trainer.py", "lineno": 180}}
:::MLLOG {"namespace": "", "time_ms": 1593113938629, "event_type": "POINT_IN_TIME", "key": "opt_adam_beta_2", "value": 0.999, "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/trainer.py", "lineno": 182}}
:::MLLOG {"namespace": "", "time_ms": 1593113938629, "event_type": "POINT_IN_TIME", "key": "opt_adam_epsilon", "value": 1e-08, "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/trainer.py", "lineno": 184}}
0: Executing preallocation
:::MLLOG {"namespace": "", "time_ms": 1593113940072, "event_type": "INTERVAL_END", "key": "init_stop", "value": null, "metadata": {"file": "train.py", "lineno": 429}}
:::MLLOG {"namespace": "", "time_ms": 1593113940073, "event_type": "INTERVAL_START", "key": "run_start", "value": null, "metadata": {"file": "train.py", "lineno": 430}}
:::MLLOG {"namespace": "", "time_ms": 1593113940073, "event_type": "POINT_IN_TIME", "key": "max_sequence_length", "value": 75, "metadata": {"file": "train.py", "lineno": 435, "method": "discard"}}
0: Reading preprocessed data file from /preproc_data/75/training.bin
0: Preprocessed data: length: 3975116 min length: 0 max length: 75
0: Opening preprocessed data /preproc_data/75/training.bin for reading
0: Processing data from /data/newstest2014.tok.bpe.32000.en
0: Filtering data, min len: 0, max len: 150
0: Pairs before: 3003, after: 3003
:::MLLOG {"namespace": "", "time_ms": 1593113940351, "event_type": "POINT_IN_TIME", "key": "global_batch_size", "value": 3072, "metadata": {"file": "train.py", "lineno": 480}}
:::MLLOG {"namespace": "", "time_ms": 1593113940352, "event_type": "POINT_IN_TIME", "key": "train_samples", "value": 3965952, "metadata": {"file": "train.py", "lineno": 488}}
:::MLLOG {"namespace": "", "time_ms": 1593113940352, "event_type": "POINT_IN_TIME", "key": "eval_samples", "value": 3003, "metadata": {"file": "train.py", "lineno": 490}}
0: Training LR schedule config: {'warmup_steps': 200, 'remain_steps': 4054, 'decay_interval': 506, 'decay_steps': 4, 'decay_factor': 0.5}
0: Scheduler warmup steps: 200
0: Scheduler remain steps: 4054
0: Scheduler decay interval: 506
0: Scheduler decay factor: 0.5
0: Scheduler max decay steps: 4
:::MLLOG {"namespace": "", "time_ms": 1593113940352, "event_type": "POINT_IN_TIME", "key": "opt_learning_rate_alt_decay_func", "value": true, "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/lr_scheduler.py", "lineno": 78}}
:::MLLOG {"namespace": "", "time_ms": 1593113940353, "event_type": "POINT_IN_TIME", "key": "opt_learning_rate_alt_warmup_func", "value": true, "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/lr_scheduler.py", "lineno": 80}}
:::MLLOG {"namespace": "", "time_ms": 1593113940353, "event_type": "POINT_IN_TIME", "key": "opt_learning_rate_decay_interval", "value": 506, "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/lr_scheduler.py", "lineno": 83}}
:::MLLOG {"namespace": "", "time_ms": 1593113940353, "event_type": "POINT_IN_TIME", "key": "opt_learning_rate_decay_factor", "value": 0.5, "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/lr_scheduler.py", "lineno": 85}}
:::MLLOG {"namespace": "", "time_ms": 1593113940353, "event_type": "POINT_IN_TIME", "key": "opt_learning_rate_decay_steps", "value": 4, "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/lr_scheduler.py", "lineno": 87}}
:::MLLOG {"namespace": "", "time_ms": 1593113940353, "event_type": "POINT_IN_TIME", "key": "opt_learning_rate_remain_steps", "value": 4054, "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/lr_scheduler.py", "lineno": 89}}
:::MLLOG {"namespace": "", "time_ms": 1593113940353, "event_type": "POINT_IN_TIME", "key": "opt_learning_rate_warmup_steps", "value": 200, "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/lr_scheduler.py", "lineno": 91}}
:::MLLOG {"namespace": "", "time_ms": 1593113940353, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "train.py", "lineno": 538, "first_epoch_num": 1, "epoch_count": 1}}
:::MLLOG {"namespace": "", "time_ms": 1593113940353, "event_type": "INTERVAL_START", "key": "epoch_start", "value": null, "metadata": {"file": "train.py", "lineno": 541, "epoch_num": 1}}
0: Starting epoch 0
0: Sampler for epoch 0 uses seed 342236853
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
0: TRAIN [0][0/1291]	Time 0.350 (0.350)	Data 1.94e-01 (1.94e-01)	Tok/s 36102 (36102)	Loss/tok 10.6659 (10.6659)	LR 2.942e-05
0: TRAIN [0][10/1291]	Time 0.042 (0.082)	Data 8.58e-05 (1.77e-02)	Tok/s 188394 (189885)	Loss/tok 9.4488 (9.9755)	LR 3.704e-05
0: Gradient norm: inf
0: Skipped batch, new scale: 512.0
0: Gradient norm: inf
0: Skipped batch, new scale: 256.0
0: TRAIN [0][20/1291]	Time 0.075 (0.065)	Data 9.70e-05 (9.31e-03)	Tok/s 234514 (193420)	Loss/tok 9.5870 (9.7042)	LR 4.453e-05
0: Gradient norm: inf
0: Skipped batch, new scale: 128.0
0: TRAIN [0][30/1291]	Time 0.044 (0.060)	Data 7.68e-05 (6.34e-03)	Tok/s 174719 (194232)	Loss/tok 8.9465 (9.5513)	LR 5.478e-05
0: TRAIN [0][40/1291]	Time 0.041 (0.057)	Data 9.13e-05 (4.81e-03)	Tok/s 190880 (195817)	Loss/tok 8.6772 (9.3874)	LR 6.897e-05
0: TRAIN [0][50/1291]	Time 0.058 (0.056)	Data 1.41e-04 (3.89e-03)	Tok/s 212728 (197362)	Loss/tok 8.4979 (9.2255)	LR 8.682e-05
0: TRAIN [0][60/1291]	Time 0.041 (0.055)	Data 1.43e-04 (3.27e-03)	Tok/s 190120 (197047)	Loss/tok 8.2395 (9.0855)	LR 1.093e-04
0: TRAIN [0][70/1291]	Time 0.041 (0.054)	Data 8.82e-05 (2.82e-03)	Tok/s 185474 (197418)	Loss/tok 8.0468 (8.9545)	LR 1.376e-04
0: TRAIN [0][80/1291]	Time 0.041 (0.055)	Data 1.38e-04 (2.49e-03)	Tok/s 190362 (199103)	Loss/tok 7.8661 (8.8285)	LR 1.732e-04
0: TRAIN [0][90/1291]	Time 0.058 (0.054)	Data 8.96e-05 (2.22e-03)	Tok/s 217911 (199377)	Loss/tok 7.9633 (8.7319)	LR 2.181e-04
0: TRAIN [0][100/1291]	Time 0.076 (0.054)	Data 8.58e-05 (2.01e-03)	Tok/s 229453 (199291)	Loss/tok 8.1008 (8.6465)	LR 2.746e-04
0: TRAIN [0][110/1291]	Time 0.076 (0.054)	Data 9.13e-05 (1.84e-03)	Tok/s 228321 (199533)	Loss/tok 8.0733 (8.5750)	LR 3.457e-04
0: TRAIN [0][120/1291]	Time 0.076 (0.055)	Data 8.54e-05 (1.70e-03)	Tok/s 231341 (200817)	Loss/tok 7.8846 (8.5041)	LR 4.351e-04
0: TRAIN [0][130/1291]	Time 0.058 (0.054)	Data 9.01e-05 (1.57e-03)	Tok/s 217109 (200709)	Loss/tok 7.7343 (8.4481)	LR 5.478e-04
0: TRAIN [0][140/1291]	Time 0.058 (0.054)	Data 9.13e-05 (1.47e-03)	Tok/s 221752 (200378)	Loss/tok 7.5626 (8.3944)	LR 6.897e-04
0: Upscaling, new scale: 256.0
0: TRAIN [0][150/1291]	Time 0.076 (0.054)	Data 9.13e-05 (1.38e-03)	Tok/s 227642 (200578)	Loss/tok 7.8086 (8.3404)	LR 8.682e-04
0: TRAIN [0][160/1291]	Time 0.041 (0.054)	Data 9.11e-05 (1.30e-03)	Tok/s 188945 (200604)	Loss/tok 7.2465 (8.2863)	LR 1.093e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 128.0
0: TRAIN [0][170/1291]	Time 0.076 (0.053)	Data 8.94e-05 (1.23e-03)	Tok/s 234802 (200629)	Loss/tok 7.4280 (8.2314)	LR 1.345e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 64.0
0: Gradient norm: inf
0: Skipped batch, new scale: 32.0
0: TRAIN [0][180/1291]	Time 0.076 (0.053)	Data 9.16e-05 (1.17e-03)	Tok/s 233454 (200974)	Loss/tok 7.5454 (8.1772)	LR 1.617e-03
0: TRAIN [0][190/1291]	Time 0.041 (0.053)	Data 1.43e-04 (1.11e-03)	Tok/s 185923 (200513)	Loss/tok 6.9410 (8.1278)	LR 2.035e-03
0: TRAIN [0][200/1291]	Time 0.075 (0.053)	Data 8.80e-05 (1.06e-03)	Tok/s 229602 (200489)	Loss/tok 7.1357 (8.0696)	LR 2.562e-03
0: TRAIN [0][210/1291]	Time 0.041 (0.053)	Data 8.96e-05 (1.01e-03)	Tok/s 189932 (200112)	Loss/tok 6.4766 (8.0109)	LR 2.875e-03
0: TRAIN [0][220/1291]	Time 0.058 (0.053)	Data 1.42e-04 (9.72e-04)	Tok/s 217760 (200347)	Loss/tok 6.4667 (7.9423)	LR 2.875e-03
0: TRAIN [0][230/1291]	Time 0.096 (0.053)	Data 8.73e-05 (9.34e-04)	Tok/s 232167 (200756)	Loss/tok 6.5703 (7.8645)	LR 2.875e-03
0: TRAIN [0][240/1291]	Time 0.058 (0.053)	Data 1.40e-04 (9.00e-04)	Tok/s 218123 (200364)	Loss/tok 6.2776 (7.8048)	LR 2.875e-03
0: TRAIN [0][250/1291]	Time 0.041 (0.053)	Data 8.18e-05 (8.68e-04)	Tok/s 191820 (200674)	Loss/tok 5.7724 (7.7335)	LR 2.875e-03
0: TRAIN [0][260/1291]	Time 0.041 (0.053)	Data 1.35e-04 (8.38e-04)	Tok/s 187187 (201053)	Loss/tok 5.8727 (7.6575)	LR 2.875e-03
0: TRAIN [0][270/1291]	Time 0.058 (0.053)	Data 8.89e-05 (8.10e-04)	Tok/s 219486 (201186)	Loss/tok 5.8059 (7.5910)	LR 2.875e-03
0: TRAIN [0][280/1291]	Time 0.041 (0.053)	Data 8.63e-05 (7.85e-04)	Tok/s 186497 (201116)	Loss/tok 5.6949 (7.5290)	LR 2.875e-03
0: TRAIN [0][290/1291]	Time 0.041 (0.053)	Data 8.37e-05 (7.61e-04)	Tok/s 188559 (200830)	Loss/tok 5.1663 (7.4709)	LR 2.875e-03
0: TRAIN [0][300/1291]	Time 0.041 (0.052)	Data 8.13e-05 (7.39e-04)	Tok/s 184373 (200682)	Loss/tok 5.1834 (7.4098)	LR 2.875e-03
0: Upscaling, new scale: 64.0
0: TRAIN [0][310/1291]	Time 0.058 (0.053)	Data 8.87e-05 (7.18e-04)	Tok/s 219427 (200300)	Loss/tok 5.4600 (7.3465)	LR 2.875e-03
0: TRAIN [0][320/1291]	Time 0.058 (0.053)	Data 8.82e-05 (6.99e-04)	Tok/s 215348 (200465)	Loss/tok 5.3164 (7.2785)	LR 2.875e-03
0: TRAIN [0][330/1291]	Time 0.042 (0.053)	Data 9.01e-05 (6.81e-04)	Tok/s 186873 (200451)	Loss/tok 4.8743 (7.2163)	LR 2.875e-03
0: TRAIN [0][340/1291]	Time 0.041 (0.052)	Data 8.68e-05 (6.64e-04)	Tok/s 186532 (200072)	Loss/tok 4.8231 (7.1635)	LR 2.875e-03
0: TRAIN [0][350/1291]	Time 0.041 (0.052)	Data 8.68e-05 (6.47e-04)	Tok/s 188413 (200341)	Loss/tok 4.6866 (7.0915)	LR 2.875e-03
0: TRAIN [0][360/1291]	Time 0.042 (0.053)	Data 1.40e-04 (6.32e-04)	Tok/s 181833 (200376)	Loss/tok 4.5639 (7.0285)	LR 2.875e-03
0: TRAIN [0][370/1291]	Time 0.041 (0.052)	Data 8.56e-05 (6.17e-04)	Tok/s 186742 (200272)	Loss/tok 4.4689 (6.9712)	LR 2.875e-03
0: TRAIN [0][380/1291]	Time 0.058 (0.052)	Data 8.99e-05 (6.03e-04)	Tok/s 215197 (200305)	Loss/tok 4.5728 (6.9109)	LR 2.875e-03
0: TRAIN [0][390/1291]	Time 0.058 (0.053)	Data 9.37e-05 (5.91e-04)	Tok/s 218598 (200529)	Loss/tok 4.6318 (6.8446)	LR 2.875e-03
0: TRAIN [0][400/1291]	Time 0.041 (0.052)	Data 9.39e-05 (5.78e-04)	Tok/s 186247 (200033)	Loss/tok 4.5019 (6.8028)	LR 2.875e-03
0: TRAIN [0][410/1291]	Time 0.059 (0.052)	Data 8.82e-05 (5.67e-04)	Tok/s 214451 (200228)	Loss/tok 4.5145 (6.7423)	LR 2.875e-03
0: TRAIN [0][420/1291]	Time 0.058 (0.052)	Data 8.34e-05 (5.55e-04)	Tok/s 214183 (200161)	Loss/tok 4.5988 (6.6869)	LR 2.875e-03
0: TRAIN [0][430/1291]	Time 0.041 (0.053)	Data 8.58e-05 (5.45e-04)	Tok/s 187630 (200429)	Loss/tok 4.2027 (6.6264)	LR 2.875e-03
0: Upscaling, new scale: 128.0
0: TRAIN [0][440/1291]	Time 0.059 (0.053)	Data 8.75e-05 (5.35e-04)	Tok/s 216922 (200551)	Loss/tok 4.3686 (6.5741)	LR 2.875e-03
0: TRAIN [0][450/1291]	Time 0.025 (0.052)	Data 8.51e-05 (5.25e-04)	Tok/s 163359 (200424)	Loss/tok 3.4358 (6.5274)	LR 2.875e-03
0: TRAIN [0][460/1291]	Time 0.041 (0.052)	Data 9.06e-05 (5.15e-04)	Tok/s 189915 (200177)	Loss/tok 4.0088 (6.4858)	LR 2.875e-03
0: TRAIN [0][470/1291]	Time 0.058 (0.052)	Data 8.61e-05 (5.06e-04)	Tok/s 217855 (200264)	Loss/tok 4.1954 (6.4364)	LR 2.875e-03
0: TRAIN [0][480/1291]	Time 0.096 (0.052)	Data 8.89e-05 (4.98e-04)	Tok/s 231981 (200218)	Loss/tok 4.6869 (6.3922)	LR 2.875e-03
0: TRAIN [0][490/1291]	Time 0.025 (0.052)	Data 8.87e-05 (4.90e-04)	Tok/s 163754 (200265)	Loss/tok 3.2497 (6.3477)	LR 2.875e-03
0: TRAIN [0][500/1291]	Time 0.058 (0.052)	Data 1.71e-04 (4.82e-04)	Tok/s 217268 (200371)	Loss/tok 4.1167 (6.3027)	LR 2.875e-03
0: TRAIN [0][510/1291]	Time 0.058 (0.052)	Data 8.68e-05 (4.74e-04)	Tok/s 211871 (200405)	Loss/tok 4.1488 (6.2600)	LR 2.875e-03
0: TRAIN [0][520/1291]	Time 0.058 (0.052)	Data 8.49e-05 (4.67e-04)	Tok/s 214811 (200559)	Loss/tok 4.1410 (6.2146)	LR 2.875e-03
0: TRAIN [0][530/1291]	Time 0.058 (0.053)	Data 9.49e-05 (4.60e-04)	Tok/s 212601 (200750)	Loss/tok 4.4337 (6.1707)	LR 2.875e-03
0: TRAIN [0][540/1291]	Time 0.058 (0.053)	Data 8.15e-05 (4.53e-04)	Tok/s 218234 (200825)	Loss/tok 4.1014 (6.1313)	LR 2.875e-03
0: TRAIN [0][550/1291]	Time 0.025 (0.053)	Data 9.87e-05 (4.47e-04)	Tok/s 162371 (200829)	Loss/tok 3.3576 (6.0929)	LR 2.875e-03
0: TRAIN [0][560/1291]	Time 0.059 (0.053)	Data 8.15e-05 (4.41e-04)	Tok/s 214983 (200901)	Loss/tok 4.0573 (6.0552)	LR 2.875e-03
0: Upscaling, new scale: 256.0
0: TRAIN [0][570/1291]	Time 0.058 (0.053)	Data 8.82e-05 (4.35e-04)	Tok/s 220694 (201053)	Loss/tok 4.0375 (6.0171)	LR 2.875e-03
0: TRAIN [0][580/1291]	Time 0.096 (0.053)	Data 8.30e-05 (4.29e-04)	Tok/s 232526 (201074)	Loss/tok 4.5309 (5.9824)	LR 2.875e-03
0: TRAIN [0][590/1291]	Time 0.058 (0.053)	Data 8.39e-05 (4.23e-04)	Tok/s 218355 (201050)	Loss/tok 3.9585 (5.9502)	LR 2.875e-03
0: TRAIN [0][600/1291]	Time 0.058 (0.053)	Data 8.06e-05 (4.18e-04)	Tok/s 217185 (201096)	Loss/tok 4.0161 (5.9166)	LR 2.875e-03
0: TRAIN [0][610/1291]	Time 0.058 (0.053)	Data 8.37e-05 (4.12e-04)	Tok/s 217629 (201129)	Loss/tok 3.8786 (5.8842)	LR 2.875e-03
0: TRAIN [0][620/1291]	Time 0.059 (0.053)	Data 7.96e-05 (4.07e-04)	Tok/s 214599 (201134)	Loss/tok 3.7903 (5.8524)	LR 2.875e-03
0: TRAIN [0][630/1291]	Time 0.041 (0.053)	Data 8.77e-05 (4.02e-04)	Tok/s 184843 (201009)	Loss/tok 3.6669 (5.8262)	LR 2.875e-03
0: TRAIN [0][640/1291]	Time 0.041 (0.052)	Data 8.34e-05 (3.97e-04)	Tok/s 186949 (200954)	Loss/tok 3.6833 (5.7981)	LR 2.875e-03
0: TRAIN [0][650/1291]	Time 0.097 (0.052)	Data 8.63e-05 (3.93e-04)	Tok/s 229987 (200980)	Loss/tok 4.3968 (5.7685)	LR 2.875e-03
0: TRAIN [0][660/1291]	Time 0.058 (0.053)	Data 8.39e-05 (3.88e-04)	Tok/s 215003 (201143)	Loss/tok 3.9052 (5.7367)	LR 2.875e-03
0: TRAIN [0][670/1291]	Time 0.076 (0.053)	Data 8.99e-05 (3.84e-04)	Tok/s 230742 (201057)	Loss/tok 4.3236 (5.7112)	LR 2.875e-03
0: TRAIN [0][680/1291]	Time 0.076 (0.053)	Data 1.41e-04 (3.80e-04)	Tok/s 230555 (201102)	Loss/tok 4.1959 (5.6849)	LR 2.875e-03
0: TRAIN [0][690/1291]	Time 0.076 (0.053)	Data 8.46e-05 (3.75e-04)	Tok/s 224975 (201123)	Loss/tok 4.2635 (5.6596)	LR 2.875e-03
0: Upscaling, new scale: 512.0
0: TRAIN [0][700/1291]	Time 0.097 (0.053)	Data 1.39e-04 (3.72e-04)	Tok/s 230527 (201148)	Loss/tok 4.3406 (5.6329)	LR 2.875e-03
0: TRAIN [0][710/1291]	Time 0.041 (0.053)	Data 1.40e-04 (3.68e-04)	Tok/s 186468 (201186)	Loss/tok 3.6244 (5.6074)	LR 2.875e-03
0: TRAIN [0][720/1291]	Time 0.024 (0.053)	Data 8.63e-05 (3.64e-04)	Tok/s 161231 (201075)	Loss/tok 3.0961 (5.5860)	LR 2.875e-03
0: TRAIN [0][730/1291]	Time 0.042 (0.053)	Data 8.37e-05 (3.60e-04)	Tok/s 185186 (201014)	Loss/tok 3.4863 (5.5628)	LR 2.875e-03
0: TRAIN [0][740/1291]	Time 0.058 (0.053)	Data 1.12e-04 (3.57e-04)	Tok/s 216753 (201007)	Loss/tok 3.8314 (5.5399)	LR 2.875e-03
0: TRAIN [0][750/1291]	Time 0.058 (0.053)	Data 9.27e-05 (3.53e-04)	Tok/s 215607 (201079)	Loss/tok 3.7672 (5.5166)	LR 2.875e-03
0: TRAIN [0][760/1291]	Time 0.058 (0.053)	Data 8.18e-05 (3.50e-04)	Tok/s 212314 (201154)	Loss/tok 3.8022 (5.4930)	LR 2.875e-03
0: TRAIN [0][770/1291]	Time 0.041 (0.053)	Data 7.99e-05 (3.46e-04)	Tok/s 186336 (201013)	Loss/tok 3.6330 (5.4742)	LR 2.875e-03
0: TRAIN [0][780/1291]	Time 0.042 (0.053)	Data 8.23e-05 (3.43e-04)	Tok/s 184639 (201009)	Loss/tok 3.4007 (5.4519)	LR 2.875e-03
0: TRAIN [0][790/1291]	Time 0.076 (0.053)	Data 9.42e-05 (3.40e-04)	Tok/s 230095 (201024)	Loss/tok 3.9934 (5.4305)	LR 2.875e-03
0: TRAIN [0][800/1291]	Time 0.058 (0.053)	Data 1.03e-04 (3.37e-04)	Tok/s 218551 (200979)	Loss/tok 3.8069 (5.4116)	LR 2.875e-03
0: TRAIN [0][810/1291]	Time 0.077 (0.053)	Data 8.73e-05 (3.34e-04)	Tok/s 228305 (201190)	Loss/tok 4.0122 (5.3858)	LR 2.875e-03
0: Upscaling, new scale: 1024.0
0: TRAIN [0][820/1291]	Time 0.076 (0.053)	Data 8.32e-05 (3.31e-04)	Tok/s 227937 (201307)	Loss/tok 3.9813 (5.3632)	LR 2.875e-03
0: TRAIN [0][830/1291]	Time 0.042 (0.053)	Data 8.32e-05 (3.28e-04)	Tok/s 186551 (201083)	Loss/tok 3.5021 (5.3484)	LR 2.875e-03
0: TRAIN [0][840/1291]	Time 0.041 (0.053)	Data 8.11e-05 (3.25e-04)	Tok/s 192118 (201081)	Loss/tok 3.5950 (5.3301)	LR 2.875e-03
0: TRAIN [0][850/1291]	Time 0.041 (0.053)	Data 8.85e-05 (3.22e-04)	Tok/s 184936 (201067)	Loss/tok 3.5584 (5.3126)	LR 2.875e-03
0: TRAIN [0][860/1291]	Time 0.059 (0.053)	Data 1.40e-04 (3.20e-04)	Tok/s 213900 (201105)	Loss/tok 3.6980 (5.2934)	LR 2.875e-03
0: TRAIN [0][870/1291]	Time 0.059 (0.053)	Data 8.51e-05 (3.17e-04)	Tok/s 212115 (201111)	Loss/tok 3.8231 (5.2758)	LR 2.875e-03
0: TRAIN [0][880/1291]	Time 0.058 (0.053)	Data 8.46e-05 (3.15e-04)	Tok/s 213558 (201073)	Loss/tok 3.7079 (5.2591)	LR 2.875e-03
0: TRAIN [0][890/1291]	Time 0.041 (0.053)	Data 8.11e-05 (3.12e-04)	Tok/s 185206 (201021)	Loss/tok 3.5219 (5.2434)	LR 2.875e-03
0: TRAIN [0][900/1291]	Time 0.042 (0.053)	Data 1.38e-04 (3.10e-04)	Tok/s 179840 (201055)	Loss/tok 3.5314 (5.2258)	LR 2.875e-03
0: TRAIN [0][910/1291]	Time 0.041 (0.053)	Data 8.15e-05 (3.07e-04)	Tok/s 186986 (201077)	Loss/tok 3.3763 (5.2091)	LR 2.875e-03
0: TRAIN [0][920/1291]	Time 0.076 (0.053)	Data 1.34e-04 (3.05e-04)	Tok/s 229405 (201194)	Loss/tok 3.9385 (5.1906)	LR 2.875e-03
0: TRAIN [0][930/1291]	Time 0.058 (0.053)	Data 8.30e-05 (3.03e-04)	Tok/s 213800 (201274)	Loss/tok 4.0050 (5.1730)	LR 2.875e-03
0: TRAIN [0][940/1291]	Time 0.041 (0.053)	Data 8.27e-05 (3.01e-04)	Tok/s 190149 (201154)	Loss/tok 3.6036 (5.1602)	LR 2.875e-03
0: Upscaling, new scale: 2048.0
0: TRAIN [0][950/1291]	Time 0.076 (0.053)	Data 8.15e-05 (2.98e-04)	Tok/s 231691 (201242)	Loss/tok 3.9409 (5.1430)	LR 2.875e-03
0: TRAIN [0][960/1291]	Time 0.058 (0.053)	Data 1.35e-04 (2.96e-04)	Tok/s 212180 (201240)	Loss/tok 3.8255 (5.1278)	LR 2.875e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 1024.0
0: TRAIN [0][970/1291]	Time 0.059 (0.053)	Data 8.06e-05 (2.94e-04)	Tok/s 210790 (201299)	Loss/tok 3.6302 (5.1122)	LR 2.875e-03
0: TRAIN [0][980/1291]	Time 0.024 (0.053)	Data 1.36e-04 (2.92e-04)	Tok/s 163384 (201211)	Loss/tok 2.8497 (5.0990)	LR 2.875e-03
0: TRAIN [0][990/1291]	Time 0.076 (0.053)	Data 1.41e-04 (2.90e-04)	Tok/s 230216 (201228)	Loss/tok 3.8141 (5.0841)	LR 2.875e-03
0: TRAIN [0][1000/1291]	Time 0.041 (0.053)	Data 1.35e-04 (2.89e-04)	Tok/s 193306 (201077)	Loss/tok 3.3030 (5.0727)	LR 2.875e-03
0: TRAIN [0][1010/1291]	Time 0.041 (0.053)	Data 1.35e-04 (2.87e-04)	Tok/s 185905 (201063)	Loss/tok 3.3965 (5.0595)	LR 2.875e-03
0: TRAIN [0][1020/1291]	Time 0.041 (0.053)	Data 8.34e-05 (2.85e-04)	Tok/s 192843 (201016)	Loss/tok 3.4272 (5.0466)	LR 2.875e-03
0: TRAIN [0][1030/1291]	Time 0.042 (0.052)	Data 8.51e-05 (2.83e-04)	Tok/s 189874 (200986)	Loss/tok 3.4487 (5.0336)	LR 2.875e-03
0: TRAIN [0][1040/1291]	Time 0.024 (0.052)	Data 8.58e-05 (2.81e-04)	Tok/s 163733 (200891)	Loss/tok 2.9982 (5.0223)	LR 2.875e-03
0: TRAIN [0][1050/1291]	Time 0.097 (0.052)	Data 8.15e-05 (2.79e-04)	Tok/s 234280 (200944)	Loss/tok 4.0588 (5.0082)	LR 2.875e-03
0: TRAIN [0][1060/1291]	Time 0.042 (0.052)	Data 7.94e-05 (2.77e-04)	Tok/s 182920 (200908)	Loss/tok 3.3186 (4.9955)	LR 2.875e-03
0: TRAIN [0][1070/1291]	Time 0.041 (0.053)	Data 8.58e-05 (2.76e-04)	Tok/s 184678 (200949)	Loss/tok 3.3123 (4.9822)	LR 2.875e-03
0: TRAIN [0][1080/1291]	Time 0.025 (0.052)	Data 8.44e-05 (2.74e-04)	Tok/s 156664 (200882)	Loss/tok 2.9751 (4.9709)	LR 2.875e-03
0: TRAIN [0][1090/1291]	Time 0.041 (0.052)	Data 8.03e-05 (2.72e-04)	Tok/s 184753 (200900)	Loss/tok 3.4061 (4.9582)	LR 2.875e-03
0: Upscaling, new scale: 2048.0
0: TRAIN [0][1100/1291]	Time 0.058 (0.053)	Data 7.89e-05 (2.71e-04)	Tok/s 214872 (201025)	Loss/tok 3.4581 (4.9435)	LR 2.875e-03
0: TRAIN [0][1110/1291]	Time 0.042 (0.053)	Data 8.80e-05 (2.69e-04)	Tok/s 186434 (201036)	Loss/tok 3.3836 (4.9314)	LR 2.875e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 1024.0
0: TRAIN [0][1120/1291]	Time 0.042 (0.053)	Data 8.37e-05 (2.68e-04)	Tok/s 189211 (201124)	Loss/tok 3.3723 (4.9176)	LR 2.875e-03
0: TRAIN [0][1130/1291]	Time 0.041 (0.053)	Data 8.20e-05 (2.66e-04)	Tok/s 185809 (201155)	Loss/tok 3.4403 (4.9058)	LR 2.875e-03
0: TRAIN [0][1140/1291]	Time 0.042 (0.053)	Data 8.11e-05 (2.64e-04)	Tok/s 192713 (201109)	Loss/tok 3.3212 (4.8955)	LR 2.875e-03
0: TRAIN [0][1150/1291]	Time 0.041 (0.053)	Data 8.30e-05 (2.63e-04)	Tok/s 189380 (201126)	Loss/tok 3.4902 (4.8838)	LR 2.875e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 512.0
0: TRAIN [0][1160/1291]	Time 0.041 (0.053)	Data 1.18e-04 (2.62e-04)	Tok/s 189406 (201083)	Loss/tok 3.4703 (4.8733)	LR 2.875e-03
0: TRAIN [0][1170/1291]	Time 0.058 (0.053)	Data 8.03e-05 (2.60e-04)	Tok/s 218259 (201209)	Loss/tok 3.5190 (4.8602)	LR 2.875e-03
0: TRAIN [0][1180/1291]	Time 0.025 (0.053)	Data 1.34e-04 (2.59e-04)	Tok/s 156549 (201141)	Loss/tok 2.7971 (4.8509)	LR 2.875e-03
0: TRAIN [0][1190/1291]	Time 0.058 (0.053)	Data 1.41e-04 (2.57e-04)	Tok/s 214956 (201136)	Loss/tok 3.6105 (4.8405)	LR 2.875e-03
0: TRAIN [0][1200/1291]	Time 0.024 (0.053)	Data 8.68e-05 (2.56e-04)	Tok/s 163040 (201113)	Loss/tok 2.9961 (4.8305)	LR 2.875e-03
0: TRAIN [0][1210/1291]	Time 0.041 (0.053)	Data 8.54e-05 (2.55e-04)	Tok/s 186698 (201221)	Loss/tok 3.3820 (4.8187)	LR 2.875e-03
0: TRAIN [0][1220/1291]	Time 0.041 (0.053)	Data 8.06e-05 (2.53e-04)	Tok/s 187525 (201242)	Loss/tok 3.4144 (4.8082)	LR 2.875e-03
0: TRAIN [0][1230/1291]	Time 0.041 (0.053)	Data 9.75e-05 (2.52e-04)	Tok/s 192527 (201270)	Loss/tok 3.3339 (4.7979)	LR 2.875e-03
0: TRAIN [0][1240/1291]	Time 0.041 (0.053)	Data 8.68e-05 (2.51e-04)	Tok/s 187078 (201162)	Loss/tok 3.3712 (4.7894)	LR 2.875e-03
0: TRAIN [0][1250/1291]	Time 0.042 (0.053)	Data 9.68e-05 (2.50e-04)	Tok/s 182778 (201240)	Loss/tok 3.3126 (4.7784)	LR 2.875e-03
0: TRAIN [0][1260/1291]	Time 0.058 (0.053)	Data 8.06e-05 (2.48e-04)	Tok/s 218307 (201283)	Loss/tok 3.6570 (4.7683)	LR 2.875e-03
0: TRAIN [0][1270/1291]	Time 0.096 (0.053)	Data 8.08e-05 (2.47e-04)	Tok/s 229307 (201309)	Loss/tok 4.1179 (4.7589)	LR 2.875e-03
0: TRAIN [0][1280/1291]	Time 0.058 (0.053)	Data 8.58e-05 (2.46e-04)	Tok/s 217019 (201361)	Loss/tok 3.5800 (4.7488)	LR 2.875e-03
0: Upscaling, new scale: 1024.0
0: TRAIN [0][1290/1291]	Time 0.042 (0.053)	Data 5.79e-05 (2.46e-04)	Tok/s 185470 (201382)	Loss/tok 3.3790 (4.7391)	LR 2.875e-03
:::MLLOG {"namespace": "", "time_ms": 1593114008626, "event_type": "INTERVAL_END", "key": "epoch_stop", "value": null, "metadata": {"file": "train.py", "lineno": 551, "epoch_num": 1}}
:::MLLOG {"namespace": "", "time_ms": 1593114008626, "event_type": "INTERVAL_START", "key": "eval_start", "value": null, "metadata": {"file": "train.py", "lineno": 556, "epoch_num": 1}}
0: Running evaluation on test set
0: TEST [0][0/3]	Time 0.394 (0.394)	Decoder iters 149.0 (149.0)	Tok/s 21534 (21534)
0: Running moses detokenizer
0: BLEU(score=19.24203795391446, counts=[33613, 15152, 7988, 4363], totals=[63195, 60192, 57190, 54193], precisions=[53.18933459925627, 25.172780435938332, 13.967476831613919, 8.050855276511726], bp=0.9768370778978913, sys_len=63195, ref_len=64676)
0: Finished evaluation on test set
:::MLLOG {"namespace": "", "time_ms": 1593114010090, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.1924, "metadata": {"file": "train.py", "lineno": 562, "epoch_num": 1}}
:::MLLOG {"namespace": "", "time_ms": 1593114010090, "event_type": "INTERVAL_END", "key": "eval_stop", "value": null, "metadata": {"file": "train.py", "lineno": 565, "epoch_num": 1}}
0: Summary: Epoch: 0	Training Loss: 4.7405	Test BLEU: 19.24
0: Performance: Epoch: 0	Training: 3221667 Tok/s
0: Finished epoch 0
:::MLLOG {"namespace": "", "time_ms": 1593114010091, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "train.py", "lineno": 584, "first_epoch_num": 1}}
:::MLLOG {"namespace": "", "time_ms": 1593114010091, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "train.py", "lineno": 538, "first_epoch_num": 2, "epoch_count": 1}}
:::MLLOG {"namespace": "", "time_ms": 1593114010091, "event_type": "INTERVAL_START", "key": "epoch_start", "value": null, "metadata": {"file": "train.py", "lineno": 541, "epoch_num": 2}}
0: Starting epoch 1
0: Sampler for epoch 1 uses seed 2846225885
0: TRAIN [1][0/1291]	Time 0.311 (0.311)	Data 1.81e-01 (1.81e-01)	Tok/s 24663 (24663)	Loss/tok 3.2132 (3.2132)	LR 2.875e-03
0: TRAIN [1][10/1291]	Time 0.077 (0.079)	Data 9.18e-05 (1.65e-02)	Tok/s 231060 (188412)	Loss/tok 3.6493 (3.5367)	LR 2.875e-03
0: TRAIN [1][20/1291]	Time 0.076 (0.068)	Data 9.13e-05 (8.69e-03)	Tok/s 232134 (195832)	Loss/tok 3.6677 (3.5315)	LR 2.875e-03
0: TRAIN [1][30/1291]	Time 0.058 (0.062)	Data 9.01e-05 (5.92e-03)	Tok/s 219205 (198313)	Loss/tok 3.4674 (3.4981)	LR 2.875e-03
0: TRAIN [1][40/1291]	Time 0.058 (0.061)	Data 8.08e-05 (4.50e-03)	Tok/s 216180 (200699)	Loss/tok 3.3800 (3.4896)	LR 2.875e-03
0: TRAIN [1][50/1291]	Time 0.058 (0.059)	Data 8.20e-05 (3.64e-03)	Tok/s 214209 (200882)	Loss/tok 3.5384 (3.4737)	LR 2.875e-03
0: TRAIN [1][60/1291]	Time 0.058 (0.058)	Data 1.38e-04 (3.06e-03)	Tok/s 222855 (202550)	Loss/tok 3.5866 (3.4848)	LR 2.875e-03
0: TRAIN [1][70/1291]	Time 0.058 (0.057)	Data 8.18e-05 (2.64e-03)	Tok/s 219230 (202231)	Loss/tok 3.4635 (3.4769)	LR 2.875e-03
0: TRAIN [1][80/1291]	Time 0.058 (0.056)	Data 7.80e-05 (2.33e-03)	Tok/s 218292 (201047)	Loss/tok 3.4454 (3.4743)	LR 2.875e-03
0: TRAIN [1][90/1291]	Time 0.041 (0.054)	Data 8.58e-05 (2.08e-03)	Tok/s 186890 (199569)	Loss/tok 3.3332 (3.4622)	LR 2.875e-03
0: TRAIN [1][100/1291]	Time 0.077 (0.054)	Data 8.75e-05 (1.88e-03)	Tok/s 226882 (199753)	Loss/tok 3.8023 (3.4623)	LR 2.875e-03
0: TRAIN [1][110/1291]	Time 0.025 (0.053)	Data 8.18e-05 (1.72e-03)	Tok/s 160753 (199115)	Loss/tok 2.9602 (3.4599)	LR 2.875e-03
0: TRAIN [1][120/1291]	Time 0.059 (0.053)	Data 7.89e-05 (1.59e-03)	Tok/s 217420 (199943)	Loss/tok 3.5014 (3.4680)	LR 2.875e-03
0: Upscaling, new scale: 2048.0
0: TRAIN [1][130/1291]	Time 0.042 (0.054)	Data 9.25e-05 (1.47e-03)	Tok/s 186813 (200068)	Loss/tok 3.1516 (3.4749)	LR 2.875e-03
0: TRAIN [1][140/1291]	Time 0.042 (0.054)	Data 8.08e-05 (1.38e-03)	Tok/s 183296 (200539)	Loss/tok 3.1499 (3.4795)	LR 2.875e-03
0: TRAIN [1][150/1291]	Time 0.041 (0.054)	Data 7.82e-05 (1.29e-03)	Tok/s 186534 (200531)	Loss/tok 3.2661 (3.4776)	LR 2.875e-03
0: TRAIN [1][160/1291]	Time 0.097 (0.054)	Data 9.04e-05 (1.22e-03)	Tok/s 234626 (200731)	Loss/tok 3.8009 (3.4814)	LR 2.875e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 1024.0
0: TRAIN [1][170/1291]	Time 0.058 (0.054)	Data 8.68e-05 (1.15e-03)	Tok/s 221071 (201345)	Loss/tok 3.5136 (3.4859)	LR 2.875e-03
0: TRAIN [1][180/1291]	Time 0.024 (0.053)	Data 8.63e-05 (1.09e-03)	Tok/s 159176 (200198)	Loss/tok 2.7218 (3.4768)	LR 2.875e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 512.0
0: TRAIN [1][190/1291]	Time 0.076 (0.053)	Data 8.37e-05 (1.04e-03)	Tok/s 225692 (200053)	Loss/tok 3.7926 (3.4840)	LR 2.875e-03
0: TRAIN [1][200/1291]	Time 0.041 (0.053)	Data 7.99e-05 (9.94e-04)	Tok/s 188672 (200179)	Loss/tok 3.3807 (3.4824)	LR 2.875e-03
0: TRAIN [1][210/1291]	Time 0.058 (0.053)	Data 8.51e-05 (9.51e-04)	Tok/s 215095 (199895)	Loss/tok 3.5609 (3.4793)	LR 2.875e-03
0: TRAIN [1][220/1291]	Time 0.096 (0.053)	Data 8.63e-05 (9.12e-04)	Tok/s 233286 (200048)	Loss/tok 3.8988 (3.4849)	LR 2.875e-03
0: TRAIN [1][230/1291]	Time 0.076 (0.053)	Data 7.87e-05 (8.76e-04)	Tok/s 230642 (200131)	Loss/tok 3.7664 (3.4898)	LR 2.875e-03
0: TRAIN [1][240/1291]	Time 0.041 (0.053)	Data 8.44e-05 (8.44e-04)	Tok/s 188434 (200572)	Loss/tok 3.3001 (3.4957)	LR 2.875e-03
0: TRAIN [1][250/1291]	Time 0.042 (0.053)	Data 8.13e-05 (8.14e-04)	Tok/s 186384 (200706)	Loss/tok 3.2444 (3.4937)	LR 2.875e-03
0: TRAIN [1][260/1291]	Time 0.097 (0.054)	Data 8.61e-05 (7.87e-04)	Tok/s 232068 (201221)	Loss/tok 3.8044 (3.4950)	LR 2.875e-03
0: TRAIN [1][270/1291]	Time 0.041 (0.054)	Data 1.35e-04 (7.61e-04)	Tok/s 182444 (201563)	Loss/tok 3.3398 (3.4952)	LR 2.875e-03
0: TRAIN [1][280/1291]	Time 0.041 (0.054)	Data 8.49e-05 (7.37e-04)	Tok/s 188441 (201487)	Loss/tok 3.3686 (3.4924)	LR 2.875e-03
0: TRAIN [1][290/1291]	Time 0.041 (0.053)	Data 8.20e-05 (7.15e-04)	Tok/s 184605 (201241)	Loss/tok 3.3034 (3.4908)	LR 2.875e-03
0: TRAIN [1][300/1291]	Time 0.041 (0.053)	Data 8.70e-05 (6.95e-04)	Tok/s 182800 (200979)	Loss/tok 3.2122 (3.4860)	LR 2.875e-03
0: Upscaling, new scale: 1024.0
0: TRAIN [1][310/1291]	Time 0.096 (0.053)	Data 8.75e-05 (6.75e-04)	Tok/s 229830 (201211)	Loss/tok 3.9062 (3.4894)	LR 2.875e-03
0: TRAIN [1][320/1291]	Time 0.025 (0.053)	Data 8.49e-05 (6.57e-04)	Tok/s 159653 (201001)	Loss/tok 2.6705 (3.4864)	LR 2.875e-03
0: TRAIN [1][330/1291]	Time 0.058 (0.053)	Data 1.57e-04 (6.40e-04)	Tok/s 216205 (201158)	Loss/tok 3.5452 (3.4867)	LR 2.875e-03
0: TRAIN [1][340/1291]	Time 0.076 (0.053)	Data 1.39e-04 (6.24e-04)	Tok/s 226785 (201228)	Loss/tok 3.7214 (3.4935)	LR 2.875e-03
0: TRAIN [1][350/1291]	Time 0.076 (0.053)	Data 9.30e-05 (6.09e-04)	Tok/s 232117 (201198)	Loss/tok 3.5531 (3.4919)	LR 2.875e-03
0: TRAIN [1][360/1291]	Time 0.058 (0.053)	Data 1.24e-04 (5.95e-04)	Tok/s 218214 (201447)	Loss/tok 3.3842 (3.4909)	LR 2.875e-03
0: TRAIN [1][370/1291]	Time 0.058 (0.053)	Data 7.77e-05 (5.81e-04)	Tok/s 214946 (201574)	Loss/tok 3.6122 (3.4912)	LR 2.875e-03
0: TRAIN [1][380/1291]	Time 0.059 (0.053)	Data 8.32e-05 (5.68e-04)	Tok/s 218980 (201711)	Loss/tok 3.3715 (3.4896)	LR 2.875e-03
0: TRAIN [1][390/1291]	Time 0.059 (0.053)	Data 8.30e-05 (5.56e-04)	Tok/s 218350 (201727)	Loss/tok 3.4044 (3.4885)	LR 2.875e-03
0: TRAIN [1][400/1291]	Time 0.058 (0.053)	Data 8.39e-05 (5.45e-04)	Tok/s 212581 (201735)	Loss/tok 3.5114 (3.4887)	LR 2.875e-03
0: TRAIN [1][410/1291]	Time 0.041 (0.053)	Data 1.51e-04 (5.34e-04)	Tok/s 184155 (201513)	Loss/tok 3.1902 (3.4876)	LR 2.875e-03
0: TRAIN [1][420/1291]	Time 0.042 (0.053)	Data 7.87e-05 (5.23e-04)	Tok/s 189941 (201528)	Loss/tok 3.1779 (3.4868)	LR 2.875e-03
0: TRAIN [1][430/1291]	Time 0.058 (0.053)	Data 1.35e-04 (5.14e-04)	Tok/s 219537 (201344)	Loss/tok 3.3416 (3.4846)	LR 2.875e-03
0: Upscaling, new scale: 2048.0
0: TRAIN [1][440/1291]	Time 0.041 (0.053)	Data 7.89e-05 (5.04e-04)	Tok/s 185729 (201172)	Loss/tok 3.2558 (3.4839)	LR 2.875e-03
0: TRAIN [1][450/1291]	Time 0.041 (0.053)	Data 7.96e-05 (4.95e-04)	Tok/s 184099 (201237)	Loss/tok 3.1758 (3.4871)	LR 2.875e-03
0: TRAIN [1][460/1291]	Time 0.076 (0.053)	Data 8.54e-05 (4.86e-04)	Tok/s 232389 (201438)	Loss/tok 3.5362 (3.4888)	LR 2.875e-03
0: TRAIN [1][470/1291]	Time 0.096 (0.053)	Data 1.37e-04 (4.78e-04)	Tok/s 231133 (201493)	Loss/tok 3.8281 (3.4890)	LR 2.875e-03
0: TRAIN [1][480/1291]	Time 0.076 (0.053)	Data 7.75e-05 (4.70e-04)	Tok/s 227975 (201472)	Loss/tok 3.6224 (3.4880)	LR 2.875e-03
0: TRAIN [1][490/1291]	Time 0.041 (0.053)	Data 8.61e-05 (4.62e-04)	Tok/s 190277 (201500)	Loss/tok 3.2387 (3.4860)	LR 2.875e-03
0: TRAIN [1][500/1291]	Time 0.041 (0.053)	Data 8.49e-05 (4.55e-04)	Tok/s 187475 (201469)	Loss/tok 3.2842 (3.4854)	LR 2.875e-03
0: TRAIN [1][510/1291]	Time 0.058 (0.053)	Data 8.01e-05 (4.47e-04)	Tok/s 218408 (201626)	Loss/tok 3.4848 (3.4847)	LR 2.875e-03
0: TRAIN [1][520/1291]	Time 0.041 (0.053)	Data 8.30e-05 (4.41e-04)	Tok/s 185144 (201528)	Loss/tok 3.0883 (3.4846)	LR 2.875e-03
0: TRAIN [1][530/1291]	Time 0.076 (0.053)	Data 8.85e-05 (4.34e-04)	Tok/s 226504 (201601)	Loss/tok 3.7387 (3.4844)	LR 2.875e-03
0: TRAIN [1][540/1291]	Time 0.025 (0.053)	Data 7.70e-05 (4.28e-04)	Tok/s 159718 (201419)	Loss/tok 2.7082 (3.4821)	LR 2.875e-03
0: TRAIN [1][550/1291]	Time 0.058 (0.053)	Data 8.65e-05 (4.22e-04)	Tok/s 216487 (201348)	Loss/tok 3.5494 (3.4806)	LR 2.875e-03
0: TRAIN [1][560/1291]	Time 0.042 (0.053)	Data 8.06e-05 (4.16e-04)	Tok/s 187015 (201296)	Loss/tok 3.2346 (3.4790)	LR 2.875e-03
0: Upscaling, new scale: 4096.0
0: TRAIN [1][570/1291]	Time 0.041 (0.053)	Data 8.39e-05 (4.10e-04)	Tok/s 192744 (201193)	Loss/tok 3.2430 (3.4758)	LR 2.875e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 2048.0
0: TRAIN [1][580/1291]	Time 0.041 (0.053)	Data 8.27e-05 (4.05e-04)	Tok/s 185444 (201145)	Loss/tok 3.2453 (3.4756)	LR 2.875e-03
0: TRAIN [1][590/1291]	Time 0.076 (0.053)	Data 8.18e-05 (3.99e-04)	Tok/s 231545 (201340)	Loss/tok 3.5319 (3.4761)	LR 2.875e-03
0: TRAIN [1][600/1291]	Time 0.058 (0.053)	Data 1.34e-04 (3.94e-04)	Tok/s 218806 (201325)	Loss/tok 3.4592 (3.4748)	LR 2.875e-03
0: TRAIN [1][610/1291]	Time 0.076 (0.053)	Data 7.92e-05 (3.89e-04)	Tok/s 231101 (201473)	Loss/tok 3.6611 (3.4742)	LR 2.875e-03
0: TRAIN [1][620/1291]	Time 0.075 (0.053)	Data 9.20e-05 (3.84e-04)	Tok/s 234325 (201407)	Loss/tok 3.5056 (3.4727)	LR 2.875e-03
0: TRAIN [1][630/1291]	Time 0.076 (0.053)	Data 1.40e-04 (3.80e-04)	Tok/s 229540 (201366)	Loss/tok 3.6184 (3.4704)	LR 2.875e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 1024.0
0: TRAIN [1][640/1291]	Time 0.096 (0.053)	Data 8.34e-05 (3.75e-04)	Tok/s 229987 (201485)	Loss/tok 3.8546 (3.4712)	LR 2.875e-03
0: TRAIN [1][650/1291]	Time 0.076 (0.053)	Data 8.30e-05 (3.71e-04)	Tok/s 227511 (201681)	Loss/tok 3.5961 (3.4725)	LR 2.875e-03
0: TRAIN [1][660/1291]	Time 0.058 (0.053)	Data 8.56e-05 (3.67e-04)	Tok/s 217305 (201697)	Loss/tok 3.5245 (3.4720)	LR 2.875e-03
0: TRAIN [1][670/1291]	Time 0.058 (0.053)	Data 8.61e-05 (3.63e-04)	Tok/s 219827 (201663)	Loss/tok 3.4829 (3.4710)	LR 2.875e-03
0: TRAIN [1][680/1291]	Time 0.041 (0.053)	Data 1.44e-04 (3.59e-04)	Tok/s 186566 (201532)	Loss/tok 3.3841 (3.4693)	LR 2.875e-03
0: TRAIN [1][690/1291]	Time 0.059 (0.053)	Data 1.02e-04 (3.55e-04)	Tok/s 213302 (201614)	Loss/tok 3.5928 (3.4689)	LR 2.875e-03
0: TRAIN [1][700/1291]	Time 0.058 (0.053)	Data 8.49e-05 (3.52e-04)	Tok/s 211576 (201659)	Loss/tok 3.5048 (3.4692)	LR 2.875e-03
0: TRAIN [1][710/1291]	Time 0.058 (0.053)	Data 8.23e-05 (3.48e-04)	Tok/s 217455 (201704)	Loss/tok 3.4034 (3.4685)	LR 2.875e-03
0: TRAIN [1][720/1291]	Time 0.025 (0.053)	Data 8.32e-05 (3.44e-04)	Tok/s 165569 (201659)	Loss/tok 2.7123 (3.4676)	LR 2.875e-03
0: TRAIN [1][730/1291]	Time 0.058 (0.053)	Data 8.03e-05 (3.41e-04)	Tok/s 216753 (201603)	Loss/tok 3.4858 (3.4670)	LR 2.875e-03
0: TRAIN [1][740/1291]	Time 0.042 (0.053)	Data 7.99e-05 (3.38e-04)	Tok/s 184400 (201566)	Loss/tok 3.3293 (3.4664)	LR 2.875e-03
0: TRAIN [1][750/1291]	Time 0.058 (0.053)	Data 8.23e-05 (3.34e-04)	Tok/s 214977 (201601)	Loss/tok 3.2926 (3.4646)	LR 2.875e-03
0: TRAIN [1][760/1291]	Time 0.058 (0.053)	Data 8.13e-05 (3.31e-04)	Tok/s 221019 (201636)	Loss/tok 3.2544 (3.4625)	LR 2.875e-03
0: Upscaling, new scale: 2048.0
0: TRAIN [1][770/1291]	Time 0.058 (0.053)	Data 1.39e-04 (3.28e-04)	Tok/s 214668 (201639)	Loss/tok 3.3929 (3.4608)	LR 2.875e-03
0: TRAIN [1][780/1291]	Time 0.058 (0.053)	Data 8.13e-05 (3.25e-04)	Tok/s 216601 (201650)	Loss/tok 3.5818 (3.4608)	LR 2.875e-03
0: TRAIN [1][790/1291]	Time 0.058 (0.053)	Data 8.68e-05 (3.22e-04)	Tok/s 216903 (201535)	Loss/tok 3.5811 (3.4587)	LR 2.875e-03
0: TRAIN [1][800/1291]	Time 0.041 (0.053)	Data 1.38e-04 (3.19e-04)	Tok/s 188728 (201559)	Loss/tok 3.1205 (3.4582)	LR 2.875e-03
0: TRAIN [1][810/1291]	Time 0.097 (0.053)	Data 7.77e-05 (3.17e-04)	Tok/s 231374 (201736)	Loss/tok 3.6968 (3.4588)	LR 2.875e-03
0: TRAIN [1][820/1291]	Time 0.042 (0.053)	Data 8.39e-05 (3.14e-04)	Tok/s 185792 (201706)	Loss/tok 3.1324 (3.4585)	LR 2.875e-03
0: TRAIN [1][830/1291]	Time 0.041 (0.053)	Data 1.37e-04 (3.11e-04)	Tok/s 184287 (201668)	Loss/tok 3.1220 (3.4565)	LR 2.875e-03
0: TRAIN [1][840/1291]	Time 0.058 (0.053)	Data 1.37e-04 (3.09e-04)	Tok/s 214676 (201729)	Loss/tok 3.2799 (3.4562)	LR 2.875e-03
0: TRAIN [1][850/1291]	Time 0.097 (0.053)	Data 8.42e-05 (3.06e-04)	Tok/s 230103 (201853)	Loss/tok 3.8005 (3.4568)	LR 2.875e-03
0: TRAIN [1][860/1291]	Time 0.041 (0.053)	Data 7.58e-05 (3.04e-04)	Tok/s 186572 (201792)	Loss/tok 3.2353 (3.4569)	LR 2.875e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 1024.0
0: TRAIN [1][870/1291]	Time 0.041 (0.053)	Data 7.96e-05 (3.01e-04)	Tok/s 186057 (201775)	Loss/tok 3.1697 (3.4578)	LR 2.875e-03
0: TRAIN [1][880/1291]	Time 0.042 (0.053)	Data 8.25e-05 (2.99e-04)	Tok/s 181922 (201675)	Loss/tok 3.2362 (3.4569)	LR 2.875e-03
0: TRAIN [1][890/1291]	Time 0.059 (0.053)	Data 1.46e-04 (2.97e-04)	Tok/s 217118 (201775)	Loss/tok 3.4266 (3.4586)	LR 2.875e-03
0: TRAIN [1][900/1291]	Time 0.041 (0.053)	Data 8.13e-05 (2.95e-04)	Tok/s 187966 (201797)	Loss/tok 3.1975 (3.4585)	LR 2.875e-03
0: TRAIN [1][910/1291]	Time 0.042 (0.053)	Data 7.72e-05 (2.93e-04)	Tok/s 180472 (201880)	Loss/tok 3.1094 (3.4582)	LR 2.875e-03
0: TRAIN [1][920/1291]	Time 0.058 (0.053)	Data 1.37e-04 (2.90e-04)	Tok/s 213000 (201793)	Loss/tok 3.4232 (3.4566)	LR 2.875e-03
0: TRAIN [1][930/1291]	Time 0.058 (0.053)	Data 8.15e-05 (2.88e-04)	Tok/s 219729 (201722)	Loss/tok 3.4449 (3.4550)	LR 2.875e-03
0: TRAIN [1][940/1291]	Time 0.025 (0.053)	Data 8.01e-05 (2.86e-04)	Tok/s 158533 (201674)	Loss/tok 2.5614 (3.4542)	LR 2.875e-03
0: TRAIN [1][950/1291]	Time 0.041 (0.053)	Data 8.03e-05 (2.84e-04)	Tok/s 189463 (201673)	Loss/tok 3.1992 (3.4542)	LR 2.875e-03
0: TRAIN [1][960/1291]	Time 0.041 (0.053)	Data 8.03e-05 (2.82e-04)	Tok/s 189162 (201599)	Loss/tok 3.1127 (3.4531)	LR 2.875e-03
0: TRAIN [1][970/1291]	Time 0.025 (0.053)	Data 7.96e-05 (2.80e-04)	Tok/s 163005 (201445)	Loss/tok 2.5830 (3.4518)	LR 2.875e-03
0: TRAIN [1][980/1291]	Time 0.041 (0.053)	Data 1.36e-04 (2.78e-04)	Tok/s 185589 (201431)	Loss/tok 3.1494 (3.4506)	LR 2.875e-03
0: Upscaling, new scale: 2048.0
0: TRAIN [1][990/1291]	Time 0.097 (0.053)	Data 8.27e-05 (2.76e-04)	Tok/s 231132 (201425)	Loss/tok 3.6620 (3.4502)	LR 2.875e-03
0: TRAIN [1][1000/1291]	Time 0.076 (0.053)	Data 8.56e-05 (2.74e-04)	Tok/s 229466 (201528)	Loss/tok 3.5139 (3.4501)	LR 2.875e-03
0: TRAIN [1][1010/1291]	Time 0.058 (0.053)	Data 8.46e-05 (2.73e-04)	Tok/s 214796 (201489)	Loss/tok 3.4906 (3.4493)	LR 2.875e-03
0: TRAIN [1][1020/1291]	Time 0.041 (0.053)	Data 8.01e-05 (2.71e-04)	Tok/s 187062 (201463)	Loss/tok 3.1624 (3.4476)	LR 2.875e-03
0: TRAIN [1][1030/1291]	Time 0.041 (0.053)	Data 1.35e-04 (2.69e-04)	Tok/s 189092 (201433)	Loss/tok 3.2278 (3.4473)	LR 2.875e-03
0: TRAIN [1][1040/1291]	Time 0.041 (0.053)	Data 7.89e-05 (2.68e-04)	Tok/s 187207 (201413)	Loss/tok 3.1154 (3.4464)	LR 2.875e-03
0: TRAIN [1][1050/1291]	Time 0.042 (0.053)	Data 7.92e-05 (2.66e-04)	Tok/s 187266 (201325)	Loss/tok 3.0917 (3.4449)	LR 2.875e-03
0: TRAIN [1][1060/1291]	Time 0.058 (0.053)	Data 1.40e-04 (2.64e-04)	Tok/s 219286 (201351)	Loss/tok 3.3060 (3.4449)	LR 2.875e-03
0: TRAIN [1][1070/1291]	Time 0.059 (0.053)	Data 7.87e-05 (2.63e-04)	Tok/s 212303 (201359)	Loss/tok 3.1992 (3.4437)	LR 2.875e-03
0: TRAIN [1][1080/1291]	Time 0.041 (0.053)	Data 1.33e-04 (2.61e-04)	Tok/s 188198 (201435)	Loss/tok 3.2484 (3.4427)	LR 2.875e-03
0: TRAIN [1][1090/1291]	Time 0.059 (0.053)	Data 8.54e-05 (2.60e-04)	Tok/s 217484 (201571)	Loss/tok 3.3970 (3.4423)	LR 2.875e-03
0: TRAIN [1][1100/1291]	Time 0.058 (0.053)	Data 7.94e-05 (2.58e-04)	Tok/s 214135 (201552)	Loss/tok 3.4960 (3.4409)	LR 2.875e-03
0: TRAIN [1][1110/1291]	Time 0.097 (0.053)	Data 7.92e-05 (2.57e-04)	Tok/s 230556 (201696)	Loss/tok 3.6936 (3.4420)	LR 2.875e-03
0: Upscaling, new scale: 4096.0
0: Gradient norm: inf
0: Skipped batch, new scale: 2048.0
0: TRAIN [1][1120/1291]	Time 0.097 (0.053)	Data 7.80e-05 (2.55e-04)	Tok/s 231048 (201826)	Loss/tok 3.9123 (3.4427)	LR 2.875e-03
0: TRAIN [1][1130/1291]	Time 0.042 (0.053)	Data 8.61e-05 (2.54e-04)	Tok/s 186777 (201673)	Loss/tok 3.1711 (3.4407)	LR 2.875e-03
0: TRAIN [1][1140/1291]	Time 0.059 (0.053)	Data 8.73e-05 (2.52e-04)	Tok/s 210488 (201720)	Loss/tok 3.5123 (3.4397)	LR 2.875e-03
0: TRAIN [1][1150/1291]	Time 0.076 (0.053)	Data 7.89e-05 (2.51e-04)	Tok/s 229015 (201761)	Loss/tok 3.5986 (3.4401)	LR 2.875e-03
0: TRAIN [1][1160/1291]	Time 0.042 (0.053)	Data 8.30e-05 (2.49e-04)	Tok/s 187731 (201732)	Loss/tok 3.1436 (3.4398)	LR 2.875e-03
0: TRAIN [1][1170/1291]	Time 0.042 (0.053)	Data 8.49e-05 (2.48e-04)	Tok/s 182467 (201722)	Loss/tok 3.0976 (3.4385)	LR 2.875e-03
0: TRAIN [1][1180/1291]	Time 0.059 (0.053)	Data 8.03e-05 (2.47e-04)	Tok/s 214429 (201758)	Loss/tok 3.3164 (3.4379)	LR 2.875e-03
0: TRAIN [1][1190/1291]	Time 0.025 (0.053)	Data 8.46e-05 (2.46e-04)	Tok/s 157290 (201727)	Loss/tok 2.6618 (3.4376)	LR 2.875e-03
0: TRAIN [1][1200/1291]	Time 0.058 (0.053)	Data 7.70e-05 (2.44e-04)	Tok/s 213602 (201716)	Loss/tok 3.4928 (3.4370)	LR 2.875e-03
0: TRAIN [1][1210/1291]	Time 0.025 (0.053)	Data 8.30e-05 (2.43e-04)	Tok/s 159055 (201645)	Loss/tok 2.5916 (3.4355)	LR 2.875e-03
0: TRAIN [1][1220/1291]	Time 0.058 (0.053)	Data 8.23e-05 (2.42e-04)	Tok/s 216286 (201646)	Loss/tok 3.4155 (3.4351)	LR 2.875e-03
0: TRAIN [1][1230/1291]	Time 0.058 (0.053)	Data 7.99e-05 (2.41e-04)	Tok/s 215525 (201616)	Loss/tok 3.4716 (3.4338)	LR 2.875e-03
0: TRAIN [1][1240/1291]	Time 0.041 (0.053)	Data 8.25e-05 (2.40e-04)	Tok/s 186293 (201555)	Loss/tok 3.1028 (3.4326)	LR 2.875e-03
0: Upscaling, new scale: 4096.0
0: Gradient norm: inf
0: Skipped batch, new scale: 2048.0
0: TRAIN [1][1250/1291]	Time 0.076 (0.053)	Data 1.61e-04 (2.39e-04)	Tok/s 229813 (201540)	Loss/tok 3.5272 (3.4320)	LR 2.875e-03
0: TRAIN [1][1260/1291]	Time 0.058 (0.053)	Data 1.38e-04 (2.37e-04)	Tok/s 219515 (201499)	Loss/tok 3.3011 (3.4309)	LR 2.875e-03
0: TRAIN [1][1270/1291]	Time 0.025 (0.053)	Data 8.63e-05 (2.36e-04)	Tok/s 159072 (201509)	Loss/tok 2.6241 (3.4302)	LR 2.875e-03
0: TRAIN [1][1280/1291]	Time 0.076 (0.053)	Data 1.37e-04 (2.35e-04)	Tok/s 228200 (201521)	Loss/tok 3.7203 (3.4303)	LR 2.875e-03
0: TRAIN [1][1290/1291]	Time 0.058 (0.053)	Data 4.17e-05 (2.36e-04)	Tok/s 217171 (201420)	Loss/tok 3.3327 (3.4293)	LR 2.875e-03
:::MLLOG {"namespace": "", "time_ms": 1593114078307, "event_type": "INTERVAL_END", "key": "epoch_stop", "value": null, "metadata": {"file": "train.py", "lineno": 551, "epoch_num": 2}}
:::MLLOG {"namespace": "", "time_ms": 1593114078307, "event_type": "INTERVAL_START", "key": "eval_start", "value": null, "metadata": {"file": "train.py", "lineno": 556, "epoch_num": 2}}
0: Running evaluation on test set
0: TEST [1][0/3]	Time 0.310 (0.310)	Decoder iters 113.0 (113.0)	Tok/s 28665 (28665)
0: Running moses detokenizer
0: BLEU(score=22.140814784425967, counts=[36040, 17365, 9584, 5494], totals=[65448, 62445, 59442, 56446], precisions=[55.0666177728884, 27.808471454880294, 16.123279835806333, 9.733196329235021], bp=1.0, sys_len=65448, ref_len=64676)
0: Finished evaluation on test set
:::MLLOG {"namespace": "", "time_ms": 1593114079664, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.2214, "metadata": {"file": "train.py", "lineno": 562, "epoch_num": 2}}
:::MLLOG {"namespace": "", "time_ms": 1593114079664, "event_type": "INTERVAL_END", "key": "eval_stop", "value": null, "metadata": {"file": "train.py", "lineno": 565, "epoch_num": 2}}
0: Summary: Epoch: 1	Training Loss: 3.4262	Test BLEU: 22.14
0: Performance: Epoch: 1	Training: 3223314 Tok/s
0: Finished epoch 1
:::MLLOG {"namespace": "", "time_ms": 1593114079665, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "train.py", "lineno": 584, "first_epoch_num": 2}}
:::MLLOG {"namespace": "", "time_ms": 1593114079665, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "train.py", "lineno": 538, "first_epoch_num": 3, "epoch_count": 1}}
:::MLLOG {"namespace": "", "time_ms": 1593114079665, "event_type": "INTERVAL_START", "key": "epoch_start", "value": null, "metadata": {"file": "train.py", "lineno": 541, "epoch_num": 3}}
0: Starting epoch 2
0: Sampler for epoch 2 uses seed 3303774051
0: TRAIN [2][0/1291]	Time 0.315 (0.315)	Data 1.91e-01 (1.91e-01)	Tok/s 55688 (55688)	Loss/tok 3.4599 (3.4599)	LR 2.875e-03
0: TRAIN [2][10/1291]	Time 0.058 (0.084)	Data 1.45e-04 (1.74e-02)	Tok/s 219011 (195592)	Loss/tok 3.2917 (3.4115)	LR 2.875e-03
0: TRAIN [2][20/1291]	Time 0.077 (0.075)	Data 9.27e-05 (9.18e-03)	Tok/s 231534 (204425)	Loss/tok 3.4083 (3.3927)	LR 2.875e-03
0: TRAIN [2][30/1291]	Time 0.096 (0.065)	Data 1.27e-04 (6.25e-03)	Tok/s 233506 (199934)	Loss/tok 3.5425 (3.3470)	LR 2.875e-03
0: TRAIN [2][40/1291]	Time 0.025 (0.062)	Data 9.68e-05 (4.75e-03)	Tok/s 158939 (201013)	Loss/tok 2.6223 (3.3283)	LR 2.875e-03
0: TRAIN [2][50/1291]	Time 0.058 (0.061)	Data 8.82e-05 (3.84e-03)	Tok/s 221071 (202570)	Loss/tok 3.1475 (3.3261)	LR 2.875e-03
0: TRAIN [2][60/1291]	Time 0.041 (0.060)	Data 9.61e-05 (3.23e-03)	Tok/s 188614 (202755)	Loss/tok 3.0794 (3.3273)	LR 2.875e-03
0: TRAIN [2][70/1291]	Time 0.042 (0.058)	Data 9.30e-05 (2.79e-03)	Tok/s 184702 (201733)	Loss/tok 3.0868 (3.3102)	LR 2.875e-03
0: TRAIN [2][80/1291]	Time 0.076 (0.058)	Data 9.61e-05 (2.46e-03)	Tok/s 228675 (202276)	Loss/tok 3.5182 (3.3087)	LR 2.875e-03
0: Upscaling, new scale: 4096.0
0: TRAIN [2][90/1291]	Time 0.042 (0.058)	Data 8.87e-05 (2.20e-03)	Tok/s 181247 (201141)	Loss/tok 3.1804 (3.3168)	LR 2.875e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 2048.0
0: TRAIN [2][100/1291]	Time 0.058 (0.057)	Data 9.37e-05 (1.99e-03)	Tok/s 216644 (200862)	Loss/tok 3.1438 (3.3162)	LR 2.875e-03
0: TRAIN [2][110/1291]	Time 0.025 (0.056)	Data 8.68e-05 (1.82e-03)	Tok/s 159195 (200294)	Loss/tok 2.6299 (3.3087)	LR 2.875e-03
0: TRAIN [2][120/1291]	Time 0.041 (0.056)	Data 8.68e-05 (1.68e-03)	Tok/s 191086 (200198)	Loss/tok 3.1011 (3.3051)	LR 2.875e-03
0: TRAIN [2][130/1291]	Time 0.058 (0.056)	Data 1.48e-04 (1.56e-03)	Tok/s 221134 (200109)	Loss/tok 3.2173 (3.3041)	LR 2.875e-03
0: TRAIN [2][140/1291]	Time 0.076 (0.055)	Data 8.89e-05 (1.45e-03)	Tok/s 229026 (199863)	Loss/tok 3.3266 (3.2989)	LR 2.875e-03
0: TRAIN [2][150/1291]	Time 0.042 (0.055)	Data 1.40e-04 (1.37e-03)	Tok/s 185090 (200274)	Loss/tok 3.0308 (3.3006)	LR 2.875e-03
0: TRAIN [2][160/1291]	Time 0.059 (0.055)	Data 8.96e-05 (1.29e-03)	Tok/s 214764 (200779)	Loss/tok 3.2025 (3.3040)	LR 2.875e-03
0: TRAIN [2][170/1291]	Time 0.076 (0.056)	Data 8.99e-05 (1.22e-03)	Tok/s 232465 (201290)	Loss/tok 3.4245 (3.3030)	LR 2.875e-03
0: TRAIN [2][180/1291]	Time 0.076 (0.055)	Data 8.61e-05 (1.15e-03)	Tok/s 232659 (201097)	Loss/tok 3.4159 (3.2976)	LR 2.875e-03
0: TRAIN [2][190/1291]	Time 0.042 (0.055)	Data 8.96e-05 (1.10e-03)	Tok/s 185990 (201234)	Loss/tok 3.0737 (3.2987)	LR 2.875e-03
0: TRAIN [2][200/1291]	Time 0.058 (0.055)	Data 9.23e-05 (1.05e-03)	Tok/s 216193 (201285)	Loss/tok 3.2550 (3.2951)	LR 2.875e-03
0: TRAIN [2][210/1291]	Time 0.042 (0.056)	Data 9.06e-05 (1.01e-03)	Tok/s 186547 (201563)	Loss/tok 3.1350 (3.2984)	LR 2.875e-03
0: Upscaling, new scale: 4096.0
0: TRAIN [2][220/1291]	Time 0.041 (0.055)	Data 8.92e-05 (9.64e-04)	Tok/s 192544 (201530)	Loss/tok 3.0764 (3.2948)	LR 2.875e-03
0: TRAIN [2][230/1291]	Time 0.025 (0.055)	Data 8.94e-05 (9.27e-04)	Tok/s 161049 (200848)	Loss/tok 2.5822 (3.2908)	LR 2.875e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 2048.0
0: TRAIN [2][240/1291]	Time 0.042 (0.055)	Data 9.92e-05 (8.92e-04)	Tok/s 188572 (200975)	Loss/tok 2.9500 (3.2923)	LR 2.875e-03
0: TRAIN [2][250/1291]	Time 0.096 (0.055)	Data 1.39e-04 (8.61e-04)	Tok/s 230766 (200802)	Loss/tok 3.5532 (3.2907)	LR 2.875e-03
0: TRAIN [2][260/1291]	Time 0.076 (0.054)	Data 8.46e-05 (8.32e-04)	Tok/s 231481 (200161)	Loss/tok 3.4203 (3.2885)	LR 2.875e-03
0: TRAIN [2][270/1291]	Time 0.077 (0.054)	Data 6.72e-04 (8.07e-04)	Tok/s 231739 (200118)	Loss/tok 3.4048 (3.2918)	LR 2.875e-03
0: TRAIN [2][280/1291]	Time 0.058 (0.054)	Data 8.99e-05 (7.82e-04)	Tok/s 222111 (200159)	Loss/tok 3.1933 (3.2910)	LR 2.875e-03
0: TRAIN [2][290/1291]	Time 0.058 (0.054)	Data 8.73e-05 (7.58e-04)	Tok/s 215892 (200063)	Loss/tok 3.3920 (3.2878)	LR 2.875e-03
0: TRAIN [2][300/1291]	Time 0.041 (0.054)	Data 8.65e-05 (7.37e-04)	Tok/s 187718 (199906)	Loss/tok 3.0604 (3.2870)	LR 2.875e-03
0: TRAIN [2][310/1291]	Time 0.042 (0.054)	Data 1.41e-04 (7.16e-04)	Tok/s 185991 (199872)	Loss/tok 3.1283 (3.2870)	LR 2.875e-03
0: TRAIN [2][320/1291]	Time 0.097 (0.054)	Data 9.20e-05 (6.97e-04)	Tok/s 230383 (200050)	Loss/tok 3.5881 (3.2881)	LR 2.875e-03
0: TRAIN [2][330/1291]	Time 0.042 (0.054)	Data 8.73e-05 (6.79e-04)	Tok/s 182977 (199748)	Loss/tok 3.0753 (3.2856)	LR 2.875e-03
0: TRAIN [2][340/1291]	Time 0.059 (0.054)	Data 8.51e-05 (6.63e-04)	Tok/s 212231 (199816)	Loss/tok 3.3180 (3.2857)	LR 2.875e-03
0: TRAIN [2][350/1291]	Time 0.042 (0.053)	Data 9.27e-05 (6.47e-04)	Tok/s 185380 (199749)	Loss/tok 3.1094 (3.2839)	LR 2.875e-03
0: TRAIN [2][360/1291]	Time 0.025 (0.053)	Data 8.77e-05 (6.32e-04)	Tok/s 165307 (199553)	Loss/tok 2.5500 (3.2823)	LR 2.875e-03
0: Upscaling, new scale: 4096.0
0: TRAIN [2][370/1291]	Time 0.041 (0.053)	Data 8.87e-05 (6.18e-04)	Tok/s 184315 (199350)	Loss/tok 3.0329 (3.2804)	LR 2.875e-03
0: TRAIN [2][380/1291]	Time 0.098 (0.053)	Data 8.54e-05 (6.04e-04)	Tok/s 226417 (199513)	Loss/tok 3.7031 (3.2809)	LR 2.875e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 2048.0
0: TRAIN [2][390/1291]	Time 0.058 (0.053)	Data 8.73e-05 (5.91e-04)	Tok/s 213293 (199521)	Loss/tok 3.3514 (3.2813)	LR 2.875e-03
0: TRAIN [2][400/1291]	Time 0.097 (0.053)	Data 8.49e-05 (5.79e-04)	Tok/s 179196 (199511)	Loss/tok 3.4015 (3.2808)	LR 2.875e-03
0: TRAIN [2][410/1291]	Time 0.024 (0.053)	Data 8.87e-05 (5.68e-04)	Tok/s 164794 (199274)	Loss/tok 2.7391 (3.2796)	LR 2.875e-03
0: TRAIN [2][420/1291]	Time 0.058 (0.053)	Data 1.41e-04 (5.57e-04)	Tok/s 219545 (199297)	Loss/tok 3.3159 (3.2777)	LR 2.875e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 1024.0
0: TRAIN [2][430/1291]	Time 0.042 (0.053)	Data 8.68e-05 (5.46e-04)	Tok/s 182259 (199317)	Loss/tok 2.9346 (3.2810)	LR 2.875e-03
0: TRAIN [2][440/1291]	Time 0.058 (0.053)	Data 1.44e-04 (5.36e-04)	Tok/s 219588 (199319)	Loss/tok 3.2951 (3.2810)	LR 2.875e-03
0: TRAIN [2][450/1291]	Time 0.097 (0.053)	Data 8.96e-05 (5.26e-04)	Tok/s 229931 (199267)	Loss/tok 3.6769 (3.2818)	LR 2.875e-03
0: TRAIN [2][460/1291]	Time 0.042 (0.053)	Data 8.51e-05 (5.17e-04)	Tok/s 179279 (199365)	Loss/tok 3.0799 (3.2838)	LR 2.875e-03
0: TRAIN [2][470/1291]	Time 0.076 (0.053)	Data 1.38e-04 (5.08e-04)	Tok/s 230848 (199681)	Loss/tok 3.4396 (3.2857)	LR 2.875e-03
0: TRAIN [2][480/1291]	Time 0.059 (0.053)	Data 8.89e-05 (5.00e-04)	Tok/s 213272 (199885)	Loss/tok 3.2332 (3.2864)	LR 2.875e-03
0: TRAIN [2][490/1291]	Time 0.077 (0.053)	Data 9.20e-05 (4.92e-04)	Tok/s 229699 (200027)	Loss/tok 3.4337 (3.2864)	LR 2.875e-03
0: TRAIN [2][500/1291]	Time 0.058 (0.053)	Data 8.75e-05 (4.84e-04)	Tok/s 219534 (200128)	Loss/tok 3.2821 (3.2869)	LR 2.875e-03
0: TRAIN [2][510/1291]	Time 0.042 (0.054)	Data 8.89e-05 (4.76e-04)	Tok/s 186741 (200088)	Loss/tok 2.9934 (3.2897)	LR 2.875e-03
0: TRAIN [2][520/1291]	Time 0.076 (0.054)	Data 1.39e-04 (4.69e-04)	Tok/s 233228 (200352)	Loss/tok 3.4678 (3.2912)	LR 2.875e-03
0: TRAIN [2][530/1291]	Time 0.042 (0.054)	Data 8.75e-05 (4.62e-04)	Tok/s 182297 (200581)	Loss/tok 3.0923 (3.2929)	LR 2.875e-03
0: TRAIN [2][540/1291]	Time 0.058 (0.054)	Data 9.04e-05 (4.55e-04)	Tok/s 216895 (200319)	Loss/tok 3.2591 (3.2906)	LR 2.875e-03
0: TRAIN [2][550/1291]	Time 0.058 (0.054)	Data 8.89e-05 (4.49e-04)	Tok/s 215274 (200318)	Loss/tok 3.2964 (3.2898)	LR 2.875e-03
0: Upscaling, new scale: 2048.0
0: TRAIN [2][560/1291]	Time 0.059 (0.054)	Data 8.75e-05 (4.43e-04)	Tok/s 214998 (200476)	Loss/tok 3.3008 (3.2911)	LR 2.875e-03
0: TRAIN [2][570/1291]	Time 0.058 (0.054)	Data 8.58e-05 (4.37e-04)	Tok/s 219276 (200379)	Loss/tok 3.2428 (3.2895)	LR 2.875e-03
0: TRAIN [2][580/1291]	Time 0.025 (0.053)	Data 8.82e-05 (4.31e-04)	Tok/s 153554 (200239)	Loss/tok 2.5648 (3.2891)	LR 2.875e-03
0: TRAIN [2][590/1291]	Time 0.058 (0.053)	Data 9.13e-05 (4.25e-04)	Tok/s 216964 (200218)	Loss/tok 3.4160 (3.2891)	LR 2.875e-03
0: TRAIN [2][600/1291]	Time 0.076 (0.054)	Data 1.39e-04 (4.20e-04)	Tok/s 231206 (200520)	Loss/tok 3.4737 (3.2917)	LR 2.875e-03
0: TRAIN [2][610/1291]	Time 0.025 (0.054)	Data 8.92e-05 (4.15e-04)	Tok/s 160290 (200609)	Loss/tok 2.8045 (3.2923)	LR 2.875e-03
0: TRAIN [2][620/1291]	Time 0.058 (0.054)	Data 8.96e-05 (4.09e-04)	Tok/s 217182 (200441)	Loss/tok 3.3105 (3.2905)	LR 2.875e-03
0: TRAIN [2][630/1291]	Time 0.042 (0.054)	Data 9.68e-05 (4.04e-04)	Tok/s 189420 (200445)	Loss/tok 3.0954 (3.2907)	LR 2.875e-03
0: TRAIN [2][640/1291]	Time 0.058 (0.053)	Data 1.44e-04 (4.00e-04)	Tok/s 218454 (200442)	Loss/tok 3.3578 (3.2895)	LR 2.875e-03
0: TRAIN [2][650/1291]	Time 0.097 (0.054)	Data 1.36e-04 (3.95e-04)	Tok/s 229856 (200575)	Loss/tok 3.5707 (3.2895)	LR 2.875e-03
0: TRAIN [2][660/1291]	Time 0.058 (0.053)	Data 9.13e-05 (3.91e-04)	Tok/s 215790 (200587)	Loss/tok 3.4090 (3.2887)	LR 2.875e-03
0: TRAIN [2][670/1291]	Time 0.059 (0.054)	Data 8.68e-05 (3.87e-04)	Tok/s 209107 (200700)	Loss/tok 3.3981 (3.2897)	LR 2.875e-03
0: TRAIN [2][680/1291]	Time 0.058 (0.054)	Data 8.82e-05 (3.82e-04)	Tok/s 216074 (200952)	Loss/tok 3.3508 (3.2907)	LR 2.875e-03
0: Upscaling, new scale: 4096.0
0: TRAIN [2][690/1291]	Time 0.025 (0.054)	Data 1.42e-04 (3.78e-04)	Tok/s 161519 (200954)	Loss/tok 2.6273 (3.2895)	LR 2.875e-03
0: TRAIN [2][700/1291]	Time 0.041 (0.054)	Data 8.94e-05 (3.74e-04)	Tok/s 189647 (200888)	Loss/tok 3.0472 (3.2885)	LR 2.875e-03
0: TRAIN [2][710/1291]	Time 0.042 (0.054)	Data 1.50e-04 (3.71e-04)	Tok/s 187555 (200779)	Loss/tok 2.9248 (3.2876)	LR 2.875e-03
0: TRAIN [2][720/1291]	Time 0.059 (0.054)	Data 9.04e-05 (3.67e-04)	Tok/s 215733 (200861)	Loss/tok 3.1553 (3.2869)	LR 2.875e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 2048.0
0: TRAIN [2][730/1291]	Time 0.041 (0.054)	Data 1.39e-04 (3.64e-04)	Tok/s 186409 (200786)	Loss/tok 3.1604 (3.2865)	LR 2.875e-03
0: TRAIN [2][740/1291]	Time 0.041 (0.054)	Data 9.13e-05 (3.60e-04)	Tok/s 190749 (200858)	Loss/tok 3.0813 (3.2864)	LR 2.875e-03
0: TRAIN [2][750/1291]	Time 0.042 (0.054)	Data 9.01e-05 (3.57e-04)	Tok/s 187811 (200844)	Loss/tok 3.0441 (3.2868)	LR 2.875e-03
0: TRAIN [2][760/1291]	Time 0.058 (0.054)	Data 8.82e-05 (3.53e-04)	Tok/s 217807 (200908)	Loss/tok 3.2004 (3.2868)	LR 2.875e-03
0: TRAIN [2][770/1291]	Time 0.058 (0.054)	Data 9.30e-05 (3.50e-04)	Tok/s 218159 (200899)	Loss/tok 3.2087 (3.2859)	LR 2.875e-03
0: TRAIN [2][780/1291]	Time 0.025 (0.054)	Data 9.04e-05 (3.47e-04)	Tok/s 166676 (200966)	Loss/tok 2.5842 (3.2867)	LR 2.875e-03
0: TRAIN [2][790/1291]	Time 0.041 (0.054)	Data 1.72e-04 (3.44e-04)	Tok/s 188906 (200959)	Loss/tok 3.1715 (3.2861)	LR 2.875e-03
0: TRAIN [2][800/1291]	Time 0.076 (0.054)	Data 8.77e-05 (3.41e-04)	Tok/s 229504 (201000)	Loss/tok 3.4685 (3.2862)	LR 2.875e-03
0: TRAIN [2][810/1291]	Time 0.042 (0.054)	Data 8.56e-05 (3.38e-04)	Tok/s 184292 (201016)	Loss/tok 3.0163 (3.2859)	LR 2.875e-03
0: TRAIN [2][820/1291]	Time 0.042 (0.054)	Data 1.04e-04 (3.35e-04)	Tok/s 186758 (200997)	Loss/tok 3.1658 (3.2857)	LR 2.875e-03
0: TRAIN [2][830/1291]	Time 0.041 (0.054)	Data 1.52e-04 (3.32e-04)	Tok/s 180949 (201063)	Loss/tok 3.0591 (3.2858)	LR 2.875e-03
0: TRAIN [2][840/1291]	Time 0.059 (0.054)	Data 1.45e-04 (3.29e-04)	Tok/s 217874 (201087)	Loss/tok 3.1443 (3.2849)	LR 2.875e-03
0: TRAIN [2][850/1291]	Time 0.043 (0.053)	Data 8.63e-05 (3.27e-04)	Tok/s 182047 (200996)	Loss/tok 2.9856 (3.2839)	LR 2.875e-03
0: Upscaling, new scale: 4096.0
0: Gradient norm: inf
0: Skipped batch, new scale: 2048.0
0: TRAIN [2][860/1291]	Time 0.043 (0.053)	Data 8.63e-05 (3.24e-04)	Tok/s 177773 (200836)	Loss/tok 2.9906 (3.2839)	LR 2.875e-03
0: TRAIN [2][870/1291]	Time 0.043 (0.053)	Data 8.80e-05 (3.21e-04)	Tok/s 180020 (200805)	Loss/tok 3.2064 (3.2842)	LR 2.875e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 1024.0
0: TRAIN [2][880/1291]	Time 0.043 (0.053)	Data 9.18e-05 (3.19e-04)	Tok/s 180755 (200778)	Loss/tok 3.1048 (3.2845)	LR 2.875e-03
0: TRAIN [2][890/1291]	Time 0.042 (0.053)	Data 9.16e-05 (3.16e-04)	Tok/s 185323 (200674)	Loss/tok 3.0653 (3.2842)	LR 2.875e-03
0: TRAIN [2][900/1291]	Time 0.042 (0.053)	Data 8.54e-05 (3.14e-04)	Tok/s 183896 (200629)	Loss/tok 2.9172 (3.2836)	LR 2.875e-03
0: TRAIN [2][910/1291]	Time 0.043 (0.053)	Data 1.41e-04 (3.11e-04)	Tok/s 184675 (200552)	Loss/tok 3.1456 (3.2826)	LR 2.875e-03
0: TRAIN [2][920/1291]	Time 0.043 (0.053)	Data 1.44e-04 (3.09e-04)	Tok/s 180961 (200486)	Loss/tok 3.0444 (3.2835)	LR 2.875e-03
0: TRAIN [2][930/1291]	Time 0.059 (0.053)	Data 1.36e-04 (3.07e-04)	Tok/s 213561 (200403)	Loss/tok 3.3831 (3.2827)	LR 2.875e-03
0: TRAIN [2][940/1291]	Time 0.077 (0.053)	Data 8.46e-05 (3.05e-04)	Tok/s 224133 (200371)	Loss/tok 3.4667 (3.2823)	LR 2.875e-03
0: TRAIN [2][950/1291]	Time 0.042 (0.053)	Data 1.40e-04 (3.02e-04)	Tok/s 180748 (200222)	Loss/tok 3.1571 (3.2809)	LR 2.875e-03
0: TRAIN [2][960/1291]	Time 0.077 (0.053)	Data 8.77e-05 (3.00e-04)	Tok/s 225034 (200196)	Loss/tok 3.4112 (3.2815)	LR 2.875e-03
0: TRAIN [2][970/1291]	Time 0.026 (0.053)	Data 8.85e-05 (2.98e-04)	Tok/s 152699 (200097)	Loss/tok 2.6771 (3.2810)	LR 2.875e-03
0: TRAIN [2][980/1291]	Time 0.043 (0.053)	Data 8.80e-05 (2.96e-04)	Tok/s 178905 (200071)	Loss/tok 3.0796 (3.2812)	LR 2.875e-03
0: TRAIN [2][990/1291]	Time 0.042 (0.053)	Data 8.73e-05 (2.94e-04)	Tok/s 180540 (200118)	Loss/tok 3.1728 (3.2826)	LR 2.875e-03
0: TRAIN [2][1000/1291]	Time 0.060 (0.053)	Data 9.06e-05 (2.92e-04)	Tok/s 209301 (200116)	Loss/tok 3.2689 (3.2826)	LR 2.875e-03
0: Upscaling, new scale: 2048.0
0: TRAIN [2][1010/1291]	Time 0.042 (0.053)	Data 8.99e-05 (2.90e-04)	Tok/s 183093 (200040)	Loss/tok 3.0845 (3.2817)	LR 2.875e-03
0: TRAIN [2][1020/1291]	Time 0.042 (0.053)	Data 1.40e-04 (2.88e-04)	Tok/s 181099 (199974)	Loss/tok 3.2601 (3.2817)	LR 2.875e-03
0: TRAIN [2][1030/1291]	Time 0.077 (0.053)	Data 8.80e-05 (2.87e-04)	Tok/s 229298 (200021)	Loss/tok 3.4350 (3.2823)	LR 2.875e-03
0: TRAIN [2][1040/1291]	Time 0.043 (0.053)	Data 9.16e-05 (2.85e-04)	Tok/s 181121 (199893)	Loss/tok 3.2047 (3.2817)	LR 2.875e-03
0: TRAIN [2][1050/1291]	Time 0.042 (0.053)	Data 9.08e-05 (2.83e-04)	Tok/s 181294 (199962)	Loss/tok 3.0698 (3.2825)	LR 2.875e-03
0: TRAIN [2][1060/1291]	Time 0.058 (0.053)	Data 1.40e-04 (2.81e-04)	Tok/s 219856 (199968)	Loss/tok 3.3199 (3.2823)	LR 2.875e-03
0: TRAIN [2][1070/1291]	Time 0.058 (0.053)	Data 1.40e-04 (2.80e-04)	Tok/s 217085 (200048)	Loss/tok 3.2387 (3.2824)	LR 2.875e-03
0: TRAIN [2][1080/1291]	Time 0.076 (0.053)	Data 9.23e-05 (2.78e-04)	Tok/s 227844 (200101)	Loss/tok 3.4110 (3.2821)	LR 2.875e-03
0: TRAIN [2][1090/1291]	Time 0.041 (0.053)	Data 1.38e-04 (2.76e-04)	Tok/s 183270 (200102)	Loss/tok 3.1456 (3.2832)	LR 2.875e-03
0: TRAIN [2][1100/1291]	Time 0.058 (0.053)	Data 8.85e-05 (2.75e-04)	Tok/s 218948 (200065)	Loss/tok 3.2509 (3.2822)	LR 2.875e-03
0: TRAIN [2][1110/1291]	Time 0.058 (0.053)	Data 1.47e-04 (2.73e-04)	Tok/s 216607 (200074)	Loss/tok 3.3190 (3.2817)	LR 2.875e-03
0: TRAIN [2][1120/1291]	Time 0.076 (0.053)	Data 8.77e-05 (2.72e-04)	Tok/s 231438 (200165)	Loss/tok 3.3865 (3.2817)	LR 2.875e-03
0: TRAIN [2][1130/1291]	Time 0.058 (0.053)	Data 1.42e-04 (2.70e-04)	Tok/s 217805 (200032)	Loss/tok 3.2021 (3.2802)	LR 2.875e-03
0: Upscaling, new scale: 4096.0
0: TRAIN [2][1140/1291]	Time 0.096 (0.053)	Data 9.11e-05 (2.69e-04)	Tok/s 231699 (200108)	Loss/tok 3.7049 (3.2817)	LR 2.875e-03
0: TRAIN [2][1150/1291]	Time 0.041 (0.053)	Data 1.39e-04 (2.67e-04)	Tok/s 188393 (200109)	Loss/tok 3.0479 (3.2808)	LR 2.875e-03
0: TRAIN [2][1160/1291]	Time 0.041 (0.053)	Data 1.41e-04 (2.66e-04)	Tok/s 187522 (200110)	Loss/tok 3.0185 (3.2798)	LR 2.875e-03
0: TRAIN [2][1170/1291]	Time 0.059 (0.053)	Data 1.43e-04 (2.64e-04)	Tok/s 218908 (200141)	Loss/tok 3.3190 (3.2797)	LR 2.875e-03
0: TRAIN [2][1180/1291]	Time 0.058 (0.053)	Data 8.82e-05 (2.63e-04)	Tok/s 213372 (200155)	Loss/tok 3.1972 (3.2800)	LR 2.875e-03
0: TRAIN [2][1190/1291]	Time 0.041 (0.053)	Data 8.56e-05 (2.61e-04)	Tok/s 186041 (200140)	Loss/tok 2.9418 (3.2797)	LR 2.875e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 2048.0
0: TRAIN [2][1200/1291]	Time 0.076 (0.053)	Data 1.67e-04 (2.60e-04)	Tok/s 230525 (200190)	Loss/tok 3.5007 (3.2808)	LR 2.875e-03
0: TRAIN [2][1210/1291]	Time 0.041 (0.053)	Data 1.49e-04 (2.59e-04)	Tok/s 192302 (200218)	Loss/tok 2.9111 (3.2806)	LR 2.875e-03
0: TRAIN [2][1220/1291]	Time 0.041 (0.053)	Data 1.55e-04 (2.58e-04)	Tok/s 190125 (200258)	Loss/tok 3.1161 (3.2806)	LR 2.875e-03
0: TRAIN [2][1230/1291]	Time 0.041 (0.053)	Data 9.58e-05 (2.56e-04)	Tok/s 187059 (200296)	Loss/tok 3.0510 (3.2798)	LR 2.875e-03
0: TRAIN [2][1240/1291]	Time 0.042 (0.053)	Data 8.34e-05 (2.55e-04)	Tok/s 182562 (200296)	Loss/tok 3.0082 (3.2793)	LR 2.875e-03
0: TRAIN [2][1250/1291]	Time 0.076 (0.053)	Data 9.39e-05 (2.54e-04)	Tok/s 229521 (200336)	Loss/tok 3.4899 (3.2803)	LR 2.875e-03
0: TRAIN [2][1260/1291]	Time 0.025 (0.053)	Data 9.04e-05 (2.53e-04)	Tok/s 157635 (200302)	Loss/tok 2.6938 (3.2800)	LR 2.875e-03
0: TRAIN [2][1270/1291]	Time 0.042 (0.053)	Data 8.63e-05 (2.51e-04)	Tok/s 183934 (200309)	Loss/tok 2.9358 (3.2801)	LR 2.875e-03
0: TRAIN [2][1280/1291]	Time 0.042 (0.053)	Data 1.44e-04 (2.50e-04)	Tok/s 182804 (200242)	Loss/tok 3.1244 (3.2797)	LR 2.875e-03
0: TRAIN [2][1290/1291]	Time 0.024 (0.053)	Data 6.56e-05 (2.51e-04)	Tok/s 164419 (200140)	Loss/tok 2.6723 (3.2789)	LR 2.875e-03
:::MLLOG {"namespace": "", "time_ms": 1593114148357, "event_type": "INTERVAL_END", "key": "epoch_stop", "value": null, "metadata": {"file": "train.py", "lineno": 551, "epoch_num": 3}}
:::MLLOG {"namespace": "", "time_ms": 1593114148357, "event_type": "INTERVAL_START", "key": "eval_start", "value": null, "metadata": {"file": "train.py", "lineno": 556, "epoch_num": 3}}
0: Running evaluation on test set
0: TEST [2][0/3]	Time 0.296 (0.296)	Decoder iters 106.0 (106.0)	Tok/s 29611 (29611)
0: Running moses detokenizer
0: BLEU(score=22.750834205911037, counts=[35863, 17488, 9764, 5673], totals=[63601, 60598, 57595, 54597], precisions=[56.38747818430528, 28.859038252087526, 16.9528604913621, 10.390680806637727], bp=0.9832397916620194, sys_len=63601, ref_len=64676)
0: Finished evaluation on test set
:::MLLOG {"namespace": "", "time_ms": 1593114149566, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.2275, "metadata": {"file": "train.py", "lineno": 562, "epoch_num": 3}}
:::MLLOG {"namespace": "", "time_ms": 1593114149567, "event_type": "INTERVAL_END", "key": "eval_stop", "value": null, "metadata": {"file": "train.py", "lineno": 565, "epoch_num": 3}}
0: Summary: Epoch: 2	Training Loss: 3.2811	Test BLEU: 22.75
0: Performance: Epoch: 2	Training: 3200881 Tok/s
0: Finished epoch 2
:::MLLOG {"namespace": "", "time_ms": 1593114149567, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "train.py", "lineno": 584, "first_epoch_num": 3}}
:::MLLOG {"namespace": "", "time_ms": 1593114149567, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "train.py", "lineno": 538, "first_epoch_num": 4, "epoch_count": 1}}
:::MLLOG {"namespace": "", "time_ms": 1593114149567, "event_type": "INTERVAL_START", "key": "epoch_start", "value": null, "metadata": {"file": "train.py", "lineno": 541, "epoch_num": 4}}
0: Starting epoch 3
0: Sampler for epoch 3 uses seed 2609187052
0: TRAIN [3][0/1291]	Time 0.283 (0.283)	Data 1.83e-01 (1.83e-01)	Tok/s 27496 (27496)	Loss/tok 2.9652 (2.9652)	LR 2.875e-03
0: TRAIN [3][10/1291]	Time 0.058 (0.063)	Data 9.49e-05 (1.67e-02)	Tok/s 220908 (171608)	Loss/tok 3.1763 (3.0792)	LR 2.875e-03
0: TRAIN [3][20/1291]	Time 0.076 (0.057)	Data 8.94e-05 (8.78e-03)	Tok/s 228815 (184803)	Loss/tok 3.3541 (3.0951)	LR 2.875e-03
0: Upscaling, new scale: 4096.0
0: TRAIN [3][30/1291]	Time 0.041 (0.054)	Data 9.92e-05 (5.98e-03)	Tok/s 184953 (188057)	Loss/tok 3.0036 (3.0986)	LR 2.875e-03
0: TRAIN [3][40/1291]	Time 0.041 (0.052)	Data 8.68e-05 (4.55e-03)	Tok/s 184009 (190221)	Loss/tok 2.9300 (3.0884)	LR 2.875e-03
0: TRAIN [3][50/1291]	Time 0.042 (0.053)	Data 9.23e-05 (3.68e-03)	Tok/s 180832 (192406)	Loss/tok 3.0349 (3.1082)	LR 2.875e-03
0: TRAIN [3][60/1291]	Time 0.041 (0.052)	Data 9.25e-05 (3.09e-03)	Tok/s 186475 (193415)	Loss/tok 3.0915 (3.1137)	LR 2.875e-03
0: TRAIN [3][70/1291]	Time 0.041 (0.053)	Data 8.63e-05 (2.67e-03)	Tok/s 192586 (195853)	Loss/tok 2.9638 (3.1370)	LR 2.875e-03
0: TRAIN [3][80/1291]	Time 0.041 (0.053)	Data 1.07e-04 (2.35e-03)	Tok/s 181613 (196614)	Loss/tok 3.0174 (3.1435)	LR 2.875e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 2048.0
0: TRAIN [3][90/1291]	Time 0.059 (0.053)	Data 9.08e-05 (2.10e-03)	Tok/s 212537 (197157)	Loss/tok 3.1587 (3.1592)	LR 2.875e-03
0: TRAIN [3][100/1291]	Time 0.058 (0.053)	Data 1.44e-04 (1.91e-03)	Tok/s 214431 (198074)	Loss/tok 3.0422 (3.1564)	LR 2.875e-03
0: TRAIN [3][110/1291]	Time 0.042 (0.053)	Data 8.85e-05 (1.74e-03)	Tok/s 184045 (198450)	Loss/tok 2.9333 (3.1581)	LR 2.875e-03
0: TRAIN [3][120/1291]	Time 0.076 (0.054)	Data 1.02e-04 (1.61e-03)	Tok/s 228422 (199396)	Loss/tok 3.4187 (3.1648)	LR 2.875e-03
0: TRAIN [3][130/1291]	Time 0.025 (0.053)	Data 8.46e-05 (1.49e-03)	Tok/s 156977 (198832)	Loss/tok 2.5678 (3.1605)	LR 2.875e-03
0: TRAIN [3][140/1291]	Time 0.058 (0.053)	Data 8.94e-05 (1.39e-03)	Tok/s 217425 (198735)	Loss/tok 3.2683 (3.1564)	LR 2.875e-03
0: TRAIN [3][150/1291]	Time 0.041 (0.052)	Data 8.44e-05 (1.31e-03)	Tok/s 189766 (198066)	Loss/tok 2.9569 (3.1500)	LR 2.875e-03
0: TRAIN [3][160/1291]	Time 0.043 (0.052)	Data 8.44e-05 (1.23e-03)	Tok/s 185658 (198308)	Loss/tok 2.9682 (3.1547)	LR 2.875e-03
0: TRAIN [3][170/1291]	Time 0.076 (0.052)	Data 1.12e-04 (1.17e-03)	Tok/s 230174 (198645)	Loss/tok 3.3776 (3.1557)	LR 2.875e-03
0: TRAIN [3][180/1291]	Time 0.041 (0.052)	Data 8.77e-05 (1.11e-03)	Tok/s 190533 (198908)	Loss/tok 3.0321 (3.1591)	LR 2.875e-03
0: TRAIN [3][190/1291]	Time 0.041 (0.052)	Data 7.87e-05 (1.05e-03)	Tok/s 184585 (198835)	Loss/tok 2.9426 (3.1598)	LR 2.875e-03
0: TRAIN [3][200/1291]	Time 0.042 (0.052)	Data 8.18e-05 (1.00e-03)	Tok/s 185982 (199350)	Loss/tok 3.1369 (3.1631)	LR 2.875e-03
0: TRAIN [3][210/1291]	Time 0.041 (0.052)	Data 8.32e-05 (9.62e-04)	Tok/s 184128 (198930)	Loss/tok 3.1065 (3.1605)	LR 1.437e-03
0: Upscaling, new scale: 4096.0
0: TRAIN [3][220/1291]	Time 0.059 (0.052)	Data 9.23e-05 (9.23e-04)	Tok/s 215064 (198865)	Loss/tok 3.2212 (3.1600)	LR 1.437e-03
0: TRAIN [3][230/1291]	Time 0.042 (0.052)	Data 8.39e-05 (8.87e-04)	Tok/s 187604 (198850)	Loss/tok 2.9191 (3.1595)	LR 1.437e-03
0: TRAIN [3][240/1291]	Time 0.096 (0.052)	Data 1.18e-04 (8.54e-04)	Tok/s 233144 (198843)	Loss/tok 3.5083 (3.1648)	LR 1.437e-03
0: TRAIN [3][250/1291]	Time 0.097 (0.052)	Data 9.82e-05 (8.24e-04)	Tok/s 230243 (198859)	Loss/tok 3.5471 (3.1672)	LR 1.437e-03
0: TRAIN [3][260/1291]	Time 0.041 (0.052)	Data 1.57e-04 (7.96e-04)	Tok/s 188914 (198438)	Loss/tok 2.9910 (3.1628)	LR 1.437e-03
0: TRAIN [3][270/1291]	Time 0.058 (0.051)	Data 8.27e-05 (7.70e-04)	Tok/s 214831 (198108)	Loss/tok 3.1079 (3.1582)	LR 1.437e-03
0: TRAIN [3][280/1291]	Time 0.059 (0.052)	Data 8.58e-05 (7.47e-04)	Tok/s 213050 (198196)	Loss/tok 3.1583 (3.1585)	LR 1.437e-03
0: TRAIN [3][290/1291]	Time 0.076 (0.052)	Data 1.32e-04 (7.24e-04)	Tok/s 230577 (198492)	Loss/tok 3.2275 (3.1627)	LR 1.437e-03
0: TRAIN [3][300/1291]	Time 0.025 (0.052)	Data 8.11e-05 (7.04e-04)	Tok/s 156626 (198365)	Loss/tok 2.5708 (3.1617)	LR 1.437e-03
0: TRAIN [3][310/1291]	Time 0.041 (0.052)	Data 1.07e-04 (6.84e-04)	Tok/s 187409 (198482)	Loss/tok 2.9524 (3.1625)	LR 1.437e-03
0: TRAIN [3][320/1291]	Time 0.041 (0.052)	Data 8.34e-05 (6.65e-04)	Tok/s 187784 (198301)	Loss/tok 2.9977 (3.1605)	LR 1.437e-03
0: TRAIN [3][330/1291]	Time 0.058 (0.051)	Data 8.70e-05 (6.48e-04)	Tok/s 211798 (198033)	Loss/tok 3.1446 (3.1564)	LR 1.437e-03
0: TRAIN [3][340/1291]	Time 0.097 (0.051)	Data 9.75e-05 (6.31e-04)	Tok/s 231028 (198201)	Loss/tok 3.5893 (3.1582)	LR 1.437e-03
0: Upscaling, new scale: 8192.0
0: TRAIN [3][350/1291]	Time 0.058 (0.051)	Data 8.30e-05 (6.16e-04)	Tok/s 218560 (198075)	Loss/tok 3.1910 (3.1571)	LR 1.437e-03
0: TRAIN [3][360/1291]	Time 0.058 (0.051)	Data 8.37e-05 (6.01e-04)	Tok/s 216576 (198087)	Loss/tok 3.1960 (3.1548)	LR 1.437e-03
0: TRAIN [3][370/1291]	Time 0.042 (0.051)	Data 8.32e-05 (5.87e-04)	Tok/s 184000 (198365)	Loss/tok 2.9404 (3.1555)	LR 1.437e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 4096.0
0: TRAIN [3][380/1291]	Time 0.059 (0.051)	Data 8.96e-05 (5.74e-04)	Tok/s 214861 (198511)	Loss/tok 3.2288 (3.1557)	LR 1.437e-03
0: TRAIN [3][390/1291]	Time 0.076 (0.052)	Data 8.20e-05 (5.61e-04)	Tok/s 227579 (198807)	Loss/tok 3.3718 (3.1584)	LR 1.437e-03
0: TRAIN [3][400/1291]	Time 0.058 (0.052)	Data 8.30e-05 (5.49e-04)	Tok/s 219480 (198885)	Loss/tok 3.1391 (3.1581)	LR 1.437e-03
0: TRAIN [3][410/1291]	Time 0.042 (0.052)	Data 8.32e-05 (5.38e-04)	Tok/s 192752 (198926)	Loss/tok 2.9832 (3.1570)	LR 1.437e-03
0: TRAIN [3][420/1291]	Time 0.076 (0.052)	Data 8.27e-05 (5.27e-04)	Tok/s 231582 (199000)	Loss/tok 3.3319 (3.1567)	LR 1.437e-03
0: TRAIN [3][430/1291]	Time 0.058 (0.052)	Data 8.15e-05 (5.17e-04)	Tok/s 219527 (198822)	Loss/tok 3.1232 (3.1550)	LR 1.437e-03
0: TRAIN [3][440/1291]	Time 0.042 (0.052)	Data 8.18e-05 (5.07e-04)	Tok/s 185925 (198896)	Loss/tok 2.9863 (3.1542)	LR 1.437e-03
0: TRAIN [3][450/1291]	Time 0.058 (0.052)	Data 8.37e-05 (4.98e-04)	Tok/s 214550 (198953)	Loss/tok 3.2106 (3.1540)	LR 1.437e-03
0: TRAIN [3][460/1291]	Time 0.042 (0.052)	Data 8.54e-05 (4.89e-04)	Tok/s 186348 (199184)	Loss/tok 2.8731 (3.1557)	LR 1.437e-03
0: TRAIN [3][470/1291]	Time 0.058 (0.052)	Data 9.63e-05 (4.80e-04)	Tok/s 210721 (199336)	Loss/tok 3.2945 (3.1563)	LR 1.437e-03
0: TRAIN [3][480/1291]	Time 0.041 (0.052)	Data 8.27e-05 (4.72e-04)	Tok/s 188353 (199561)	Loss/tok 2.9061 (3.1555)	LR 1.437e-03
0: TRAIN [3][490/1291]	Time 0.058 (0.052)	Data 8.42e-05 (4.64e-04)	Tok/s 218649 (199836)	Loss/tok 3.0980 (3.1584)	LR 1.437e-03
0: TRAIN [3][500/1291]	Time 0.058 (0.052)	Data 9.35e-05 (4.56e-04)	Tok/s 217004 (199706)	Loss/tok 3.0954 (3.1561)	LR 1.437e-03
0: Upscaling, new scale: 8192.0
0: TRAIN [3][510/1291]	Time 0.042 (0.052)	Data 8.54e-05 (4.49e-04)	Tok/s 187051 (199693)	Loss/tok 2.9463 (3.1574)	LR 1.437e-03
0: TRAIN [3][520/1291]	Time 0.041 (0.052)	Data 8.34e-05 (4.42e-04)	Tok/s 191698 (199815)	Loss/tok 2.8561 (3.1590)	LR 1.437e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 4096.0
0: TRAIN [3][530/1291]	Time 0.058 (0.052)	Data 1.02e-04 (4.36e-04)	Tok/s 217922 (199897)	Loss/tok 3.1148 (3.1583)	LR 1.437e-03
0: TRAIN [3][540/1291]	Time 0.042 (0.052)	Data 8.37e-05 (4.29e-04)	Tok/s 187223 (199852)	Loss/tok 3.0775 (3.1576)	LR 1.437e-03
0: TRAIN [3][550/1291]	Time 0.042 (0.052)	Data 8.73e-05 (4.23e-04)	Tok/s 184446 (199985)	Loss/tok 2.9696 (3.1603)	LR 1.437e-03
0: TRAIN [3][560/1291]	Time 0.058 (0.052)	Data 8.89e-05 (4.17e-04)	Tok/s 216167 (200186)	Loss/tok 3.1787 (3.1600)	LR 1.437e-03
0: TRAIN [3][570/1291]	Time 0.058 (0.053)	Data 8.68e-05 (4.11e-04)	Tok/s 216044 (200255)	Loss/tok 3.1280 (3.1598)	LR 1.437e-03
0: TRAIN [3][580/1291]	Time 0.024 (0.053)	Data 8.99e-05 (4.06e-04)	Tok/s 155405 (200403)	Loss/tok 2.5132 (3.1597)	LR 1.437e-03
0: TRAIN [3][590/1291]	Time 0.041 (0.053)	Data 8.73e-05 (4.00e-04)	Tok/s 189705 (200552)	Loss/tok 2.8014 (3.1595)	LR 1.437e-03
0: TRAIN [3][600/1291]	Time 0.058 (0.053)	Data 8.68e-05 (3.95e-04)	Tok/s 216291 (200435)	Loss/tok 3.1598 (3.1593)	LR 1.437e-03
0: TRAIN [3][610/1291]	Time 0.041 (0.053)	Data 8.20e-05 (3.90e-04)	Tok/s 187518 (200334)	Loss/tok 3.0463 (3.1587)	LR 1.437e-03
0: TRAIN [3][620/1291]	Time 0.041 (0.053)	Data 8.44e-05 (3.85e-04)	Tok/s 187762 (200353)	Loss/tok 3.0308 (3.1574)	LR 1.437e-03
0: TRAIN [3][630/1291]	Time 0.097 (0.053)	Data 8.27e-05 (3.80e-04)	Tok/s 231110 (200498)	Loss/tok 3.5121 (3.1586)	LR 1.437e-03
0: TRAIN [3][640/1291]	Time 0.041 (0.053)	Data 8.58e-05 (3.76e-04)	Tok/s 187976 (200357)	Loss/tok 3.0105 (3.1579)	LR 1.437e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 2048.0
0: TRAIN [3][650/1291]	Time 0.076 (0.053)	Data 8.68e-05 (3.71e-04)	Tok/s 230586 (200492)	Loss/tok 3.2683 (3.1598)	LR 1.437e-03
0: TRAIN [3][660/1291]	Time 0.041 (0.053)	Data 8.37e-05 (3.67e-04)	Tok/s 181681 (200617)	Loss/tok 2.9900 (3.1619)	LR 1.437e-03
0: TRAIN [3][670/1291]	Time 0.042 (0.053)	Data 8.39e-05 (3.63e-04)	Tok/s 188117 (200708)	Loss/tok 3.0460 (3.1629)	LR 1.437e-03
0: TRAIN [3][680/1291]	Time 0.025 (0.053)	Data 8.65e-05 (3.59e-04)	Tok/s 158376 (200584)	Loss/tok 2.5351 (3.1621)	LR 1.437e-03
0: TRAIN [3][690/1291]	Time 0.058 (0.053)	Data 8.75e-05 (3.55e-04)	Tok/s 217376 (200447)	Loss/tok 3.1792 (3.1617)	LR 1.437e-03
0: TRAIN [3][700/1291]	Time 0.041 (0.053)	Data 8.32e-05 (3.51e-04)	Tok/s 188937 (200396)	Loss/tok 2.9864 (3.1607)	LR 1.437e-03
0: TRAIN [3][710/1291]	Time 0.042 (0.053)	Data 8.39e-05 (3.47e-04)	Tok/s 188398 (200354)	Loss/tok 2.9476 (3.1594)	LR 1.437e-03
0: TRAIN [3][720/1291]	Time 0.059 (0.053)	Data 8.56e-05 (3.43e-04)	Tok/s 216544 (200459)	Loss/tok 3.1020 (3.1613)	LR 7.187e-04
0: TRAIN [3][730/1291]	Time 0.059 (0.053)	Data 8.63e-05 (3.40e-04)	Tok/s 214401 (200537)	Loss/tok 3.1527 (3.1635)	LR 7.187e-04
0: TRAIN [3][740/1291]	Time 0.041 (0.053)	Data 8.49e-05 (3.37e-04)	Tok/s 183465 (200504)	Loss/tok 2.9450 (3.1635)	LR 7.187e-04
0: TRAIN [3][750/1291]	Time 0.058 (0.053)	Data 9.11e-05 (3.33e-04)	Tok/s 219077 (200623)	Loss/tok 3.1810 (3.1649)	LR 7.187e-04
0: TRAIN [3][760/1291]	Time 0.058 (0.053)	Data 1.04e-04 (3.30e-04)	Tok/s 218601 (200666)	Loss/tok 3.0798 (3.1644)	LR 7.187e-04
0: Upscaling, new scale: 4096.0
0: TRAIN [3][770/1291]	Time 0.058 (0.053)	Data 9.25e-05 (3.27e-04)	Tok/s 215324 (200725)	Loss/tok 3.1594 (3.1639)	LR 7.187e-04
0: TRAIN [3][780/1291]	Time 0.059 (0.053)	Data 8.54e-05 (3.24e-04)	Tok/s 215443 (200740)	Loss/tok 3.0595 (3.1647)	LR 7.187e-04
0: TRAIN [3][790/1291]	Time 0.042 (0.053)	Data 9.06e-05 (3.21e-04)	Tok/s 188482 (200824)	Loss/tok 2.9757 (3.1649)	LR 7.187e-04
0: TRAIN [3][800/1291]	Time 0.041 (0.053)	Data 8.23e-05 (3.18e-04)	Tok/s 181951 (200797)	Loss/tok 2.8863 (3.1646)	LR 7.187e-04
0: TRAIN [3][810/1291]	Time 0.042 (0.053)	Data 8.23e-05 (3.15e-04)	Tok/s 190745 (200768)	Loss/tok 3.0265 (3.1636)	LR 7.187e-04
0: TRAIN [3][820/1291]	Time 0.042 (0.053)	Data 8.27e-05 (3.12e-04)	Tok/s 185670 (200761)	Loss/tok 3.1376 (3.1632)	LR 7.187e-04
0: TRAIN [3][830/1291]	Time 0.041 (0.053)	Data 8.63e-05 (3.09e-04)	Tok/s 193225 (200582)	Loss/tok 2.8907 (3.1611)	LR 7.187e-04
0: TRAIN [3][840/1291]	Time 0.042 (0.053)	Data 8.08e-05 (3.07e-04)	Tok/s 182724 (200608)	Loss/tok 2.8988 (3.1609)	LR 7.187e-04
0: TRAIN [3][850/1291]	Time 0.041 (0.053)	Data 8.11e-05 (3.04e-04)	Tok/s 187657 (200507)	Loss/tok 2.9023 (3.1592)	LR 7.187e-04
0: TRAIN [3][860/1291]	Time 0.042 (0.053)	Data 8.27e-05 (3.01e-04)	Tok/s 188204 (200395)	Loss/tok 3.0601 (3.1578)	LR 7.187e-04
0: TRAIN [3][870/1291]	Time 0.097 (0.053)	Data 8.20e-05 (2.99e-04)	Tok/s 232411 (200475)	Loss/tok 3.4874 (3.1596)	LR 7.187e-04
0: TRAIN [3][880/1291]	Time 0.076 (0.053)	Data 9.82e-05 (2.96e-04)	Tok/s 226188 (200571)	Loss/tok 3.5387 (3.1604)	LR 7.187e-04
0: TRAIN [3][890/1291]	Time 0.058 (0.053)	Data 8.37e-05 (2.94e-04)	Tok/s 218038 (200530)	Loss/tok 3.2775 (3.1597)	LR 7.187e-04
0: Upscaling, new scale: 8192.0
0: TRAIN [3][900/1291]	Time 0.041 (0.053)	Data 8.03e-05 (2.92e-04)	Tok/s 186679 (200570)	Loss/tok 2.8336 (3.1595)	LR 7.187e-04
0: TRAIN [3][910/1291]	Time 0.042 (0.053)	Data 8.03e-05 (2.89e-04)	Tok/s 190500 (200560)	Loss/tok 2.8881 (3.1589)	LR 7.187e-04
0: TRAIN [3][920/1291]	Time 0.042 (0.053)	Data 8.25e-05 (2.87e-04)	Tok/s 182504 (200514)	Loss/tok 3.0203 (3.1580)	LR 7.187e-04
0: TRAIN [3][930/1291]	Time 0.025 (0.053)	Data 8.32e-05 (2.85e-04)	Tok/s 165741 (200489)	Loss/tok 2.6561 (3.1572)	LR 7.187e-04
0: TRAIN [3][940/1291]	Time 0.041 (0.053)	Data 8.42e-05 (2.83e-04)	Tok/s 189860 (200481)	Loss/tok 2.8826 (3.1562)	LR 7.187e-04
0: TRAIN [3][950/1291]	Time 0.042 (0.053)	Data 8.13e-05 (2.81e-04)	Tok/s 185859 (200542)	Loss/tok 2.9065 (3.1556)	LR 7.187e-04
0: TRAIN [3][960/1291]	Time 0.076 (0.053)	Data 8.37e-05 (2.79e-04)	Tok/s 228816 (200563)	Loss/tok 3.2670 (3.1545)	LR 7.187e-04
0: TRAIN [3][970/1291]	Time 0.058 (0.053)	Data 8.42e-05 (2.77e-04)	Tok/s 217436 (200520)	Loss/tok 3.1286 (3.1541)	LR 7.187e-04
0: Gradient norm: inf
0: Skipped batch, new scale: 4096.0
0: TRAIN [3][980/1291]	Time 0.097 (0.053)	Data 8.25e-05 (2.75e-04)	Tok/s 233303 (200608)	Loss/tok 3.3858 (3.1554)	LR 7.187e-04
0: TRAIN [3][990/1291]	Time 0.024 (0.053)	Data 8.08e-05 (2.73e-04)	Tok/s 161921 (200590)	Loss/tok 2.7230 (3.1550)	LR 7.187e-04
0: TRAIN [3][1000/1291]	Time 0.041 (0.053)	Data 8.11e-05 (2.71e-04)	Tok/s 187605 (200573)	Loss/tok 2.9507 (3.1543)	LR 7.187e-04
0: TRAIN [3][1010/1291]	Time 0.025 (0.053)	Data 8.06e-05 (2.69e-04)	Tok/s 156656 (200523)	Loss/tok 2.4672 (3.1533)	LR 7.187e-04
0: TRAIN [3][1020/1291]	Time 0.058 (0.053)	Data 8.34e-05 (2.67e-04)	Tok/s 215417 (200555)	Loss/tok 3.0332 (3.1529)	LR 7.187e-04
0: TRAIN [3][1030/1291]	Time 0.059 (0.053)	Data 8.54e-05 (2.65e-04)	Tok/s 216763 (200525)	Loss/tok 3.0651 (3.1520)	LR 7.187e-04
0: TRAIN [3][1040/1291]	Time 0.058 (0.053)	Data 8.20e-05 (2.64e-04)	Tok/s 216260 (200624)	Loss/tok 3.2361 (3.1527)	LR 7.187e-04
0: TRAIN [3][1050/1291]	Time 0.042 (0.053)	Data 8.06e-05 (2.62e-04)	Tok/s 179315 (200562)	Loss/tok 2.9434 (3.1516)	LR 7.187e-04
0: TRAIN [3][1060/1291]	Time 0.041 (0.053)	Data 8.15e-05 (2.60e-04)	Tok/s 187543 (200496)	Loss/tok 2.7964 (3.1502)	LR 7.187e-04
0: TRAIN [3][1070/1291]	Time 0.025 (0.053)	Data 8.18e-05 (2.58e-04)	Tok/s 161574 (200432)	Loss/tok 2.4947 (3.1499)	LR 7.187e-04
0: TRAIN [3][1080/1291]	Time 0.042 (0.053)	Data 8.32e-05 (2.57e-04)	Tok/s 186388 (200412)	Loss/tok 2.7790 (3.1496)	LR 7.187e-04
0: TRAIN [3][1090/1291]	Time 0.041 (0.053)	Data 8.42e-05 (2.55e-04)	Tok/s 184626 (200477)	Loss/tok 2.8982 (3.1502)	LR 7.187e-04
0: TRAIN [3][1100/1291]	Time 0.058 (0.053)	Data 8.15e-05 (2.54e-04)	Tok/s 216780 (200465)	Loss/tok 3.0836 (3.1494)	LR 7.187e-04
0: Upscaling, new scale: 8192.0
0: TRAIN [3][1110/1291]	Time 0.041 (0.053)	Data 8.44e-05 (2.52e-04)	Tok/s 190246 (200484)	Loss/tok 2.8811 (3.1495)	LR 7.187e-04
0: TRAIN [3][1120/1291]	Time 0.025 (0.053)	Data 8.54e-05 (2.51e-04)	Tok/s 156413 (200362)	Loss/tok 2.5874 (3.1484)	LR 7.187e-04
0: TRAIN [3][1130/1291]	Time 0.041 (0.053)	Data 8.75e-05 (2.49e-04)	Tok/s 187076 (200376)	Loss/tok 2.9124 (3.1479)	LR 7.187e-04
0: TRAIN [3][1140/1291]	Time 0.059 (0.053)	Data 8.30e-05 (2.48e-04)	Tok/s 210902 (200422)	Loss/tok 3.0357 (3.1476)	LR 7.187e-04
0: TRAIN [3][1150/1291]	Time 0.042 (0.053)	Data 8.01e-05 (2.46e-04)	Tok/s 185274 (200502)	Loss/tok 2.9831 (3.1481)	LR 7.187e-04
0: TRAIN [3][1160/1291]	Time 0.041 (0.053)	Data 8.39e-05 (2.45e-04)	Tok/s 182784 (200528)	Loss/tok 2.9607 (3.1486)	LR 7.187e-04
0: TRAIN [3][1170/1291]	Time 0.058 (0.053)	Data 8.15e-05 (2.44e-04)	Tok/s 212534 (200629)	Loss/tok 3.0752 (3.1486)	LR 7.187e-04
0: Gradient norm: inf
0: Skipped batch, new scale: 4096.0
0: TRAIN [3][1180/1291]	Time 0.097 (0.053)	Data 8.15e-05 (2.42e-04)	Tok/s 232164 (200631)	Loss/tok 3.5002 (3.1490)	LR 7.187e-04
0: TRAIN [3][1190/1291]	Time 0.058 (0.053)	Data 8.32e-05 (2.41e-04)	Tok/s 218181 (200550)	Loss/tok 3.0415 (3.1480)	LR 7.187e-04
0: TRAIN [3][1200/1291]	Time 0.058 (0.053)	Data 8.06e-05 (2.40e-04)	Tok/s 218799 (200580)	Loss/tok 3.0370 (3.1484)	LR 7.187e-04
0: TRAIN [3][1210/1291]	Time 0.076 (0.053)	Data 8.37e-05 (2.38e-04)	Tok/s 226015 (200642)	Loss/tok 3.4061 (3.1487)	LR 7.187e-04
0: TRAIN [3][1220/1291]	Time 0.076 (0.053)	Data 8.03e-05 (2.37e-04)	Tok/s 230624 (200705)	Loss/tok 3.3041 (3.1484)	LR 7.187e-04
0: TRAIN [3][1230/1291]	Time 0.058 (0.053)	Data 8.18e-05 (2.36e-04)	Tok/s 212942 (200736)	Loss/tok 2.9965 (3.1483)	LR 3.594e-04
0: TRAIN [3][1240/1291]	Time 0.059 (0.053)	Data 8.27e-05 (2.34e-04)	Tok/s 214930 (200768)	Loss/tok 3.2189 (3.1487)	LR 3.594e-04
0: TRAIN [3][1250/1291]	Time 0.059 (0.053)	Data 8.30e-05 (2.33e-04)	Tok/s 217861 (200744)	Loss/tok 3.0678 (3.1475)	LR 3.594e-04
0: TRAIN [3][1260/1291]	Time 0.076 (0.053)	Data 8.20e-05 (2.32e-04)	Tok/s 230536 (200712)	Loss/tok 3.2902 (3.1473)	LR 3.594e-04
0: TRAIN [3][1270/1291]	Time 0.058 (0.053)	Data 7.94e-05 (2.31e-04)	Tok/s 213823 (200658)	Loss/tok 3.1458 (3.1466)	LR 3.594e-04
0: TRAIN [3][1280/1291]	Time 0.059 (0.053)	Data 8.15e-05 (2.30e-04)	Tok/s 218756 (200644)	Loss/tok 3.1395 (3.1461)	LR 3.594e-04
0: TRAIN [3][1290/1291]	Time 0.058 (0.053)	Data 4.32e-05 (2.30e-04)	Tok/s 218085 (200722)	Loss/tok 3.0782 (3.1464)	LR 3.594e-04
:::MLLOG {"namespace": "", "time_ms": 1593114218035, "event_type": "INTERVAL_END", "key": "epoch_stop", "value": null, "metadata": {"file": "train.py", "lineno": 551, "epoch_num": 4}}
:::MLLOG {"namespace": "", "time_ms": 1593114218035, "event_type": "INTERVAL_START", "key": "eval_start", "value": null, "metadata": {"file": "train.py", "lineno": 556, "epoch_num": 4}}
0: Running evaluation on test set
0: TEST [3][0/3]	Time 0.294 (0.294)	Decoder iters 97.0 (97.0)	Tok/s 30635 (30635)
0: Running moses detokenizer
0: BLEU(score=24.035397904965766, counts=[37164, 18586, 10642, 6335], totals=[65713, 62710, 59707, 56710], precisions=[56.55501955473042, 29.63801626534843, 17.82370576314335, 11.170869335214247], bp=1.0, sys_len=65713, ref_len=64676)
0: Target accuracy reached
0: Finished evaluation on test set
:::MLLOG {"namespace": "", "time_ms": 1593114219211, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.2404, "metadata": {"file": "train.py", "lineno": 562, "epoch_num": 4}}
:::MLLOG {"namespace": "", "time_ms": 1593114219211, "event_type": "INTERVAL_END", "key": "eval_stop", "value": null, "metadata": {"file": "train.py", "lineno": 565, "epoch_num": 4}}
0: Summary: Epoch: 3	Training Loss: 3.1472	Test BLEU: 24.04
0: Performance: Epoch: 3	Training: 3211694 Tok/s
0: Finished epoch 3
:::MLLOG {"namespace": "", "time_ms": 1593114219211, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "train.py", "lineno": 584, "first_epoch_num": 4}}
0: Closing preprocessed data file
:::MLLOG {"namespace": "", "time_ms": 1593114219211, "event_type": "INTERVAL_END", "key": "run_stop", "value": null, "metadata": {"file": "train.py", "lineno": 595, "status": "success"}}
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
ENDING TIMING RUN AT 2020-06-25 12:43:45 PM
RESULT,RNN_TRANSLATOR,,310,nvidia,2020-06-25 12:38:35 PM
ENDING TIMING RUN AT 2020-06-25 12:43:45 PM
RESULT,RNN_TRANSLATOR,,310,nvidia,2020-06-25 12:38:35 PM
ENDING TIMING RUN AT 2020-06-25 12:43:46 PM
RESULT,RNN_TRANSLATOR,,311,nvidia,2020-06-25 12:38:35 PM
ENDING TIMING RUN AT 2020-06-25 12:43:46 PM
ENDING TIMING RUN AT 2020-06-25 12:43:46 PM
RESULT,RNN_TRANSLATOR,,311,nvidia,2020-06-25 12:38:35 PM
ENDING TIMING RUN AT 2020-06-25 12:43:46 PM
RESULT,RNN_TRANSLATOR,,311,nvidia,2020-06-25 12:38:35 PM
RESULT,RNN_TRANSLATOR,,311,nvidia,2020-06-25 12:38:35 PM
ENDING TIMING RUN AT 2020-06-25 12:43:46 PM
RESULT,RNN_TRANSLATOR,,311,nvidia,2020-06-25 12:38:35 PM
ENDING TIMING RUN AT 2020-06-25 12:43:46 PM
RESULT,RNN_TRANSLATOR,,311,nvidia,2020-06-25 12:38:35 PM
ENDING TIMING RUN AT 2020-06-25 12:43:46 PM
ENDING TIMING RUN AT 2020-06-25 12:43:46 PM
RESULT,RNN_TRANSLATOR,,311,nvidia,2020-06-25 12:38:35 PM
RESULT,RNN_TRANSLATOR,,311,nvidia,2020-06-25 12:38:35 PM
ENDING TIMING RUN AT 2020-06-25 12:43:46 PM
RESULT,RNN_TRANSLATOR,,311,nvidia,2020-06-25 12:38:35 PM
slurmstepd: error: _is_a_lwp: open() /proc/148350/status failed: No such file or directory
ENDING TIMING RUN AT 2020-06-25 12:43:46 PM
ENDING TIMING RUN AT 2020-06-25 12:43:46 PM
ENDING TIMING RUN AT 2020-06-25 12:43:46 PM
RESULT,RNN_TRANSLATOR,,311,nvidia,2020-06-25 12:38:35 PM
RESULT,RNN_TRANSLATOR,,311,nvidia,2020-06-25 12:38:35 PM
RESULT,RNN_TRANSLATOR,,311,nvidia,2020-06-25 12:38:35 PM
ENDING TIMING RUN AT 2020-06-25 12:43:47 PM
RESULT,RNN_TRANSLATOR,,312,nvidia,2020-06-25 12:38:35 PM
ENDING TIMING RUN AT 2020-06-25 12:43:47 PM
RESULT,RNN_TRANSLATOR,,312,nvidia,2020-06-25 12:38:35 PM
