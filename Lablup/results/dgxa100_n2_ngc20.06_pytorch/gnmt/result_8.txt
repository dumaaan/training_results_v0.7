+ echo 'Beginning trial 4 of 5'
Beginning trial 4 of 5
+ srun --ntasks=2 --container-name=rnn_translator python -c '
import mlperf_log_utils
from mlperf_logging.mllog import constants
mlperf_log_utils.mlperf_submission_log(constants.GNMT)'
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
:::MLLOG {"namespace": "", "time_ms": 1593114008901, "event_type": "POINT_IN_TIME", "key": "submission_benchmark", "value": "gnmt", "metadata": {"file": "/workspace/rnn_translator/mlperf_log_utils.py", "lineno": 19}}
:::MLLOG {"namespace": "", "time_ms": 1593114008939, "event_type": "POINT_IN_TIME", "key": "submission_org", "value": "NVIDIA", "metadata": {"file": "/workspace/rnn_translator/mlperf_log_utils.py", "lineno": 24}}
:::MLLOG {"namespace": "", "time_ms": 1593114008939, "event_type": "POINT_IN_TIME", "key": "submission_division", "value": "closed", "metadata": {"file": "/workspace/rnn_translator/mlperf_log_utils.py", "lineno": 28}}
:::MLLOG {"namespace": "", "time_ms": 1593114008939, "event_type": "POINT_IN_TIME", "key": "submission_status", "value": "onprem", "metadata": {"file": "/workspace/rnn_translator/mlperf_log_utils.py", "lineno": 32}}
:::MLLOG {"namespace": "", "time_ms": 1593114008939, "event_type": "POINT_IN_TIME", "key": "submission_platform", "value": "2xNVIDIA DGX A100", "metadata": {"file": "/workspace/rnn_translator/mlperf_log_utils.py", "lineno": 36}}
+ '[' 1 -eq 1 ']'
+ srun --ntasks=2 bash -c 'echo -n '\''Clearing cache on '\'' && hostname && sync && sudo /sbin/sysctl vm.drop_caches=3'
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
Clearing cache on luna-0101
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
Clearing cache on luna-0102
vm.drop_caches = 3
vm.drop_caches = 3
+ srun --ntasks=2 --container-name=rnn_translator python -c '
from mlperf_logging.mllog import constants
from seq2seq.utils import log_event
log_event(key=constants.CACHE_CLEAR, value=True)'
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
:::MLLOG {"namespace": "", "time_ms": 1593114013899, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
:::MLLOG {"namespace": "", "time_ms": 1593114013963, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
+ srun --mpi=none --ntasks=16 --ntasks-per-node=8 --container-name=rnn_translator --container-mounts=/lustre/fsr/datasets/wmt16_de_en:/data,/raid/scratch/svcnvdlfw/gnmt:/preproc_data,/lustre/fsw/mlperf-ci/14251653/results:/results ./run_and_time.sh
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
STARTING TIMING RUN AT 2020-06-25 12:40:16 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=2.875e-3
+ TRAIN_BATCH_SIZE=192
+ TEST_BATCH_SIZE=64
+ WARMUP_STEPS=200
+ REMAIN_STEPS=4054
+ DECAY_INTERVAL=506
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=8
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 3    --dwu-num-chunks 2    --dwu-num-rs-pg 1    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-overlap-reductions    --dwu-grad-norm    '
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 0 ']'
+ '[' 16 -gt 2 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGXA100_multi_2x8x192_dist == *\D\G\X\A\1\0\0* ]]
STARTING TIMING RUN AT 2020-06-25 12:40:16 PM
Using TCMalloc
+ echo 'Using TCMalloc'
+ export LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
running benchmark
+ echo 'running benchmark'
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 192 --test-batch-size 64 --optimizer FusedAdam --lr 2.875e-3 --warmup-steps 200 --remain-steps 4054 --decay-interval 506 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
+ LR=2.875e-3
+ TRAIN_BATCH_SIZE=192
+ TEST_BATCH_SIZE=64
+ WARMUP_STEPS=200
+ REMAIN_STEPS=4054
+ DECAY_INTERVAL=506
STARTING TIMING RUN AT 2020-06-25 12:40:16 PM
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=8
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 3    --dwu-num-chunks 2    --dwu-num-rs-pg 1    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-overlap-reductions    --dwu-grad-norm    '
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    '
+ MATH=fp16
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ declare -a CMD
+ RESULTS_DIR=gnmt_wmt16
+ LR=2.875e-3
+ TRAIN_BATCH_SIZE=192
+ TEST_BATCH_SIZE=64
+ WARMUP_STEPS=200
+ REMAIN_STEPS=4054
+ '[' -n 5 ']'
+ DECAY_INTERVAL=506
+ TARGET=24.0
+ '[' 16 -gt 2 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=8
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 3    --dwu-num-chunks 2    --dwu-num-rs-pg 1    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-overlap-reductions    --dwu-grad-norm    '
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    '
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ MATH=fp16
+ declare -a CMD
+ [[ DGXA100_multi_2x8x192_dist == *\D\G\X\A\1\0\0* ]]
Using TCMalloc
+ echo 'Using TCMalloc'
+ export LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
running benchmark
+ echo 'running benchmark'
+ '[' -n 7 ']'
+ '[' 16 -gt 2 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 192 --test-batch-size 64 --optimizer FusedAdam --lr 2.875e-3 --warmup-steps 200 --remain-steps 4054 --decay-interval 506 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
Using TCMalloc
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGXA100_multi_2x8x192_dist == *\D\G\X\A\1\0\0* ]]
+ echo 'Using TCMalloc'
+ export LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
running benchmark
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 192 --test-batch-size 64 --optimizer FusedAdam --lr 2.875e-3 --warmup-steps 200 --remain-steps 4054 --decay-interval 506 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
STARTING TIMING RUN AT 2020-06-25 12:40:16 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=2.875e-3
+ TRAIN_BATCH_SIZE=192
+ TEST_BATCH_SIZE=64
+ WARMUP_STEPS=200
+ REMAIN_STEPS=4054
+ DECAY_INTERVAL=506
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=8
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 3    --dwu-num-chunks 2    --dwu-num-rs-pg 1    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-overlap-reductions    --dwu-grad-norm    '
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    '
+ MATH=fp16
+ declare -a CMD
STARTING TIMING RUN AT 2020-06-25 12:40:16 PM
+ '[' -n 4 ']'
+ '[' 16 -gt 2 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=2.875e-3
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGXA100_multi_2x8x192_dist == *\D\G\X\A\1\0\0* ]]
+ TRAIN_BATCH_SIZE=192
+ TEST_BATCH_SIZE=64
+ WARMUP_STEPS=200
+ REMAIN_STEPS=4054
+ DECAY_INTERVAL=506
Using TCMalloc
+ echo 'Using TCMalloc'
+ export LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ echo 'running benchmark'
+ NUMEPOCHS=8
running benchmark
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 3    --dwu-num-chunks 2    --dwu-num-rs-pg 1    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-overlap-reductions    --dwu-grad-norm    '
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 6 ']'
+ '[' 16 -gt 2 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 192 --test-batch-size 64 --optimizer FusedAdam --lr 2.875e-3 --warmup-steps 200 --remain-steps 4054 --decay-interval 506 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGXA100_multi_2x8x192_dist == *\D\G\X\A\1\0\0* ]]
Using TCMalloc
+ echo 'Using TCMalloc'
+ export LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ echo 'running benchmark'
running benchmark
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 192 --test-batch-size 64 --optimizer FusedAdam --lr 2.875e-3 --warmup-steps 200 --remain-steps 4054 --decay-interval 506 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
STARTING TIMING RUN AT 2020-06-25 12:40:16 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=2.875e-3
+ TRAIN_BATCH_SIZE=192
+ TEST_BATCH_SIZE=64
+ WARMUP_STEPS=200
+ REMAIN_STEPS=4054
+ DECAY_INTERVAL=506
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=8
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 3    --dwu-num-chunks 2    --dwu-num-rs-pg 1    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-overlap-reductions    --dwu-grad-norm    '
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 2 ']'
+ '[' 16 -gt 2 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGXA100_multi_2x8x192_dist == *\D\G\X\A\1\0\0* ]]
+ echo 'Using TCMalloc'
Using TCMalloc
+ export LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
running benchmark
+ echo 'running benchmark'
STARTING TIMING RUN AT 2020-06-25 12:40:16 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=2.875e-3
+ TRAIN_BATCH_SIZE=192
+ TEST_BATCH_SIZE=64
+ WARMUP_STEPS=200
+ REMAIN_STEPS=4054
+ DECAY_INTERVAL=506
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 192 --test-batch-size 64 --optimizer FusedAdam --lr 2.875e-3 --warmup-steps 200 --remain-steps 4054 --decay-interval 506 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
+ NUMEPOCHS=8
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 3    --dwu-num-chunks 2    --dwu-num-rs-pg 1    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-overlap-reductions    --dwu-grad-norm    '
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 1 ']'
+ '[' 16 -gt 2 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGXA100_multi_2x8x192_dist == *\D\G\X\A\1\0\0* ]]
Using TCMalloc
+ echo 'Using TCMalloc'
+ export LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
running benchmark
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 192 --test-batch-size 64 --optimizer FusedAdam --lr 2.875e-3 --warmup-steps 200 --remain-steps 4054 --decay-interval 506 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
STARTING TIMING RUN AT 2020-06-25 12:40:16 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=2.875e-3
+ TRAIN_BATCH_SIZE=192
+ TEST_BATCH_SIZE=64
+ WARMUP_STEPS=200
+ REMAIN_STEPS=4054
+ DECAY_INTERVAL=506
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=8
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 3    --dwu-num-chunks 2    --dwu-num-rs-pg 1    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-overlap-reductions    --dwu-grad-norm    '
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 3 ']'
+ '[' 16 -gt 2 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGXA100_multi_2x8x192_dist == *\D\G\X\A\1\0\0* ]]
Using TCMalloc
+ echo 'Using TCMalloc'
+ export LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
running benchmark
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 192 --test-batch-size 64 --optimizer FusedAdam --lr 2.875e-3 --warmup-steps 200 --remain-steps 4054 --decay-interval 506 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
STARTING TIMING RUN AT 2020-06-25 12:40:16 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=2.875e-3
+ TRAIN_BATCH_SIZE=192
+ TEST_BATCH_SIZE=64
+ WARMUP_STEPS=200
+ REMAIN_STEPS=4054
+ DECAY_INTERVAL=506
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=8
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 3    --dwu-num-chunks 2    --dwu-num-rs-pg 1    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-overlap-reductions    --dwu-grad-norm    '
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 5 ']'
+ '[' 16 -gt 2 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
Using TCMalloc
running benchmark
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGXA100_multi_2x8x192_dist == *\D\G\X\A\1\0\0* ]]
+ echo 'Using TCMalloc'
+ export LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 192 --test-batch-size 64 --optimizer FusedAdam --lr 2.875e-3 --warmup-steps 200 --remain-steps 4054 --decay-interval 506 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
STARTING TIMING RUN AT 2020-06-25 12:40:16 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=2.875e-3
+ TRAIN_BATCH_SIZE=192
+ TEST_BATCH_SIZE=64
+ WARMUP_STEPS=200
+ REMAIN_STEPS=4054
+ DECAY_INTERVAL=506
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=8
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 3    --dwu-num-chunks 2    --dwu-num-rs-pg 1    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-overlap-reductions    --dwu-grad-norm    '
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    '
+ MATH=fp16
STARTING TIMING RUN AT 2020-06-25 12:40:16 PM
+ declare -a CMD
+ DATASET_DIR=/data
+ '[' -n 2 ']'
+ PREPROC_DATADIR=/preproc_data
+ '[' 16 -gt 2 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ RESULTS_DIR=gnmt_wmt16
+ LR=2.875e-3
+ TRAIN_BATCH_SIZE=192
+ TEST_BATCH_SIZE=64
+ WARMUP_STEPS=200
+ REMAIN_STEPS=4054
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ DECAY_INTERVAL=506
+ [[ DGXA100_multi_2x8x192_dist == *\D\G\X\A\1\0\0* ]]
+ TARGET=24.0
+ MAX_SEQ_LEN=75
Using TCMalloc
+ echo 'Using TCMalloc'
+ export LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ NUMEPOCHS=8
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 3    --dwu-num-chunks 2    --dwu-num-rs-pg 1    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-overlap-reductions    --dwu-grad-norm    '
+ echo 'running benchmark'
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    '
running benchmark
+ MATH=fp16
+ declare -a CMD
+ '[' -n 1 ']'
+ '[' 16 -gt 2 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 192 --test-batch-size 64 --optimizer FusedAdam --lr 2.875e-3 --warmup-steps 200 --remain-steps 4054 --decay-interval 506 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGXA100_multi_2x8x192_dist == *\D\G\X\A\1\0\0* ]]
Using TCMalloc
+ echo 'Using TCMalloc'
+ export LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ echo 'running benchmark'
running benchmark
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 192 --test-batch-size 64 --optimizer FusedAdam --lr 2.875e-3 --warmup-steps 200 --remain-steps 4054 --decay-interval 506 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
STARTING TIMING RUN AT 2020-06-25 12:40:16 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=2.875e-3
+ TRAIN_BATCH_SIZE=192
+ TEST_BATCH_SIZE=64
+ WARMUP_STEPS=200
+ REMAIN_STEPS=4054
+ DECAY_INTERVAL=506
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=8
STARTING TIMING RUN AT 2020-06-25 12:40:16 PM
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 3    --dwu-num-chunks 2    --dwu-num-rs-pg 1    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-overlap-reductions    --dwu-grad-norm    '
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    '
+ MATH=fp16
+ declare -a CMD
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ '[' -n 4 ']'
+ RESULTS_DIR=gnmt_wmt16
+ '[' 16 -gt 2 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ LR=2.875e-3
+ TRAIN_BATCH_SIZE=192
+ TEST_BATCH_SIZE=64
+ WARMUP_STEPS=200
+ REMAIN_STEPS=4054
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ DECAY_INTERVAL=506
+ TARGET=24.0
+ [[ DGXA100_multi_2x8x192_dist == *\D\G\X\A\1\0\0* ]]
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=8
Using TCMalloc
+ echo 'Using TCMalloc'
+ export LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 3    --dwu-num-chunks 2    --dwu-num-rs-pg 1    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-overlap-reductions    --dwu-grad-norm    '
running benchmark
+ echo 'running benchmark'
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 0 ']'
STARTING TIMING RUN AT 2020-06-25 12:40:16 PM
+ '[' 16 -gt 2 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 192 --test-batch-size 64 --optimizer FusedAdam --lr 2.875e-3 --warmup-steps 200 --remain-steps 4054 --decay-interval 506 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGXA100_multi_2x8x192_dist == *\D\G\X\A\1\0\0* ]]
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
Using TCMalloc
+ echo 'Using TCMalloc'
+ export LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ LR=2.875e-3
+ TRAIN_BATCH_SIZE=192
+ TEST_BATCH_SIZE=64
running benchmark
+ echo 'running benchmark'
+ WARMUP_STEPS=200
+ REMAIN_STEPS=4054
+ DECAY_INTERVAL=506
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=8
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 3    --dwu-num-chunks 2    --dwu-num-rs-pg 1    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-overlap-reductions    --dwu-grad-norm    '
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    '
+ MATH=fp16
+ declare -a CMD
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 192 --test-batch-size 64 --optimizer FusedAdam --lr 2.875e-3 --warmup-steps 200 --remain-steps 4054 --decay-interval 506 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
+ '[' -n 6 ']'
+ '[' 16 -gt 2 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGXA100_multi_2x8x192_dist == *\D\G\X\A\1\0\0* ]]
Using TCMalloc
+ echo 'Using TCMalloc'
+ export LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
running benchmark
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 192 --test-batch-size 64 --optimizer FusedAdam --lr 2.875e-3 --warmup-steps 200 --remain-steps 4054 --decay-interval 506 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
STARTING TIMING RUN AT 2020-06-25 12:40:16 PM
STARTING TIMING RUN AT 2020-06-25 12:40:16 PM
+ DATASET_DIR=/data
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=2.875e-3
+ LR=2.875e-3
+ TRAIN_BATCH_SIZE=192
+ TRAIN_BATCH_SIZE=192
+ TEST_BATCH_SIZE=64
+ TEST_BATCH_SIZE=64
+ WARMUP_STEPS=200
+ WARMUP_STEPS=200
+ REMAIN_STEPS=4054
+ REMAIN_STEPS=4054
+ DECAY_INTERVAL=506
+ DECAY_INTERVAL=506
+ TARGET=24.0
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=8
+ NUMEPOCHS=8
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 3    --dwu-num-chunks 2    --dwu-num-rs-pg 1    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-overlap-reductions    --dwu-grad-norm    '
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 3    --dwu-num-chunks 2    --dwu-num-rs-pg 1    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-overlap-reductions    --dwu-grad-norm    '
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    '
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    '
+ MATH=fp16
+ MATH=fp16
+ declare -a CMD
+ declare -a CMD
+ '[' -n 3 ']'
+ '[' -n 7 ']'
+ '[' 16 -gt 2 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' 16 -gt 2 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGXA100_multi_2x8x192_dist == *\D\G\X\A\1\0\0* ]]
+ [[ DGXA100_multi_2x8x192_dist == *\D\G\X\A\1\0\0* ]]
Using TCMalloc
+ echo 'Using TCMalloc'
Using TCMalloc
+ echo 'Using TCMalloc'
+ export LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ export LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
running benchmark
+ echo 'running benchmark'
running benchmark
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 192 --test-batch-size 64 --optimizer FusedAdam --lr 2.875e-3 --warmup-steps 200 --remain-steps 4054 --decay-interval 506 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 192 --test-batch-size 64 --optimizer FusedAdam --lr 2.875e-3 --warmup-steps 200 --remain-steps 4054 --decay-interval 506 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
num_sockets = 2 num_nodes=8 cores_per_socket=64
+ exec numactl --physcpubind=32-47,160-175 --membind=2 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 192 --test-batch-size 64 --optimizer FusedAdam --lr 2.875e-3 --warmup-steps 200 --remain-steps 4054 --decay-interval 506 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
num_sockets = 2 num_nodes=8 cores_per_socket=64
+ exec numactl --physcpubind=64-79,192-207 --membind=4 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 192 --test-batch-size 64 --optimizer FusedAdam --lr 2.875e-3 --warmup-steps 200 --remain-steps 4054 --decay-interval 506 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
num_sockets = 2 num_nodes=8 cores_per_socket=64
num_sockets = 2 num_nodes=8 cores_per_socket=64
num_sockets = 2 num_nodes=8 cores_per_socket=64
num_sockets = 2 num_nodes=8 cores_per_socket=64
num_sockets = 2 num_nodes=8 cores_per_socket=64
num_sockets = 2 num_nodes=8 cores_per_socket=64
+ exec numactl --physcpubind=48-63,176-191 --membind=3 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 192 --test-batch-size 64 --optimizer FusedAdam --lr 2.875e-3 --warmup-steps 200 --remain-steps 4054 --decay-interval 506 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
+ exec numactl --physcpubind=0-15,128-143 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 192 --test-batch-size 64 --optimizer FusedAdam --lr 2.875e-3 --warmup-steps 200 --remain-steps 4054 --decay-interval 506 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
+ exec numactl --physcpubind=112-127,240-255 --membind=7 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 192 --test-batch-size 64 --optimizer FusedAdam --lr 2.875e-3 --warmup-steps 200 --remain-steps 4054 --decay-interval 506 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
+ exec numactl --physcpubind=80-95,208-223 --membind=5 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 192 --test-batch-size 64 --optimizer FusedAdam --lr 2.875e-3 --warmup-steps 200 --remain-steps 4054 --decay-interval 506 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
+ exec numactl --physcpubind=96-111,224-239 --membind=6 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 192 --test-batch-size 64 --optimizer FusedAdam --lr 2.875e-3 --warmup-steps 200 --remain-steps 4054 --decay-interval 506 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
+ exec numactl --physcpubind=16-31,144-159 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 192 --test-batch-size 64 --optimizer FusedAdam --lr 2.875e-3 --warmup-steps 200 --remain-steps 4054 --decay-interval 506 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
num_sockets = 2 num_nodes=8 cores_per_socket=64
+ exec numactl --physcpubind=64-79,192-207 --membind=4 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 192 --test-batch-size 64 --optimizer FusedAdam --lr 2.875e-3 --warmup-steps 200 --remain-steps 4054 --decay-interval 506 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
num_sockets = 2 num_nodes=8 cores_per_socket=64
+ exec numactl --physcpubind=16-31,144-159 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 192 --test-batch-size 64 --optimizer FusedAdam --lr 2.875e-3 --warmup-steps 200 --remain-steps 4054 --decay-interval 506 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
num_sockets = 2 num_nodes=8 cores_per_socket=64
num_sockets = 2 num_nodes=8 cores_per_socket=64
+ exec numactl --physcpubind=48-63,176-191 --membind=3 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 192 --test-batch-size 64 --optimizer FusedAdam --lr 2.875e-3 --warmup-steps 200 --remain-steps 4054 --decay-interval 506 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
+ exec numactl --physcpubind=112-127,240-255 --membind=7 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 192 --test-batch-size 64 --optimizer FusedAdam --lr 2.875e-3 --warmup-steps 200 --remain-steps 4054 --decay-interval 506 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
num_sockets = 2 num_nodes=8 cores_per_socket=64
+ exec numactl --physcpubind=96-111,224-239 --membind=6 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 192 --test-batch-size 64 --optimizer FusedAdam --lr 2.875e-3 --warmup-steps 200 --remain-steps 4054 --decay-interval 506 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
num_sockets = 2 num_nodes=8 cores_per_socket=64
num_sockets = 2 num_nodes=8 cores_per_socket=64
num_sockets = 2 num_nodes=8 cores_per_socket=64
+ exec numactl --physcpubind=0-15,128-143 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 192 --test-batch-size 64 --optimizer FusedAdam --lr 2.875e-3 --warmup-steps 200 --remain-steps 4054 --decay-interval 506 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
+ exec numactl --physcpubind=32-47,160-175 --membind=2 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 192 --test-batch-size 64 --optimizer FusedAdam --lr 2.875e-3 --warmup-steps 200 --remain-steps 4054 --decay-interval 506 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
+ exec numactl --physcpubind=80-95,208-223 --membind=5 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 192 --test-batch-size 64 --optimizer FusedAdam --lr 2.875e-3 --warmup-steps 200 --remain-steps 4054 --decay-interval 506 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
:::MLLOG {"namespace": "", "time_ms": 1593114017925, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593114017995, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593114018073, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593114018079, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593114018087, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593114018098, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593114018099, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593114018110, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593114018125, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593114018160, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593114018161, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593114018189, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593114018240, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593114018267, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593114018268, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593114018273, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
NCCL version 2.7.5+cuda11.0
0: Saving results to: results/gnmt_wmt16
0: Run arguments: Namespace(apex_message_size=10000000.0, apex_num_allreduce_streams=1, batching='bucketing', beam_size=5, cov_penalty_factor=0.1, cuda=True, cudnn=True, dataset_dir='/data', decay_factor=0.5, decay_interval=506, decay_steps=4, distributed_weight_update=2, dropout=0.2, dwu_e5m2_allgather=False, dwu_full_pipeline=False, dwu_grad_norm=True, dwu_group_size=0, dwu_num_ag_pg=0, dwu_num_ar_pg=2, dwu_num_blocks=3, dwu_num_chunks=2, dwu_num_rs_pg=1, dwu_overlap_reductions=True, enable_apex_allreduce_overlap=False, env=False, epochs=8, eval=True, fused_attention=True, fused_xentropy=True, grad_clip=5.0, hidden_size=1024, init_scale=1024, intra_epoch_eval=0, keep_checkpoints=0, len_norm_const=5.0, len_norm_factor=0.6, local_rank=0, log_all_ranks=False, lr=0.002875, math='fp16', max_length_test=150, max_length_train=75, max_size=None, min_length_test=0, min_length_train=0, num_buckets=5, num_layers=4, optimizer='FusedAdam', optimizer_extra='{}', prealloc_mode='once', preproc_data_dir='/preproc_data/75', print_freq=10, rank=0, remain_steps=4054, results_dir='results', resume=None, save='gnmt_wmt16', save_all=False, save_freq=5000, save_path='results/gnmt_wmt16', seed=None, shard_size=80, share_embedding=True, smoothing=0.1, start_epoch=0, target_bleu=24.0, test_batch_size=64, test_loader_workers=0, train_batch_size=192, train_global_batch_size=None, train_iter_size=1, train_loader_workers=1, upscale_interval=128, use_preproc_data=True, warmup_steps=200)
0: L2 promotion: 128B
0: Using random master seed: 2393960718
:::MLLOG {"namespace": "", "time_ms": 1593114028338, "event_type": "POINT_IN_TIME", "key": "seed", "value": 2393960718, "metadata": {"file": "/workspace/rnn_translator/seq2seq/utils.py", "lineno": 144}}
0: Worker 0 is using worker seed: 4146376851
0: Building vocabulary from /data/vocab.bpe.32000
0: Size of vocabulary: 32320
0: Fused attention flag set to True
0: GNMT(
  (embedder): Embedding(32320, 1024, padding_idx=0)
  (encoder): ResidualRecurrentEncoder(
    (rnn_layers): ModuleList(
      (0): EmuBidirLSTM(
        (bidir): LSTM(1024, 1024, bidirectional=True)
        (layer1): LSTM(1024, 1024)
        (layer2): LSTM(1024, 1024)
      )
      (1): LSTM(2048, 1024)
      (2): LSTM(1024, 1024)
      (3): LSTM(1024, 1024)
    )
    (dropout): Dropout(p=0.2, inplace=False)
    (embedder): Embedding(32320, 1024, padding_idx=0)
  )
  (decoder): ResidualRecurrentDecoder(
    (att_rnn): RecurrentAttention(
      (rnn): LSTM(1024, 1024)
      (attn): BahdanauAttention(
        (linear_q): Linear(in_features=1024, out_features=1024, bias=False)
        (linear_k): Linear(in_features=1024, out_features=1024, bias=False)
      )
      (dropout): Dropout(p=0.2, inplace=False)
    )
    (rnn_layers): ModuleList(
      (0): LSTM(2048, 1024)
      (1): LSTM(2048, 1024)
      (2): LSTM(2048, 1024)
    )
    (embedder): Embedding(32320, 1024, padding_idx=0)
    (classifier): Classifier(
      (classifier): Linear(in_features=1024, out_features=32320, bias=True)
    )
    (dropout): Dropout(p=0.2, inplace=False)
  )
)
0: Building LabelSmoothingLoss (smoothing: 0.1)
0: Fused xentropy flag set to True
0: Training optimizer config: {'optimizer': 'FusedAdam', 'lr': 0.002875}
0: Number of parameters: 160671297
0: Saving state of the tokenizer
0: Initializing dwu fp16 optimizer
self._net_total_param_size=160671360, self._total_param_size=160677888, dwu_min_page_size=12288, self._block_size=53559296, self._chunk_size=26779648, self._shard_size=3347456
[0, 9, 34]
self._net_total_param_size=160671360, self._total_param_size=160677888, dwu_min_page_size=12288, self._block_size=53559296, self._chunk_size=26779648, self._shard_size=3347456
[0, 9, 34]
self._net_total_param_size=160671360, self._total_param_size=160677888, dwu_min_page_size=12288, self._block_size=53559296, self._chunk_size=26779648, self._shard_size=3347456
[0, 9, 34]
self._net_total_param_size=160671360, self._total_param_size=160677888, dwu_min_page_size=12288, self._block_size=53559296, self._chunk_size=26779648, self._shard_size=3347456
[0, 9, 34]
self._net_total_param_size=160671360, self._total_param_size=160677888, dwu_min_page_size=12288, self._block_size=53559296, self._chunk_size=26779648, self._shard_size=3347456
[0, 9, 34]
self._net_total_param_size=160671360, self._total_param_size=160677888, dwu_min_page_size=12288, self._block_size=53559296, self._chunk_size=26779648, self._shard_size=3347456
[0, 9, 34]
self._net_total_param_size=160671360, self._total_param_size=160677888, dwu_min_page_size=12288, self._block_size=53559296, self._chunk_size=26779648, self._shard_size=3347456
[0, 9, 34]
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([2968576]), new_param_packed_fragment.size()=torch.Size([2968576]), master_param_fragment.size()=torch.Size([2968576])
model_param_fragment.size()=torch.Size([378880]), new_param_packed_fragment.size()=torch.Size([378880]), master_param_fragment.size()=torch.Size([378880])
model_param_fragment.size()=torch.Size([1371136]), new_param_packed_fragment.size()=torch.Size([1371136]), master_param_fragment.size()=torch.Size([1371136])
model_param_fragment.size()=torch.Size([1976320]), new_param_packed_fragment.size()=torch.Size([1976320]), master_param_fragment.size()=torch.Size([1976320])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([283712]), new_param_packed_fragment.size()=torch.Size([283712]), master_param_fragment.size()=torch.Size([283712])
model_param_fragment.size()=torch.Size([3063744]), new_param_packed_fragment.size()=torch.Size([3063744]), master_param_fragment.size()=torch.Size([3063744])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
NCCL version 2.7.5+cuda11.0
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3129344]), new_param_packed_fragment.size()=torch.Size([3129344]), master_param_fragment.size()=torch.Size([3129344])
model_param_fragment.size()=torch.Size([218112]), new_param_packed_fragment.size()=torch.Size([218112]), master_param_fragment.size()=torch.Size([218112])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([2169856]), new_param_packed_fragment.size()=torch.Size([2169856]), master_param_fragment.size()=torch.Size([2169856])
model_param_fragment.size()=torch.Size([1177600]), new_param_packed_fragment.size()=torch.Size([1177600]), master_param_fragment.size()=torch.Size([1177600])
model_param_fragment.size()=torch.Size([572416]), new_param_packed_fragment.size()=torch.Size([572416]), master_param_fragment.size()=torch.Size([572416])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([2766848]), new_param_packed_fragment.size()=torch.Size([2766848]), master_param_fragment.size()=torch.Size([2766848])
model_param_fragment.size()=torch.Size([1082432]), new_param_packed_fragment.size()=torch.Size([1082432]), master_param_fragment.size()=torch.Size([1082432])
model_param_fragment.size()=torch.Size([2265024]), new_param_packed_fragment.size()=torch.Size([2265024]), master_param_fragment.size()=torch.Size([2265024])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
NCCL version 2.7.5+cuda11.0
self._net_total_param_size=160671360, self._total_param_size=160677888, dwu_min_page_size=12288, self._block_size=53559296, self._chunk_size=26779648, self._shard_size=3347456
[0, 9, 34]
self._net_total_param_size=160671360, self._total_param_size=160677888, dwu_min_page_size=12288, self._block_size=53559296, self._chunk_size=26779648, self._shard_size=3347456
[0, 9, 34]
self._net_total_param_size=160671360, self._total_param_size=160677888, dwu_min_page_size=12288, self._block_size=53559296, self._chunk_size=26779648, self._shard_size=3347456
[0, 9, 34]
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([1314816]), new_param_packed_fragment.size()=torch.Size([1314816]), master_param_fragment.size()=torch.Size([1314816])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([2024448]), new_param_packed_fragment.size()=torch.Size([2024448]), master_param_fragment.size()=torch.Size([2024448])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([2832448]), new_param_packed_fragment.size()=torch.Size([2832448]), master_param_fragment.size()=torch.Size([2832448])
model_param_fragment.size()=torch.Size([515008]), new_param_packed_fragment.size()=torch.Size([515008]), master_param_fragment.size()=torch.Size([515008])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
self._net_total_param_size=160671360, self._total_param_size=160677888, dwu_min_page_size=12288, self._block_size=53559296, self._chunk_size=26779648, self._shard_size=3347456
[0, 9, 34]
NCCL version 2.7.5+cuda11.0
self._net_total_param_size=160671360, self._total_param_size=160677888, dwu_min_page_size=12288, self._block_size=53559296, self._chunk_size=26779648, self._shard_size=3347456
[0, 9, 34]
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([2274304]), new_param_packed_fragment.size()=torch.Size([2274304]), master_param_fragment.size()=torch.Size([2274304])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([1064960]), new_param_packed_fragment.size()=torch.Size([1064960]), master_param_fragment.size()=torch.Size([1064960])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3308608]), new_param_packed_fragment.size()=torch.Size([3308608]), master_param_fragment.size()=torch.Size([3308608])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([32320]), new_param_packed_fragment.size()=torch.Size([32320]), master_param_fragment.size()=torch.Size([32320])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3129344]), new_param_packed_fragment.size()=torch.Size([3129344]), master_param_fragment.size()=torch.Size([3129344])
model_param_fragment.size()=torch.Size([218112]), new_param_packed_fragment.size()=torch.Size([218112]), master_param_fragment.size()=torch.Size([218112])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
self._net_total_param_size=160671360, self._total_param_size=160677888, dwu_min_page_size=12288, self._block_size=53559296, self._chunk_size=26779648, self._shard_size=3347456
NCCL version 2.7.5+cuda11.0
[0, 9, 34]
self._net_total_param_size=160671360, self._total_param_size=160677888, dwu_min_page_size=12288, self._block_size=53559296, self._chunk_size=26779648, self._shard_size=3347456
[0, 9, 34]
self._net_total_param_size=160671360, self._total_param_size=160677888, dwu_min_page_size=12288, self._block_size=53559296, self._chunk_size=26779648, self._shard_size=3347456
[0, 9, 34]
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([2169856]), new_param_packed_fragment.size()=torch.Size([2169856]), master_param_fragment.size()=torch.Size([2169856])
model_param_fragment.size()=torch.Size([1177600]), new_param_packed_fragment.size()=torch.Size([1177600]), master_param_fragment.size()=torch.Size([1177600])
model_param_fragment.size()=torch.Size([572416]), new_param_packed_fragment.size()=torch.Size([572416]), master_param_fragment.size()=torch.Size([572416])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([2766848]), new_param_packed_fragment.size()=torch.Size([2766848]), master_param_fragment.size()=torch.Size([2766848])
model_param_fragment.size()=torch.Size([1082432]), new_param_packed_fragment.size()=torch.Size([1082432]), master_param_fragment.size()=torch.Size([1082432])
model_param_fragment.size()=torch.Size([2265024]), new_param_packed_fragment.size()=torch.Size([2265024]), master_param_fragment.size()=torch.Size([2265024])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([467968]), new_param_packed_fragment.size()=torch.Size([467968]), master_param_fragment.size()=torch.Size([467968])
model_param_fragment.size()=torch.Size([2879488]), new_param_packed_fragment.size()=torch.Size([2879488]), master_param_fragment.size()=torch.Size([2879488])
model_param_fragment.size()=torch.Size([3073024]), new_param_packed_fragment.size()=torch.Size([3073024]), master_param_fragment.size()=torch.Size([3073024])
model_param_fragment.size()=torch.Size([274432]), new_param_packed_fragment.size()=torch.Size([274432]), master_param_fragment.size()=torch.Size([274432])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([1314816]), new_param_packed_fragment.size()=torch.Size([1314816]), master_param_fragment.size()=torch.Size([1314816])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([2024448]), new_param_packed_fragment.size()=torch.Size([2024448]), master_param_fragment.size()=torch.Size([2024448])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([2832448]), new_param_packed_fragment.size()=torch.Size([2832448]), master_param_fragment.size()=torch.Size([2832448])
model_param_fragment.size()=torch.Size([515008]), new_param_packed_fragment.size()=torch.Size([515008]), master_param_fragment.size()=torch.Size([515008])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
NCCL version 2.7.5+cuda11.0
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3016704]), new_param_packed_fragment.size()=torch.Size([3016704]), master_param_fragment.size()=torch.Size([3016704])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([322560]), new_param_packed_fragment.size()=torch.Size([322560]), master_param_fragment.size()=torch.Size([322560])
model_param_fragment.size()=torch.Size([1427456]), new_param_packed_fragment.size()=torch.Size([1427456]), master_param_fragment.size()=torch.Size([1427456])
model_param_fragment.size()=torch.Size([1920000]), new_param_packed_fragment.size()=torch.Size([1920000]), master_param_fragment.size()=torch.Size([1920000])
model_param_fragment.size()=torch.Size([1929280]), new_param_packed_fragment.size()=torch.Size([1929280]), master_param_fragment.size()=torch.Size([1929280])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([1409984]), new_param_packed_fragment.size()=torch.Size([1409984]), master_param_fragment.size()=torch.Size([1409984])
model_param_fragment.size()=torch.Size([331840]), new_param_packed_fragment.size()=torch.Size([331840]), master_param_fragment.size()=torch.Size([331840])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([3007424]), new_param_packed_fragment.size()=torch.Size([3007424]), master_param_fragment.size()=torch.Size([3007424])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([2217984]), new_param_packed_fragment.size()=torch.Size([2217984]), master_param_fragment.size()=torch.Size([2217984])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([1121280]), new_param_packed_fragment.size()=torch.Size([1121280]), master_param_fragment.size()=torch.Size([1121280])
model_param_fragment.size()=torch.Size([628736]), new_param_packed_fragment.size()=torch.Size([628736]), master_param_fragment.size()=torch.Size([628736])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([1024]), new_param_packed_fragment.size()=torch.Size([1024]), master_param_fragment.size()=torch.Size([1024])
model_param_fragment.size()=torch.Size([1]), new_param_packed_fragment.size()=torch.Size([1]), master_param_fragment.size()=torch.Size([1])
model_param_fragment.size()=torch.Size([1024]), new_param_packed_fragment.size()=torch.Size([1024]), master_param_fragment.size()=torch.Size([1024])
model_param_fragment.size()=torch.Size([1048576]), new_param_packed_fragment.size()=torch.Size([1048576]), master_param_fragment.size()=torch.Size([1048576])
model_param_fragment.size()=torch.Size([1048576]), new_param_packed_fragment.size()=torch.Size([1048576]), master_param_fragment.size()=torch.Size([1048576])
model_param_fragment.size()=torch.Size([611264]), new_param_packed_fragment.size()=torch.Size([611264]), master_param_fragment.size()=torch.Size([611264])
model_param_fragment.size()=torch.Size([1130560]), new_param_packed_fragment.size()=torch.Size([1130560]), master_param_fragment.size()=torch.Size([1130560])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([2208704]), new_param_packed_fragment.size()=torch.Size([2208704]), master_param_fragment.size()=torch.Size([2208704])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([2217984]), new_param_packed_fragment.size()=torch.Size([2217984]), master_param_fragment.size()=torch.Size([2217984])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([1121280]), new_param_packed_fragment.size()=torch.Size([1121280]), master_param_fragment.size()=torch.Size([1121280])
model_param_fragment.size()=torch.Size([628736]), new_param_packed_fragment.size()=torch.Size([628736]), master_param_fragment.size()=torch.Size([628736])
model_param_fragment.size()=torch.Size([2274304]), new_param_packed_fragment.size()=torch.Size([2274304]), master_param_fragment.size()=torch.Size([2274304])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([1024]), new_param_packed_fragment.size()=torch.Size([1024]), master_param_fragment.size()=torch.Size([1024])
model_param_fragment.size()=torch.Size([1]), new_param_packed_fragment.size()=torch.Size([1]), master_param_fragment.size()=torch.Size([1])
model_param_fragment.size()=torch.Size([1064960]), new_param_packed_fragment.size()=torch.Size([1064960]), master_param_fragment.size()=torch.Size([1064960])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([1024]), new_param_packed_fragment.size()=torch.Size([1024]), master_param_fragment.size()=torch.Size([1024])
model_param_fragment.size()=torch.Size([1048576]), new_param_packed_fragment.size()=torch.Size([1048576]), master_param_fragment.size()=torch.Size([1048576])
model_param_fragment.size()=torch.Size([1048576]), new_param_packed_fragment.size()=torch.Size([1048576]), master_param_fragment.size()=torch.Size([1048576])
model_param_fragment.size()=torch.Size([611264]), new_param_packed_fragment.size()=torch.Size([611264]), master_param_fragment.size()=torch.Size([611264])
model_param_fragment.size()=torch.Size([3308608]), new_param_packed_fragment.size()=torch.Size([3308608]), master_param_fragment.size()=torch.Size([3308608])
model_param_fragment.size()=torch.Size([32320]), new_param_packed_fragment.size()=torch.Size([32320]), master_param_fragment.size()=torch.Size([32320])
model_param_fragment.size()=torch.Size([1130560]), new_param_packed_fragment.size()=torch.Size([1130560]), master_param_fragment.size()=torch.Size([1130560])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([2208704]), new_param_packed_fragment.size()=torch.Size([2208704]), master_param_fragment.size()=torch.Size([2208704])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
NCCL version 2.7.5+cuda11.0
model_param_fragment.size()=torch.Size([467968]), new_param_packed_fragment.size()=torch.Size([467968]), master_param_fragment.size()=torch.Size([467968])
model_param_fragment.size()=torch.Size([2879488]), new_param_packed_fragment.size()=torch.Size([2879488]), master_param_fragment.size()=torch.Size([2879488])
model_param_fragment.size()=torch.Size([3073024]), new_param_packed_fragment.size()=torch.Size([3073024]), master_param_fragment.size()=torch.Size([3073024])
model_param_fragment.size()=torch.Size([274432]), new_param_packed_fragment.size()=torch.Size([274432]), master_param_fragment.size()=torch.Size([274432])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([2968576]), new_param_packed_fragment.size()=torch.Size([2968576]), master_param_fragment.size()=torch.Size([2968576])
model_param_fragment.size()=torch.Size([378880]), new_param_packed_fragment.size()=torch.Size([378880]), master_param_fragment.size()=torch.Size([378880])
model_param_fragment.size()=torch.Size([1371136]), new_param_packed_fragment.size()=torch.Size([1371136]), master_param_fragment.size()=torch.Size([1371136])
model_param_fragment.size()=torch.Size([1976320]), new_param_packed_fragment.size()=torch.Size([1976320]), master_param_fragment.size()=torch.Size([1976320])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([283712]), new_param_packed_fragment.size()=torch.Size([283712]), master_param_fragment.size()=torch.Size([283712])
model_param_fragment.size()=torch.Size([3063744]), new_param_packed_fragment.size()=torch.Size([3063744]), master_param_fragment.size()=torch.Size([3063744])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
self._net_total_param_size=160671360, self._total_param_size=160677888, dwu_min_page_size=12288, self._block_size=53559296, self._chunk_size=26779648, self._shard_size=3347456
[0, 9, 34]
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3016704]), new_param_packed_fragment.size()=torch.Size([3016704]), master_param_fragment.size()=torch.Size([3016704])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([322560]), new_param_packed_fragment.size()=torch.Size([322560]), master_param_fragment.size()=torch.Size([322560])
model_param_fragment.size()=torch.Size([1427456]), new_param_packed_fragment.size()=torch.Size([1427456]), master_param_fragment.size()=torch.Size([1427456])
model_param_fragment.size()=torch.Size([1920000]), new_param_packed_fragment.size()=torch.Size([1920000]), master_param_fragment.size()=torch.Size([1920000])
model_param_fragment.size()=torch.Size([1929280]), new_param_packed_fragment.size()=torch.Size([1929280]), master_param_fragment.size()=torch.Size([1929280])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([1409984]), new_param_packed_fragment.size()=torch.Size([1409984]), master_param_fragment.size()=torch.Size([1409984])
model_param_fragment.size()=torch.Size([331840]), new_param_packed_fragment.size()=torch.Size([331840]), master_param_fragment.size()=torch.Size([331840])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([3007424]), new_param_packed_fragment.size()=torch.Size([3007424]), master_param_fragment.size()=torch.Size([3007424])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
NCCL version 2.7.5+cuda11.0
NCCL version 2.7.5+cuda11.0
0: Using optimizer: DistributedFusedAdam (
Parameter Group 0
    betas: (0.9, 0.999)
    bias_correction: True
    eps: 1e-08
    lr: 0.002875
    max_grad_norm: 5.0
    weight_decay: 0.0
)
:::MLLOG {"namespace": "", "time_ms": 1593114040307, "event_type": "POINT_IN_TIME", "key": "opt_name", "value": "adam", "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/trainer.py", "lineno": 176}}
:::MLLOG {"namespace": "", "time_ms": 1593114040307, "event_type": "POINT_IN_TIME", "key": "opt_base_learning_rate", "value": 0.002875, "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/trainer.py", "lineno": 178}}
:::MLLOG {"namespace": "", "time_ms": 1593114040307, "event_type": "POINT_IN_TIME", "key": "opt_adam_beta_1", "value": 0.9, "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/trainer.py", "lineno": 180}}
:::MLLOG {"namespace": "", "time_ms": 1593114040307, "event_type": "POINT_IN_TIME", "key": "opt_adam_beta_2", "value": 0.999, "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/trainer.py", "lineno": 182}}
:::MLLOG {"namespace": "", "time_ms": 1593114040307, "event_type": "POINT_IN_TIME", "key": "opt_adam_epsilon", "value": 1e-08, "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/trainer.py", "lineno": 184}}
0: Executing preallocation
:::MLLOG {"namespace": "", "time_ms": 1593114041754, "event_type": "INTERVAL_END", "key": "init_stop", "value": null, "metadata": {"file": "train.py", "lineno": 429}}
:::MLLOG {"namespace": "", "time_ms": 1593114041755, "event_type": "INTERVAL_START", "key": "run_start", "value": null, "metadata": {"file": "train.py", "lineno": 430}}
:::MLLOG {"namespace": "", "time_ms": 1593114041755, "event_type": "POINT_IN_TIME", "key": "max_sequence_length", "value": 75, "metadata": {"file": "train.py", "lineno": 435, "method": "discard"}}
0: Reading preprocessed data file from /preproc_data/75/training.bin
0: Preprocessed data: length: 3975116 min length: 0 max length: 75
0: Opening preprocessed data /preproc_data/75/training.bin for reading
0: Processing data from /data/newstest2014.tok.bpe.32000.en
0: Filtering data, min len: 0, max len: 150
0: Pairs before: 3003, after: 3003
:::MLLOG {"namespace": "", "time_ms": 1593114042016, "event_type": "POINT_IN_TIME", "key": "global_batch_size", "value": 3072, "metadata": {"file": "train.py", "lineno": 480}}
:::MLLOG {"namespace": "", "time_ms": 1593114042017, "event_type": "POINT_IN_TIME", "key": "train_samples", "value": 3965952, "metadata": {"file": "train.py", "lineno": 488}}
:::MLLOG {"namespace": "", "time_ms": 1593114042017, "event_type": "POINT_IN_TIME", "key": "eval_samples", "value": 3003, "metadata": {"file": "train.py", "lineno": 490}}
0: Training LR schedule config: {'warmup_steps': 200, 'remain_steps': 4054, 'decay_interval': 506, 'decay_steps': 4, 'decay_factor': 0.5}
0: Scheduler warmup steps: 200
0: Scheduler remain steps: 4054
0: Scheduler decay interval: 506
0: Scheduler decay factor: 0.5
0: Scheduler max decay steps: 4
:::MLLOG {"namespace": "", "time_ms": 1593114042018, "event_type": "POINT_IN_TIME", "key": "opt_learning_rate_alt_decay_func", "value": true, "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/lr_scheduler.py", "lineno": 78}}
:::MLLOG {"namespace": "", "time_ms": 1593114042018, "event_type": "POINT_IN_TIME", "key": "opt_learning_rate_alt_warmup_func", "value": true, "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/lr_scheduler.py", "lineno": 80}}
:::MLLOG {"namespace": "", "time_ms": 1593114042018, "event_type": "POINT_IN_TIME", "key": "opt_learning_rate_decay_interval", "value": 506, "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/lr_scheduler.py", "lineno": 83}}
:::MLLOG {"namespace": "", "time_ms": 1593114042018, "event_type": "POINT_IN_TIME", "key": "opt_learning_rate_decay_factor", "value": 0.5, "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/lr_scheduler.py", "lineno": 85}}
:::MLLOG {"namespace": "", "time_ms": 1593114042018, "event_type": "POINT_IN_TIME", "key": "opt_learning_rate_decay_steps", "value": 4, "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/lr_scheduler.py", "lineno": 87}}
:::MLLOG {"namespace": "", "time_ms": 1593114042019, "event_type": "POINT_IN_TIME", "key": "opt_learning_rate_remain_steps", "value": 4054, "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/lr_scheduler.py", "lineno": 89}}
:::MLLOG {"namespace": "", "time_ms": 1593114042019, "event_type": "POINT_IN_TIME", "key": "opt_learning_rate_warmup_steps", "value": 200, "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/lr_scheduler.py", "lineno": 91}}
:::MLLOG {"namespace": "", "time_ms": 1593114042019, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "train.py", "lineno": 538, "first_epoch_num": 1, "epoch_count": 1}}
:::MLLOG {"namespace": "", "time_ms": 1593114042019, "event_type": "INTERVAL_START", "key": "epoch_start", "value": null, "metadata": {"file": "train.py", "lineno": 541, "epoch_num": 1}}
0: Starting epoch 0
0: Sampler for epoch 0 uses seed 3850321932
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
0: TRAIN [0][0/1291]	Time 0.343 (0.343)	Data 1.93e-01 (1.93e-01)	Tok/s 22765 (22765)	Loss/tok 10.6067 (10.6067)	LR 2.942e-05
0: TRAIN [0][10/1291]	Time 0.041 (0.078)	Data 1.04e-04 (1.76e-02)	Tok/s 190815 (185557)	Loss/tok 9.4763 (9.9671)	LR 3.704e-05
0: TRAIN [0][20/1291]	Time 0.058 (0.066)	Data 1.01e-04 (9.27e-03)	Tok/s 220951 (194397)	Loss/tok 9.2242 (9.6371)	LR 4.663e-05
0: TRAIN [0][30/1291]	Time 0.041 (0.063)	Data 8.80e-05 (6.31e-03)	Tok/s 195350 (198965)	Loss/tok 8.7670 (9.4202)	LR 5.870e-05
0: Gradient norm: inf
0: Skipped batch, new scale: 512.0
0: Gradient norm: inf
0: Skipped batch, new scale: 256.0
0: TRAIN [0][40/1291]	Time 0.059 (0.061)	Data 9.18e-05 (4.80e-03)	Tok/s 219174 (200129)	Loss/tok 8.6958 (9.2675)	LR 7.057e-05
0: TRAIN [0][50/1291]	Time 0.042 (0.059)	Data 9.61e-05 (3.88e-03)	Tok/s 185114 (199260)	Loss/tok 8.2840 (9.1331)	LR 8.885e-05
0: TRAIN [0][60/1291]	Time 0.041 (0.059)	Data 8.54e-05 (3.25e-03)	Tok/s 184339 (200627)	Loss/tok 8.0163 (8.9889)	LR 1.119e-04
0: Gradient norm: inf
0: Skipped batch, new scale: 128.0
0: TRAIN [0][70/1291]	Time 0.025 (0.058)	Data 8.63e-05 (2.81e-03)	Tok/s 156715 (201462)	Loss/tok 7.6218 (8.8664)	LR 1.376e-04
0: TRAIN [0][80/1291]	Time 0.041 (0.057)	Data 9.37e-05 (2.47e-03)	Tok/s 191921 (201402)	Loss/tok 7.8414 (8.7742)	LR 1.732e-04
0: Gradient norm: inf
0: Skipped batch, new scale: 64.0
0: TRAIN [0][90/1291]	Time 0.025 (0.057)	Data 8.34e-05 (2.21e-03)	Tok/s 161339 (201373)	Loss/tok 7.4863 (8.6906)	LR 2.131e-04
0: TRAIN [0][100/1291]	Time 0.042 (0.057)	Data 8.56e-05 (2.00e-03)	Tok/s 186038 (202499)	Loss/tok 7.7700 (8.6076)	LR 2.683e-04
0: TRAIN [0][110/1291]	Time 0.058 (0.057)	Data 8.44e-05 (1.83e-03)	Tok/s 216394 (202577)	Loss/tok 7.9208 (8.5463)	LR 3.378e-04
0: TRAIN [0][120/1291]	Time 0.024 (0.056)	Data 8.58e-05 (1.69e-03)	Tok/s 167491 (202469)	Loss/tok 7.0582 (8.4919)	LR 4.252e-04
0: TRAIN [0][130/1291]	Time 0.076 (0.057)	Data 8.65e-05 (1.57e-03)	Tok/s 227798 (203155)	Loss/tok 8.0728 (8.4418)	LR 5.354e-04
0: TRAIN [0][140/1291]	Time 0.042 (0.056)	Data 1.01e-04 (1.46e-03)	Tok/s 180939 (203148)	Loss/tok 7.4836 (8.3974)	LR 6.740e-04
0: TRAIN [0][150/1291]	Time 0.059 (0.056)	Data 8.20e-05 (1.37e-03)	Tok/s 217368 (203411)	Loss/tok 7.6357 (8.3484)	LR 8.485e-04
0: TRAIN [0][160/1291]	Time 0.059 (0.056)	Data 8.20e-05 (1.29e-03)	Tok/s 215627 (203308)	Loss/tok 7.5224 (8.3017)	LR 1.068e-03
0: TRAIN [0][170/1291]	Time 0.058 (0.056)	Data 8.11e-05 (1.22e-03)	Tok/s 216801 (203096)	Loss/tok 7.1958 (8.2501)	LR 1.345e-03
0: TRAIN [0][180/1291]	Time 0.059 (0.055)	Data 8.18e-05 (1.16e-03)	Tok/s 211588 (202915)	Loss/tok 7.1539 (8.1963)	LR 1.693e-03
0: TRAIN [0][190/1291]	Time 0.059 (0.056)	Data 8.13e-05 (1.10e-03)	Tok/s 212327 (203159)	Loss/tok 6.9538 (8.1319)	LR 2.131e-03
0: TRAIN [0][200/1291]	Time 0.059 (0.056)	Data 8.20e-05 (1.05e-03)	Tok/s 218276 (203536)	Loss/tok 6.8711 (8.0645)	LR 2.683e-03
0: TRAIN [0][210/1291]	Time 0.042 (0.055)	Data 7.94e-05 (1.01e-03)	Tok/s 186186 (202778)	Loss/tok 6.5363 (8.0130)	LR 2.875e-03
0: Upscaling, new scale: 128.0
0: TRAIN [0][220/1291]	Time 0.076 (0.055)	Data 8.44e-05 (9.64e-04)	Tok/s 230536 (202932)	Loss/tok 6.5064 (7.9429)	LR 2.875e-03
0: TRAIN [0][230/1291]	Time 0.077 (0.055)	Data 1.42e-04 (9.26e-04)	Tok/s 229072 (202637)	Loss/tok 6.5380 (7.8812)	LR 2.875e-03
0: TRAIN [0][240/1291]	Time 0.076 (0.055)	Data 7.51e-05 (8.91e-04)	Tok/s 230879 (202438)	Loss/tok 6.4178 (7.8169)	LR 2.875e-03
0: TRAIN [0][250/1291]	Time 0.042 (0.055)	Data 7.84e-05 (8.59e-04)	Tok/s 183809 (202547)	Loss/tok 5.8797 (7.7493)	LR 2.875e-03
0: TRAIN [0][260/1291]	Time 0.042 (0.055)	Data 8.39e-05 (8.29e-04)	Tok/s 181103 (202438)	Loss/tok 5.6776 (7.6844)	LR 2.875e-03
0: TRAIN [0][270/1291]	Time 0.042 (0.055)	Data 7.82e-05 (8.02e-04)	Tok/s 187482 (202411)	Loss/tok 5.6771 (7.6182)	LR 2.875e-03
0: TRAIN [0][280/1291]	Time 0.042 (0.055)	Data 8.25e-05 (7.77e-04)	Tok/s 181087 (201917)	Loss/tok 5.4404 (7.5638)	LR 2.875e-03
0: TRAIN [0][290/1291]	Time 0.025 (0.055)	Data 1.43e-04 (7.53e-04)	Tok/s 162412 (201624)	Loss/tok 4.6514 (7.5046)	LR 2.875e-03
0: TRAIN [0][300/1291]	Time 0.059 (0.055)	Data 7.87e-05 (7.31e-04)	Tok/s 212029 (202197)	Loss/tok 5.5767 (7.4262)	LR 2.875e-03
0: TRAIN [0][310/1291]	Time 0.077 (0.055)	Data 7.80e-05 (7.10e-04)	Tok/s 229006 (202232)	Loss/tok 5.5942 (7.3589)	LR 2.875e-03
0: TRAIN [0][320/1291]	Time 0.025 (0.055)	Data 7.82e-05 (6.90e-04)	Tok/s 149276 (202393)	Loss/tok 4.0676 (7.2896)	LR 2.875e-03
0: TRAIN [0][330/1291]	Time 0.058 (0.055)	Data 7.80e-05 (6.72e-04)	Tok/s 217512 (201843)	Loss/tok 5.2589 (7.2429)	LR 2.875e-03
0: TRAIN [0][340/1291]	Time 0.041 (0.055)	Data 7.58e-05 (6.55e-04)	Tok/s 180510 (201931)	Loss/tok 4.7568 (7.1791)	LR 2.875e-03
0: Upscaling, new scale: 256.0
0: TRAIN [0][350/1291]	Time 0.041 (0.055)	Data 7.82e-05 (6.39e-04)	Tok/s 184973 (201456)	Loss/tok 4.6075 (7.1301)	LR 2.875e-03
0: TRAIN [0][360/1291]	Time 0.042 (0.054)	Data 1.39e-04 (6.24e-04)	Tok/s 180721 (201323)	Loss/tok 4.4003 (7.0740)	LR 2.875e-03
0: TRAIN [0][370/1291]	Time 0.058 (0.054)	Data 7.75e-05 (6.10e-04)	Tok/s 219703 (201197)	Loss/tok 4.8808 (7.0169)	LR 2.875e-03
0: TRAIN [0][380/1291]	Time 0.059 (0.054)	Data 7.68e-05 (5.96e-04)	Tok/s 215388 (201029)	Loss/tok 4.8218 (6.9623)	LR 2.875e-03
0: TRAIN [0][390/1291]	Time 0.042 (0.054)	Data 7.61e-05 (5.83e-04)	Tok/s 186258 (200491)	Loss/tok 4.1749 (6.9174)	LR 2.875e-03
0: TRAIN [0][400/1291]	Time 0.026 (0.054)	Data 7.84e-05 (5.70e-04)	Tok/s 156840 (200144)	Loss/tok 3.6947 (6.8695)	LR 2.875e-03
0: TRAIN [0][410/1291]	Time 0.043 (0.054)	Data 7.70e-05 (5.59e-04)	Tok/s 183318 (200071)	Loss/tok 4.0793 (6.8107)	LR 2.875e-03
0: TRAIN [0][420/1291]	Time 0.076 (0.054)	Data 7.89e-05 (5.47e-04)	Tok/s 234563 (200189)	Loss/tok 4.7883 (6.7508)	LR 2.875e-03
0: TRAIN [0][430/1291]	Time 0.058 (0.054)	Data 7.63e-05 (5.37e-04)	Tok/s 214575 (199957)	Loss/tok 4.5244 (6.7058)	LR 2.875e-03
0: TRAIN [0][440/1291]	Time 0.041 (0.054)	Data 7.84e-05 (5.26e-04)	Tok/s 190905 (200036)	Loss/tok 4.2009 (6.6533)	LR 2.875e-03
0: TRAIN [0][450/1291]	Time 0.059 (0.054)	Data 7.92e-05 (5.16e-04)	Tok/s 219885 (200144)	Loss/tok 4.3715 (6.6029)	LR 2.875e-03
0: TRAIN [0][460/1291]	Time 0.041 (0.053)	Data 8.11e-05 (5.07e-04)	Tok/s 188541 (200084)	Loss/tok 4.0635 (6.5588)	LR 2.875e-03
0: TRAIN [0][470/1291]	Time 0.041 (0.053)	Data 7.63e-05 (4.98e-04)	Tok/s 190310 (200045)	Loss/tok 4.0733 (6.5130)	LR 2.875e-03
0: Upscaling, new scale: 512.0
0: TRAIN [0][480/1291]	Time 0.025 (0.053)	Data 7.70e-05 (4.89e-04)	Tok/s 166204 (199943)	Loss/tok 3.4677 (6.4700)	LR 2.875e-03
0: TRAIN [0][490/1291]	Time 0.058 (0.053)	Data 7.77e-05 (4.81e-04)	Tok/s 215336 (200020)	Loss/tok 4.3428 (6.4235)	LR 2.875e-03
0: TRAIN [0][500/1291]	Time 0.041 (0.053)	Data 9.06e-05 (4.73e-04)	Tok/s 186463 (200008)	Loss/tok 3.8509 (6.3780)	LR 2.875e-03
0: TRAIN [0][510/1291]	Time 0.041 (0.053)	Data 1.32e-04 (4.66e-04)	Tok/s 186930 (200175)	Loss/tok 3.8760 (6.3296)	LR 2.875e-03
0: TRAIN [0][520/1291]	Time 0.097 (0.053)	Data 9.04e-05 (4.59e-04)	Tok/s 233695 (200267)	Loss/tok 4.4406 (6.2847)	LR 2.875e-03
0: TRAIN [0][530/1291]	Time 0.042 (0.053)	Data 1.40e-04 (4.52e-04)	Tok/s 184084 (200179)	Loss/tok 3.9220 (6.2484)	LR 2.875e-03
0: TRAIN [0][540/1291]	Time 0.076 (0.053)	Data 8.87e-05 (4.45e-04)	Tok/s 229511 (200343)	Loss/tok 4.4397 (6.2056)	LR 2.875e-03
0: TRAIN [0][550/1291]	Time 0.041 (0.053)	Data 8.92e-05 (4.39e-04)	Tok/s 187639 (200222)	Loss/tok 3.7940 (6.1731)	LR 2.875e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 256.0
0: TRAIN [0][560/1291]	Time 0.076 (0.053)	Data 9.49e-05 (4.33e-04)	Tok/s 227537 (200277)	Loss/tok 4.3877 (6.1353)	LR 2.875e-03
0: TRAIN [0][570/1291]	Time 0.058 (0.053)	Data 8.94e-05 (4.27e-04)	Tok/s 215041 (200265)	Loss/tok 3.9132 (6.1004)	LR 2.875e-03
0: TRAIN [0][580/1291]	Time 0.041 (0.053)	Data 8.92e-05 (4.21e-04)	Tok/s 186924 (200128)	Loss/tok 3.8732 (6.0707)	LR 2.875e-03
0: TRAIN [0][590/1291]	Time 0.042 (0.053)	Data 8.65e-05 (4.15e-04)	Tok/s 185900 (200122)	Loss/tok 3.7125 (6.0372)	LR 2.875e-03
0: TRAIN [0][600/1291]	Time 0.076 (0.053)	Data 8.92e-05 (4.10e-04)	Tok/s 232689 (200083)	Loss/tok 4.3184 (6.0052)	LR 2.875e-03
0: TRAIN [0][610/1291]	Time 0.042 (0.053)	Data 8.99e-05 (4.05e-04)	Tok/s 186710 (200028)	Loss/tok 3.7551 (5.9732)	LR 2.875e-03
0: TRAIN [0][620/1291]	Time 0.058 (0.053)	Data 8.82e-05 (3.99e-04)	Tok/s 215686 (200014)	Loss/tok 3.9724 (5.9430)	LR 2.875e-03
0: TRAIN [0][630/1291]	Time 0.024 (0.053)	Data 8.82e-05 (3.95e-04)	Tok/s 160009 (199911)	Loss/tok 3.0334 (5.9147)	LR 2.875e-03
0: TRAIN [0][640/1291]	Time 0.041 (0.053)	Data 8.92e-05 (3.90e-04)	Tok/s 189075 (199893)	Loss/tok 3.6758 (5.8846)	LR 2.875e-03
0: TRAIN [0][650/1291]	Time 0.058 (0.053)	Data 8.92e-05 (3.85e-04)	Tok/s 214962 (199925)	Loss/tok 3.9987 (5.8562)	LR 2.875e-03
0: TRAIN [0][660/1291]	Time 0.042 (0.053)	Data 8.89e-05 (3.81e-04)	Tok/s 185602 (199912)	Loss/tok 3.7693 (5.8282)	LR 2.875e-03
0: TRAIN [0][670/1291]	Time 0.041 (0.053)	Data 8.70e-05 (3.77e-04)	Tok/s 192607 (200071)	Loss/tok 3.5820 (5.7964)	LR 2.875e-03
0: TRAIN [0][680/1291]	Time 0.076 (0.053)	Data 9.18e-05 (3.73e-04)	Tok/s 227526 (200084)	Loss/tok 4.0940 (5.7690)	LR 2.875e-03
0: Upscaling, new scale: 512.0
0: TRAIN [0][690/1291]	Time 0.076 (0.053)	Data 1.39e-04 (3.69e-04)	Tok/s 231886 (200308)	Loss/tok 4.1296 (5.7358)	LR 2.875e-03
0: TRAIN [0][700/1291]	Time 0.076 (0.053)	Data 8.99e-05 (3.65e-04)	Tok/s 231659 (200377)	Loss/tok 4.1134 (5.7079)	LR 2.875e-03
0: TRAIN [0][710/1291]	Time 0.041 (0.053)	Data 1.39e-04 (3.61e-04)	Tok/s 189939 (200153)	Loss/tok 3.6017 (5.6879)	LR 2.875e-03
0: TRAIN [0][720/1291]	Time 0.059 (0.053)	Data 8.68e-05 (3.57e-04)	Tok/s 215072 (200304)	Loss/tok 4.0196 (5.6597)	LR 2.875e-03
0: TRAIN [0][730/1291]	Time 0.059 (0.053)	Data 8.89e-05 (3.53e-04)	Tok/s 212909 (200243)	Loss/tok 3.9480 (5.6371)	LR 2.875e-03
0: TRAIN [0][740/1291]	Time 0.076 (0.053)	Data 8.99e-05 (3.50e-04)	Tok/s 228019 (200241)	Loss/tok 4.0438 (5.6136)	LR 2.875e-03
0: TRAIN [0][750/1291]	Time 0.059 (0.053)	Data 7.80e-05 (3.46e-04)	Tok/s 218452 (200248)	Loss/tok 3.7331 (5.5899)	LR 2.875e-03
0: TRAIN [0][760/1291]	Time 0.041 (0.053)	Data 7.72e-05 (3.43e-04)	Tok/s 182972 (200232)	Loss/tok 3.5930 (5.5666)	LR 2.875e-03
0: TRAIN [0][770/1291]	Time 0.058 (0.053)	Data 7.63e-05 (3.40e-04)	Tok/s 213594 (200324)	Loss/tok 3.7656 (5.5421)	LR 2.875e-03
0: TRAIN [0][780/1291]	Time 0.076 (0.053)	Data 9.42e-05 (3.37e-04)	Tok/s 232668 (200384)	Loss/tok 3.9561 (5.5193)	LR 2.875e-03
0: TRAIN [0][790/1291]	Time 0.041 (0.053)	Data 1.35e-04 (3.34e-04)	Tok/s 189049 (200524)	Loss/tok 3.5388 (5.4942)	LR 2.875e-03
0: TRAIN [0][800/1291]	Time 0.097 (0.053)	Data 7.61e-05 (3.30e-04)	Tok/s 230452 (200361)	Loss/tok 4.3187 (5.4767)	LR 2.875e-03
0: TRAIN [0][810/1291]	Time 0.058 (0.053)	Data 1.39e-04 (3.28e-04)	Tok/s 216126 (200465)	Loss/tok 3.7721 (5.4540)	LR 2.875e-03
0: Upscaling, new scale: 1024.0
0: TRAIN [0][820/1291]	Time 0.042 (0.053)	Data 7.84e-05 (3.25e-04)	Tok/s 185083 (200329)	Loss/tok 3.5057 (5.4370)	LR 2.875e-03
0: TRAIN [0][830/1291]	Time 0.041 (0.053)	Data 8.87e-05 (3.22e-04)	Tok/s 187647 (200408)	Loss/tok 3.6594 (5.4162)	LR 2.875e-03
0: TRAIN [0][840/1291]	Time 0.058 (0.053)	Data 8.01e-05 (3.19e-04)	Tok/s 220829 (200557)	Loss/tok 3.8604 (5.3937)	LR 2.875e-03
0: TRAIN [0][850/1291]	Time 0.024 (0.053)	Data 1.37e-04 (3.16e-04)	Tok/s 164690 (200450)	Loss/tok 2.9630 (5.3773)	LR 2.875e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 512.0
0: TRAIN [0][860/1291]	Time 0.042 (0.053)	Data 7.34e-05 (3.14e-04)	Tok/s 182449 (200562)	Loss/tok 3.3111 (5.3556)	LR 2.875e-03
0: TRAIN [0][870/1291]	Time 0.042 (0.053)	Data 7.75e-05 (3.11e-04)	Tok/s 185252 (200632)	Loss/tok 3.4103 (5.3356)	LR 2.875e-03
0: TRAIN [0][880/1291]	Time 0.041 (0.053)	Data 7.65e-05 (3.09e-04)	Tok/s 186988 (200736)	Loss/tok 3.5482 (5.3148)	LR 2.875e-03
0: TRAIN [0][890/1291]	Time 0.058 (0.053)	Data 7.70e-05 (3.06e-04)	Tok/s 221249 (200805)	Loss/tok 3.7558 (5.2963)	LR 2.875e-03
0: TRAIN [0][900/1291]	Time 0.024 (0.053)	Data 8.77e-05 (3.04e-04)	Tok/s 159196 (200806)	Loss/tok 3.0482 (5.2789)	LR 2.875e-03
0: TRAIN [0][910/1291]	Time 0.097 (0.053)	Data 7.75e-05 (3.01e-04)	Tok/s 232586 (200831)	Loss/tok 4.0938 (5.2612)	LR 2.875e-03
0: TRAIN [0][920/1291]	Time 0.042 (0.053)	Data 8.70e-05 (2.99e-04)	Tok/s 181570 (200888)	Loss/tok 3.5856 (5.2429)	LR 2.875e-03
0: TRAIN [0][930/1291]	Time 0.042 (0.053)	Data 7.51e-05 (2.97e-04)	Tok/s 185530 (200761)	Loss/tok 3.4772 (5.2292)	LR 2.875e-03
0: TRAIN [0][940/1291]	Time 0.042 (0.053)	Data 7.68e-05 (2.95e-04)	Tok/s 185371 (200824)	Loss/tok 3.3286 (5.2120)	LR 2.875e-03
0: TRAIN [0][950/1291]	Time 0.042 (0.053)	Data 7.75e-05 (2.92e-04)	Tok/s 186693 (200740)	Loss/tok 3.3983 (5.1980)	LR 2.875e-03
0: TRAIN [0][960/1291]	Time 0.058 (0.053)	Data 7.72e-05 (2.90e-04)	Tok/s 221556 (200711)	Loss/tok 3.7247 (5.1834)	LR 2.875e-03
0: TRAIN [0][970/1291]	Time 0.076 (0.053)	Data 1.38e-04 (2.88e-04)	Tok/s 237031 (200779)	Loss/tok 3.7222 (5.1670)	LR 2.875e-03
0: TRAIN [0][980/1291]	Time 0.042 (0.053)	Data 7.89e-05 (2.86e-04)	Tok/s 183728 (200652)	Loss/tok 3.4781 (5.1540)	LR 2.875e-03
0: Upscaling, new scale: 1024.0
0: TRAIN [0][990/1291]	Time 0.059 (0.053)	Data 8.85e-05 (2.84e-04)	Tok/s 213620 (200680)	Loss/tok 3.5440 (5.1391)	LR 2.875e-03
0: TRAIN [0][1000/1291]	Time 0.041 (0.053)	Data 9.04e-05 (2.82e-04)	Tok/s 189282 (200712)	Loss/tok 3.4983 (5.1245)	LR 2.875e-03
0: TRAIN [0][1010/1291]	Time 0.076 (0.053)	Data 8.68e-05 (2.80e-04)	Tok/s 227110 (200754)	Loss/tok 3.7684 (5.1095)	LR 2.875e-03
0: TRAIN [0][1020/1291]	Time 0.042 (0.053)	Data 8.92e-05 (2.79e-04)	Tok/s 189299 (200806)	Loss/tok 3.3951 (5.0948)	LR 2.875e-03
0: TRAIN [0][1030/1291]	Time 0.042 (0.053)	Data 8.94e-05 (2.77e-04)	Tok/s 181477 (200839)	Loss/tok 3.5360 (5.0802)	LR 2.875e-03
0: TRAIN [0][1040/1291]	Time 0.024 (0.053)	Data 8.87e-05 (2.75e-04)	Tok/s 162582 (200892)	Loss/tok 2.9388 (5.0656)	LR 2.875e-03
0: TRAIN [0][1050/1291]	Time 0.059 (0.053)	Data 8.85e-05 (2.73e-04)	Tok/s 214163 (200951)	Loss/tok 3.5479 (5.0513)	LR 2.875e-03
0: TRAIN [0][1060/1291]	Time 0.042 (0.053)	Data 8.70e-05 (2.72e-04)	Tok/s 186062 (200914)	Loss/tok 3.5193 (5.0386)	LR 2.875e-03
0: TRAIN [0][1070/1291]	Time 0.097 (0.053)	Data 8.96e-05 (2.70e-04)	Tok/s 230824 (201023)	Loss/tok 3.9667 (5.0236)	LR 2.875e-03
0: TRAIN [0][1080/1291]	Time 0.041 (0.053)	Data 8.73e-05 (2.68e-04)	Tok/s 188717 (200998)	Loss/tok 3.4073 (5.0116)	LR 2.875e-03
0: TRAIN [0][1090/1291]	Time 0.058 (0.053)	Data 8.99e-05 (2.67e-04)	Tok/s 216908 (201047)	Loss/tok 3.7952 (4.9985)	LR 2.875e-03
0: TRAIN [0][1100/1291]	Time 0.041 (0.053)	Data 1.47e-04 (2.65e-04)	Tok/s 197763 (201051)	Loss/tok 3.4342 (4.9865)	LR 2.875e-03
0: Upscaling, new scale: 2048.0
0: TRAIN [0][1110/1291]	Time 0.096 (0.053)	Data 8.77e-05 (2.63e-04)	Tok/s 233441 (201093)	Loss/tok 4.0547 (4.9740)	LR 2.875e-03
0: TRAIN [0][1120/1291]	Time 0.058 (0.053)	Data 8.63e-05 (2.62e-04)	Tok/s 215677 (201093)	Loss/tok 3.6265 (4.9619)	LR 2.875e-03
0: TRAIN [0][1130/1291]	Time 0.058 (0.053)	Data 8.89e-05 (2.60e-04)	Tok/s 215745 (201081)	Loss/tok 3.6164 (4.9502)	LR 2.875e-03
0: TRAIN [0][1140/1291]	Time 0.041 (0.053)	Data 8.73e-05 (2.59e-04)	Tok/s 186710 (201147)	Loss/tok 3.3818 (4.9368)	LR 2.875e-03
0: TRAIN [0][1150/1291]	Time 0.041 (0.053)	Data 8.92e-05 (2.57e-04)	Tok/s 190834 (201134)	Loss/tok 3.3804 (4.9257)	LR 2.875e-03
0: TRAIN [0][1160/1291]	Time 0.025 (0.053)	Data 8.70e-05 (2.56e-04)	Tok/s 161127 (201090)	Loss/tok 2.8467 (4.9161)	LR 2.875e-03
0: TRAIN [0][1170/1291]	Time 0.042 (0.053)	Data 8.80e-05 (2.55e-04)	Tok/s 181252 (201044)	Loss/tok 3.4070 (4.9057)	LR 2.875e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 1024.0
0: TRAIN [0][1180/1291]	Time 0.076 (0.053)	Data 8.82e-05 (2.53e-04)	Tok/s 230844 (201150)	Loss/tok 3.8163 (4.8928)	LR 2.875e-03
0: TRAIN [0][1190/1291]	Time 0.058 (0.053)	Data 8.94e-05 (2.52e-04)	Tok/s 215755 (201090)	Loss/tok 3.5517 (4.8828)	LR 2.875e-03
0: TRAIN [0][1200/1291]	Time 0.042 (0.053)	Data 8.80e-05 (2.50e-04)	Tok/s 186764 (201087)	Loss/tok 3.2782 (4.8718)	LR 2.875e-03
0: TRAIN [0][1210/1291]	Time 0.041 (0.053)	Data 8.96e-05 (2.49e-04)	Tok/s 191538 (201068)	Loss/tok 3.4430 (4.8616)	LR 2.875e-03
0: TRAIN [0][1220/1291]	Time 0.042 (0.053)	Data 8.68e-05 (2.48e-04)	Tok/s 184113 (201030)	Loss/tok 3.4153 (4.8519)	LR 2.875e-03
0: TRAIN [0][1230/1291]	Time 0.042 (0.053)	Data 9.04e-05 (2.47e-04)	Tok/s 188146 (201061)	Loss/tok 3.4075 (4.8415)	LR 2.875e-03
0: TRAIN [0][1240/1291]	Time 0.097 (0.053)	Data 8.87e-05 (2.45e-04)	Tok/s 232201 (201068)	Loss/tok 4.0281 (4.8314)	LR 2.875e-03
0: TRAIN [0][1250/1291]	Time 0.042 (0.053)	Data 8.70e-05 (2.44e-04)	Tok/s 185604 (200983)	Loss/tok 3.3431 (4.8224)	LR 2.875e-03
0: TRAIN [0][1260/1291]	Time 0.058 (0.053)	Data 8.82e-05 (2.43e-04)	Tok/s 218596 (201004)	Loss/tok 3.5099 (4.8124)	LR 2.875e-03
0: TRAIN [0][1270/1291]	Time 0.059 (0.053)	Data 1.01e-04 (2.42e-04)	Tok/s 214634 (200998)	Loss/tok 3.6435 (4.8026)	LR 2.875e-03
0: TRAIN [0][1280/1291]	Time 0.076 (0.053)	Data 8.92e-05 (2.40e-04)	Tok/s 227992 (201031)	Loss/tok 3.6527 (4.7921)	LR 2.875e-03
0: TRAIN [0][1290/1291]	Time 0.096 (0.053)	Data 4.27e-05 (2.41e-04)	Tok/s 224314 (201013)	Loss/tok 4.1994 (4.7831)	LR 2.875e-03
:::MLLOG {"namespace": "", "time_ms": 1593114110443, "event_type": "INTERVAL_END", "key": "epoch_stop", "value": null, "metadata": {"file": "train.py", "lineno": 551, "epoch_num": 1}}
:::MLLOG {"namespace": "", "time_ms": 1593114110443, "event_type": "INTERVAL_START", "key": "eval_start", "value": null, "metadata": {"file": "train.py", "lineno": 556, "epoch_num": 1}}
0: Running evaluation on test set
0: TEST [0][0/3]	Time 0.392 (0.392)	Decoder iters 149.0 (149.0)	Tok/s 22897 (22897)
0: Running moses detokenizer
0: BLEU(score=19.573854349955543, counts=[34585, 15687, 8303, 4598], totals=[65884, 62881, 59878, 56880], precisions=[52.493776941290754, 24.947122342201936, 13.866528608169945, 8.083684950773558], bp=1.0, sys_len=65884, ref_len=64676)
0: Finished evaluation on test set
:::MLLOG {"namespace": "", "time_ms": 1593114111731, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.1957, "metadata": {"file": "train.py", "lineno": 562, "epoch_num": 1}}
:::MLLOG {"namespace": "", "time_ms": 1593114111732, "event_type": "INTERVAL_END", "key": "eval_stop", "value": null, "metadata": {"file": "train.py", "lineno": 565, "epoch_num": 1}}
0: Summary: Epoch: 0	Training Loss: 4.7813	Test BLEU: 19.57
0: Performance: Epoch: 0	Training: 3213925 Tok/s
0: Finished epoch 0
:::MLLOG {"namespace": "", "time_ms": 1593114111732, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "train.py", "lineno": 584, "first_epoch_num": 1}}
:::MLLOG {"namespace": "", "time_ms": 1593114111732, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "train.py", "lineno": 538, "first_epoch_num": 2, "epoch_count": 1}}
:::MLLOG {"namespace": "", "time_ms": 1593114111732, "event_type": "INTERVAL_START", "key": "epoch_start", "value": null, "metadata": {"file": "train.py", "lineno": 541, "epoch_num": 2}}
0: Starting epoch 1
0: Sampler for epoch 1 uses seed 4219521062
0: TRAIN [1][0/1291]	Time 0.275 (0.275)	Data 1.84e-01 (1.84e-01)	Tok/s 27545 (27545)	Loss/tok 3.3777 (3.3777)	LR 2.875e-03
0: TRAIN [1][10/1291]	Time 0.058 (0.072)	Data 8.46e-05 (1.68e-02)	Tok/s 217116 (184422)	Loss/tok 3.5210 (3.5408)	LR 2.875e-03
0: Upscaling, new scale: 2048.0
0: TRAIN [1][20/1291]	Time 0.058 (0.067)	Data 8.73e-05 (8.87e-03)	Tok/s 211926 (199052)	Loss/tok 3.6290 (3.5793)	LR 2.875e-03
0: TRAIN [1][30/1291]	Time 0.042 (0.061)	Data 8.70e-05 (6.03e-03)	Tok/s 184066 (197596)	Loss/tok 3.4031 (3.5490)	LR 2.875e-03
0: TRAIN [1][40/1291]	Time 0.042 (0.058)	Data 8.68e-05 (4.58e-03)	Tok/s 186874 (198594)	Loss/tok 3.2495 (3.5286)	LR 2.875e-03
0: TRAIN [1][50/1291]	Time 0.042 (0.055)	Data 9.01e-05 (3.70e-03)	Tok/s 190452 (197152)	Loss/tok 3.2174 (3.4897)	LR 2.875e-03
0: TRAIN [1][60/1291]	Time 0.041 (0.055)	Data 8.68e-05 (3.11e-03)	Tok/s 191416 (197349)	Loss/tok 3.3586 (3.4991)	LR 2.875e-03
0: TRAIN [1][70/1291]	Time 0.025 (0.054)	Data 8.65e-05 (2.68e-03)	Tok/s 154601 (197505)	Loss/tok 2.7533 (3.4933)	LR 2.875e-03
0: TRAIN [1][80/1291]	Time 0.025 (0.054)	Data 8.89e-05 (2.36e-03)	Tok/s 163179 (197427)	Loss/tok 2.6794 (3.4955)	LR 2.875e-03
0: TRAIN [1][90/1291]	Time 0.041 (0.053)	Data 8.73e-05 (2.11e-03)	Tok/s 186312 (196473)	Loss/tok 3.2271 (3.4830)	LR 2.875e-03
0: TRAIN [1][100/1291]	Time 0.042 (0.052)	Data 8.75e-05 (1.91e-03)	Tok/s 183899 (196718)	Loss/tok 3.1474 (3.4791)	LR 2.875e-03
0: TRAIN [1][110/1291]	Time 0.058 (0.052)	Data 8.46e-05 (1.75e-03)	Tok/s 216627 (197482)	Loss/tok 3.4522 (3.4794)	LR 2.875e-03
0: TRAIN [1][120/1291]	Time 0.042 (0.053)	Data 8.80e-05 (1.61e-03)	Tok/s 186012 (198521)	Loss/tok 3.1579 (3.4826)	LR 2.875e-03
0: TRAIN [1][130/1291]	Time 0.024 (0.053)	Data 8.49e-05 (1.49e-03)	Tok/s 160346 (198510)	Loss/tok 2.6858 (3.4828)	LR 2.875e-03
0: TRAIN [1][140/1291]	Time 0.059 (0.053)	Data 8.08e-05 (1.39e-03)	Tok/s 215936 (198947)	Loss/tok 3.4996 (3.4809)	LR 2.875e-03
0: Upscaling, new scale: 4096.0
0: Gradient norm: inf
0: Skipped batch, new scale: 2048.0
0: TRAIN [1][150/1291]	Time 0.041 (0.053)	Data 8.23e-05 (1.31e-03)	Tok/s 185856 (199156)	Loss/tok 3.1698 (3.4759)	LR 2.875e-03
0: TRAIN [1][160/1291]	Time 0.058 (0.053)	Data 8.49e-05 (1.23e-03)	Tok/s 217377 (200060)	Loss/tok 3.5692 (3.4879)	LR 2.875e-03
0: TRAIN [1][170/1291]	Time 0.097 (0.054)	Data 8.85e-05 (1.16e-03)	Tok/s 227330 (200303)	Loss/tok 3.9683 (3.4926)	LR 2.875e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 1024.0
0: TRAIN [1][180/1291]	Time 0.042 (0.054)	Data 9.20e-05 (1.11e-03)	Tok/s 182017 (200254)	Loss/tok 3.2894 (3.4951)	LR 2.875e-03
0: TRAIN [1][190/1291]	Time 0.025 (0.053)	Data 8.61e-05 (1.05e-03)	Tok/s 157950 (200056)	Loss/tok 2.7862 (3.4938)	LR 2.875e-03
0: TRAIN [1][200/1291]	Time 0.041 (0.053)	Data 8.65e-05 (1.01e-03)	Tok/s 189396 (200322)	Loss/tok 3.3028 (3.4948)	LR 2.875e-03
0: TRAIN [1][210/1291]	Time 0.076 (0.053)	Data 8.92e-05 (9.62e-04)	Tok/s 228088 (200177)	Loss/tok 3.5492 (3.4958)	LR 2.875e-03
0: TRAIN [1][220/1291]	Time 0.058 (0.053)	Data 9.18e-05 (9.24e-04)	Tok/s 217019 (200022)	Loss/tok 3.4480 (3.4917)	LR 2.875e-03
0: TRAIN [1][230/1291]	Time 0.042 (0.053)	Data 8.70e-05 (8.88e-04)	Tok/s 184571 (199985)	Loss/tok 3.2607 (3.4881)	LR 2.875e-03
0: TRAIN [1][240/1291]	Time 0.042 (0.053)	Data 8.63e-05 (8.55e-04)	Tok/s 182734 (199652)	Loss/tok 3.2602 (3.4862)	LR 2.875e-03
0: TRAIN [1][250/1291]	Time 0.041 (0.053)	Data 8.70e-05 (8.24e-04)	Tok/s 193927 (199756)	Loss/tok 3.1821 (3.4874)	LR 2.875e-03
0: TRAIN [1][260/1291]	Time 0.097 (0.053)	Data 8.80e-05 (7.96e-04)	Tok/s 231127 (200076)	Loss/tok 3.8655 (3.4921)	LR 2.875e-03
0: TRAIN [1][270/1291]	Time 0.041 (0.053)	Data 8.56e-05 (7.71e-04)	Tok/s 189632 (199967)	Loss/tok 3.3931 (3.4888)	LR 2.875e-03
0: TRAIN [1][280/1291]	Time 0.076 (0.053)	Data 8.87e-05 (7.47e-04)	Tok/s 231432 (199974)	Loss/tok 3.6849 (3.4894)	LR 2.875e-03
0: TRAIN [1][290/1291]	Time 0.042 (0.053)	Data 8.39e-05 (7.24e-04)	Tok/s 182994 (199901)	Loss/tok 3.0103 (3.4869)	LR 2.875e-03
0: Upscaling, new scale: 2048.0
0: TRAIN [1][300/1291]	Time 0.059 (0.053)	Data 1.22e-04 (7.03e-04)	Tok/s 212713 (200311)	Loss/tok 3.4968 (3.4900)	LR 2.875e-03
0: TRAIN [1][310/1291]	Time 0.058 (0.053)	Data 8.56e-05 (6.84e-04)	Tok/s 212286 (200145)	Loss/tok 3.5640 (3.4872)	LR 2.875e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 1024.0
0: TRAIN [1][320/1291]	Time 0.042 (0.053)	Data 9.11e-05 (6.65e-04)	Tok/s 191875 (200139)	Loss/tok 3.2082 (3.4913)	LR 2.875e-03
0: TRAIN [1][330/1291]	Time 0.025 (0.053)	Data 8.94e-05 (6.48e-04)	Tok/s 164500 (199624)	Loss/tok 2.9263 (3.4876)	LR 2.875e-03
0: TRAIN [1][340/1291]	Time 0.042 (0.053)	Data 8.85e-05 (6.32e-04)	Tok/s 185366 (199758)	Loss/tok 3.2904 (3.4861)	LR 2.875e-03
0: TRAIN [1][350/1291]	Time 0.059 (0.053)	Data 8.44e-05 (6.17e-04)	Tok/s 212490 (200167)	Loss/tok 3.4749 (3.4879)	LR 2.875e-03
0: TRAIN [1][360/1291]	Time 0.042 (0.053)	Data 1.53e-04 (6.02e-04)	Tok/s 185498 (200248)	Loss/tok 3.3811 (3.4872)	LR 2.875e-03
0: TRAIN [1][370/1291]	Time 0.058 (0.053)	Data 8.70e-05 (5.89e-04)	Tok/s 215208 (200464)	Loss/tok 3.5468 (3.4890)	LR 2.875e-03
0: TRAIN [1][380/1291]	Time 0.041 (0.053)	Data 8.73e-05 (5.76e-04)	Tok/s 188341 (199903)	Loss/tok 3.1595 (3.4841)	LR 2.875e-03
0: TRAIN [1][390/1291]	Time 0.058 (0.053)	Data 1.02e-04 (5.64e-04)	Tok/s 215238 (199925)	Loss/tok 3.4666 (3.4835)	LR 2.875e-03
0: TRAIN [1][400/1291]	Time 0.041 (0.053)	Data 8.73e-05 (5.52e-04)	Tok/s 187625 (200107)	Loss/tok 3.2295 (3.4850)	LR 2.875e-03
0: TRAIN [1][410/1291]	Time 0.058 (0.053)	Data 8.68e-05 (5.41e-04)	Tok/s 220185 (200209)	Loss/tok 3.4384 (3.4873)	LR 2.875e-03
0: TRAIN [1][420/1291]	Time 0.097 (0.053)	Data 1.44e-04 (5.30e-04)	Tok/s 228057 (200306)	Loss/tok 3.8845 (3.4896)	LR 2.875e-03
0: TRAIN [1][430/1291]	Time 0.097 (0.053)	Data 1.41e-04 (5.20e-04)	Tok/s 230946 (200511)	Loss/tok 3.8375 (3.4923)	LR 2.875e-03
0: Upscaling, new scale: 2048.0
0: TRAIN [1][440/1291]	Time 0.076 (0.053)	Data 8.42e-05 (5.11e-04)	Tok/s 229292 (200591)	Loss/tok 3.5230 (3.4913)	LR 2.875e-03
0: TRAIN [1][450/1291]	Time 0.024 (0.053)	Data 1.43e-04 (5.02e-04)	Tok/s 167382 (200380)	Loss/tok 2.8763 (3.4888)	LR 2.875e-03
0: TRAIN [1][460/1291]	Time 0.097 (0.053)	Data 9.16e-05 (4.93e-04)	Tok/s 225681 (200383)	Loss/tok 3.9230 (3.4886)	LR 2.875e-03
0: TRAIN [1][470/1291]	Time 0.041 (0.053)	Data 9.32e-05 (4.85e-04)	Tok/s 186986 (200288)	Loss/tok 3.1352 (3.4871)	LR 2.875e-03
0: TRAIN [1][480/1291]	Time 0.041 (0.053)	Data 8.63e-05 (4.77e-04)	Tok/s 188216 (200160)	Loss/tok 3.2597 (3.4847)	LR 2.875e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 1024.0
0: TRAIN [1][490/1291]	Time 0.076 (0.053)	Data 9.13e-05 (4.69e-04)	Tok/s 226804 (199929)	Loss/tok 3.7994 (3.4822)	LR 2.875e-03
0: TRAIN [1][500/1291]	Time 0.025 (0.053)	Data 8.85e-05 (4.61e-04)	Tok/s 156798 (199937)	Loss/tok 2.7130 (3.4826)	LR 2.875e-03
0: TRAIN [1][510/1291]	Time 0.042 (0.053)	Data 8.56e-05 (4.54e-04)	Tok/s 187688 (199854)	Loss/tok 3.2946 (3.4798)	LR 2.875e-03
0: TRAIN [1][520/1291]	Time 0.042 (0.053)	Data 8.73e-05 (4.47e-04)	Tok/s 186339 (199649)	Loss/tok 3.3160 (3.4785)	LR 2.875e-03
0: TRAIN [1][530/1291]	Time 0.076 (0.053)	Data 1.48e-04 (4.40e-04)	Tok/s 228010 (199785)	Loss/tok 3.5499 (3.4768)	LR 2.875e-03
0: TRAIN [1][540/1291]	Time 0.058 (0.053)	Data 7.30e-05 (4.34e-04)	Tok/s 215763 (199804)	Loss/tok 3.3699 (3.4756)	LR 2.875e-03
0: TRAIN [1][550/1291]	Time 0.041 (0.053)	Data 8.30e-05 (4.28e-04)	Tok/s 181272 (199905)	Loss/tok 3.3031 (3.4756)	LR 2.875e-03
0: TRAIN [1][560/1291]	Time 0.041 (0.053)	Data 7.58e-05 (4.21e-04)	Tok/s 192497 (199977)	Loss/tok 3.2484 (3.4757)	LR 2.875e-03
0: TRAIN [1][570/1291]	Time 0.059 (0.053)	Data 7.68e-05 (4.15e-04)	Tok/s 214407 (199922)	Loss/tok 3.4126 (3.4741)	LR 2.875e-03
0: TRAIN [1][580/1291]	Time 0.076 (0.053)	Data 7.65e-05 (4.10e-04)	Tok/s 230998 (200109)	Loss/tok 3.5917 (3.4739)	LR 2.875e-03
0: TRAIN [1][590/1291]	Time 0.041 (0.053)	Data 7.44e-05 (4.04e-04)	Tok/s 188071 (200120)	Loss/tok 3.2887 (3.4737)	LR 2.875e-03
0: TRAIN [1][600/1291]	Time 0.058 (0.053)	Data 7.46e-05 (3.99e-04)	Tok/s 220207 (200156)	Loss/tok 3.4073 (3.4729)	LR 2.875e-03
0: TRAIN [1][610/1291]	Time 0.042 (0.053)	Data 7.72e-05 (3.94e-04)	Tok/s 184251 (200254)	Loss/tok 3.2690 (3.4719)	LR 2.875e-03
0: Upscaling, new scale: 2048.0
0: TRAIN [1][620/1291]	Time 0.042 (0.053)	Data 7.96e-05 (3.89e-04)	Tok/s 181936 (200210)	Loss/tok 3.2563 (3.4710)	LR 2.875e-03
0: TRAIN [1][630/1291]	Time 0.059 (0.053)	Data 7.94e-05 (3.84e-04)	Tok/s 211474 (200436)	Loss/tok 3.4950 (3.4737)	LR 2.875e-03
0: TRAIN [1][640/1291]	Time 0.024 (0.053)	Data 7.75e-05 (3.79e-04)	Tok/s 160338 (200411)	Loss/tok 2.7070 (3.4719)	LR 2.875e-03
0: TRAIN [1][650/1291]	Time 0.058 (0.053)	Data 7.72e-05 (3.74e-04)	Tok/s 221556 (200293)	Loss/tok 3.4328 (3.4697)	LR 2.875e-03
0: TRAIN [1][660/1291]	Time 0.041 (0.053)	Data 8.11e-05 (3.70e-04)	Tok/s 187901 (200371)	Loss/tok 3.2191 (3.4680)	LR 2.875e-03
0: TRAIN [1][670/1291]	Time 0.042 (0.053)	Data 1.03e-04 (3.66e-04)	Tok/s 189155 (200462)	Loss/tok 3.1694 (3.4674)	LR 2.875e-03
0: TRAIN [1][680/1291]	Time 0.097 (0.053)	Data 7.84e-05 (3.61e-04)	Tok/s 236672 (200361)	Loss/tok 3.7473 (3.4661)	LR 2.875e-03
0: TRAIN [1][690/1291]	Time 0.041 (0.053)	Data 7.58e-05 (3.57e-04)	Tok/s 185559 (200335)	Loss/tok 3.2770 (3.4663)	LR 2.875e-03
0: TRAIN [1][700/1291]	Time 0.076 (0.053)	Data 8.20e-05 (3.53e-04)	Tok/s 228616 (200407)	Loss/tok 3.6777 (3.4655)	LR 2.875e-03
0: TRAIN [1][710/1291]	Time 0.058 (0.053)	Data 7.56e-05 (3.50e-04)	Tok/s 214014 (200417)	Loss/tok 3.4953 (3.4648)	LR 2.875e-03
0: TRAIN [1][720/1291]	Time 0.024 (0.053)	Data 1.38e-04 (3.46e-04)	Tok/s 166670 (200271)	Loss/tok 2.7440 (3.4640)	LR 2.875e-03
0: TRAIN [1][730/1291]	Time 0.042 (0.053)	Data 7.70e-05 (3.42e-04)	Tok/s 186656 (200276)	Loss/tok 3.2624 (3.4625)	LR 2.875e-03
0: TRAIN [1][740/1291]	Time 0.097 (0.053)	Data 1.39e-04 (3.39e-04)	Tok/s 227925 (200130)	Loss/tok 3.8490 (3.4624)	LR 2.875e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 1024.0
0: TRAIN [1][750/1291]	Time 0.076 (0.053)	Data 7.89e-05 (3.36e-04)	Tok/s 229618 (200146)	Loss/tok 3.5252 (3.4627)	LR 2.875e-03
0: TRAIN [1][760/1291]	Time 0.059 (0.053)	Data 7.84e-05 (3.32e-04)	Tok/s 213649 (200069)	Loss/tok 3.4599 (3.4614)	LR 2.875e-03
0: TRAIN [1][770/1291]	Time 0.097 (0.053)	Data 7.87e-05 (3.29e-04)	Tok/s 229047 (200107)	Loss/tok 3.7087 (3.4609)	LR 2.875e-03
0: TRAIN [1][780/1291]	Time 0.042 (0.053)	Data 7.80e-05 (3.26e-04)	Tok/s 187858 (200000)	Loss/tok 3.2409 (3.4607)	LR 2.875e-03
0: TRAIN [1][790/1291]	Time 0.041 (0.052)	Data 7.49e-05 (3.23e-04)	Tok/s 192544 (199948)	Loss/tok 3.1931 (3.4594)	LR 2.875e-03
0: TRAIN [1][800/1291]	Time 0.041 (0.052)	Data 7.96e-05 (3.20e-04)	Tok/s 188039 (199821)	Loss/tok 3.2991 (3.4578)	LR 2.875e-03
0: TRAIN [1][810/1291]	Time 0.058 (0.052)	Data 7.84e-05 (3.17e-04)	Tok/s 211686 (199800)	Loss/tok 3.6270 (3.4566)	LR 2.875e-03
0: TRAIN [1][820/1291]	Time 0.058 (0.052)	Data 1.36e-04 (3.14e-04)	Tok/s 216600 (199893)	Loss/tok 3.3753 (3.4557)	LR 2.875e-03
0: TRAIN [1][830/1291]	Time 0.042 (0.052)	Data 7.70e-05 (3.12e-04)	Tok/s 186480 (199977)	Loss/tok 3.0890 (3.4551)	LR 2.875e-03
0: TRAIN [1][840/1291]	Time 0.059 (0.052)	Data 1.39e-04 (3.09e-04)	Tok/s 215199 (200058)	Loss/tok 3.3968 (3.4548)	LR 2.875e-03
0: TRAIN [1][850/1291]	Time 0.041 (0.052)	Data 9.18e-05 (3.07e-04)	Tok/s 190802 (200084)	Loss/tok 3.2369 (3.4544)	LR 2.875e-03
0: TRAIN [1][860/1291]	Time 0.042 (0.052)	Data 7.75e-05 (3.04e-04)	Tok/s 188247 (200098)	Loss/tok 3.1422 (3.4535)	LR 2.875e-03
0: Upscaling, new scale: 2048.0
0: TRAIN [1][870/1291]	Time 0.059 (0.052)	Data 7.94e-05 (3.01e-04)	Tok/s 216348 (200061)	Loss/tok 3.2712 (3.4520)	LR 2.875e-03
0: TRAIN [1][880/1291]	Time 0.041 (0.052)	Data 7.51e-05 (2.99e-04)	Tok/s 190052 (200078)	Loss/tok 3.2857 (3.4522)	LR 2.875e-03
0: TRAIN [1][890/1291]	Time 0.058 (0.053)	Data 7.77e-05 (2.96e-04)	Tok/s 219223 (200155)	Loss/tok 3.4158 (3.4519)	LR 2.875e-03
0: TRAIN [1][900/1291]	Time 0.041 (0.053)	Data 7.84e-05 (2.94e-04)	Tok/s 184257 (200265)	Loss/tok 3.1093 (3.4521)	LR 2.875e-03
0: TRAIN [1][910/1291]	Time 0.041 (0.053)	Data 7.56e-05 (2.92e-04)	Tok/s 193718 (200229)	Loss/tok 3.1040 (3.4514)	LR 2.875e-03
0: TRAIN [1][920/1291]	Time 0.058 (0.053)	Data 7.89e-05 (2.89e-04)	Tok/s 218156 (200241)	Loss/tok 3.4366 (3.4507)	LR 2.875e-03
0: TRAIN [1][930/1291]	Time 0.058 (0.053)	Data 8.51e-05 (2.87e-04)	Tok/s 215968 (200250)	Loss/tok 3.3361 (3.4502)	LR 2.875e-03
0: TRAIN [1][940/1291]	Time 0.058 (0.053)	Data 1.33e-04 (2.85e-04)	Tok/s 213822 (200382)	Loss/tok 3.3955 (3.4493)	LR 2.875e-03
0: TRAIN [1][950/1291]	Time 0.042 (0.053)	Data 7.72e-05 (2.83e-04)	Tok/s 182938 (200362)	Loss/tok 3.0953 (3.4489)	LR 2.875e-03
0: TRAIN [1][960/1291]	Time 0.059 (0.053)	Data 7.53e-05 (2.81e-04)	Tok/s 211561 (200458)	Loss/tok 3.4994 (3.4482)	LR 2.875e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 1024.0
0: TRAIN [1][970/1291]	Time 0.096 (0.053)	Data 8.03e-05 (2.79e-04)	Tok/s 228963 (200415)	Loss/tok 3.6233 (3.4469)	LR 2.875e-03
0: TRAIN [1][980/1291]	Time 0.097 (0.053)	Data 8.13e-05 (2.77e-04)	Tok/s 232322 (200414)	Loss/tok 3.7671 (3.4466)	LR 2.875e-03
0: TRAIN [1][990/1291]	Time 0.058 (0.053)	Data 7.80e-05 (2.75e-04)	Tok/s 215096 (200322)	Loss/tok 3.4689 (3.4452)	LR 2.875e-03
0: TRAIN [1][1000/1291]	Time 0.041 (0.053)	Data 7.77e-05 (2.73e-04)	Tok/s 189404 (200274)	Loss/tok 3.1914 (3.4437)	LR 2.875e-03
0: TRAIN [1][1010/1291]	Time 0.041 (0.053)	Data 7.70e-05 (2.71e-04)	Tok/s 186837 (200315)	Loss/tok 3.2233 (3.4429)	LR 2.875e-03
0: TRAIN [1][1020/1291]	Time 0.059 (0.053)	Data 8.18e-05 (2.69e-04)	Tok/s 217522 (200356)	Loss/tok 3.5010 (3.4435)	LR 2.875e-03
0: TRAIN [1][1030/1291]	Time 0.041 (0.053)	Data 7.51e-05 (2.68e-04)	Tok/s 185375 (200384)	Loss/tok 3.0958 (3.4426)	LR 2.875e-03
0: TRAIN [1][1040/1291]	Time 0.058 (0.053)	Data 7.68e-05 (2.66e-04)	Tok/s 218475 (200382)	Loss/tok 3.3681 (3.4416)	LR 2.875e-03
0: TRAIN [1][1050/1291]	Time 0.076 (0.053)	Data 7.80e-05 (2.64e-04)	Tok/s 227206 (200295)	Loss/tok 3.6198 (3.4402)	LR 2.875e-03
0: TRAIN [1][1060/1291]	Time 0.041 (0.052)	Data 7.46e-05 (2.62e-04)	Tok/s 186331 (200192)	Loss/tok 3.2772 (3.4385)	LR 2.875e-03
0: TRAIN [1][1070/1291]	Time 0.058 (0.053)	Data 7.72e-05 (2.61e-04)	Tok/s 219183 (200303)	Loss/tok 3.3642 (3.4392)	LR 2.875e-03
0: TRAIN [1][1080/1291]	Time 0.042 (0.053)	Data 1.39e-04 (2.59e-04)	Tok/s 190110 (200379)	Loss/tok 3.1824 (3.4394)	LR 2.875e-03
0: TRAIN [1][1090/1291]	Time 0.097 (0.053)	Data 8.20e-05 (2.58e-04)	Tok/s 228317 (200402)	Loss/tok 3.8059 (3.4393)	LR 2.875e-03
0: Upscaling, new scale: 2048.0
0: TRAIN [1][1100/1291]	Time 0.042 (0.053)	Data 7.92e-05 (2.56e-04)	Tok/s 186893 (200401)	Loss/tok 3.1016 (3.4383)	LR 2.875e-03
0: TRAIN [1][1110/1291]	Time 0.042 (0.053)	Data 7.61e-05 (2.55e-04)	Tok/s 180267 (200386)	Loss/tok 3.1691 (3.4379)	LR 2.875e-03
0: TRAIN [1][1120/1291]	Time 0.041 (0.053)	Data 7.68e-05 (2.53e-04)	Tok/s 189285 (200359)	Loss/tok 3.1530 (3.4373)	LR 2.875e-03
0: TRAIN [1][1130/1291]	Time 0.097 (0.053)	Data 7.65e-05 (2.52e-04)	Tok/s 229254 (200437)	Loss/tok 3.7921 (3.4376)	LR 2.875e-03
0: TRAIN [1][1140/1291]	Time 0.041 (0.053)	Data 9.78e-05 (2.50e-04)	Tok/s 182849 (200357)	Loss/tok 3.3087 (3.4362)	LR 2.875e-03
0: TRAIN [1][1150/1291]	Time 0.077 (0.053)	Data 7.89e-05 (2.49e-04)	Tok/s 224517 (200468)	Loss/tok 3.6915 (3.4371)	LR 2.875e-03
0: TRAIN [1][1160/1291]	Time 0.041 (0.053)	Data 7.63e-05 (2.47e-04)	Tok/s 180304 (200396)	Loss/tok 3.1806 (3.4359)	LR 2.875e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 1024.0
0: TRAIN [1][1170/1291]	Time 0.042 (0.053)	Data 7.75e-05 (2.46e-04)	Tok/s 184278 (200441)	Loss/tok 3.2161 (3.4358)	LR 2.875e-03
0: TRAIN [1][1180/1291]	Time 0.041 (0.053)	Data 7.46e-05 (2.45e-04)	Tok/s 185432 (200497)	Loss/tok 3.2333 (3.4355)	LR 2.875e-03
0: TRAIN [1][1190/1291]	Time 0.059 (0.053)	Data 8.39e-05 (2.43e-04)	Tok/s 209397 (200519)	Loss/tok 3.2644 (3.4347)	LR 2.875e-03
0: TRAIN [1][1200/1291]	Time 0.097 (0.053)	Data 7.70e-05 (2.42e-04)	Tok/s 230442 (200543)	Loss/tok 3.6902 (3.4349)	LR 2.875e-03
0: TRAIN [1][1210/1291]	Time 0.059 (0.053)	Data 7.68e-05 (2.41e-04)	Tok/s 212948 (200699)	Loss/tok 3.2552 (3.4347)	LR 2.875e-03
0: TRAIN [1][1220/1291]	Time 0.058 (0.053)	Data 1.39e-04 (2.40e-04)	Tok/s 213990 (200757)	Loss/tok 3.5689 (3.4344)	LR 2.875e-03
0: TRAIN [1][1230/1291]	Time 0.025 (0.053)	Data 8.56e-05 (2.38e-04)	Tok/s 163017 (200790)	Loss/tok 2.6345 (3.4337)	LR 2.875e-03
0: TRAIN [1][1240/1291]	Time 0.058 (0.053)	Data 7.68e-05 (2.37e-04)	Tok/s 216950 (200813)	Loss/tok 3.2776 (3.4327)	LR 2.875e-03
0: TRAIN [1][1250/1291]	Time 0.058 (0.053)	Data 1.11e-04 (2.36e-04)	Tok/s 218814 (200855)	Loss/tok 3.3069 (3.4323)	LR 2.875e-03
0: TRAIN [1][1260/1291]	Time 0.041 (0.053)	Data 7.75e-05 (2.35e-04)	Tok/s 186676 (200811)	Loss/tok 3.2021 (3.4312)	LR 2.875e-03
0: TRAIN [1][1270/1291]	Time 0.041 (0.053)	Data 8.46e-05 (2.34e-04)	Tok/s 191745 (200836)	Loss/tok 3.1725 (3.4306)	LR 2.875e-03
0: TRAIN [1][1280/1291]	Time 0.041 (0.053)	Data 7.70e-05 (2.33e-04)	Tok/s 189151 (200903)	Loss/tok 3.2348 (3.4311)	LR 2.875e-03
0: TRAIN [1][1290/1291]	Time 0.024 (0.053)	Data 4.32e-05 (2.33e-04)	Tok/s 161026 (200892)	Loss/tok 2.6847 (3.4303)	LR 2.875e-03
:::MLLOG {"namespace": "", "time_ms": 1593114180079, "event_type": "INTERVAL_END", "key": "epoch_stop", "value": null, "metadata": {"file": "train.py", "lineno": 551, "epoch_num": 2}}
:::MLLOG {"namespace": "", "time_ms": 1593114180079, "event_type": "INTERVAL_START", "key": "eval_start", "value": null, "metadata": {"file": "train.py", "lineno": 556, "epoch_num": 2}}
0: Running evaluation on test set
0: TEST [1][0/3]	Time 0.402 (0.402)	Decoder iters 149.0 (149.0)	Tok/s 21972 (21972)
0: Running moses detokenizer
0: BLEU(score=21.99962441835265, counts=[35599, 17092, 9435, 5408], totals=[64933, 61930, 58929, 55931], precisions=[54.82420340966843, 27.598901986113354, 16.010792648780736, 9.669056516064437], bp=1.0, sys_len=64933, ref_len=64676)
0: Finished evaluation on test set
:::MLLOG {"namespace": "", "time_ms": 1593114181420, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.22, "metadata": {"file": "train.py", "lineno": 562, "epoch_num": 2}}
:::MLLOG {"namespace": "", "time_ms": 1593114181420, "event_type": "INTERVAL_END", "key": "eval_stop", "value": null, "metadata": {"file": "train.py", "lineno": 565, "epoch_num": 2}}
0: Summary: Epoch: 1	Training Loss: 3.4278	Test BLEU: 22.00
0: Performance: Epoch: 1	Training: 3215786 Tok/s
0: Finished epoch 1
:::MLLOG {"namespace": "", "time_ms": 1593114181421, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "train.py", "lineno": 584, "first_epoch_num": 2}}
:::MLLOG {"namespace": "", "time_ms": 1593114181421, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "train.py", "lineno": 538, "first_epoch_num": 3, "epoch_count": 1}}
:::MLLOG {"namespace": "", "time_ms": 1593114181421, "event_type": "INTERVAL_START", "key": "epoch_start", "value": null, "metadata": {"file": "train.py", "lineno": 541, "epoch_num": 3}}
0: Starting epoch 2
0: Sampler for epoch 2 uses seed 2341730761
0: TRAIN [2][0/1291]	Time 0.306 (0.306)	Data 1.72e-01 (1.72e-01)	Tok/s 41416 (41416)	Loss/tok 3.2845 (3.2845)	LR 2.875e-03
0: Upscaling, new scale: 2048.0
0: TRAIN [2][10/1291]	Time 0.059 (0.081)	Data 8.65e-05 (1.57e-02)	Tok/s 214011 (197551)	Loss/tok 3.2674 (3.3471)	LR 2.875e-03
0: TRAIN [2][20/1291]	Time 0.058 (0.061)	Data 8.63e-05 (8.28e-03)	Tok/s 214320 (191308)	Loss/tok 3.2376 (3.2375)	LR 2.875e-03
0: TRAIN [2][30/1291]	Time 0.041 (0.060)	Data 9.68e-05 (5.64e-03)	Tok/s 193601 (196121)	Loss/tok 3.0842 (3.2671)	LR 2.875e-03
0: TRAIN [2][40/1291]	Time 0.041 (0.057)	Data 8.54e-05 (4.29e-03)	Tok/s 184870 (196079)	Loss/tok 3.0956 (3.2424)	LR 2.875e-03
0: TRAIN [2][50/1291]	Time 0.041 (0.055)	Data 8.63e-05 (3.46e-03)	Tok/s 184339 (195951)	Loss/tok 3.0118 (3.2404)	LR 2.875e-03
0: TRAIN [2][60/1291]	Time 0.077 (0.055)	Data 8.61e-05 (2.91e-03)	Tok/s 231916 (196605)	Loss/tok 3.4467 (3.2470)	LR 2.875e-03
0: TRAIN [2][70/1291]	Time 0.059 (0.055)	Data 1.09e-04 (2.52e-03)	Tok/s 207654 (197573)	Loss/tok 3.4038 (3.2577)	LR 2.875e-03
0: TRAIN [2][80/1291]	Time 0.024 (0.055)	Data 8.65e-05 (2.22e-03)	Tok/s 165258 (198153)	Loss/tok 2.6853 (3.2653)	LR 2.875e-03
0: TRAIN [2][90/1291]	Time 0.041 (0.054)	Data 8.89e-05 (1.99e-03)	Tok/s 186145 (197973)	Loss/tok 3.0380 (3.2610)	LR 2.875e-03
0: TRAIN [2][100/1291]	Time 0.058 (0.054)	Data 9.68e-05 (1.80e-03)	Tok/s 216389 (198824)	Loss/tok 3.4249 (3.2714)	LR 2.875e-03
0: TRAIN [2][110/1291]	Time 0.041 (0.054)	Data 8.56e-05 (1.64e-03)	Tok/s 190478 (198987)	Loss/tok 3.1618 (3.2746)	LR 2.875e-03
0: TRAIN [2][120/1291]	Time 0.076 (0.054)	Data 8.65e-05 (1.52e-03)	Tok/s 232252 (199821)	Loss/tok 3.3603 (3.2784)	LR 2.875e-03
0: Upscaling, new scale: 4096.0
0: TRAIN [2][130/1291]	Time 0.041 (0.054)	Data 9.35e-05 (1.41e-03)	Tok/s 190222 (199264)	Loss/tok 3.1040 (3.2696)	LR 2.875e-03
0: TRAIN [2][140/1291]	Time 0.058 (0.054)	Data 8.51e-05 (1.32e-03)	Tok/s 215828 (199495)	Loss/tok 3.2396 (3.2728)	LR 2.875e-03
0: TRAIN [2][150/1291]	Time 0.041 (0.053)	Data 1.01e-04 (1.23e-03)	Tok/s 192373 (198711)	Loss/tok 3.1233 (3.2676)	LR 2.875e-03
0: TRAIN [2][160/1291]	Time 0.042 (0.052)	Data 9.11e-05 (1.16e-03)	Tok/s 191767 (198625)	Loss/tok 2.9243 (3.2601)	LR 2.875e-03
0: TRAIN [2][170/1291]	Time 0.059 (0.052)	Data 8.87e-05 (1.10e-03)	Tok/s 211458 (198368)	Loss/tok 3.2090 (3.2552)	LR 2.875e-03
0: TRAIN [2][180/1291]	Time 0.058 (0.052)	Data 8.94e-05 (1.05e-03)	Tok/s 216485 (198414)	Loss/tok 3.1772 (3.2531)	LR 2.875e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 2048.0
0: TRAIN [2][190/1291]	Time 0.058 (0.053)	Data 9.30e-05 (9.97e-04)	Tok/s 219161 (199355)	Loss/tok 3.4007 (3.2674)	LR 2.875e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 1024.0
0: TRAIN [2][200/1291]	Time 0.076 (0.053)	Data 9.04e-05 (9.52e-04)	Tok/s 227776 (199639)	Loss/tok 3.5385 (3.2737)	LR 2.875e-03
0: TRAIN [2][210/1291]	Time 0.059 (0.053)	Data 8.99e-05 (9.11e-04)	Tok/s 219706 (199941)	Loss/tok 3.3422 (3.2785)	LR 2.875e-03
0: TRAIN [2][220/1291]	Time 0.041 (0.053)	Data 8.96e-05 (8.75e-04)	Tok/s 188902 (200190)	Loss/tok 3.1397 (3.2808)	LR 2.875e-03
0: TRAIN [2][230/1291]	Time 0.076 (0.053)	Data 8.99e-05 (8.41e-04)	Tok/s 229508 (200169)	Loss/tok 3.4621 (3.2788)	LR 2.875e-03
0: TRAIN [2][240/1291]	Time 0.041 (0.053)	Data 9.27e-05 (8.10e-04)	Tok/s 186742 (200350)	Loss/tok 3.2862 (3.2812)	LR 2.875e-03
0: TRAIN [2][250/1291]	Time 0.058 (0.053)	Data 8.85e-05 (7.81e-04)	Tok/s 217462 (200156)	Loss/tok 3.3843 (3.2783)	LR 2.875e-03
0: TRAIN [2][260/1291]	Time 0.076 (0.053)	Data 1.46e-04 (7.55e-04)	Tok/s 229834 (200017)	Loss/tok 3.5016 (3.2763)	LR 2.875e-03
0: TRAIN [2][270/1291]	Time 0.041 (0.053)	Data 8.85e-05 (7.31e-04)	Tok/s 189520 (199972)	Loss/tok 3.1312 (3.2756)	LR 2.875e-03
0: TRAIN [2][280/1291]	Time 0.041 (0.052)	Data 1.10e-04 (7.09e-04)	Tok/s 183393 (199599)	Loss/tok 3.1277 (3.2713)	LR 2.875e-03
0: TRAIN [2][290/1291]	Time 0.058 (0.052)	Data 8.70e-05 (6.87e-04)	Tok/s 218194 (199329)	Loss/tok 3.3111 (3.2716)	LR 2.875e-03
0: TRAIN [2][300/1291]	Time 0.058 (0.052)	Data 8.61e-05 (6.68e-04)	Tok/s 217308 (199638)	Loss/tok 3.3136 (3.2745)	LR 2.875e-03
0: TRAIN [2][310/1291]	Time 0.058 (0.052)	Data 8.87e-05 (6.49e-04)	Tok/s 216311 (199514)	Loss/tok 3.1964 (3.2717)	LR 2.875e-03
0: Upscaling, new scale: 2048.0
0: TRAIN [2][320/1291]	Time 0.025 (0.052)	Data 9.08e-05 (6.32e-04)	Tok/s 156895 (199157)	Loss/tok 2.6527 (3.2696)	LR 2.875e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 1024.0
0: Gradient norm: inf
0: Skipped batch, new scale: 512.0
0: TRAIN [2][330/1291]	Time 0.058 (0.052)	Data 8.73e-05 (6.16e-04)	Tok/s 213795 (199199)	Loss/tok 3.1513 (3.2749)	LR 2.875e-03
0: TRAIN [2][340/1291]	Time 0.041 (0.052)	Data 8.87e-05 (6.00e-04)	Tok/s 189265 (199098)	Loss/tok 3.0354 (3.2734)	LR 2.875e-03
0: TRAIN [2][350/1291]	Time 0.041 (0.052)	Data 8.87e-05 (5.86e-04)	Tok/s 185269 (199061)	Loss/tok 2.9573 (3.2744)	LR 2.875e-03
0: TRAIN [2][360/1291]	Time 0.042 (0.052)	Data 1.07e-04 (5.72e-04)	Tok/s 188359 (199253)	Loss/tok 3.1043 (3.2760)	LR 2.875e-03
0: TRAIN [2][370/1291]	Time 0.058 (0.052)	Data 8.85e-05 (5.59e-04)	Tok/s 216628 (199335)	Loss/tok 3.2342 (3.2769)	LR 2.875e-03
0: TRAIN [2][380/1291]	Time 0.024 (0.052)	Data 1.39e-04 (5.47e-04)	Tok/s 161740 (199282)	Loss/tok 2.6019 (3.2767)	LR 2.875e-03
0: TRAIN [2][390/1291]	Time 0.024 (0.052)	Data 1.45e-04 (5.36e-04)	Tok/s 165816 (199485)	Loss/tok 2.6685 (3.2794)	LR 2.875e-03
0: TRAIN [2][400/1291]	Time 0.041 (0.052)	Data 8.49e-05 (5.25e-04)	Tok/s 186102 (199490)	Loss/tok 3.0828 (3.2783)	LR 2.875e-03
0: TRAIN [2][410/1291]	Time 0.042 (0.052)	Data 1.55e-04 (5.15e-04)	Tok/s 189231 (199632)	Loss/tok 3.1801 (3.2794)	LR 2.875e-03
0: TRAIN [2][420/1291]	Time 0.058 (0.052)	Data 1.10e-04 (5.05e-04)	Tok/s 215216 (199665)	Loss/tok 3.3583 (3.2798)	LR 2.875e-03
0: TRAIN [2][430/1291]	Time 0.042 (0.053)	Data 9.44e-05 (4.95e-04)	Tok/s 184651 (199933)	Loss/tok 3.1459 (3.2830)	LR 2.875e-03
0: TRAIN [2][440/1291]	Time 0.025 (0.052)	Data 8.70e-05 (4.86e-04)	Tok/s 161778 (199858)	Loss/tok 2.6208 (3.2810)	LR 2.875e-03
0: TRAIN [2][450/1291]	Time 0.060 (0.053)	Data 1.08e-04 (4.78e-04)	Tok/s 211629 (200031)	Loss/tok 3.1396 (3.2836)	LR 2.875e-03
0: Upscaling, new scale: 1024.0
0: TRAIN [2][460/1291]	Time 0.058 (0.053)	Data 1.41e-04 (4.70e-04)	Tok/s 217368 (200139)	Loss/tok 3.3401 (3.2835)	LR 2.875e-03
0: TRAIN [2][470/1291]	Time 0.097 (0.053)	Data 8.89e-05 (4.62e-04)	Tok/s 229348 (200407)	Loss/tok 3.7968 (3.2897)	LR 2.875e-03
0: TRAIN [2][480/1291]	Time 0.058 (0.053)	Data 8.75e-05 (4.54e-04)	Tok/s 218325 (200497)	Loss/tok 3.4006 (3.2905)	LR 2.875e-03
0: TRAIN [2][490/1291]	Time 0.059 (0.053)	Data 8.51e-05 (4.47e-04)	Tok/s 214272 (200548)	Loss/tok 3.1443 (3.2921)	LR 2.875e-03
0: TRAIN [2][500/1291]	Time 0.058 (0.053)	Data 1.43e-04 (4.40e-04)	Tok/s 216935 (200671)	Loss/tok 3.4100 (3.2931)	LR 2.875e-03
0: TRAIN [2][510/1291]	Time 0.077 (0.053)	Data 9.11e-05 (4.34e-04)	Tok/s 227368 (200819)	Loss/tok 3.6494 (3.2945)	LR 2.875e-03
0: TRAIN [2][520/1291]	Time 0.097 (0.053)	Data 9.08e-05 (4.27e-04)	Tok/s 230840 (200939)	Loss/tok 3.7382 (3.2979)	LR 2.875e-03
0: TRAIN [2][530/1291]	Time 0.042 (0.053)	Data 1.47e-04 (4.21e-04)	Tok/s 184655 (200912)	Loss/tok 3.0771 (3.2984)	LR 2.875e-03
0: TRAIN [2][540/1291]	Time 0.042 (0.053)	Data 1.45e-04 (4.15e-04)	Tok/s 184653 (200938)	Loss/tok 2.9992 (3.2978)	LR 2.875e-03
0: TRAIN [2][550/1291]	Time 0.042 (0.053)	Data 8.77e-05 (4.09e-04)	Tok/s 181739 (201067)	Loss/tok 3.0995 (3.2975)	LR 2.875e-03
0: TRAIN [2][560/1291]	Time 0.058 (0.053)	Data 8.80e-05 (4.04e-04)	Tok/s 215043 (201050)	Loss/tok 3.2859 (3.2971)	LR 2.875e-03
0: TRAIN [2][570/1291]	Time 0.041 (0.053)	Data 8.73e-05 (3.98e-04)	Tok/s 190728 (201040)	Loss/tok 3.0318 (3.2958)	LR 2.875e-03
0: TRAIN [2][580/1291]	Time 0.025 (0.053)	Data 8.89e-05 (3.93e-04)	Tok/s 159054 (200880)	Loss/tok 2.5976 (3.2944)	LR 2.875e-03
0: Upscaling, new scale: 2048.0
0: TRAIN [2][590/1291]	Time 0.058 (0.053)	Data 8.73e-05 (3.88e-04)	Tok/s 221967 (200785)	Loss/tok 3.2533 (3.2930)	LR 2.875e-03
0: TRAIN [2][600/1291]	Time 0.058 (0.053)	Data 8.82e-05 (3.83e-04)	Tok/s 216542 (200776)	Loss/tok 3.2995 (3.2923)	LR 2.875e-03
0: TRAIN [2][610/1291]	Time 0.076 (0.053)	Data 8.42e-05 (3.78e-04)	Tok/s 230019 (200896)	Loss/tok 3.3728 (3.2942)	LR 2.875e-03
0: TRAIN [2][620/1291]	Time 0.058 (0.053)	Data 9.89e-05 (3.74e-04)	Tok/s 214733 (200821)	Loss/tok 3.2859 (3.2924)	LR 2.875e-03
0: TRAIN [2][630/1291]	Time 0.025 (0.053)	Data 8.94e-05 (3.69e-04)	Tok/s 159119 (200705)	Loss/tok 2.7220 (3.2905)	LR 2.875e-03
0: TRAIN [2][640/1291]	Time 0.042 (0.053)	Data 1.55e-04 (3.65e-04)	Tok/s 186692 (200772)	Loss/tok 2.8946 (3.2915)	LR 2.875e-03
0: TRAIN [2][650/1291]	Time 0.058 (0.053)	Data 8.70e-05 (3.61e-04)	Tok/s 216520 (200820)	Loss/tok 3.3441 (3.2908)	LR 2.875e-03
0: TRAIN [2][660/1291]	Time 0.042 (0.053)	Data 8.63e-05 (3.58e-04)	Tok/s 185713 (200660)	Loss/tok 3.1555 (3.2892)	LR 2.875e-03
0: TRAIN [2][670/1291]	Time 0.041 (0.053)	Data 8.70e-05 (3.54e-04)	Tok/s 185092 (200597)	Loss/tok 3.0638 (3.2880)	LR 2.875e-03
0: TRAIN [2][680/1291]	Time 0.025 (0.053)	Data 8.58e-05 (3.50e-04)	Tok/s 156237 (200509)	Loss/tok 2.5967 (3.2875)	LR 2.875e-03
0: TRAIN [2][690/1291]	Time 0.076 (0.053)	Data 8.58e-05 (3.47e-04)	Tok/s 232124 (200502)	Loss/tok 3.4289 (3.2870)	LR 2.875e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 1024.0
0: TRAIN [2][700/1291]	Time 0.042 (0.053)	Data 8.82e-05 (3.43e-04)	Tok/s 185216 (200517)	Loss/tok 3.0159 (3.2874)	LR 2.875e-03
0: TRAIN [2][710/1291]	Time 0.042 (0.053)	Data 8.92e-05 (3.40e-04)	Tok/s 186045 (200425)	Loss/tok 2.9283 (3.2857)	LR 2.875e-03
0: TRAIN [2][720/1291]	Time 0.076 (0.053)	Data 1.11e-04 (3.36e-04)	Tok/s 229371 (200422)	Loss/tok 3.4926 (3.2859)	LR 2.875e-03
0: TRAIN [2][730/1291]	Time 0.076 (0.053)	Data 1.06e-04 (3.33e-04)	Tok/s 228028 (200450)	Loss/tok 3.5981 (3.2858)	LR 2.875e-03
0: TRAIN [2][740/1291]	Time 0.097 (0.053)	Data 9.01e-05 (3.30e-04)	Tok/s 235588 (200509)	Loss/tok 3.4672 (3.2863)	LR 2.875e-03
0: TRAIN [2][750/1291]	Time 0.025 (0.053)	Data 8.75e-05 (3.27e-04)	Tok/s 158567 (200494)	Loss/tok 2.7112 (3.2857)	LR 2.875e-03
0: TRAIN [2][760/1291]	Time 0.058 (0.053)	Data 9.01e-05 (3.24e-04)	Tok/s 216494 (200403)	Loss/tok 3.3149 (3.2845)	LR 2.875e-03
0: TRAIN [2][770/1291]	Time 0.058 (0.053)	Data 1.46e-04 (3.21e-04)	Tok/s 217962 (200458)	Loss/tok 3.3548 (3.2847)	LR 2.875e-03
0: TRAIN [2][780/1291]	Time 0.059 (0.053)	Data 9.70e-05 (3.18e-04)	Tok/s 216189 (200404)	Loss/tok 3.3110 (3.2836)	LR 2.875e-03
0: TRAIN [2][790/1291]	Time 0.058 (0.053)	Data 8.96e-05 (3.15e-04)	Tok/s 215132 (200433)	Loss/tok 3.2968 (3.2833)	LR 2.875e-03
0: TRAIN [2][800/1291]	Time 0.097 (0.053)	Data 8.39e-05 (3.13e-04)	Tok/s 233114 (200494)	Loss/tok 3.5056 (3.2843)	LR 2.875e-03
0: TRAIN [2][810/1291]	Time 0.025 (0.053)	Data 8.87e-05 (3.10e-04)	Tok/s 156577 (200456)	Loss/tok 2.6903 (3.2840)	LR 2.875e-03
0: TRAIN [2][820/1291]	Time 0.058 (0.053)	Data 8.68e-05 (3.07e-04)	Tok/s 219888 (200391)	Loss/tok 3.3047 (3.2841)	LR 2.875e-03
0: Upscaling, new scale: 2048.0
0: TRAIN [2][830/1291]	Time 0.059 (0.053)	Data 8.99e-05 (3.05e-04)	Tok/s 217202 (200490)	Loss/tok 3.3443 (3.2843)	LR 2.875e-03
0: TRAIN [2][840/1291]	Time 0.041 (0.053)	Data 1.48e-04 (3.03e-04)	Tok/s 187262 (200488)	Loss/tok 3.0743 (3.2837)	LR 2.875e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 1024.0
0: TRAIN [2][850/1291]	Time 0.077 (0.053)	Data 1.05e-04 (3.00e-04)	Tok/s 231229 (200706)	Loss/tok 3.4166 (3.2867)	LR 2.875e-03
0: TRAIN [2][860/1291]	Time 0.041 (0.053)	Data 9.08e-05 (2.98e-04)	Tok/s 189120 (200625)	Loss/tok 3.1519 (3.2856)	LR 2.875e-03
0: TRAIN [2][870/1291]	Time 0.042 (0.053)	Data 8.34e-05 (2.96e-04)	Tok/s 184052 (200600)	Loss/tok 2.9807 (3.2850)	LR 2.875e-03
0: TRAIN [2][880/1291]	Time 0.076 (0.053)	Data 8.85e-05 (2.94e-04)	Tok/s 229403 (200611)	Loss/tok 3.5906 (3.2852)	LR 2.875e-03
0: TRAIN [2][890/1291]	Time 0.058 (0.053)	Data 9.39e-05 (2.91e-04)	Tok/s 220431 (200620)	Loss/tok 3.2354 (3.2842)	LR 2.875e-03
0: TRAIN [2][900/1291]	Time 0.042 (0.053)	Data 9.89e-05 (2.89e-04)	Tok/s 187125 (200621)	Loss/tok 3.0165 (3.2836)	LR 2.875e-03
0: TRAIN [2][910/1291]	Time 0.024 (0.053)	Data 9.30e-05 (2.87e-04)	Tok/s 162894 (200524)	Loss/tok 2.6589 (3.2821)	LR 2.875e-03
0: TRAIN [2][920/1291]	Time 0.025 (0.053)	Data 8.54e-05 (2.85e-04)	Tok/s 154886 (200565)	Loss/tok 2.6831 (3.2829)	LR 2.875e-03
0: TRAIN [2][930/1291]	Time 0.042 (0.053)	Data 8.56e-05 (2.83e-04)	Tok/s 182561 (200490)	Loss/tok 3.0860 (3.2818)	LR 2.875e-03
0: TRAIN [2][940/1291]	Time 0.042 (0.052)	Data 8.70e-05 (2.81e-04)	Tok/s 185701 (200441)	Loss/tok 3.0374 (3.2806)	LR 2.875e-03
0: TRAIN [2][950/1291]	Time 0.042 (0.053)	Data 1.51e-04 (2.79e-04)	Tok/s 186200 (200539)	Loss/tok 3.0354 (3.2815)	LR 2.875e-03
0: TRAIN [2][960/1291]	Time 0.025 (0.053)	Data 8.77e-05 (2.77e-04)	Tok/s 156878 (200507)	Loss/tok 2.5931 (3.2810)	LR 2.875e-03
0: TRAIN [2][970/1291]	Time 0.058 (0.053)	Data 8.96e-05 (2.75e-04)	Tok/s 217471 (200588)	Loss/tok 3.3198 (3.2814)	LR 2.875e-03
0: Upscaling, new scale: 2048.0
0: TRAIN [2][980/1291]	Time 0.041 (0.053)	Data 1.05e-04 (2.73e-04)	Tok/s 188989 (200666)	Loss/tok 3.2549 (3.2832)	LR 2.875e-03
0: TRAIN [2][990/1291]	Time 0.059 (0.053)	Data 8.82e-05 (2.72e-04)	Tok/s 210776 (200767)	Loss/tok 3.2870 (3.2837)	LR 2.875e-03
0: TRAIN [2][1000/1291]	Time 0.058 (0.053)	Data 9.11e-05 (2.70e-04)	Tok/s 217213 (200790)	Loss/tok 3.2905 (3.2836)	LR 2.875e-03
0: TRAIN [2][1010/1291]	Time 0.025 (0.053)	Data 8.54e-05 (2.68e-04)	Tok/s 160892 (200839)	Loss/tok 2.6916 (3.2855)	LR 2.875e-03
0: TRAIN [2][1020/1291]	Time 0.041 (0.053)	Data 8.77e-05 (2.67e-04)	Tok/s 190373 (200807)	Loss/tok 3.0526 (3.2848)	LR 2.875e-03
0: TRAIN [2][1030/1291]	Time 0.025 (0.053)	Data 9.35e-05 (2.65e-04)	Tok/s 165228 (200834)	Loss/tok 2.6338 (3.2845)	LR 2.875e-03
0: TRAIN [2][1040/1291]	Time 0.076 (0.053)	Data 9.25e-05 (2.64e-04)	Tok/s 225228 (200825)	Loss/tok 3.4972 (3.2846)	LR 2.875e-03
0: TRAIN [2][1050/1291]	Time 0.076 (0.053)	Data 8.82e-05 (2.62e-04)	Tok/s 228622 (200836)	Loss/tok 3.3802 (3.2850)	LR 2.875e-03
0: TRAIN [2][1060/1291]	Time 0.025 (0.053)	Data 1.01e-04 (2.60e-04)	Tok/s 158276 (200883)	Loss/tok 2.6616 (3.2854)	LR 2.875e-03
0: TRAIN [2][1070/1291]	Time 0.058 (0.053)	Data 1.49e-04 (2.59e-04)	Tok/s 214412 (200975)	Loss/tok 3.3948 (3.2865)	LR 2.875e-03
0: TRAIN [2][1080/1291]	Time 0.059 (0.053)	Data 8.51e-05 (2.57e-04)	Tok/s 216073 (201042)	Loss/tok 3.2000 (3.2876)	LR 2.875e-03
0: TRAIN [2][1090/1291]	Time 0.041 (0.053)	Data 8.61e-05 (2.56e-04)	Tok/s 188937 (201019)	Loss/tok 2.9941 (3.2873)	LR 2.875e-03
0: TRAIN [2][1100/1291]	Time 0.058 (0.053)	Data 8.89e-05 (2.54e-04)	Tok/s 217403 (201094)	Loss/tok 3.3301 (3.2875)	LR 2.875e-03
0: Upscaling, new scale: 4096.0
0: TRAIN [2][1110/1291]	Time 0.024 (0.053)	Data 8.61e-05 (2.53e-04)	Tok/s 160861 (201006)	Loss/tok 2.7353 (3.2870)	LR 2.875e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 2048.0
0: TRAIN [2][1120/1291]	Time 0.076 (0.053)	Data 8.85e-05 (2.52e-04)	Tok/s 224230 (201058)	Loss/tok 3.4001 (3.2868)	LR 2.875e-03
0: TRAIN [2][1130/1291]	Time 0.058 (0.053)	Data 8.70e-05 (2.50e-04)	Tok/s 220673 (201103)	Loss/tok 3.2502 (3.2864)	LR 2.875e-03
0: TRAIN [2][1140/1291]	Time 0.058 (0.053)	Data 1.03e-04 (2.49e-04)	Tok/s 220231 (201159)	Loss/tok 3.0862 (3.2865)	LR 2.875e-03
0: TRAIN [2][1150/1291]	Time 0.025 (0.053)	Data 1.17e-04 (2.47e-04)	Tok/s 163573 (201193)	Loss/tok 2.6529 (3.2864)	LR 2.875e-03
0: TRAIN [2][1160/1291]	Time 0.076 (0.053)	Data 1.05e-04 (2.46e-04)	Tok/s 232912 (201219)	Loss/tok 3.3305 (3.2870)	LR 2.875e-03
0: TRAIN [2][1170/1291]	Time 0.058 (0.053)	Data 1.08e-04 (2.45e-04)	Tok/s 213855 (201205)	Loss/tok 3.4389 (3.2873)	LR 2.875e-03
0: TRAIN [2][1180/1291]	Time 0.041 (0.053)	Data 1.46e-04 (2.44e-04)	Tok/s 187451 (201181)	Loss/tok 3.0472 (3.2874)	LR 2.875e-03
0: TRAIN [2][1190/1291]	Time 0.042 (0.053)	Data 9.18e-05 (2.42e-04)	Tok/s 183321 (201145)	Loss/tok 3.0259 (3.2875)	LR 2.875e-03
0: TRAIN [2][1200/1291]	Time 0.042 (0.053)	Data 1.76e-04 (2.41e-04)	Tok/s 187629 (201162)	Loss/tok 3.0609 (3.2873)	LR 2.875e-03
0: TRAIN [2][1210/1291]	Time 0.042 (0.053)	Data 1.32e-04 (2.40e-04)	Tok/s 186739 (201049)	Loss/tok 2.9217 (3.2868)	LR 2.875e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 1024.0
0: TRAIN [2][1220/1291]	Time 0.059 (0.053)	Data 8.70e-05 (2.39e-04)	Tok/s 213047 (201099)	Loss/tok 3.2725 (3.2865)	LR 2.875e-03
0: TRAIN [2][1230/1291]	Time 0.042 (0.053)	Data 8.89e-05 (2.38e-04)	Tok/s 186611 (201164)	Loss/tok 3.1478 (3.2871)	LR 2.875e-03
0: TRAIN [2][1240/1291]	Time 0.058 (0.053)	Data 8.68e-05 (2.37e-04)	Tok/s 217865 (201152)	Loss/tok 3.2214 (3.2863)	LR 2.875e-03
0: TRAIN [2][1250/1291]	Time 0.042 (0.053)	Data 9.13e-05 (2.35e-04)	Tok/s 188290 (201168)	Loss/tok 3.0755 (3.2861)	LR 2.875e-03
0: TRAIN [2][1260/1291]	Time 0.025 (0.053)	Data 8.87e-05 (2.34e-04)	Tok/s 162252 (201250)	Loss/tok 2.7298 (3.2867)	LR 2.875e-03
0: TRAIN [2][1270/1291]	Time 0.041 (0.053)	Data 8.99e-05 (2.33e-04)	Tok/s 187743 (201191)	Loss/tok 2.9899 (3.2860)	LR 2.875e-03
0: TRAIN [2][1280/1291]	Time 0.042 (0.053)	Data 8.75e-05 (2.32e-04)	Tok/s 187894 (201050)	Loss/tok 3.0213 (3.2848)	LR 2.875e-03
0: TRAIN [2][1290/1291]	Time 0.024 (0.053)	Data 4.51e-05 (2.32e-04)	Tok/s 167336 (201013)	Loss/tok 2.6580 (3.2840)	LR 2.875e-03
:::MLLOG {"namespace": "", "time_ms": 1593114249765, "event_type": "INTERVAL_END", "key": "epoch_stop", "value": null, "metadata": {"file": "train.py", "lineno": 551, "epoch_num": 3}}
:::MLLOG {"namespace": "", "time_ms": 1593114249766, "event_type": "INTERVAL_START", "key": "eval_start", "value": null, "metadata": {"file": "train.py", "lineno": 556, "epoch_num": 3}}
0: Running evaluation on test set
0: TEST [2][0/3]	Time 0.294 (0.294)	Decoder iters 100.0 (100.0)	Tok/s 29044 (29044)
0: Running moses detokenizer
0: BLEU(score=22.772106305028913, counts=[35652, 17443, 9750, 5741], totals=[63128, 60125, 57123, 54125], precisions=[56.4757318464073, 29.01122661122661, 17.068431279869756, 10.606928406466512], bp=0.9757766051890238, sys_len=63128, ref_len=64676)
0: Finished evaluation on test set
:::MLLOG {"namespace": "", "time_ms": 1593114250931, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.22769999999999999, "metadata": {"file": "train.py", "lineno": 562, "epoch_num": 3}}
:::MLLOG {"namespace": "", "time_ms": 1593114250931, "event_type": "INTERVAL_END", "key": "eval_stop", "value": null, "metadata": {"file": "train.py", "lineno": 565, "epoch_num": 3}}
0: Summary: Epoch: 2	Training Loss: 3.2814	Test BLEU: 22.77
0: Performance: Epoch: 2	Training: 3215984 Tok/s
0: Finished epoch 2
:::MLLOG {"namespace": "", "time_ms": 1593114250931, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "train.py", "lineno": 584, "first_epoch_num": 3}}
:::MLLOG {"namespace": "", "time_ms": 1593114250931, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "train.py", "lineno": 538, "first_epoch_num": 4, "epoch_count": 1}}
:::MLLOG {"namespace": "", "time_ms": 1593114250931, "event_type": "INTERVAL_START", "key": "epoch_start", "value": null, "metadata": {"file": "train.py", "lineno": 541, "epoch_num": 4}}
0: Starting epoch 3
0: Sampler for epoch 3 uses seed 2184278408
0: TRAIN [3][0/1291]	Time 0.290 (0.290)	Data 1.78e-01 (1.78e-01)	Tok/s 27223 (27223)	Loss/tok 2.9714 (2.9714)	LR 2.875e-03
0: TRAIN [3][10/1291]	Time 0.041 (0.069)	Data 8.70e-05 (1.63e-02)	Tok/s 190538 (179671)	Loss/tok 2.9382 (3.0824)	LR 2.875e-03
0: TRAIN [3][20/1291]	Time 0.042 (0.060)	Data 8.03e-05 (8.56e-03)	Tok/s 189465 (189503)	Loss/tok 2.9754 (3.1100)	LR 2.875e-03
0: TRAIN [3][30/1291]	Time 0.059 (0.058)	Data 9.54e-05 (5.83e-03)	Tok/s 214359 (195313)	Loss/tok 3.1839 (3.1480)	LR 2.875e-03
0: TRAIN [3][40/1291]	Time 0.058 (0.055)	Data 1.05e-04 (4.43e-03)	Tok/s 217563 (194894)	Loss/tok 3.2042 (3.1293)	LR 2.875e-03
0: Upscaling, new scale: 2048.0
0: TRAIN [3][50/1291]	Time 0.058 (0.053)	Data 9.16e-05 (3.58e-03)	Tok/s 213433 (194166)	Loss/tok 3.1221 (3.1158)	LR 2.875e-03
0: TRAIN [3][60/1291]	Time 0.042 (0.052)	Data 1.04e-04 (3.01e-03)	Tok/s 184684 (194254)	Loss/tok 3.1410 (3.1175)	LR 2.875e-03
0: TRAIN [3][70/1291]	Time 0.041 (0.054)	Data 1.10e-04 (2.60e-03)	Tok/s 184062 (197055)	Loss/tok 3.1220 (3.1505)	LR 2.875e-03
0: TRAIN [3][80/1291]	Time 0.041 (0.054)	Data 8.39e-05 (2.30e-03)	Tok/s 192948 (199046)	Loss/tok 3.0034 (3.1561)	LR 2.875e-03
0: TRAIN [3][90/1291]	Time 0.041 (0.054)	Data 9.16e-05 (2.05e-03)	Tok/s 187343 (199194)	Loss/tok 2.9096 (3.1541)	LR 2.875e-03
0: TRAIN [3][100/1291]	Time 0.058 (0.054)	Data 8.03e-05 (1.86e-03)	Tok/s 215981 (199506)	Loss/tok 3.2330 (3.1539)	LR 2.875e-03
0: TRAIN [3][110/1291]	Time 0.096 (0.054)	Data 1.00e-04 (1.70e-03)	Tok/s 230607 (199073)	Loss/tok 3.6129 (3.1642)	LR 2.875e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 1024.0
0: TRAIN [3][120/1291]	Time 0.041 (0.054)	Data 9.70e-05 (1.57e-03)	Tok/s 189117 (199205)	Loss/tok 2.8838 (3.1700)	LR 2.875e-03
0: TRAIN [3][130/1291]	Time 0.041 (0.054)	Data 8.56e-05 (1.46e-03)	Tok/s 187436 (200139)	Loss/tok 3.0559 (3.1844)	LR 2.875e-03
0: TRAIN [3][140/1291]	Time 0.076 (0.054)	Data 8.73e-05 (1.36e-03)	Tok/s 230140 (200365)	Loss/tok 3.3450 (3.1847)	LR 2.875e-03
0: TRAIN [3][150/1291]	Time 0.041 (0.053)	Data 8.13e-05 (1.28e-03)	Tok/s 184333 (199528)	Loss/tok 2.9988 (3.1777)	LR 2.875e-03
0: TRAIN [3][160/1291]	Time 0.058 (0.053)	Data 8.20e-05 (1.20e-03)	Tok/s 216161 (199780)	Loss/tok 3.0500 (3.1801)	LR 2.875e-03
0: TRAIN [3][170/1291]	Time 0.024 (0.053)	Data 8.20e-05 (1.14e-03)	Tok/s 155976 (199151)	Loss/tok 2.6468 (3.1758)	LR 2.875e-03
0: TRAIN [3][180/1291]	Time 0.076 (0.053)	Data 8.11e-05 (1.08e-03)	Tok/s 228667 (199290)	Loss/tok 3.3986 (3.1793)	LR 2.875e-03
0: TRAIN [3][190/1291]	Time 0.076 (0.053)	Data 1.41e-04 (1.03e-03)	Tok/s 230596 (199504)	Loss/tok 3.3795 (3.1839)	LR 2.875e-03
0: TRAIN [3][200/1291]	Time 0.042 (0.053)	Data 8.13e-05 (9.83e-04)	Tok/s 188118 (199451)	Loss/tok 2.9947 (3.1868)	LR 2.875e-03
0: TRAIN [3][210/1291]	Time 0.041 (0.052)	Data 1.44e-04 (9.41e-04)	Tok/s 186043 (198984)	Loss/tok 2.9483 (3.1825)	LR 1.437e-03
0: TRAIN [3][220/1291]	Time 0.076 (0.053)	Data 8.92e-05 (9.03e-04)	Tok/s 231193 (199439)	Loss/tok 3.3378 (3.1887)	LR 1.437e-03
0: TRAIN [3][230/1291]	Time 0.042 (0.053)	Data 9.39e-05 (8.68e-04)	Tok/s 188783 (199581)	Loss/tok 2.8571 (3.1926)	LR 1.437e-03
0: TRAIN [3][240/1291]	Time 0.041 (0.053)	Data 8.32e-05 (8.36e-04)	Tok/s 186011 (199602)	Loss/tok 2.9621 (3.1895)	LR 1.437e-03
0: Upscaling, new scale: 2048.0
0: TRAIN [3][250/1291]	Time 0.042 (0.053)	Data 7.94e-05 (8.06e-04)	Tok/s 188852 (199812)	Loss/tok 2.9898 (3.1901)	LR 1.437e-03
0: TRAIN [3][260/1291]	Time 0.042 (0.053)	Data 8.25e-05 (7.79e-04)	Tok/s 188502 (199982)	Loss/tok 3.0341 (3.1886)	LR 1.437e-03
0: TRAIN [3][270/1291]	Time 0.041 (0.053)	Data 8.13e-05 (7.53e-04)	Tok/s 183106 (199753)	Loss/tok 2.9276 (3.1869)	LR 1.437e-03
0: TRAIN [3][280/1291]	Time 0.058 (0.053)	Data 8.20e-05 (7.30e-04)	Tok/s 213162 (200087)	Loss/tok 3.1434 (3.1866)	LR 1.437e-03
0: TRAIN [3][290/1291]	Time 0.042 (0.053)	Data 9.68e-05 (7.08e-04)	Tok/s 182369 (200117)	Loss/tok 3.0858 (3.1852)	LR 1.437e-03
0: TRAIN [3][300/1291]	Time 0.024 (0.053)	Data 8.39e-05 (6.88e-04)	Tok/s 160479 (199744)	Loss/tok 2.5566 (3.1820)	LR 1.437e-03
0: TRAIN [3][310/1291]	Time 0.042 (0.053)	Data 9.23e-05 (6.69e-04)	Tok/s 182285 (199612)	Loss/tok 2.8766 (3.1822)	LR 1.437e-03
0: TRAIN [3][320/1291]	Time 0.041 (0.052)	Data 1.61e-04 (6.51e-04)	Tok/s 185343 (199561)	Loss/tok 2.8996 (3.1795)	LR 1.437e-03
0: TRAIN [3][330/1291]	Time 0.096 (0.053)	Data 9.87e-05 (6.34e-04)	Tok/s 233385 (199889)	Loss/tok 3.3861 (3.1795)	LR 1.437e-03
0: TRAIN [3][340/1291]	Time 0.041 (0.053)	Data 8.15e-05 (6.19e-04)	Tok/s 190544 (200013)	Loss/tok 2.8564 (3.1778)	LR 1.437e-03
0: TRAIN [3][350/1291]	Time 0.058 (0.053)	Data 8.18e-05 (6.04e-04)	Tok/s 213349 (200339)	Loss/tok 3.1718 (3.1788)	LR 1.437e-03
0: TRAIN [3][360/1291]	Time 0.076 (0.053)	Data 8.94e-05 (5.90e-04)	Tok/s 231840 (200754)	Loss/tok 3.1368 (3.1816)	LR 1.437e-03
0: Upscaling, new scale: 4096.0
0: TRAIN [3][370/1291]	Time 0.041 (0.053)	Data 1.43e-04 (5.76e-04)	Tok/s 187527 (200816)	Loss/tok 3.0019 (3.1812)	LR 1.437e-03
0: TRAIN [3][380/1291]	Time 0.041 (0.053)	Data 8.01e-05 (5.64e-04)	Tok/s 189250 (200587)	Loss/tok 3.0889 (3.1801)	LR 1.437e-03
0: TRAIN [3][390/1291]	Time 0.058 (0.053)	Data 8.75e-05 (5.51e-04)	Tok/s 217837 (200689)	Loss/tok 3.0871 (3.1794)	LR 1.437e-03
0: TRAIN [3][400/1291]	Time 0.025 (0.053)	Data 8.20e-05 (5.40e-04)	Tok/s 158377 (200452)	Loss/tok 2.5748 (3.1767)	LR 1.437e-03
0: TRAIN [3][410/1291]	Time 0.097 (0.053)	Data 1.46e-04 (5.29e-04)	Tok/s 233503 (200776)	Loss/tok 3.4779 (3.1819)	LR 1.437e-03
0: TRAIN [3][420/1291]	Time 0.041 (0.053)	Data 8.42e-05 (5.19e-04)	Tok/s 193915 (200715)	Loss/tok 2.9520 (3.1790)	LR 1.437e-03
0: TRAIN [3][430/1291]	Time 0.059 (0.053)	Data 1.04e-04 (5.09e-04)	Tok/s 211612 (200764)	Loss/tok 3.1728 (3.1787)	LR 1.437e-03
0: TRAIN [3][440/1291]	Time 0.041 (0.053)	Data 8.51e-05 (5.00e-04)	Tok/s 187228 (200730)	Loss/tok 2.8951 (3.1776)	LR 1.437e-03
0: TRAIN [3][450/1291]	Time 0.058 (0.053)	Data 8.11e-05 (4.91e-04)	Tok/s 214084 (200923)	Loss/tok 3.1501 (3.1778)	LR 1.437e-03
0: TRAIN [3][460/1291]	Time 0.077 (0.053)	Data 8.15e-05 (4.82e-04)	Tok/s 228166 (201091)	Loss/tok 3.3914 (3.1791)	LR 1.437e-03
0: TRAIN [3][470/1291]	Time 0.077 (0.053)	Data 7.75e-05 (4.74e-04)	Tok/s 225599 (200827)	Loss/tok 3.3794 (3.1784)	LR 1.437e-03
0: TRAIN [3][480/1291]	Time 0.059 (0.053)	Data 7.87e-05 (4.66e-04)	Tok/s 216686 (200947)	Loss/tok 3.2076 (3.1771)	LR 1.437e-03
0: TRAIN [3][490/1291]	Time 0.076 (0.053)	Data 7.84e-05 (4.58e-04)	Tok/s 229806 (200773)	Loss/tok 3.3188 (3.1758)	LR 1.437e-03
0: Upscaling, new scale: 8192.0
0: TRAIN [3][500/1291]	Time 0.058 (0.053)	Data 8.54e-05 (4.50e-04)	Tok/s 214557 (200547)	Loss/tok 3.0554 (3.1726)	LR 1.437e-03
0: TRAIN [3][510/1291]	Time 0.042 (0.053)	Data 7.84e-05 (4.43e-04)	Tok/s 183807 (200322)	Loss/tok 2.9306 (3.1708)	LR 1.437e-03
0: TRAIN [3][520/1291]	Time 0.041 (0.053)	Data 8.15e-05 (4.36e-04)	Tok/s 191058 (200286)	Loss/tok 3.0172 (3.1700)	LR 1.437e-03
0: TRAIN [3][530/1291]	Time 0.058 (0.053)	Data 7.94e-05 (4.30e-04)	Tok/s 217399 (200293)	Loss/tok 3.1331 (3.1690)	LR 1.437e-03
0: TRAIN [3][540/1291]	Time 0.058 (0.052)	Data 8.11e-05 (4.23e-04)	Tok/s 214108 (200240)	Loss/tok 3.1769 (3.1682)	LR 1.437e-03
0: TRAIN [3][550/1291]	Time 0.042 (0.052)	Data 7.87e-05 (4.17e-04)	Tok/s 189447 (200070)	Loss/tok 3.0422 (3.1675)	LR 1.437e-03
0: TRAIN [3][560/1291]	Time 0.041 (0.052)	Data 7.84e-05 (4.11e-04)	Tok/s 186089 (200100)	Loss/tok 2.9582 (3.1663)	LR 1.437e-03
0: TRAIN [3][570/1291]	Time 0.077 (0.052)	Data 8.01e-05 (4.05e-04)	Tok/s 227429 (200152)	Loss/tok 3.3604 (3.1672)	LR 1.437e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 4096.0
0: TRAIN [3][580/1291]	Time 0.059 (0.052)	Data 7.96e-05 (4.00e-04)	Tok/s 218894 (200184)	Loss/tok 2.9857 (3.1658)	LR 1.437e-03
0: TRAIN [3][590/1291]	Time 0.041 (0.053)	Data 7.68e-05 (3.94e-04)	Tok/s 188019 (200405)	Loss/tok 2.8451 (3.1670)	LR 1.437e-03
0: TRAIN [3][600/1291]	Time 0.058 (0.052)	Data 7.84e-05 (3.89e-04)	Tok/s 216975 (200369)	Loss/tok 3.1141 (3.1657)	LR 1.437e-03
0: TRAIN [3][610/1291]	Time 0.058 (0.052)	Data 8.58e-05 (3.84e-04)	Tok/s 219730 (200312)	Loss/tok 3.0637 (3.1650)	LR 1.437e-03
0: TRAIN [3][620/1291]	Time 0.058 (0.052)	Data 8.30e-05 (3.79e-04)	Tok/s 214414 (200318)	Loss/tok 3.3265 (3.1646)	LR 1.437e-03
0: TRAIN [3][630/1291]	Time 0.025 (0.052)	Data 8.18e-05 (3.74e-04)	Tok/s 159170 (200375)	Loss/tok 2.5005 (3.1637)	LR 1.437e-03
0: TRAIN [3][640/1291]	Time 0.096 (0.052)	Data 8.25e-05 (3.70e-04)	Tok/s 230531 (200394)	Loss/tok 3.4717 (3.1647)	LR 1.437e-03
0: TRAIN [3][650/1291]	Time 0.041 (0.052)	Data 8.20e-05 (3.66e-04)	Tok/s 188074 (200456)	Loss/tok 2.9968 (3.1638)	LR 1.437e-03
0: TRAIN [3][660/1291]	Time 0.041 (0.052)	Data 8.20e-05 (3.61e-04)	Tok/s 190515 (200371)	Loss/tok 2.8868 (3.1628)	LR 1.437e-03
0: TRAIN [3][670/1291]	Time 0.042 (0.052)	Data 8.20e-05 (3.57e-04)	Tok/s 182917 (200402)	Loss/tok 2.8361 (3.1636)	LR 1.437e-03
0: TRAIN [3][680/1291]	Time 0.059 (0.053)	Data 8.15e-05 (3.53e-04)	Tok/s 216639 (200511)	Loss/tok 3.0784 (3.1638)	LR 1.437e-03
0: TRAIN [3][690/1291]	Time 0.059 (0.052)	Data 8.25e-05 (3.49e-04)	Tok/s 216097 (200435)	Loss/tok 3.0833 (3.1625)	LR 1.437e-03
0: Upscaling, new scale: 8192.0
0: TRAIN [3][700/1291]	Time 0.042 (0.052)	Data 8.34e-05 (3.45e-04)	Tok/s 186961 (200386)	Loss/tok 2.9851 (3.1615)	LR 1.437e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 4096.0
0: TRAIN [3][710/1291]	Time 0.042 (0.052)	Data 8.30e-05 (3.42e-04)	Tok/s 184557 (200435)	Loss/tok 2.9449 (3.1620)	LR 1.437e-03
0: TRAIN [3][720/1291]	Time 0.042 (0.052)	Data 8.18e-05 (3.38e-04)	Tok/s 184905 (200405)	Loss/tok 3.0154 (3.1622)	LR 7.187e-04
0: TRAIN [3][730/1291]	Time 0.058 (0.052)	Data 8.25e-05 (3.35e-04)	Tok/s 216137 (200443)	Loss/tok 3.1701 (3.1622)	LR 7.187e-04
0: TRAIN [3][740/1291]	Time 0.041 (0.052)	Data 8.39e-05 (3.31e-04)	Tok/s 185095 (200312)	Loss/tok 2.9539 (3.1605)	LR 7.187e-04
0: TRAIN [3][750/1291]	Time 0.041 (0.052)	Data 8.34e-05 (3.28e-04)	Tok/s 187005 (200284)	Loss/tok 2.8193 (3.1590)	LR 7.187e-04
0: TRAIN [3][760/1291]	Time 0.059 (0.052)	Data 8.27e-05 (3.25e-04)	Tok/s 216572 (200389)	Loss/tok 3.2135 (3.1609)	LR 7.187e-04
0: TRAIN [3][770/1291]	Time 0.058 (0.052)	Data 8.23e-05 (3.22e-04)	Tok/s 215172 (200485)	Loss/tok 3.2550 (3.1610)	LR 7.187e-04
0: TRAIN [3][780/1291]	Time 0.059 (0.052)	Data 8.65e-05 (3.19e-04)	Tok/s 217206 (200505)	Loss/tok 3.2012 (3.1604)	LR 7.187e-04
0: TRAIN [3][790/1291]	Time 0.058 (0.052)	Data 8.34e-05 (3.16e-04)	Tok/s 214234 (200493)	Loss/tok 3.1465 (3.1598)	LR 7.187e-04
0: TRAIN [3][800/1291]	Time 0.077 (0.052)	Data 8.34e-05 (3.13e-04)	Tok/s 225813 (200578)	Loss/tok 3.3869 (3.1607)	LR 7.187e-04
0: TRAIN [3][810/1291]	Time 0.042 (0.052)	Data 8.20e-05 (3.10e-04)	Tok/s 185907 (200429)	Loss/tok 3.0521 (3.1589)	LR 7.187e-04
0: TRAIN [3][820/1291]	Time 0.041 (0.052)	Data 8.42e-05 (3.07e-04)	Tok/s 188282 (200466)	Loss/tok 2.9641 (3.1577)	LR 7.187e-04
0: TRAIN [3][830/1291]	Time 0.097 (0.052)	Data 8.49e-05 (3.05e-04)	Tok/s 229602 (200474)	Loss/tok 3.5362 (3.1589)	LR 7.187e-04
0: Upscaling, new scale: 8192.0
0: TRAIN [3][840/1291]	Time 0.077 (0.052)	Data 8.13e-05 (3.02e-04)	Tok/s 226689 (200551)	Loss/tok 3.3564 (3.1578)	LR 7.187e-04
0: TRAIN [3][850/1291]	Time 0.058 (0.053)	Data 7.84e-05 (2.99e-04)	Tok/s 215612 (200632)	Loss/tok 3.0766 (3.1584)	LR 7.187e-04
0: TRAIN [3][860/1291]	Time 0.041 (0.053)	Data 8.49e-05 (2.97e-04)	Tok/s 187902 (200770)	Loss/tok 2.9332 (3.1606)	LR 7.187e-04
0: TRAIN [3][870/1291]	Time 0.041 (0.053)	Data 8.06e-05 (2.94e-04)	Tok/s 184813 (200704)	Loss/tok 2.9802 (3.1597)	LR 7.187e-04
0: TRAIN [3][880/1291]	Time 0.065 (0.053)	Data 8.13e-05 (2.92e-04)	Tok/s 192845 (200698)	Loss/tok 3.0633 (3.1591)	LR 7.187e-04
0: TRAIN [3][890/1291]	Time 0.041 (0.053)	Data 8.20e-05 (2.90e-04)	Tok/s 188322 (200683)	Loss/tok 2.9774 (3.1582)	LR 7.187e-04
0: TRAIN [3][900/1291]	Time 0.025 (0.053)	Data 8.13e-05 (2.88e-04)	Tok/s 158739 (200758)	Loss/tok 2.5634 (3.1587)	LR 7.187e-04
0: TRAIN [3][910/1291]	Time 0.041 (0.053)	Data 8.18e-05 (2.86e-04)	Tok/s 193559 (200745)	Loss/tok 2.9432 (3.1581)	LR 7.187e-04
0: TRAIN [3][920/1291]	Time 0.042 (0.053)	Data 1.17e-04 (2.84e-04)	Tok/s 184858 (200745)	Loss/tok 2.8728 (3.1585)	LR 7.187e-04
0: TRAIN [3][930/1291]	Time 0.058 (0.053)	Data 8.32e-05 (2.82e-04)	Tok/s 218599 (200758)	Loss/tok 3.1096 (3.1580)	LR 7.187e-04
0: TRAIN [3][940/1291]	Time 0.025 (0.053)	Data 9.16e-05 (2.80e-04)	Tok/s 163965 (200636)	Loss/tok 2.6106 (3.1568)	LR 7.187e-04
0: TRAIN [3][950/1291]	Time 0.076 (0.052)	Data 9.97e-05 (2.78e-04)	Tok/s 231170 (200618)	Loss/tok 3.2404 (3.1558)	LR 7.187e-04
0: Upscaling, new scale: 8192.0
0: TRAIN [3][960/1291]	Time 0.024 (0.052)	Data 1.59e-04 (2.76e-04)	Tok/s 160979 (200571)	Loss/tok 2.4401 (3.1546)	LR 7.187e-04
0: TRAIN [3][970/1291]	Time 0.041 (0.052)	Data 8.87e-05 (2.74e-04)	Tok/s 188266 (200644)	Loss/tok 3.0500 (3.1546)	LR 7.187e-04
0: TRAIN [3][980/1291]	Time 0.058 (0.052)	Data 8.13e-05 (2.72e-04)	Tok/s 216096 (200671)	Loss/tok 3.1338 (3.1543)	LR 7.187e-04
0: TRAIN [3][990/1291]	Time 0.058 (0.052)	Data 7.87e-05 (2.70e-04)	Tok/s 219606 (200712)	Loss/tok 3.0251 (3.1541)	LR 7.187e-04
0: TRAIN [3][1000/1291]	Time 0.042 (0.052)	Data 8.01e-05 (2.69e-04)	Tok/s 183823 (200699)	Loss/tok 2.9687 (3.1535)	LR 7.187e-04
0: TRAIN [3][1010/1291]	Time 0.042 (0.052)	Data 8.27e-05 (2.67e-04)	Tok/s 190488 (200724)	Loss/tok 2.9572 (3.1537)	LR 7.187e-04
0: TRAIN [3][1020/1291]	Time 0.025 (0.052)	Data 7.94e-05 (2.65e-04)	Tok/s 159958 (200632)	Loss/tok 2.6653 (3.1536)	LR 7.187e-04
0: TRAIN [3][1030/1291]	Time 0.076 (0.052)	Data 7.89e-05 (2.63e-04)	Tok/s 228276 (200641)	Loss/tok 3.3391 (3.1534)	LR 7.187e-04
0: TRAIN [3][1040/1291]	Time 0.077 (0.053)	Data 9.94e-05 (2.62e-04)	Tok/s 227636 (200716)	Loss/tok 3.3323 (3.1539)	LR 7.187e-04
0: TRAIN [3][1050/1291]	Time 0.025 (0.053)	Data 7.99e-05 (2.60e-04)	Tok/s 159170 (200723)	Loss/tok 2.4554 (3.1535)	LR 7.187e-04
0: TRAIN [3][1060/1291]	Time 0.024 (0.052)	Data 1.40e-04 (2.59e-04)	Tok/s 159064 (200614)	Loss/tok 2.4790 (3.1532)	LR 7.187e-04
0: TRAIN [3][1070/1291]	Time 0.041 (0.052)	Data 8.06e-05 (2.57e-04)	Tok/s 185578 (200527)	Loss/tok 2.8543 (3.1522)	LR 7.187e-04
0: Gradient norm: inf
0: Skipped batch, new scale: 4096.0
0: TRAIN [3][1080/1291]	Time 0.097 (0.052)	Data 1.38e-04 (2.56e-04)	Tok/s 231660 (200562)	Loss/tok 3.4221 (3.1525)	LR 7.187e-04
0: TRAIN [3][1090/1291]	Time 0.042 (0.052)	Data 8.18e-05 (2.54e-04)	Tok/s 182569 (200505)	Loss/tok 2.9641 (3.1520)	LR 7.187e-04
0: TRAIN [3][1100/1291]	Time 0.076 (0.052)	Data 1.47e-04 (2.53e-04)	Tok/s 230543 (200554)	Loss/tok 3.3549 (3.1519)	LR 7.187e-04
0: TRAIN [3][1110/1291]	Time 0.058 (0.052)	Data 7.89e-05 (2.51e-04)	Tok/s 216930 (200505)	Loss/tok 3.1741 (3.1514)	LR 7.187e-04
0: TRAIN [3][1120/1291]	Time 0.058 (0.052)	Data 1.41e-04 (2.50e-04)	Tok/s 215860 (200513)	Loss/tok 3.0736 (3.1509)	LR 7.187e-04
0: TRAIN [3][1130/1291]	Time 0.058 (0.052)	Data 8.03e-05 (2.49e-04)	Tok/s 219229 (200621)	Loss/tok 3.0381 (3.1515)	LR 7.187e-04
0: TRAIN [3][1140/1291]	Time 0.042 (0.052)	Data 8.54e-05 (2.47e-04)	Tok/s 182887 (200586)	Loss/tok 2.9347 (3.1521)	LR 7.187e-04
0: TRAIN [3][1150/1291]	Time 0.058 (0.053)	Data 1.37e-04 (2.46e-04)	Tok/s 218734 (200634)	Loss/tok 3.1462 (3.1515)	LR 7.187e-04
0: TRAIN [3][1160/1291]	Time 0.097 (0.053)	Data 8.06e-05 (2.45e-04)	Tok/s 228390 (200655)	Loss/tok 3.4518 (3.1528)	LR 7.187e-04
0: TRAIN [3][1170/1291]	Time 0.041 (0.053)	Data 8.06e-05 (2.43e-04)	Tok/s 188252 (200647)	Loss/tok 2.8775 (3.1532)	LR 7.187e-04
0: TRAIN [3][1180/1291]	Time 0.058 (0.053)	Data 8.15e-05 (2.42e-04)	Tok/s 215750 (200663)	Loss/tok 3.0745 (3.1524)	LR 7.187e-04
0: TRAIN [3][1190/1291]	Time 0.076 (0.053)	Data 7.94e-05 (2.41e-04)	Tok/s 231035 (200668)	Loss/tok 3.3796 (3.1521)	LR 7.187e-04
0: Upscaling, new scale: 8192.0
0: TRAIN [3][1200/1291]	Time 0.041 (0.053)	Data 7.99e-05 (2.39e-04)	Tok/s 183965 (200580)	Loss/tok 2.8345 (3.1508)	LR 7.187e-04
0: TRAIN [3][1210/1291]	Time 0.041 (0.053)	Data 9.06e-05 (2.38e-04)	Tok/s 188533 (200652)	Loss/tok 2.8901 (3.1513)	LR 7.187e-04
0: TRAIN [3][1220/1291]	Time 0.058 (0.053)	Data 8.13e-05 (2.37e-04)	Tok/s 216726 (200773)	Loss/tok 3.2952 (3.1530)	LR 3.594e-04
0: TRAIN [3][1230/1291]	Time 0.041 (0.053)	Data 9.61e-05 (2.36e-04)	Tok/s 186204 (200797)	Loss/tok 3.0272 (3.1531)	LR 3.594e-04
0: TRAIN [3][1240/1291]	Time 0.076 (0.053)	Data 8.23e-05 (2.35e-04)	Tok/s 231430 (200879)	Loss/tok 3.3241 (3.1534)	LR 3.594e-04
0: TRAIN [3][1250/1291]	Time 0.041 (0.053)	Data 8.11e-05 (2.34e-04)	Tok/s 187237 (200824)	Loss/tok 2.9581 (3.1528)	LR 3.594e-04
0: TRAIN [3][1260/1291]	Time 0.042 (0.053)	Data 8.15e-05 (2.32e-04)	Tok/s 188766 (200857)	Loss/tok 2.7988 (3.1533)	LR 3.594e-04
0: TRAIN [3][1270/1291]	Time 0.058 (0.053)	Data 8.92e-05 (2.31e-04)	Tok/s 215033 (200936)	Loss/tok 3.1094 (3.1531)	LR 3.594e-04
0: TRAIN [3][1280/1291]	Time 0.058 (0.053)	Data 8.82e-05 (2.30e-04)	Tok/s 217897 (200992)	Loss/tok 2.9811 (3.1524)	LR 3.594e-04
0: TRAIN [3][1290/1291]	Time 0.058 (0.053)	Data 4.29e-05 (2.31e-04)	Tok/s 215325 (201059)	Loss/tok 3.0526 (3.1521)	LR 3.594e-04
:::MLLOG {"namespace": "", "time_ms": 1593114319272, "event_type": "INTERVAL_END", "key": "epoch_stop", "value": null, "metadata": {"file": "train.py", "lineno": 551, "epoch_num": 4}}
:::MLLOG {"namespace": "", "time_ms": 1593114319272, "event_type": "INTERVAL_START", "key": "eval_start", "value": null, "metadata": {"file": "train.py", "lineno": 556, "epoch_num": 4}}
0: Running evaluation on test set
0: TEST [3][0/3]	Time 0.320 (0.320)	Decoder iters 106.0 (106.0)	Tok/s 27814 (27814)
0: Running moses detokenizer
0: BLEU(score=24.150926302196023, counts=[37130, 18663, 10700, 6422], totals=[65760, 62757, 59755, 56758], precisions=[56.46289537712895, 29.738515225393183, 17.90645134298385, 11.314704535043518], bp=1.0, sys_len=65760, ref_len=64676)
0: Target accuracy reached
0: Finished evaluation on test set
:::MLLOG {"namespace": "", "time_ms": 1593114320509, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.2415, "metadata": {"file": "train.py", "lineno": 562, "epoch_num": 4}}
:::MLLOG {"namespace": "", "time_ms": 1593114320510, "event_type": "INTERVAL_END", "key": "eval_stop", "value": null, "metadata": {"file": "train.py", "lineno": 565, "epoch_num": 4}}
0: Summary: Epoch: 3	Training Loss: 3.1494	Test BLEU: 24.15
0: Performance: Epoch: 3	Training: 3217050 Tok/s
0: Finished epoch 3
:::MLLOG {"namespace": "", "time_ms": 1593114320510, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "train.py", "lineno": 584, "first_epoch_num": 4}}
0: Closing preprocessed data file
:::MLLOG {"namespace": "", "time_ms": 1593114320510, "event_type": "INTERVAL_END", "key": "run_stop", "value": null, "metadata": {"file": "train.py", "lineno": 595, "status": "success"}}
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
ENDING TIMING RUN AT 2020-06-25 12:45:27 PM
RESULT,RNN_TRANSLATOR,,311,nvidia,2020-06-25 12:40:16 PM
ENDING TIMING RUN AT 2020-06-25 12:45:27 PM
RESULT,RNN_TRANSLATOR,,311,nvidia,2020-06-25 12:40:16 PM
ENDING TIMING RUN AT 2020-06-25 12:45:28 PM
RESULT,RNN_TRANSLATOR,,312,nvidia,2020-06-25 12:40:16 PM
ENDING TIMING RUN AT 2020-06-25 12:45:28 PM
RESULT,RNN_TRANSLATOR,,312,nvidia,2020-06-25 12:40:16 PM
ENDING TIMING RUN AT 2020-06-25 12:45:28 PM
RESULT,RNN_TRANSLATOR,,312,nvidia,2020-06-25 12:40:16 PM
ENDING TIMING RUN AT 2020-06-25 12:45:28 PM
ENDING TIMING RUN AT 2020-06-25 12:45:28 PM
RESULT,RNN_TRANSLATOR,,312,nvidia,2020-06-25 12:40:16 PM
RESULT,RNN_TRANSLATOR,,312,nvidia,2020-06-25 12:40:16 PM
ENDING TIMING RUN AT 2020-06-25 12:45:28 PM
RESULT,RNN_TRANSLATOR,,312,nvidia,2020-06-25 12:40:16 PM
ENDING TIMING RUN AT 2020-06-25 12:45:28 PM
RESULT,RNN_TRANSLATOR,,312,nvidia,2020-06-25 12:40:16 PM
ENDING TIMING RUN AT 2020-06-25 12:45:28 PM
RESULT,RNN_TRANSLATOR,,312,nvidia,2020-06-25 12:40:16 PM
ENDING TIMING RUN AT 2020-06-25 12:45:28 PM
ENDING TIMING RUN AT 2020-06-25 12:45:28 PM
RESULT,RNN_TRANSLATOR,,312,nvidia,2020-06-25 12:40:16 PM
RESULT,RNN_TRANSLATOR,,312,nvidia,2020-06-25 12:40:16 PM
ENDING TIMING RUN AT 2020-06-25 12:45:28 PM
RESULT,RNN_TRANSLATOR,,312,nvidia,2020-06-25 12:40:16 PM
ENDING TIMING RUN AT 2020-06-25 12:45:28 PM
RESULT,RNN_TRANSLATOR,,312,nvidia,2020-06-25 12:40:16 PM
ENDING TIMING RUN AT 2020-06-25 12:45:28 PM
RESULT,RNN_TRANSLATOR,,312,nvidia,2020-06-25 12:40:16 PM
ENDING TIMING RUN AT 2020-06-25 12:45:28 PM
RESULT,RNN_TRANSLATOR,,312,nvidia,2020-06-25 12:40:16 PM
