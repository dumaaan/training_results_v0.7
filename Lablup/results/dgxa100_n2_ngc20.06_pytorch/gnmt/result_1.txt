+ echo 'Beginning trial 2 of 5'
Beginning trial 2 of 5
+ srun --ntasks=2 --container-name=rnn_translator python -c '
import mlperf_log_utils
from mlperf_logging.mllog import constants
mlperf_log_utils.mlperf_submission_log(constants.GNMT)'
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
:::MLLOG {"namespace": "", "time_ms": 1593113051758, "event_type": "POINT_IN_TIME", "key": "submission_benchmark", "value": "gnmt", "metadata": {"file": "/workspace/rnn_translator/mlperf_log_utils.py", "lineno": 19}}
:::MLLOG {"namespace": "", "time_ms": 1593113051797, "event_type": "POINT_IN_TIME", "key": "submission_org", "value": "NVIDIA", "metadata": {"file": "/workspace/rnn_translator/mlperf_log_utils.py", "lineno": 24}}
:::MLLOG {"namespace": "", "time_ms": 1593113051797, "event_type": "POINT_IN_TIME", "key": "submission_division", "value": "closed", "metadata": {"file": "/workspace/rnn_translator/mlperf_log_utils.py", "lineno": 28}}
:::MLLOG {"namespace": "", "time_ms": 1593113051797, "event_type": "POINT_IN_TIME", "key": "submission_status", "value": "onprem", "metadata": {"file": "/workspace/rnn_translator/mlperf_log_utils.py", "lineno": 32}}
:::MLLOG {"namespace": "", "time_ms": 1593113051797, "event_type": "POINT_IN_TIME", "key": "submission_platform", "value": "2xNVIDIA DGX A100", "metadata": {"file": "/workspace/rnn_translator/mlperf_log_utils.py", "lineno": 36}}
+ '[' 1 -eq 1 ']'
+ srun --ntasks=2 bash -c 'echo -n '\''Clearing cache on '\'' && hostname && sync && sudo /sbin/sysctl vm.drop_caches=3'
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
Clearing cache on luna-0100
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
Clearing cache on luna-0099
vm.drop_caches = 3
vm.drop_caches = 3
+ srun --ntasks=2 --container-name=rnn_translator python -c '
from mlperf_logging.mllog import constants
from seq2seq.utils import log_event
log_event(key=constants.CACHE_CLEAR, value=True)'
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
:::MLLOG {"namespace": "", "time_ms": 1593113057320, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
:::MLLOG {"namespace": "", "time_ms": 1593113057329, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
+ srun --mpi=none --ntasks=16 --ntasks-per-node=8 --container-name=rnn_translator --container-mounts=/lustre/fsr/datasets/wmt16_de_en:/data,/raid/scratch/svcnvdlfw/gnmt:/preproc_data,/lustre/fsw/mlperf-ci/14251651/results:/results ./run_and_time.sh
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
STARTING TIMING RUN AT 2020-06-25 12:24:19 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=2.875e-3
+ TRAIN_BATCH_SIZE=192
+ TEST_BATCH_SIZE=64
+ WARMUP_STEPS=200
+ REMAIN_STEPS=4054
+ DECAY_INTERVAL=506
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=8
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 3    --dwu-num-chunks 2    --dwu-num-rs-pg 1    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-overlap-reductions    --dwu-grad-norm    '
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 0 ']'
+ '[' 16 -gt 2 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGXA100_multi_2x8x192_dist == *\D\G\X\A\1\0\0* ]]
Using TCMalloc
+ echo 'Using TCMalloc'
+ export LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
running benchmark
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 192 --test-batch-size 64 --optimizer FusedAdam --lr 2.875e-3 --warmup-steps 200 --remain-steps 4054 --decay-interval 506 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
STARTING TIMING RUN AT 2020-06-25 12:24:19 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=2.875e-3
+ TRAIN_BATCH_SIZE=192
+ TEST_BATCH_SIZE=64
+ WARMUP_STEPS=200
+ REMAIN_STEPS=4054
+ DECAY_INTERVAL=506
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=8
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 3    --dwu-num-chunks 2    --dwu-num-rs-pg 1    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-overlap-reductions    --dwu-grad-norm    '
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 6 ']'
+ '[' 16 -gt 2 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGXA100_multi_2x8x192_dist == *\D\G\X\A\1\0\0* ]]
Using TCMalloc
+ echo 'Using TCMalloc'
+ export LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
running benchmark
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 192 --test-batch-size 64 --optimizer FusedAdam --lr 2.875e-3 --warmup-steps 200 --remain-steps 4054 --decay-interval 506 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
STARTING TIMING RUN AT 2020-06-25 12:24:19 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=2.875e-3
+ TRAIN_BATCH_SIZE=192
+ TEST_BATCH_SIZE=64
+ WARMUP_STEPS=200
+ REMAIN_STEPS=4054
+ DECAY_INTERVAL=506
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=8
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 3    --dwu-num-chunks 2    --dwu-num-rs-pg 1    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-overlap-reductions    --dwu-grad-norm    '
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 3 ']'
+ '[' 16 -gt 2 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGXA100_multi_2x8x192_dist == *\D\G\X\A\1\0\0* ]]
Using TCMalloc
+ echo 'Using TCMalloc'
+ export LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ echo 'running benchmark'
running benchmark
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 192 --test-batch-size 64 --optimizer FusedAdam --lr 2.875e-3 --warmup-steps 200 --remain-steps 4054 --decay-interval 506 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
STARTING TIMING RUN AT 2020-06-25 12:24:19 PM
STARTING TIMING RUN AT 2020-06-25 12:24:19 PM
+ DATASET_DIR=/data
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=2.875e-3
+ TRAIN_BATCH_SIZE=192
+ LR=2.875e-3
+ TRAIN_BATCH_SIZE=192
+ TEST_BATCH_SIZE=64
+ WARMUP_STEPS=200
+ TEST_BATCH_SIZE=64
+ REMAIN_STEPS=4054
+ DECAY_INTERVAL=506
+ WARMUP_STEPS=200
+ TARGET=24.0
+ REMAIN_STEPS=4054
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=8
+ DECAY_INTERVAL=506
+ TARGET=24.0
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 3    --dwu-num-chunks 2    --dwu-num-rs-pg 1    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-overlap-reductions    --dwu-grad-norm    '
+ MAX_SEQ_LEN=75
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    '
+ MATH=fp16
+ declare -a CMD
+ NUMEPOCHS=8
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 3    --dwu-num-chunks 2    --dwu-num-rs-pg 1    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-overlap-reductions    --dwu-grad-norm    '
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 4 ']'
+ '[' 16 -gt 2 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGXA100_multi_2x8x192_dist == *\D\G\X\A\1\0\0* ]]
Using TCMalloc
+ echo 'Using TCMalloc'
+ export LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ '[' -n 1 ']'
+ LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ '[' 16 -gt 2 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
running benchmark
+ echo 'running benchmark'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGXA100_multi_2x8x192_dist == *\D\G\X\A\1\0\0* ]]
Using TCMalloc
+ echo 'Using TCMalloc'
+ export LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
running benchmark
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 192 --test-batch-size 64 --optimizer FusedAdam --lr 2.875e-3 --warmup-steps 200 --remain-steps 4054 --decay-interval 506 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 192 --test-batch-size 64 --optimizer FusedAdam --lr 2.875e-3 --warmup-steps 200 --remain-steps 4054 --decay-interval 506 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
STARTING TIMING RUN AT 2020-06-25 12:24:19 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=2.875e-3
+ TRAIN_BATCH_SIZE=192
STARTING TIMING RUN AT 2020-06-25 12:24:19 PM
+ TEST_BATCH_SIZE=64
+ WARMUP_STEPS=200
+ REMAIN_STEPS=4054
+ DECAY_INTERVAL=506
+ TARGET=24.0
+ DATASET_DIR=/data
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=8
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 3    --dwu-num-chunks 2    --dwu-num-rs-pg 1    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-overlap-reductions    --dwu-grad-norm    '
+ LR=2.875e-3
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    '
+ TRAIN_BATCH_SIZE=192
+ MATH=fp16
+ TEST_BATCH_SIZE=64
+ declare -a CMD
+ WARMUP_STEPS=200
+ REMAIN_STEPS=4054
+ DECAY_INTERVAL=506
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=8
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 3    --dwu-num-chunks 2    --dwu-num-rs-pg 1    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-overlap-reductions    --dwu-grad-norm    '
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    '
+ '[' -n 2 ']'
+ MATH=fp16
+ declare -a CMD
+ '[' 16 -gt 2 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGXA100_multi_2x8x192_dist == *\D\G\X\A\1\0\0* ]]
+ '[' -n 5 ']'
Using TCMalloc
+ echo 'Using TCMalloc'
+ export LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ '[' 16 -gt 2 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
running benchmark
+ echo 'running benchmark'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGXA100_multi_2x8x192_dist == *\D\G\X\A\1\0\0* ]]
+ echo 'Using TCMalloc'
Using TCMalloc
+ export LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ echo 'running benchmark'
running benchmark
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 192 --test-batch-size 64 --optimizer FusedAdam --lr 2.875e-3 --warmup-steps 200 --remain-steps 4054 --decay-interval 506 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 192 --test-batch-size 64 --optimizer FusedAdam --lr 2.875e-3 --warmup-steps 200 --remain-steps 4054 --decay-interval 506 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
STARTING TIMING RUN AT 2020-06-25 12:24:19 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=2.875e-3
+ TRAIN_BATCH_SIZE=192
+ TEST_BATCH_SIZE=64
+ WARMUP_STEPS=200
+ REMAIN_STEPS=4054
+ DECAY_INTERVAL=506
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=8
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 3    --dwu-num-chunks 2    --dwu-num-rs-pg 1    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-overlap-reductions    --dwu-grad-norm    '
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 7 ']'
+ '[' 16 -gt 2 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGXA100_multi_2x8x192_dist == *\D\G\X\A\1\0\0* ]]
Using TCMalloc
+ echo 'Using TCMalloc'
+ export LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
running benchmark
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 192 --test-batch-size 64 --optimizer FusedAdam --lr 2.875e-3 --warmup-steps 200 --remain-steps 4054 --decay-interval 506 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
STARTING TIMING RUN AT 2020-06-25 12:24:19 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=2.875e-3
+ TRAIN_BATCH_SIZE=192
+ TEST_BATCH_SIZE=64
+ WARMUP_STEPS=200
+ REMAIN_STEPS=4054
+ DECAY_INTERVAL=506
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=8
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 3    --dwu-num-chunks 2    --dwu-num-rs-pg 1    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-overlap-reductions    --dwu-grad-norm    '
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 3 ']'
+ '[' 16 -gt 2 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
Using TCMalloc
running benchmark
+ [[ DGXA100_multi_2x8x192_dist == *\D\G\X\A\1\0\0* ]]
+ echo 'Using TCMalloc'
+ export LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 192 --test-batch-size 64 --optimizer FusedAdam --lr 2.875e-3 --warmup-steps 200 --remain-steps 4054 --decay-interval 506 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
STARTING TIMING RUN AT 2020-06-25 12:24:19 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=2.875e-3
+ TRAIN_BATCH_SIZE=192
+ TEST_BATCH_SIZE=64
+ WARMUP_STEPS=200
+ REMAIN_STEPS=4054
+ DECAY_INTERVAL=506
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=8
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 3    --dwu-num-chunks 2    --dwu-num-rs-pg 1    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-overlap-reductions    --dwu-grad-norm    '
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 0 ']'
+ '[' 16 -gt 2 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGXA100_multi_2x8x192_dist == *\D\G\X\A\1\0\0* ]]
+ echo 'Using TCMalloc'
Using TCMalloc
+ export LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ echo 'running benchmark'
running benchmark
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 192 --test-batch-size 64 --optimizer FusedAdam --lr 2.875e-3 --warmup-steps 200 --remain-steps 4054 --decay-interval 506 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
STARTING TIMING RUN AT 2020-06-25 12:24:19 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=2.875e-3
+ TRAIN_BATCH_SIZE=192
+ TEST_BATCH_SIZE=64
+ WARMUP_STEPS=200
+ REMAIN_STEPS=4054
+ DECAY_INTERVAL=506
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=8
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 3    --dwu-num-chunks 2    --dwu-num-rs-pg 1    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-overlap-reductions    --dwu-grad-norm    '
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 7 ']'
+ '[' 16 -gt 2 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGXA100_multi_2x8x192_dist == *\D\G\X\A\1\0\0* ]]
Using TCMalloc
+ echo 'Using TCMalloc'
+ export LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ echo 'running benchmark'
running benchmark
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 192 --test-batch-size 64 --optimizer FusedAdam --lr 2.875e-3 --warmup-steps 200 --remain-steps 4054 --decay-interval 506 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
STARTING TIMING RUN AT 2020-06-25 12:24:19 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=2.875e-3
+ TRAIN_BATCH_SIZE=192
+ TEST_BATCH_SIZE=64
+ WARMUP_STEPS=200
+ REMAIN_STEPS=4054
+ DECAY_INTERVAL=506
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=8
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 3    --dwu-num-chunks 2    --dwu-num-rs-pg 1    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-overlap-reductions    --dwu-grad-norm    '
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 6 ']'
+ '[' 16 -gt 2 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGXA100_multi_2x8x192_dist == *\D\G\X\A\1\0\0* ]]
Using TCMalloc
+ echo 'Using TCMalloc'
+ export LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
running benchmark
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 192 --test-batch-size 64 --optimizer FusedAdam --lr 2.875e-3 --warmup-steps 200 --remain-steps 4054 --decay-interval 506 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
STARTING TIMING RUN AT 2020-06-25 12:24:19 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=2.875e-3
+ TRAIN_BATCH_SIZE=192
+ TEST_BATCH_SIZE=64
+ WARMUP_STEPS=200
+ REMAIN_STEPS=4054
+ DECAY_INTERVAL=506
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=8
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 3    --dwu-num-chunks 2    --dwu-num-rs-pg 1    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-overlap-reductions    --dwu-grad-norm    '
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 2 ']'
+ '[' 16 -gt 2 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGXA100_multi_2x8x192_dist == *\D\G\X\A\1\0\0* ]]
+ echo 'Using TCMalloc'
Using TCMalloc
+ export LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ echo 'running benchmark'
running benchmark
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 192 --test-batch-size 64 --optimizer FusedAdam --lr 2.875e-3 --warmup-steps 200 --remain-steps 4054 --decay-interval 506 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
STARTING TIMING RUN AT 2020-06-25 12:24:19 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=2.875e-3
+ TRAIN_BATCH_SIZE=192
+ TEST_BATCH_SIZE=64
+ WARMUP_STEPS=200
+ REMAIN_STEPS=4054
+ DECAY_INTERVAL=506
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=8
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 3    --dwu-num-chunks 2    --dwu-num-rs-pg 1    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-overlap-reductions    --dwu-grad-norm    '
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 1 ']'
+ '[' 16 -gt 2 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGXA100_multi_2x8x192_dist == *\D\G\X\A\1\0\0* ]]
Using TCMalloc
+ echo 'Using TCMalloc'
+ export LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
running benchmark
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 192 --test-batch-size 64 --optimizer FusedAdam --lr 2.875e-3 --warmup-steps 200 --remain-steps 4054 --decay-interval 506 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
STARTING TIMING RUN AT 2020-06-25 12:24:19 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=2.875e-3
+ TRAIN_BATCH_SIZE=192
+ TEST_BATCH_SIZE=64
+ WARMUP_STEPS=200
+ REMAIN_STEPS=4054
+ DECAY_INTERVAL=506
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=8
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 3    --dwu-num-chunks 2    --dwu-num-rs-pg 1    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-overlap-reductions    --dwu-grad-norm    '
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    '
+ MATH=fp16
+ declare -a CMD
STARTING TIMING RUN AT 2020-06-25 12:24:19 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=2.875e-3
+ '[' -n 4 ']'
+ TRAIN_BATCH_SIZE=192
+ TEST_BATCH_SIZE=64
+ '[' 16 -gt 2 ']'
+ WARMUP_STEPS=200
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ REMAIN_STEPS=4054
+ DECAY_INTERVAL=506
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=8
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 3    --dwu-num-chunks 2    --dwu-num-rs-pg 1    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-overlap-reductions    --dwu-grad-norm    '
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    '
+ MATH=fp16
+ [[ DGXA100_multi_2x8x192_dist == *\D\G\X\A\1\0\0* ]]
+ echo 'Using TCMalloc'
+ declare -a CMD
Using TCMalloc
+ export LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
running benchmark
+ echo 'running benchmark'
+ '[' -n 5 ']'
+ '[' 16 -gt 2 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGXA100_multi_2x8x192_dist == *\D\G\X\A\1\0\0* ]]
Using TCMalloc
+ echo 'Using TCMalloc'
+ export LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
running benchmark
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 192 --test-batch-size 64 --optimizer FusedAdam --lr 2.875e-3 --warmup-steps 200 --remain-steps 4054 --decay-interval 506 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 192 --test-batch-size 64 --optimizer FusedAdam --lr 2.875e-3 --warmup-steps 200 --remain-steps 4054 --decay-interval 506 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
num_sockets = 2 num_nodes=8 cores_per_socket=64
+ exec numactl --physcpubind=96-111,224-239 --membind=6 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 192 --test-batch-size 64 --optimizer FusedAdam --lr 2.875e-3 --warmup-steps 200 --remain-steps 4054 --decay-interval 506 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
num_sockets = 2 num_nodes=8 cores_per_socket=64
+ exec numactl --physcpubind=48-63,176-191 --membind=3 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 192 --test-batch-size 64 --optimizer FusedAdam --lr 2.875e-3 --warmup-steps 200 --remain-steps 4054 --decay-interval 506 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
num_sockets = 2 num_nodes=8 cores_per_socket=64
+ exec numactl --physcpubind=48-63,176-191 --membind=3 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 192 --test-batch-size 64 --optimizer FusedAdam --lr 2.875e-3 --warmup-steps 200 --remain-steps 4054 --decay-interval 506 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
num_sockets = 2 num_nodes=8 cores_per_socket=64
num_sockets = 2 num_nodes=8 cores_per_socket=64
+ exec numactl --physcpubind=112-127,240-255 --membind=7 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 192 --test-batch-size 64 --optimizer FusedAdam --lr 2.875e-3 --warmup-steps 200 --remain-steps 4054 --decay-interval 506 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
+ exec numactl --physcpubind=16-31,144-159 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 192 --test-batch-size 64 --optimizer FusedAdam --lr 2.875e-3 --warmup-steps 200 --remain-steps 4054 --decay-interval 506 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
num_sockets = 2 num_nodes=8 cores_per_socket=64
+ exec numactl --physcpubind=112-127,240-255 --membind=7 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 192 --test-batch-size 64 --optimizer FusedAdam --lr 2.875e-3 --warmup-steps 200 --remain-steps 4054 --decay-interval 506 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
num_sockets = 2 num_nodes=8 cores_per_socket=64
num_sockets = 2 num_nodes=8 cores_per_socket=64
num_sockets = 2 num_nodes=8 cores_per_socket=64
num_sockets = 2 num_nodes=8 cores_per_socket=64
+ exec numactl --physcpubind=64-79,192-207 --membind=4 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 192 --test-batch-size 64 --optimizer FusedAdam --lr 2.875e-3 --warmup-steps 200 --remain-steps 4054 --decay-interval 506 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
+ exec numactl --physcpubind=80-95,208-223 --membind=5 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 192 --test-batch-size 64 --optimizer FusedAdam --lr 2.875e-3 --warmup-steps 200 --remain-steps 4054 --decay-interval 506 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
+ exec numactl --physcpubind=0-15,128-143 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 192 --test-batch-size 64 --optimizer FusedAdam --lr 2.875e-3 --warmup-steps 200 --remain-steps 4054 --decay-interval 506 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
+ exec numactl --physcpubind=32-47,160-175 --membind=2 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 192 --test-batch-size 64 --optimizer FusedAdam --lr 2.875e-3 --warmup-steps 200 --remain-steps 4054 --decay-interval 506 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
num_sockets = 2 num_nodes=8 cores_per_socket=64
+ exec numactl --physcpubind=32-47,160-175 --membind=2 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 192 --test-batch-size 64 --optimizer FusedAdam --lr 2.875e-3 --warmup-steps 200 --remain-steps 4054 --decay-interval 506 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
num_sockets = 2 num_nodes=8 cores_per_socket=64
+ exec numactl --physcpubind=96-111,224-239 --membind=6 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 192 --test-batch-size 64 --optimizer FusedAdam --lr 2.875e-3 --warmup-steps 200 --remain-steps 4054 --decay-interval 506 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
num_sockets = 2 num_nodes=8 cores_per_socket=64
num_sockets = 2 num_nodes=8 cores_per_socket=64
num_sockets = 2 num_nodes=8 cores_per_socket=64
num_sockets = 2 num_nodes=8 cores_per_socket=64
+ exec numactl --physcpubind=64-79,192-207 --membind=4 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 192 --test-batch-size 64 --optimizer FusedAdam --lr 2.875e-3 --warmup-steps 200 --remain-steps 4054 --decay-interval 506 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
+ exec numactl --physcpubind=16-31,144-159 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 192 --test-batch-size 64 --optimizer FusedAdam --lr 2.875e-3 --warmup-steps 200 --remain-steps 4054 --decay-interval 506 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
+ exec numactl --physcpubind=80-95,208-223 --membind=5 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 192 --test-batch-size 64 --optimizer FusedAdam --lr 2.875e-3 --warmup-steps 200 --remain-steps 4054 --decay-interval 506 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
+ exec numactl --physcpubind=0-15,128-143 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 192 --test-batch-size 64 --optimizer FusedAdam --lr 2.875e-3 --warmup-steps 200 --remain-steps 4054 --decay-interval 506 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
:::MLLOG {"namespace": "", "time_ms": 1593113061300, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593113061364, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593113061408, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593113061457, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593113061469, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593113061479, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593113061485, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593113061491, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593113061497, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593113061517, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593113061642, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593113061666, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593113061677, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593113061701, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593113061708, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593113061719, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
NCCL version 2.7.5+cuda11.0
0: Saving results to: results/gnmt_wmt16
0: Run arguments: Namespace(apex_message_size=10000000.0, apex_num_allreduce_streams=1, batching='bucketing', beam_size=5, cov_penalty_factor=0.1, cuda=True, cudnn=True, dataset_dir='/data', decay_factor=0.5, decay_interval=506, decay_steps=4, distributed_weight_update=2, dropout=0.2, dwu_e5m2_allgather=False, dwu_full_pipeline=False, dwu_grad_norm=True, dwu_group_size=0, dwu_num_ag_pg=0, dwu_num_ar_pg=2, dwu_num_blocks=3, dwu_num_chunks=2, dwu_num_rs_pg=1, dwu_overlap_reductions=True, enable_apex_allreduce_overlap=False, env=False, epochs=8, eval=True, fused_attention=True, fused_xentropy=True, grad_clip=5.0, hidden_size=1024, init_scale=1024, intra_epoch_eval=0, keep_checkpoints=0, len_norm_const=5.0, len_norm_factor=0.6, local_rank=0, log_all_ranks=False, lr=0.002875, math='fp16', max_length_test=150, max_length_train=75, max_size=None, min_length_test=0, min_length_train=0, num_buckets=5, num_layers=4, optimizer='FusedAdam', optimizer_extra='{}', prealloc_mode='once', preproc_data_dir='/preproc_data/75', print_freq=10, rank=0, remain_steps=4054, results_dir='results', resume=None, save='gnmt_wmt16', save_all=False, save_freq=5000, save_path='results/gnmt_wmt16', seed=None, shard_size=80, share_embedding=True, smoothing=0.1, start_epoch=0, target_bleu=24.0, test_batch_size=64, test_loader_workers=0, train_batch_size=192, train_global_batch_size=None, train_iter_size=1, train_loader_workers=1, upscale_interval=128, use_preproc_data=True, warmup_steps=200)
0: L2 promotion: 128B
0: Using random master seed: 2849060626
:::MLLOG {"namespace": "", "time_ms": 1593113071370, "event_type": "POINT_IN_TIME", "key": "seed", "value": 2849060626, "metadata": {"file": "/workspace/rnn_translator/seq2seq/utils.py", "lineno": 144}}
0: Worker 0 is using worker seed: 2644998420
0: Building vocabulary from /data/vocab.bpe.32000
0: Size of vocabulary: 32320
0: Fused attention flag set to True
0: GNMT(
  (embedder): Embedding(32320, 1024, padding_idx=0)
  (encoder): ResidualRecurrentEncoder(
    (rnn_layers): ModuleList(
      (0): EmuBidirLSTM(
        (bidir): LSTM(1024, 1024, bidirectional=True)
        (layer1): LSTM(1024, 1024)
        (layer2): LSTM(1024, 1024)
      )
      (1): LSTM(2048, 1024)
      (2): LSTM(1024, 1024)
      (3): LSTM(1024, 1024)
    )
    (dropout): Dropout(p=0.2, inplace=False)
    (embedder): Embedding(32320, 1024, padding_idx=0)
  )
  (decoder): ResidualRecurrentDecoder(
    (att_rnn): RecurrentAttention(
      (rnn): LSTM(1024, 1024)
      (attn): BahdanauAttention(
        (linear_q): Linear(in_features=1024, out_features=1024, bias=False)
        (linear_k): Linear(in_features=1024, out_features=1024, bias=False)
      )
      (dropout): Dropout(p=0.2, inplace=False)
    )
    (rnn_layers): ModuleList(
      (0): LSTM(2048, 1024)
      (1): LSTM(2048, 1024)
      (2): LSTM(2048, 1024)
    )
    (embedder): Embedding(32320, 1024, padding_idx=0)
    (classifier): Classifier(
      (classifier): Linear(in_features=1024, out_features=32320, bias=True)
    )
    (dropout): Dropout(p=0.2, inplace=False)
  )
)
0: Building LabelSmoothingLoss (smoothing: 0.1)
0: Fused xentropy flag set to True
0: Training optimizer config: {'optimizer': 'FusedAdam', 'lr': 0.002875}
0: Number of parameters: 160671297
0: Saving state of the tokenizer
self._net_total_param_size=160671360, self._total_param_size=160677888, dwu_min_page_size=12288, self._block_size=53559296, self._chunk_size=26779648, self._shard_size=3347456
[0, 9, 34]
self._net_total_param_size=160671360, self._total_param_size=160677888, dwu_min_page_size=12288, self._block_size=53559296, self._chunk_size=26779648, self._shard_size=3347456
[0, 9, 34]
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([1314816]), new_param_packed_fragment.size()=torch.Size([1314816]), master_param_fragment.size()=torch.Size([1314816])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([2024448]), new_param_packed_fragment.size()=torch.Size([2024448]), master_param_fragment.size()=torch.Size([2024448])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([2832448]), new_param_packed_fragment.size()=torch.Size([2832448]), master_param_fragment.size()=torch.Size([2832448])
model_param_fragment.size()=torch.Size([515008]), new_param_packed_fragment.size()=torch.Size([515008]), master_param_fragment.size()=torch.Size([515008])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
self._net_total_param_size=160671360, self._total_param_size=160677888, dwu_min_page_size=12288, self._block_size=53559296, self._chunk_size=26779648, self._shard_size=3347456
[0, 9, 34]
self._net_total_param_size=160671360, self._total_param_size=160677888, dwu_min_page_size=12288, self._block_size=53559296, self._chunk_size=26779648, self._shard_size=3347456
[0, 9, 34]
self._net_total_param_size=160671360, self._total_param_size=160677888, dwu_min_page_size=12288, self._block_size=53559296, self._chunk_size=26779648, self._shard_size=3347456
[0, 9, 34]
self._net_total_param_size=160671360, self._total_param_size=160677888, dwu_min_page_size=12288, self._block_size=53559296, self._chunk_size=26779648, self._shard_size=3347456
[0, 9, 34]
self._net_total_param_size=160671360, self._total_param_size=160677888, dwu_min_page_size=12288, self._block_size=53559296, self._chunk_size=26779648, self._shard_size=3347456
[0, 9, 34]
self._net_total_param_size=160671360, self._total_param_size=160677888, dwu_min_page_size=12288, self._block_size=53559296, self._chunk_size=26779648, self._shard_size=3347456
[0, 9, 34]
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3016704]), new_param_packed_fragment.size()=torch.Size([3016704]), master_param_fragment.size()=torch.Size([3016704])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([322560]), new_param_packed_fragment.size()=torch.Size([322560]), master_param_fragment.size()=torch.Size([322560])
model_param_fragment.size()=torch.Size([1427456]), new_param_packed_fragment.size()=torch.Size([1427456]), master_param_fragment.size()=torch.Size([1427456])
model_param_fragment.size()=torch.Size([1920000]), new_param_packed_fragment.size()=torch.Size([1920000]), master_param_fragment.size()=torch.Size([1920000])
model_param_fragment.size()=torch.Size([1929280]), new_param_packed_fragment.size()=torch.Size([1929280]), master_param_fragment.size()=torch.Size([1929280])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([1409984]), new_param_packed_fragment.size()=torch.Size([1409984]), master_param_fragment.size()=torch.Size([1409984])
model_param_fragment.size()=torch.Size([331840]), new_param_packed_fragment.size()=torch.Size([331840]), master_param_fragment.size()=torch.Size([331840])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([3007424]), new_param_packed_fragment.size()=torch.Size([3007424]), master_param_fragment.size()=torch.Size([3007424])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
self._net_total_param_size=160671360, self._total_param_size=160677888, dwu_min_page_size=12288, self._block_size=53559296, self._chunk_size=26779648, self._shard_size=3347456
[0, 9, 34]
0: Initializing dwu fp16 optimizer
self._net_total_param_size=160671360, self._total_param_size=160677888, dwu_min_page_size=12288, self._block_size=53559296, self._chunk_size=26779648, self._shard_size=3347456
[0, 9, 34]
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([2274304]), new_param_packed_fragment.size()=torch.Size([2274304]), master_param_fragment.size()=torch.Size([2274304])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([1064960]), new_param_packed_fragment.size()=torch.Size([1064960]), master_param_fragment.size()=torch.Size([1064960])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([467968]), new_param_packed_fragment.size()=torch.Size([467968]), master_param_fragment.size()=torch.Size([467968])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([2879488]), new_param_packed_fragment.size()=torch.Size([2879488]), master_param_fragment.size()=torch.Size([2879488])
model_param_fragment.size()=torch.Size([3308608]), new_param_packed_fragment.size()=torch.Size([3308608]), master_param_fragment.size()=torch.Size([3308608])
model_param_fragment.size()=torch.Size([32320]), new_param_packed_fragment.size()=torch.Size([32320]), master_param_fragment.size()=torch.Size([32320])
model_param_fragment.size()=torch.Size([3073024]), new_param_packed_fragment.size()=torch.Size([3073024]), master_param_fragment.size()=torch.Size([3073024])
model_param_fragment.size()=torch.Size([274432]), new_param_packed_fragment.size()=torch.Size([274432]), master_param_fragment.size()=torch.Size([274432])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([2968576]), new_param_packed_fragment.size()=torch.Size([2968576]), master_param_fragment.size()=torch.Size([2968576])
model_param_fragment.size()=torch.Size([378880]), new_param_packed_fragment.size()=torch.Size([378880]), master_param_fragment.size()=torch.Size([378880])
model_param_fragment.size()=torch.Size([1371136]), new_param_packed_fragment.size()=torch.Size([1371136]), master_param_fragment.size()=torch.Size([1371136])
model_param_fragment.size()=torch.Size([1976320]), new_param_packed_fragment.size()=torch.Size([1976320]), master_param_fragment.size()=torch.Size([1976320])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([283712]), new_param_packed_fragment.size()=torch.Size([283712]), master_param_fragment.size()=torch.Size([283712])
model_param_fragment.size()=torch.Size([3063744]), new_param_packed_fragment.size()=torch.Size([3063744]), master_param_fragment.size()=torch.Size([3063744])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([2217984]), new_param_packed_fragment.size()=torch.Size([2217984]), master_param_fragment.size()=torch.Size([2217984])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
NCCL version 2.7.5+cuda11.0
model_param_fragment.size()=torch.Size([1121280]), new_param_packed_fragment.size()=torch.Size([1121280]), master_param_fragment.size()=torch.Size([1121280])
model_param_fragment.size()=torch.Size([628736]), new_param_packed_fragment.size()=torch.Size([628736]), master_param_fragment.size()=torch.Size([628736])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([1024]), new_param_packed_fragment.size()=torch.Size([1024]), master_param_fragment.size()=torch.Size([1024])
model_param_fragment.size()=torch.Size([1]), new_param_packed_fragment.size()=torch.Size([1]), master_param_fragment.size()=torch.Size([1])
model_param_fragment.size()=torch.Size([1024]), new_param_packed_fragment.size()=torch.Size([1024]), master_param_fragment.size()=torch.Size([1024])
model_param_fragment.size()=torch.Size([1048576]), new_param_packed_fragment.size()=torch.Size([1048576]), master_param_fragment.size()=torch.Size([1048576])
model_param_fragment.size()=torch.Size([1048576]), new_param_packed_fragment.size()=torch.Size([1048576]), master_param_fragment.size()=torch.Size([1048576])
model_param_fragment.size()=torch.Size([611264]), new_param_packed_fragment.size()=torch.Size([611264]), master_param_fragment.size()=torch.Size([611264])
model_param_fragment.size()=torch.Size([1130560]), new_param_packed_fragment.size()=torch.Size([1130560]), master_param_fragment.size()=torch.Size([1130560])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([2208704]), new_param_packed_fragment.size()=torch.Size([2208704]), master_param_fragment.size()=torch.Size([2208704])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([467968]), new_param_packed_fragment.size()=torch.Size([467968]), master_param_fragment.size()=torch.Size([467968])
model_param_fragment.size()=torch.Size([2879488]), new_param_packed_fragment.size()=torch.Size([2879488]), master_param_fragment.size()=torch.Size([2879488])
model_param_fragment.size()=torch.Size([3073024]), new_param_packed_fragment.size()=torch.Size([3073024]), master_param_fragment.size()=torch.Size([3073024])
model_param_fragment.size()=torch.Size([274432]), new_param_packed_fragment.size()=torch.Size([274432]), master_param_fragment.size()=torch.Size([274432])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3129344]), new_param_packed_fragment.size()=torch.Size([3129344]), master_param_fragment.size()=torch.Size([3129344])
model_param_fragment.size()=torch.Size([218112]), new_param_packed_fragment.size()=torch.Size([218112]), master_param_fragment.size()=torch.Size([218112])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
self._net_total_param_size=160671360, self._total_param_size=160677888, dwu_min_page_size=12288, self._block_size=53559296, self._chunk_size=26779648, self._shard_size=3347456
[0, 9, 34]
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([2217984]), new_param_packed_fragment.size()=torch.Size([2217984]), master_param_fragment.size()=torch.Size([2217984])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([1121280]), new_param_packed_fragment.size()=torch.Size([1121280]), master_param_fragment.size()=torch.Size([1121280])
model_param_fragment.size()=torch.Size([628736]), new_param_packed_fragment.size()=torch.Size([628736]), master_param_fragment.size()=torch.Size([628736])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([1024]), new_param_packed_fragment.size()=torch.Size([1024]), master_param_fragment.size()=torch.Size([1024])
model_param_fragment.size()=torch.Size([1]), new_param_packed_fragment.size()=torch.Size([1]), master_param_fragment.size()=torch.Size([1])
model_param_fragment.size()=torch.Size([1024]), new_param_packed_fragment.size()=torch.Size([1024]), master_param_fragment.size()=torch.Size([1024])
model_param_fragment.size()=torch.Size([1048576]), new_param_packed_fragment.size()=torch.Size([1048576]), master_param_fragment.size()=torch.Size([1048576])
model_param_fragment.size()=torch.Size([1048576]), new_param_packed_fragment.size()=torch.Size([1048576]), master_param_fragment.size()=torch.Size([1048576])
model_param_fragment.size()=torch.Size([611264]), new_param_packed_fragment.size()=torch.Size([611264]), master_param_fragment.size()=torch.Size([611264])
model_param_fragment.size()=torch.Size([1130560]), new_param_packed_fragment.size()=torch.Size([1130560]), master_param_fragment.size()=torch.Size([1130560])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([2208704]), new_param_packed_fragment.size()=torch.Size([2208704]), master_param_fragment.size()=torch.Size([2208704])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
NCCL version 2.7.5+cuda11.0
self._net_total_param_size=160671360, self._total_param_size=160677888, dwu_min_page_size=12288, self._block_size=53559296, self._chunk_size=26779648, self._shard_size=3347456
[0, 9, 34]
self._net_total_param_size=160671360, self._total_param_size=160677888, dwu_min_page_size=12288, self._block_size=53559296, self._chunk_size=26779648, self._shard_size=3347456
[0, 9, 34]
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3129344]), new_param_packed_fragment.size()=torch.Size([3129344]), master_param_fragment.size()=torch.Size([3129344])
model_param_fragment.size()=torch.Size([218112]), new_param_packed_fragment.size()=torch.Size([218112]), master_param_fragment.size()=torch.Size([218112])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
self._net_total_param_size=160671360, self._total_param_size=160677888, dwu_min_page_size=12288, self._block_size=53559296, self._chunk_size=26779648, self._shard_size=3347456
[0, 9, 34]
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([2169856]), new_param_packed_fragment.size()=torch.Size([2169856]), master_param_fragment.size()=torch.Size([2169856])
model_param_fragment.size()=torch.Size([1177600]), new_param_packed_fragment.size()=torch.Size([1177600]), master_param_fragment.size()=torch.Size([1177600])
model_param_fragment.size()=torch.Size([572416]), new_param_packed_fragment.size()=torch.Size([572416]), master_param_fragment.size()=torch.Size([572416])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([2766848]), new_param_packed_fragment.size()=torch.Size([2766848]), master_param_fragment.size()=torch.Size([2766848])
model_param_fragment.size()=torch.Size([1082432]), new_param_packed_fragment.size()=torch.Size([1082432]), master_param_fragment.size()=torch.Size([1082432])
model_param_fragment.size()=torch.Size([2265024]), new_param_packed_fragment.size()=torch.Size([2265024]), master_param_fragment.size()=torch.Size([2265024])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
NCCL version 2.7.5+cuda11.0
self._net_total_param_size=160671360, self._total_param_size=160677888, dwu_min_page_size=12288, self._block_size=53559296, self._chunk_size=26779648, self._shard_size=3347456
[0, 9, 34]
self._net_total_param_size=160671360, self._total_param_size=160677888, dwu_min_page_size=12288, self._block_size=53559296, self._chunk_size=26779648, self._shard_size=3347456
[0, 9, 34]
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3016704]), new_param_packed_fragment.size()=torch.Size([3016704]), master_param_fragment.size()=torch.Size([3016704])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([322560]), new_param_packed_fragment.size()=torch.Size([322560]), master_param_fragment.size()=torch.Size([322560])
model_param_fragment.size()=torch.Size([1427456]), new_param_packed_fragment.size()=torch.Size([1427456]), master_param_fragment.size()=torch.Size([1427456])
model_param_fragment.size()=torch.Size([1920000]), new_param_packed_fragment.size()=torch.Size([1920000]), master_param_fragment.size()=torch.Size([1920000])
model_param_fragment.size()=torch.Size([1929280]), new_param_packed_fragment.size()=torch.Size([1929280]), master_param_fragment.size()=torch.Size([1929280])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([1409984]), new_param_packed_fragment.size()=torch.Size([1409984]), master_param_fragment.size()=torch.Size([1409984])
model_param_fragment.size()=torch.Size([331840]), new_param_packed_fragment.size()=torch.Size([331840]), master_param_fragment.size()=torch.Size([331840])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([3007424]), new_param_packed_fragment.size()=torch.Size([3007424]), master_param_fragment.size()=torch.Size([3007424])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
NCCL version 2.7.5+cuda11.0
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([1314816]), new_param_packed_fragment.size()=torch.Size([1314816]), master_param_fragment.size()=torch.Size([1314816])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([2024448]), new_param_packed_fragment.size()=torch.Size([2024448]), master_param_fragment.size()=torch.Size([2024448])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([2832448]), new_param_packed_fragment.size()=torch.Size([2832448]), master_param_fragment.size()=torch.Size([2832448])
model_param_fragment.size()=torch.Size([515008]), new_param_packed_fragment.size()=torch.Size([515008]), master_param_fragment.size()=torch.Size([515008])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
NCCL version 2.7.5+cuda11.0
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([2968576]), new_param_packed_fragment.size()=torch.Size([2968576]), master_param_fragment.size()=torch.Size([2968576])
model_param_fragment.size()=torch.Size([378880]), new_param_packed_fragment.size()=torch.Size([378880]), master_param_fragment.size()=torch.Size([378880])
model_param_fragment.size()=torch.Size([1371136]), new_param_packed_fragment.size()=torch.Size([1371136]), master_param_fragment.size()=torch.Size([1371136])
model_param_fragment.size()=torch.Size([1976320]), new_param_packed_fragment.size()=torch.Size([1976320]), master_param_fragment.size()=torch.Size([1976320])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([283712]), new_param_packed_fragment.size()=torch.Size([283712]), master_param_fragment.size()=torch.Size([283712])
model_param_fragment.size()=torch.Size([3063744]), new_param_packed_fragment.size()=torch.Size([3063744]), master_param_fragment.size()=torch.Size([3063744])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
NCCL version 2.7.5+cuda11.0
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([2169856]), new_param_packed_fragment.size()=torch.Size([2169856]), master_param_fragment.size()=torch.Size([2169856])
model_param_fragment.size()=torch.Size([1177600]), new_param_packed_fragment.size()=torch.Size([1177600]), master_param_fragment.size()=torch.Size([1177600])
model_param_fragment.size()=torch.Size([572416]), new_param_packed_fragment.size()=torch.Size([572416]), master_param_fragment.size()=torch.Size([572416])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([2766848]), new_param_packed_fragment.size()=torch.Size([2766848]), master_param_fragment.size()=torch.Size([2766848])
model_param_fragment.size()=torch.Size([1082432]), new_param_packed_fragment.size()=torch.Size([1082432]), master_param_fragment.size()=torch.Size([1082432])
model_param_fragment.size()=torch.Size([2265024]), new_param_packed_fragment.size()=torch.Size([2265024]), master_param_fragment.size()=torch.Size([2265024])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([2274304]), new_param_packed_fragment.size()=torch.Size([2274304]), master_param_fragment.size()=torch.Size([2274304])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([1064960]), new_param_packed_fragment.size()=torch.Size([1064960]), master_param_fragment.size()=torch.Size([1064960])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3308608]), new_param_packed_fragment.size()=torch.Size([3308608]), master_param_fragment.size()=torch.Size([3308608])
model_param_fragment.size()=torch.Size([32320]), new_param_packed_fragment.size()=torch.Size([32320]), master_param_fragment.size()=torch.Size([32320])
NCCL version 2.7.5+cuda11.0
NCCL version 2.7.5+cuda11.0
0: Using optimizer: DistributedFusedAdam (
Parameter Group 0
    betas: (0.9, 0.999)
    bias_correction: True
    eps: 1e-08
    lr: 0.002875
    max_grad_norm: 5.0
    weight_decay: 0.0
)
:::MLLOG {"namespace": "", "time_ms": 1593113083273, "event_type": "POINT_IN_TIME", "key": "opt_name", "value": "adam", "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/trainer.py", "lineno": 176}}
:::MLLOG {"namespace": "", "time_ms": 1593113083273, "event_type": "POINT_IN_TIME", "key": "opt_base_learning_rate", "value": 0.002875, "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/trainer.py", "lineno": 178}}
:::MLLOG {"namespace": "", "time_ms": 1593113083273, "event_type": "POINT_IN_TIME", "key": "opt_adam_beta_1", "value": 0.9, "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/trainer.py", "lineno": 180}}
:::MLLOG {"namespace": "", "time_ms": 1593113083273, "event_type": "POINT_IN_TIME", "key": "opt_adam_beta_2", "value": 0.999, "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/trainer.py", "lineno": 182}}
:::MLLOG {"namespace": "", "time_ms": 1593113083273, "event_type": "POINT_IN_TIME", "key": "opt_adam_epsilon", "value": 1e-08, "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/trainer.py", "lineno": 184}}
0: Executing preallocation
:::MLLOG {"namespace": "", "time_ms": 1593113084727, "event_type": "INTERVAL_END", "key": "init_stop", "value": null, "metadata": {"file": "train.py", "lineno": 429}}
:::MLLOG {"namespace": "", "time_ms": 1593113084728, "event_type": "INTERVAL_START", "key": "run_start", "value": null, "metadata": {"file": "train.py", "lineno": 430}}
:::MLLOG {"namespace": "", "time_ms": 1593113084728, "event_type": "POINT_IN_TIME", "key": "max_sequence_length", "value": 75, "metadata": {"file": "train.py", "lineno": 435, "method": "discard"}}
0: Reading preprocessed data file from /preproc_data/75/training.bin
0: Preprocessed data: length: 3975116 min length: 0 max length: 75
0: Opening preprocessed data /preproc_data/75/training.bin for reading
0: Processing data from /data/newstest2014.tok.bpe.32000.en
0: Filtering data, min len: 0, max len: 150
0: Pairs before: 3003, after: 3003
:::MLLOG {"namespace": "", "time_ms": 1593113084969, "event_type": "POINT_IN_TIME", "key": "global_batch_size", "value": 3072, "metadata": {"file": "train.py", "lineno": 480}}
:::MLLOG {"namespace": "", "time_ms": 1593113084970, "event_type": "POINT_IN_TIME", "key": "train_samples", "value": 3965952, "metadata": {"file": "train.py", "lineno": 488}}
:::MLLOG {"namespace": "", "time_ms": 1593113084970, "event_type": "POINT_IN_TIME", "key": "eval_samples", "value": 3003, "metadata": {"file": "train.py", "lineno": 490}}
0: Training LR schedule config: {'warmup_steps': 200, 'remain_steps': 4054, 'decay_interval': 506, 'decay_steps': 4, 'decay_factor': 0.5}
0: Scheduler warmup steps: 200
0: Scheduler remain steps: 4054
0: Scheduler decay interval: 506
0: Scheduler decay factor: 0.5
0: Scheduler max decay steps: 4
:::MLLOG {"namespace": "", "time_ms": 1593113084970, "event_type": "POINT_IN_TIME", "key": "opt_learning_rate_alt_decay_func", "value": true, "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/lr_scheduler.py", "lineno": 78}}
:::MLLOG {"namespace": "", "time_ms": 1593113084970, "event_type": "POINT_IN_TIME", "key": "opt_learning_rate_alt_warmup_func", "value": true, "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/lr_scheduler.py", "lineno": 80}}
:::MLLOG {"namespace": "", "time_ms": 1593113084970, "event_type": "POINT_IN_TIME", "key": "opt_learning_rate_decay_interval", "value": 506, "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/lr_scheduler.py", "lineno": 83}}
:::MLLOG {"namespace": "", "time_ms": 1593113084971, "event_type": "POINT_IN_TIME", "key": "opt_learning_rate_decay_factor", "value": 0.5, "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/lr_scheduler.py", "lineno": 85}}
:::MLLOG {"namespace": "", "time_ms": 1593113084971, "event_type": "POINT_IN_TIME", "key": "opt_learning_rate_decay_steps", "value": 4, "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/lr_scheduler.py", "lineno": 87}}
:::MLLOG {"namespace": "", "time_ms": 1593113084971, "event_type": "POINT_IN_TIME", "key": "opt_learning_rate_remain_steps", "value": 4054, "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/lr_scheduler.py", "lineno": 89}}
:::MLLOG {"namespace": "", "time_ms": 1593113084971, "event_type": "POINT_IN_TIME", "key": "opt_learning_rate_warmup_steps", "value": 200, "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/lr_scheduler.py", "lineno": 91}}
:::MLLOG {"namespace": "", "time_ms": 1593113084971, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "train.py", "lineno": 538, "first_epoch_num": 1, "epoch_count": 1}}
:::MLLOG {"namespace": "", "time_ms": 1593113084971, "event_type": "INTERVAL_START", "key": "epoch_start", "value": null, "metadata": {"file": "train.py", "lineno": 541, "epoch_num": 1}}
0: Starting epoch 0
0: Sampler for epoch 0 uses seed 1842960966
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
0: TRAIN [0][0/1291]	Time 0.416 (0.416)	Data 1.96e-01 (1.96e-01)	Tok/s 29905 (29905)	Loss/tok 10.6330 (10.6330)	LR 2.942e-05
0: TRAIN [0][10/1291]	Time 0.058 (0.085)	Data 9.37e-05 (1.79e-02)	Tok/s 216092 (187552)	Loss/tok 9.6172 (9.9848)	LR 3.704e-05
0: TRAIN [0][20/1291]	Time 0.041 (0.064)	Data 8.85e-05 (9.41e-03)	Tok/s 185145 (186337)	Loss/tok 9.1236 (9.7049)	LR 4.663e-05
0: TRAIN [0][30/1291]	Time 0.076 (0.063)	Data 8.75e-05 (6.41e-03)	Tok/s 233967 (193616)	Loss/tok 9.0256 (9.4539)	LR 5.870e-05
0: Gradient norm: inf
0: Skipped batch, new scale: 512.0
0: TRAIN [0][40/1291]	Time 0.041 (0.062)	Data 8.63e-05 (4.87e-03)	Tok/s 187979 (196974)	Loss/tok 8.5303 (9.2737)	LR 7.222e-05
0: Gradient norm: inf
0: Skipped batch, new scale: 256.0
0: Gradient norm: inf
0: Skipped batch, new scale: 128.0
0: TRAIN [0][50/1291]	Time 0.041 (0.059)	Data 8.61e-05 (3.93e-03)	Tok/s 186473 (195085)	Loss/tok 8.3072 (9.1499)	LR 8.682e-05
0: TRAIN [0][60/1291]	Time 0.041 (0.059)	Data 8.99e-05 (3.30e-03)	Tok/s 188081 (196828)	Loss/tok 8.1354 (9.0066)	LR 1.093e-04
0: TRAIN [0][70/1291]	Time 0.041 (0.058)	Data 8.68e-05 (2.85e-03)	Tok/s 189996 (197733)	Loss/tok 8.1009 (8.8775)	LR 1.376e-04
0: TRAIN [0][80/1291]	Time 0.096 (0.059)	Data 8.25e-05 (2.51e-03)	Tok/s 236081 (199301)	Loss/tok 8.1918 (8.7896)	LR 1.732e-04
0: TRAIN [0][90/1291]	Time 0.058 (0.058)	Data 1.29e-04 (2.24e-03)	Tok/s 217950 (199832)	Loss/tok 8.0764 (8.7070)	LR 2.181e-04
0: TRAIN [0][100/1291]	Time 0.058 (0.058)	Data 8.44e-05 (2.03e-03)	Tok/s 215583 (200570)	Loss/tok 7.8902 (8.6306)	LR 2.746e-04
0: TRAIN [0][110/1291]	Time 0.058 (0.058)	Data 8.77e-05 (1.86e-03)	Tok/s 218495 (201443)	Loss/tok 7.9678 (8.5663)	LR 3.457e-04
0: TRAIN [0][120/1291]	Time 0.041 (0.057)	Data 1.41e-04 (1.71e-03)	Tok/s 184859 (201262)	Loss/tok 7.7490 (8.5128)	LR 4.351e-04
0: TRAIN [0][130/1291]	Time 0.041 (0.056)	Data 8.32e-05 (1.59e-03)	Tok/s 187682 (200734)	Loss/tok 7.6025 (8.4644)	LR 5.478e-04
0: TRAIN [0][140/1291]	Time 0.058 (0.056)	Data 1.22e-04 (1.48e-03)	Tok/s 217838 (200970)	Loss/tok 7.6811 (8.4143)	LR 6.897e-04
0: TRAIN [0][150/1291]	Time 0.024 (0.056)	Data 1.42e-04 (1.39e-03)	Tok/s 161619 (201352)	Loss/tok 6.8086 (8.3651)	LR 8.682e-04
0: Gradient norm: inf
0: Skipped batch, new scale: 64.0
0: TRAIN [0][160/1291]	Time 0.058 (0.056)	Data 8.68e-05 (1.31e-03)	Tok/s 214975 (201414)	Loss/tok 7.6192 (8.3219)	LR 1.068e-03
0: TRAIN [0][170/1291]	Time 0.058 (0.055)	Data 8.06e-05 (1.24e-03)	Tok/s 215350 (201505)	Loss/tok 7.4169 (8.2735)	LR 1.345e-03
0: TRAIN [0][180/1291]	Time 0.076 (0.055)	Data 8.15e-05 (1.18e-03)	Tok/s 227808 (201306)	Loss/tok 7.4156 (8.2212)	LR 1.693e-03
0: TRAIN [0][190/1291]	Time 0.024 (0.054)	Data 8.15e-05 (1.12e-03)	Tok/s 159310 (200887)	Loss/tok 6.1957 (8.1698)	LR 2.131e-03
0: TRAIN [0][200/1291]	Time 0.041 (0.054)	Data 8.46e-05 (1.07e-03)	Tok/s 188692 (201053)	Loss/tok 6.8511 (8.1133)	LR 2.683e-03
0: TRAIN [0][210/1291]	Time 0.042 (0.054)	Data 1.36e-04 (1.02e-03)	Tok/s 184220 (200608)	Loss/tok 6.6477 (8.0605)	LR 2.875e-03
0: TRAIN [0][220/1291]	Time 0.041 (0.054)	Data 8.51e-05 (9.79e-04)	Tok/s 187477 (200791)	Loss/tok 6.4042 (7.9973)	LR 2.875e-03
0: TRAIN [0][230/1291]	Time 0.058 (0.054)	Data 1.36e-04 (9.41e-04)	Tok/s 217701 (200642)	Loss/tok 6.4625 (7.9367)	LR 2.875e-03
0: TRAIN [0][240/1291]	Time 0.041 (0.054)	Data 1.42e-04 (9.05e-04)	Tok/s 192760 (200810)	Loss/tok 6.0235 (7.8690)	LR 2.875e-03
0: TRAIN [0][250/1291]	Time 0.041 (0.054)	Data 8.08e-05 (8.73e-04)	Tok/s 189764 (201162)	Loss/tok 5.8667 (7.7936)	LR 2.875e-03
0: TRAIN [0][260/1291]	Time 0.042 (0.054)	Data 8.18e-05 (8.42e-04)	Tok/s 185450 (201233)	Loss/tok 5.7480 (7.7255)	LR 2.875e-03
0: TRAIN [0][270/1291]	Time 0.041 (0.054)	Data 8.13e-05 (8.14e-04)	Tok/s 184854 (201275)	Loss/tok 5.5762 (7.6571)	LR 2.875e-03
0: Upscaling, new scale: 128.0
0: TRAIN [0][280/1291]	Time 0.076 (0.054)	Data 8.25e-05 (7.89e-04)	Tok/s 228283 (201114)	Loss/tok 6.0967 (7.5958)	LR 2.875e-03
0: TRAIN [0][290/1291]	Time 0.041 (0.054)	Data 8.82e-05 (7.65e-04)	Tok/s 187688 (201244)	Loss/tok 5.5375 (7.5272)	LR 2.875e-03
0: TRAIN [0][300/1291]	Time 0.024 (0.054)	Data 8.27e-05 (7.43e-04)	Tok/s 164513 (201239)	Loss/tok 4.4235 (7.4609)	LR 2.875e-03
0: TRAIN [0][310/1291]	Time 0.025 (0.054)	Data 8.51e-05 (7.22e-04)	Tok/s 161186 (201317)	Loss/tok 4.2780 (7.3948)	LR 2.875e-03
0: TRAIN [0][320/1291]	Time 0.041 (0.054)	Data 7.92e-05 (7.02e-04)	Tok/s 182391 (201102)	Loss/tok 5.0855 (7.3412)	LR 2.875e-03
0: TRAIN [0][330/1291]	Time 0.058 (0.054)	Data 1.39e-04 (6.84e-04)	Tok/s 219803 (201038)	Loss/tok 5.2722 (7.2842)	LR 2.875e-03
0: TRAIN [0][340/1291]	Time 0.042 (0.054)	Data 8.54e-05 (6.67e-04)	Tok/s 185572 (200933)	Loss/tok 4.8675 (7.2251)	LR 2.875e-03
0: TRAIN [0][350/1291]	Time 0.042 (0.054)	Data 8.70e-05 (6.51e-04)	Tok/s 188512 (200948)	Loss/tok 4.8733 (7.1620)	LR 2.875e-03
0: TRAIN [0][360/1291]	Time 0.076 (0.054)	Data 1.40e-04 (6.36e-04)	Tok/s 231963 (200926)	Loss/tok 5.2147 (7.1060)	LR 2.875e-03
0: TRAIN [0][370/1291]	Time 0.042 (0.054)	Data 1.38e-04 (6.21e-04)	Tok/s 189940 (201307)	Loss/tok 4.5697 (7.0336)	LR 2.875e-03
0: TRAIN [0][380/1291]	Time 0.058 (0.054)	Data 8.34e-05 (6.07e-04)	Tok/s 216741 (201211)	Loss/tok 4.6694 (6.9782)	LR 2.875e-03
0: TRAIN [0][390/1291]	Time 0.042 (0.054)	Data 7.82e-05 (5.94e-04)	Tok/s 184298 (201353)	Loss/tok 4.4423 (6.9200)	LR 2.875e-03
0: TRAIN [0][400/1291]	Time 0.042 (0.054)	Data 8.20e-05 (5.82e-04)	Tok/s 181680 (201488)	Loss/tok 4.3111 (6.8602)	LR 2.875e-03
0: Upscaling, new scale: 256.0
0: TRAIN [0][410/1291]	Time 0.042 (0.054)	Data 7.65e-05 (5.70e-04)	Tok/s 182406 (201283)	Loss/tok 4.3877 (6.8133)	LR 2.875e-03
0: TRAIN [0][420/1291]	Time 0.041 (0.054)	Data 7.99e-05 (5.58e-04)	Tok/s 194711 (201252)	Loss/tok 4.3522 (6.7596)	LR 2.875e-03
0: TRAIN [0][430/1291]	Time 0.076 (0.054)	Data 1.35e-04 (5.48e-04)	Tok/s 230161 (201622)	Loss/tok 4.7757 (6.6958)	LR 2.875e-03
0: TRAIN [0][440/1291]	Time 0.076 (0.054)	Data 7.80e-05 (5.37e-04)	Tok/s 229406 (201898)	Loss/tok 4.5815 (6.6373)	LR 2.875e-03
0: TRAIN [0][450/1291]	Time 0.058 (0.054)	Data 8.23e-05 (5.27e-04)	Tok/s 219910 (202057)	Loss/tok 4.1960 (6.5835)	LR 2.875e-03
0: TRAIN [0][460/1291]	Time 0.041 (0.054)	Data 7.84e-05 (5.18e-04)	Tok/s 187943 (202073)	Loss/tok 4.1178 (6.5350)	LR 2.875e-03
0: TRAIN [0][470/1291]	Time 0.042 (0.054)	Data 1.32e-04 (5.09e-04)	Tok/s 192286 (202086)	Loss/tok 3.9714 (6.4881)	LR 2.875e-03
0: TRAIN [0][480/1291]	Time 0.041 (0.054)	Data 7.68e-05 (5.00e-04)	Tok/s 186025 (202182)	Loss/tok 4.0557 (6.4413)	LR 2.875e-03
0: TRAIN [0][490/1291]	Time 0.041 (0.054)	Data 7.80e-05 (4.91e-04)	Tok/s 187361 (202215)	Loss/tok 3.9893 (6.3940)	LR 2.875e-03
0: TRAIN [0][500/1291]	Time 0.076 (0.054)	Data 7.96e-05 (4.83e-04)	Tok/s 226131 (202137)	Loss/tok 4.4558 (6.3534)	LR 2.875e-03
0: TRAIN [0][510/1291]	Time 0.042 (0.054)	Data 7.65e-05 (4.75e-04)	Tok/s 186191 (202107)	Loss/tok 4.0278 (6.3100)	LR 2.875e-03
0: TRAIN [0][520/1291]	Time 0.042 (0.054)	Data 1.08e-04 (4.68e-04)	Tok/s 186147 (202221)	Loss/tok 4.1380 (6.2671)	LR 2.875e-03
0: TRAIN [0][530/1291]	Time 0.058 (0.054)	Data 1.41e-04 (4.61e-04)	Tok/s 211701 (202170)	Loss/tok 4.2341 (6.2290)	LR 2.875e-03
0: Upscaling, new scale: 512.0
0: TRAIN [0][540/1291]	Time 0.042 (0.054)	Data 7.61e-05 (4.54e-04)	Tok/s 183830 (202189)	Loss/tok 3.8042 (6.1889)	LR 2.875e-03
0: TRAIN [0][550/1291]	Time 0.042 (0.054)	Data 1.34e-04 (4.48e-04)	Tok/s 185267 (201896)	Loss/tok 3.9173 (6.1592)	LR 2.875e-03
0: TRAIN [0][560/1291]	Time 0.041 (0.054)	Data 1.34e-04 (4.41e-04)	Tok/s 189170 (201934)	Loss/tok 3.7810 (6.1218)	LR 2.875e-03
0: TRAIN [0][570/1291]	Time 0.041 (0.054)	Data 8.11e-05 (4.35e-04)	Tok/s 186743 (202086)	Loss/tok 3.9252 (6.0832)	LR 2.875e-03
0: TRAIN [0][580/1291]	Time 0.058 (0.054)	Data 7.92e-05 (4.29e-04)	Tok/s 213271 (202099)	Loss/tok 4.1032 (6.0497)	LR 2.875e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 256.0
0: TRAIN [0][590/1291]	Time 0.042 (0.054)	Data 7.87e-05 (4.23e-04)	Tok/s 184257 (202079)	Loss/tok 3.6559 (6.0165)	LR 2.875e-03
0: TRAIN [0][600/1291]	Time 0.042 (0.054)	Data 1.29e-04 (4.18e-04)	Tok/s 188153 (202042)	Loss/tok 3.6775 (5.9857)	LR 2.875e-03
0: TRAIN [0][610/1291]	Time 0.058 (0.054)	Data 1.36e-04 (4.12e-04)	Tok/s 213665 (202120)	Loss/tok 3.8634 (5.9519)	LR 2.875e-03
0: TRAIN [0][620/1291]	Time 0.059 (0.054)	Data 1.40e-04 (4.07e-04)	Tok/s 213816 (202380)	Loss/tok 3.9684 (5.9127)	LR 2.875e-03
0: TRAIN [0][630/1291]	Time 0.076 (0.054)	Data 8.13e-05 (4.02e-04)	Tok/s 232265 (202603)	Loss/tok 4.1163 (5.8771)	LR 2.875e-03
0: TRAIN [0][640/1291]	Time 0.042 (0.054)	Data 7.39e-05 (3.97e-04)	Tok/s 184083 (202426)	Loss/tok 3.7372 (5.8512)	LR 2.875e-03
0: TRAIN [0][650/1291]	Time 0.058 (0.054)	Data 8.03e-05 (3.92e-04)	Tok/s 215391 (202369)	Loss/tok 4.0057 (5.8248)	LR 2.875e-03
0: TRAIN [0][660/1291]	Time 0.058 (0.054)	Data 1.35e-04 (3.88e-04)	Tok/s 215092 (202478)	Loss/tok 3.8393 (5.7947)	LR 2.875e-03
0: TRAIN [0][670/1291]	Time 0.041 (0.054)	Data 7.82e-05 (3.83e-04)	Tok/s 186733 (202284)	Loss/tok 3.6297 (5.7716)	LR 2.875e-03
0: TRAIN [0][680/1291]	Time 0.097 (0.054)	Data 7.63e-05 (3.79e-04)	Tok/s 230837 (202335)	Loss/tok 4.1566 (5.7429)	LR 2.875e-03
0: TRAIN [0][690/1291]	Time 0.058 (0.054)	Data 1.08e-04 (3.75e-04)	Tok/s 212435 (202375)	Loss/tok 3.9350 (5.7151)	LR 2.875e-03
0: TRAIN [0][700/1291]	Time 0.058 (0.054)	Data 8.06e-05 (3.71e-04)	Tok/s 215168 (202332)	Loss/tok 3.9274 (5.6909)	LR 2.875e-03
0: TRAIN [0][710/1291]	Time 0.025 (0.054)	Data 8.18e-05 (3.67e-04)	Tok/s 161205 (202248)	Loss/tok 3.1403 (5.6674)	LR 2.875e-03
0: Upscaling, new scale: 512.0
0: TRAIN [0][720/1291]	Time 0.097 (0.054)	Data 1.33e-04 (3.63e-04)	Tok/s 229187 (202336)	Loss/tok 4.3232 (5.6410)	LR 2.875e-03
0: TRAIN [0][730/1291]	Time 0.042 (0.054)	Data 7.96e-05 (3.59e-04)	Tok/s 188346 (202393)	Loss/tok 3.5835 (5.6161)	LR 2.875e-03
0: TRAIN [0][740/1291]	Time 0.042 (0.054)	Data 8.06e-05 (3.55e-04)	Tok/s 180752 (202336)	Loss/tok 3.5558 (5.5932)	LR 2.875e-03
0: TRAIN [0][750/1291]	Time 0.058 (0.054)	Data 7.75e-05 (3.52e-04)	Tok/s 213252 (202286)	Loss/tok 3.9600 (5.5698)	LR 2.875e-03
0: TRAIN [0][760/1291]	Time 0.042 (0.054)	Data 7.99e-05 (3.48e-04)	Tok/s 187300 (202207)	Loss/tok 3.5484 (5.5486)	LR 2.875e-03
0: TRAIN [0][770/1291]	Time 0.042 (0.054)	Data 7.87e-05 (3.45e-04)	Tok/s 185859 (201989)	Loss/tok 3.5389 (5.5313)	LR 2.875e-03
0: TRAIN [0][780/1291]	Time 0.025 (0.054)	Data 7.84e-05 (3.41e-04)	Tok/s 158038 (201830)	Loss/tok 2.9685 (5.5127)	LR 2.875e-03
0: TRAIN [0][790/1291]	Time 0.058 (0.054)	Data 7.61e-05 (3.38e-04)	Tok/s 217371 (201839)	Loss/tok 3.7425 (5.4913)	LR 2.875e-03
0: TRAIN [0][800/1291]	Time 0.058 (0.053)	Data 1.32e-04 (3.35e-04)	Tok/s 214663 (201765)	Loss/tok 3.8957 (5.4714)	LR 2.875e-03
0: TRAIN [0][810/1291]	Time 0.041 (0.053)	Data 7.32e-05 (3.32e-04)	Tok/s 186793 (201682)	Loss/tok 3.6147 (5.4528)	LR 2.875e-03
0: TRAIN [0][820/1291]	Time 0.025 (0.053)	Data 8.27e-05 (3.29e-04)	Tok/s 163312 (201526)	Loss/tok 2.9246 (5.4358)	LR 2.875e-03
0: TRAIN [0][830/1291]	Time 0.042 (0.053)	Data 7.58e-05 (3.26e-04)	Tok/s 190185 (201477)	Loss/tok 3.5404 (5.4172)	LR 2.875e-03
0: TRAIN [0][840/1291]	Time 0.041 (0.053)	Data 7.89e-05 (3.23e-04)	Tok/s 188270 (201378)	Loss/tok 3.4624 (5.3998)	LR 2.875e-03
0: Upscaling, new scale: 1024.0
0: TRAIN [0][850/1291]	Time 0.059 (0.053)	Data 7.49e-05 (3.21e-04)	Tok/s 216097 (201357)	Loss/tok 3.7823 (5.3815)	LR 2.875e-03
0: TRAIN [0][860/1291]	Time 0.058 (0.053)	Data 8.13e-05 (3.18e-04)	Tok/s 217181 (201413)	Loss/tok 3.7281 (5.3618)	LR 2.875e-03
0: TRAIN [0][870/1291]	Time 0.041 (0.053)	Data 1.34e-04 (3.15e-04)	Tok/s 185603 (201293)	Loss/tok 3.6133 (5.3457)	LR 2.875e-03
0: TRAIN [0][880/1291]	Time 0.042 (0.053)	Data 8.01e-05 (3.13e-04)	Tok/s 183112 (201336)	Loss/tok 3.5772 (5.3272)	LR 2.875e-03
0: TRAIN [0][890/1291]	Time 0.076 (0.053)	Data 1.34e-04 (3.10e-04)	Tok/s 232792 (201360)	Loss/tok 3.8184 (5.3088)	LR 2.875e-03
0: TRAIN [0][900/1291]	Time 0.076 (0.053)	Data 7.65e-05 (3.08e-04)	Tok/s 232051 (201432)	Loss/tok 3.8945 (5.2902)	LR 2.875e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 512.0
0: TRAIN [0][910/1291]	Time 0.097 (0.053)	Data 9.42e-05 (3.05e-04)	Tok/s 230447 (201388)	Loss/tok 4.2256 (5.2742)	LR 2.875e-03
0: TRAIN [0][920/1291]	Time 0.058 (0.053)	Data 8.03e-05 (3.03e-04)	Tok/s 217566 (201398)	Loss/tok 3.7062 (5.2571)	LR 2.875e-03
0: TRAIN [0][930/1291]	Time 0.058 (0.053)	Data 7.89e-05 (3.01e-04)	Tok/s 216949 (201405)	Loss/tok 3.8339 (5.2405)	LR 2.875e-03
0: TRAIN [0][940/1291]	Time 0.059 (0.053)	Data 7.56e-05 (2.98e-04)	Tok/s 214400 (201402)	Loss/tok 3.7370 (5.2241)	LR 2.875e-03
0: TRAIN [0][950/1291]	Time 0.042 (0.053)	Data 7.72e-05 (2.96e-04)	Tok/s 184446 (201363)	Loss/tok 3.4783 (5.2094)	LR 2.875e-03
0: TRAIN [0][960/1291]	Time 0.076 (0.053)	Data 7.68e-05 (2.94e-04)	Tok/s 227240 (201413)	Loss/tok 3.9232 (5.1934)	LR 2.875e-03
0: TRAIN [0][970/1291]	Time 0.097 (0.053)	Data 7.99e-05 (2.92e-04)	Tok/s 232224 (201447)	Loss/tok 4.1996 (5.1778)	LR 2.875e-03
0: TRAIN [0][980/1291]	Time 0.042 (0.053)	Data 7.70e-05 (2.90e-04)	Tok/s 177825 (201471)	Loss/tok 3.4884 (5.1619)	LR 2.875e-03
0: TRAIN [0][990/1291]	Time 0.097 (0.053)	Data 1.26e-04 (2.88e-04)	Tok/s 230316 (201556)	Loss/tok 4.0655 (5.1451)	LR 2.875e-03
0: TRAIN [0][1000/1291]	Time 0.059 (0.053)	Data 8.37e-05 (2.86e-04)	Tok/s 215758 (201519)	Loss/tok 3.7564 (5.1312)	LR 2.875e-03
0: TRAIN [0][1010/1291]	Time 0.041 (0.053)	Data 7.72e-05 (2.84e-04)	Tok/s 187215 (201544)	Loss/tok 3.3626 (5.1166)	LR 2.875e-03
0: TRAIN [0][1020/1291]	Time 0.076 (0.053)	Data 8.15e-05 (2.82e-04)	Tok/s 231643 (201605)	Loss/tok 3.8212 (5.1013)	LR 2.875e-03
0: TRAIN [0][1030/1291]	Time 0.076 (0.053)	Data 8.25e-05 (2.80e-04)	Tok/s 229895 (201599)	Loss/tok 3.8345 (5.0879)	LR 2.875e-03
0: Upscaling, new scale: 1024.0
0: TRAIN [0][1040/1291]	Time 0.076 (0.053)	Data 7.65e-05 (2.78e-04)	Tok/s 230110 (201514)	Loss/tok 4.0191 (5.0761)	LR 2.875e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 512.0
0: TRAIN [0][1050/1291]	Time 0.041 (0.053)	Data 1.57e-04 (2.76e-04)	Tok/s 188425 (201426)	Loss/tok 3.4607 (5.0639)	LR 2.875e-03
0: TRAIN [0][1060/1291]	Time 0.076 (0.053)	Data 8.03e-05 (2.74e-04)	Tok/s 230679 (201494)	Loss/tok 3.8828 (5.0498)	LR 2.875e-03
0: TRAIN [0][1070/1291]	Time 0.058 (0.053)	Data 7.87e-05 (2.73e-04)	Tok/s 217563 (201513)	Loss/tok 3.5799 (5.0359)	LR 2.875e-03
0: TRAIN [0][1080/1291]	Time 0.041 (0.053)	Data 7.96e-05 (2.71e-04)	Tok/s 189002 (201571)	Loss/tok 3.3833 (5.0224)	LR 2.875e-03
0: TRAIN [0][1090/1291]	Time 0.042 (0.053)	Data 7.84e-05 (2.69e-04)	Tok/s 182110 (201579)	Loss/tok 3.3760 (5.0096)	LR 2.875e-03
0: TRAIN [0][1100/1291]	Time 0.041 (0.053)	Data 7.96e-05 (2.68e-04)	Tok/s 188149 (201568)	Loss/tok 3.3838 (4.9969)	LR 2.875e-03
0: TRAIN [0][1110/1291]	Time 0.041 (0.053)	Data 7.65e-05 (2.66e-04)	Tok/s 191146 (201480)	Loss/tok 3.4017 (4.9857)	LR 2.875e-03
0: TRAIN [0][1120/1291]	Time 0.076 (0.053)	Data 1.34e-04 (2.65e-04)	Tok/s 224697 (201508)	Loss/tok 4.0864 (4.9730)	LR 2.875e-03
0: TRAIN [0][1130/1291]	Time 0.076 (0.053)	Data 7.84e-05 (2.63e-04)	Tok/s 229367 (201460)	Loss/tok 3.9119 (4.9621)	LR 2.875e-03
0: TRAIN [0][1140/1291]	Time 0.041 (0.053)	Data 7.82e-05 (2.61e-04)	Tok/s 190091 (201460)	Loss/tok 3.3936 (4.9506)	LR 2.875e-03
0: TRAIN [0][1150/1291]	Time 0.041 (0.053)	Data 8.99e-05 (2.60e-04)	Tok/s 185186 (201508)	Loss/tok 3.2545 (4.9383)	LR 2.875e-03
0: TRAIN [0][1160/1291]	Time 0.041 (0.053)	Data 8.80e-05 (2.58e-04)	Tok/s 189517 (201513)	Loss/tok 3.4408 (4.9272)	LR 2.875e-03
0: TRAIN [0][1170/1291]	Time 0.058 (0.053)	Data 8.03e-05 (2.57e-04)	Tok/s 215279 (201484)	Loss/tok 3.5106 (4.9164)	LR 2.875e-03
0: Upscaling, new scale: 1024.0
0: TRAIN [0][1180/1291]	Time 0.024 (0.053)	Data 8.80e-05 (2.55e-04)	Tok/s 159836 (201444)	Loss/tok 2.9103 (4.9062)	LR 2.875e-03
0: TRAIN [0][1190/1291]	Time 0.042 (0.053)	Data 8.23e-05 (2.54e-04)	Tok/s 191450 (201407)	Loss/tok 3.2444 (4.8956)	LR 2.875e-03
0: TRAIN [0][1200/1291]	Time 0.076 (0.053)	Data 7.70e-05 (2.53e-04)	Tok/s 225735 (201433)	Loss/tok 3.8224 (4.8847)	LR 2.875e-03
0: TRAIN [0][1210/1291]	Time 0.042 (0.053)	Data 8.08e-05 (2.51e-04)	Tok/s 184247 (201328)	Loss/tok 3.3080 (4.8751)	LR 2.875e-03
0: TRAIN [0][1220/1291]	Time 0.058 (0.053)	Data 8.34e-05 (2.50e-04)	Tok/s 217825 (201369)	Loss/tok 3.5986 (4.8635)	LR 2.875e-03
0: TRAIN [0][1230/1291]	Time 0.058 (0.053)	Data 7.41e-05 (2.49e-04)	Tok/s 211119 (201314)	Loss/tok 3.6903 (4.8542)	LR 2.875e-03
0: TRAIN [0][1240/1291]	Time 0.042 (0.053)	Data 7.68e-05 (2.47e-04)	Tok/s 186717 (201321)	Loss/tok 3.3451 (4.8442)	LR 2.875e-03
0: TRAIN [0][1250/1291]	Time 0.058 (0.053)	Data 7.89e-05 (2.46e-04)	Tok/s 215039 (201272)	Loss/tok 3.5108 (4.8348)	LR 2.875e-03
0: TRAIN [0][1260/1291]	Time 0.059 (0.053)	Data 1.35e-04 (2.45e-04)	Tok/s 211787 (201355)	Loss/tok 3.5434 (4.8238)	LR 2.875e-03
0: TRAIN [0][1270/1291]	Time 0.058 (0.053)	Data 7.61e-05 (2.43e-04)	Tok/s 215137 (201333)	Loss/tok 3.6335 (4.8143)	LR 2.875e-03
0: TRAIN [0][1280/1291]	Time 0.042 (0.053)	Data 7.92e-05 (2.42e-04)	Tok/s 185546 (201340)	Loss/tok 3.3176 (4.8046)	LR 2.875e-03
0: TRAIN [0][1290/1291]	Time 0.042 (0.053)	Data 4.22e-05 (2.43e-04)	Tok/s 187726 (201379)	Loss/tok 3.4263 (4.7944)	LR 2.875e-03
:::MLLOG {"namespace": "", "time_ms": 1593113153314, "event_type": "INTERVAL_END", "key": "epoch_stop", "value": null, "metadata": {"file": "train.py", "lineno": 551, "epoch_num": 1}}
:::MLLOG {"namespace": "", "time_ms": 1593113153314, "event_type": "INTERVAL_START", "key": "eval_start", "value": null, "metadata": {"file": "train.py", "lineno": 556, "epoch_num": 1}}
0: Running evaluation on test set
0: TEST [0][0/3]	Time 0.399 (0.399)	Decoder iters 149.0 (149.0)	Tok/s 22114 (22114)
0: Running moses detokenizer
0: BLEU(score=19.10336511394918, counts=[33680, 15119, 7926, 4316], totals=[64735, 61732, 58729, 55730], precisions=[52.02749671738627, 24.491349705177218, 13.495887891842191, 7.744482325497937], bp=1.0, sys_len=64735, ref_len=64676)
0: Finished evaluation on test set
:::MLLOG {"namespace": "", "time_ms": 1593113154623, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.191, "metadata": {"file": "train.py", "lineno": 562, "epoch_num": 1}}
:::MLLOG {"namespace": "", "time_ms": 1593113154623, "event_type": "INTERVAL_END", "key": "eval_stop", "value": null, "metadata": {"file": "train.py", "lineno": 565, "epoch_num": 1}}
0: Summary: Epoch: 0	Training Loss: 4.7922	Test BLEU: 19.10
0: Performance: Epoch: 0	Training: 3220714 Tok/s
0: Finished epoch 0
:::MLLOG {"namespace": "", "time_ms": 1593113154623, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "train.py", "lineno": 584, "first_epoch_num": 1}}
:::MLLOG {"namespace": "", "time_ms": 1593113154623, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "train.py", "lineno": 538, "first_epoch_num": 2, "epoch_count": 1}}
:::MLLOG {"namespace": "", "time_ms": 1593113154624, "event_type": "INTERVAL_START", "key": "epoch_start", "value": null, "metadata": {"file": "train.py", "lineno": 541, "epoch_num": 2}}
0: Starting epoch 1
0: Sampler for epoch 1 uses seed 4152748289
0: TRAIN [1][0/1291]	Time 0.274 (0.274)	Data 1.89e-01 (1.89e-01)	Tok/s 27237 (27237)	Loss/tok 3.1960 (3.1960)	LR 2.875e-03
0: Upscaling, new scale: 2048.0
0: TRAIN [1][10/1291]	Time 0.058 (0.079)	Data 9.06e-05 (1.72e-02)	Tok/s 212629 (191897)	Loss/tok 3.4304 (3.5898)	LR 2.875e-03
0: TRAIN [1][20/1291]	Time 0.058 (0.067)	Data 8.96e-05 (9.08e-03)	Tok/s 215041 (197106)	Loss/tok 3.5007 (3.5648)	LR 2.875e-03
0: TRAIN [1][30/1291]	Time 0.058 (0.063)	Data 1.53e-04 (6.19e-03)	Tok/s 215141 (199553)	Loss/tok 3.5549 (3.5522)	LR 2.875e-03
0: TRAIN [1][40/1291]	Time 0.025 (0.061)	Data 9.06e-05 (4.70e-03)	Tok/s 159925 (200536)	Loss/tok 2.8353 (3.5421)	LR 2.875e-03
0: TRAIN [1][50/1291]	Time 0.077 (0.059)	Data 1.02e-04 (3.80e-03)	Tok/s 230107 (200814)	Loss/tok 3.6763 (3.5401)	LR 2.875e-03
0: TRAIN [1][60/1291]	Time 0.025 (0.059)	Data 9.35e-05 (3.20e-03)	Tok/s 157005 (201349)	Loss/tok 2.8961 (3.5404)	LR 2.875e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 1024.0
0: TRAIN [1][70/1291]	Time 0.042 (0.059)	Data 1.42e-04 (2.76e-03)	Tok/s 186806 (202652)	Loss/tok 3.1280 (3.5548)	LR 2.875e-03
0: TRAIN [1][80/1291]	Time 0.058 (0.057)	Data 9.08e-05 (2.43e-03)	Tok/s 216091 (201169)	Loss/tok 3.4354 (3.5327)	LR 2.875e-03
0: TRAIN [1][90/1291]	Time 0.096 (0.057)	Data 1.43e-04 (2.18e-03)	Tok/s 230910 (201319)	Loss/tok 4.0313 (3.5309)	LR 2.875e-03
0: TRAIN [1][100/1291]	Time 0.042 (0.057)	Data 8.92e-05 (1.97e-03)	Tok/s 185650 (201236)	Loss/tok 3.1903 (3.5254)	LR 2.875e-03
0: TRAIN [1][110/1291]	Time 0.076 (0.055)	Data 8.08e-05 (1.80e-03)	Tok/s 231940 (200569)	Loss/tok 3.6026 (3.5122)	LR 2.875e-03
0: TRAIN [1][120/1291]	Time 0.042 (0.055)	Data 1.10e-04 (1.66e-03)	Tok/s 181526 (199759)	Loss/tok 3.3136 (3.5029)	LR 2.875e-03
0: TRAIN [1][130/1291]	Time 0.058 (0.055)	Data 9.49e-05 (1.54e-03)	Tok/s 216341 (200130)	Loss/tok 3.4113 (3.5064)	LR 2.875e-03
0: TRAIN [1][140/1291]	Time 0.041 (0.055)	Data 8.96e-05 (1.44e-03)	Tok/s 185832 (200484)	Loss/tok 3.2280 (3.5051)	LR 2.875e-03
0: TRAIN [1][150/1291]	Time 0.041 (0.055)	Data 1.42e-04 (1.35e-03)	Tok/s 186006 (200776)	Loss/tok 3.3575 (3.5044)	LR 2.875e-03
0: TRAIN [1][160/1291]	Time 0.042 (0.055)	Data 8.85e-05 (1.27e-03)	Tok/s 180760 (200794)	Loss/tok 3.1025 (3.5049)	LR 2.875e-03
0: TRAIN [1][170/1291]	Time 0.025 (0.055)	Data 8.92e-05 (1.20e-03)	Tok/s 163773 (200929)	Loss/tok 2.7732 (3.5044)	LR 2.875e-03
0: TRAIN [1][180/1291]	Time 0.058 (0.054)	Data 9.11e-05 (1.14e-03)	Tok/s 213245 (200412)	Loss/tok 3.5171 (3.5020)	LR 2.875e-03
0: TRAIN [1][190/1291]	Time 0.059 (0.055)	Data 8.68e-05 (1.09e-03)	Tok/s 218988 (200987)	Loss/tok 3.4205 (3.5148)	LR 2.875e-03
0: Upscaling, new scale: 2048.0
0: TRAIN [1][200/1291]	Time 0.041 (0.054)	Data 8.61e-05 (1.04e-03)	Tok/s 188180 (200412)	Loss/tok 3.2186 (3.5099)	LR 2.875e-03
0: TRAIN [1][210/1291]	Time 0.059 (0.055)	Data 8.94e-05 (9.96e-04)	Tok/s 212525 (200749)	Loss/tok 3.3891 (3.5119)	LR 2.875e-03
0: TRAIN [1][220/1291]	Time 0.041 (0.054)	Data 8.80e-05 (9.56e-04)	Tok/s 189058 (200533)	Loss/tok 3.2244 (3.5079)	LR 2.875e-03
0: TRAIN [1][230/1291]	Time 0.097 (0.054)	Data 8.75e-05 (9.19e-04)	Tok/s 224624 (200371)	Loss/tok 3.9701 (3.5111)	LR 2.875e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 1024.0
0: TRAIN [1][240/1291]	Time 0.096 (0.054)	Data 1.47e-04 (8.85e-04)	Tok/s 232244 (200393)	Loss/tok 3.8017 (3.5087)	LR 2.875e-03
0: TRAIN [1][250/1291]	Time 0.058 (0.054)	Data 8.94e-05 (8.54e-04)	Tok/s 218557 (200867)	Loss/tok 3.4716 (3.5074)	LR 2.875e-03
0: TRAIN [1][260/1291]	Time 0.042 (0.054)	Data 1.41e-04 (8.25e-04)	Tok/s 187112 (200776)	Loss/tok 3.1617 (3.5025)	LR 2.875e-03
0: TRAIN [1][270/1291]	Time 0.041 (0.054)	Data 8.94e-05 (7.99e-04)	Tok/s 189837 (201067)	Loss/tok 3.3042 (3.5054)	LR 2.875e-03
0: TRAIN [1][280/1291]	Time 0.058 (0.054)	Data 8.96e-05 (7.74e-04)	Tok/s 216865 (200806)	Loss/tok 3.4541 (3.5020)	LR 2.875e-03
0: TRAIN [1][290/1291]	Time 0.076 (0.054)	Data 8.85e-05 (7.52e-04)	Tok/s 229002 (200892)	Loss/tok 3.7697 (3.5047)	LR 2.875e-03
0: TRAIN [1][300/1291]	Time 0.058 (0.054)	Data 8.73e-05 (7.30e-04)	Tok/s 216249 (200826)	Loss/tok 3.4204 (3.5028)	LR 2.875e-03
0: TRAIN [1][310/1291]	Time 0.076 (0.054)	Data 1.44e-04 (7.09e-04)	Tok/s 231158 (200793)	Loss/tok 3.7364 (3.5004)	LR 2.875e-03
0: TRAIN [1][320/1291]	Time 0.041 (0.054)	Data 1.41e-04 (6.91e-04)	Tok/s 182866 (200550)	Loss/tok 3.1750 (3.4957)	LR 2.875e-03
0: TRAIN [1][330/1291]	Time 0.042 (0.054)	Data 1.44e-04 (6.73e-04)	Tok/s 187062 (200847)	Loss/tok 3.1380 (3.5000)	LR 2.875e-03
0: TRAIN [1][340/1291]	Time 0.059 (0.054)	Data 8.58e-05 (6.56e-04)	Tok/s 215341 (201108)	Loss/tok 3.2590 (3.4990)	LR 2.875e-03
0: TRAIN [1][350/1291]	Time 0.025 (0.054)	Data 8.92e-05 (6.40e-04)	Tok/s 157105 (200790)	Loss/tok 2.6672 (3.4944)	LR 2.875e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 512.0
0: TRAIN [1][360/1291]	Time 0.024 (0.053)	Data 9.13e-05 (6.25e-04)	Tok/s 164484 (200409)	Loss/tok 2.9184 (3.4917)	LR 2.875e-03
0: TRAIN [1][370/1291]	Time 0.059 (0.054)	Data 8.37e-05 (6.11e-04)	Tok/s 214863 (200651)	Loss/tok 3.4054 (3.4902)	LR 2.875e-03
0: TRAIN [1][380/1291]	Time 0.076 (0.053)	Data 8.61e-05 (5.97e-04)	Tok/s 230128 (200536)	Loss/tok 3.5828 (3.4882)	LR 2.875e-03
0: TRAIN [1][390/1291]	Time 0.041 (0.053)	Data 1.44e-04 (5.85e-04)	Tok/s 189151 (200388)	Loss/tok 3.1577 (3.4846)	LR 2.875e-03
0: TRAIN [1][400/1291]	Time 0.058 (0.053)	Data 8.56e-05 (5.73e-04)	Tok/s 215782 (200583)	Loss/tok 3.5221 (3.4828)	LR 2.875e-03
0: TRAIN [1][410/1291]	Time 0.059 (0.053)	Data 8.32e-05 (5.61e-04)	Tok/s 215589 (200817)	Loss/tok 3.3760 (3.4827)	LR 2.875e-03
0: TRAIN [1][420/1291]	Time 0.076 (0.053)	Data 8.68e-05 (5.50e-04)	Tok/s 225634 (200771)	Loss/tok 3.5586 (3.4818)	LR 2.875e-03
0: TRAIN [1][430/1291]	Time 0.025 (0.053)	Data 8.32e-05 (5.40e-04)	Tok/s 157929 (200420)	Loss/tok 2.7501 (3.4786)	LR 2.875e-03
0: TRAIN [1][440/1291]	Time 0.042 (0.053)	Data 1.41e-04 (5.30e-04)	Tok/s 184331 (200447)	Loss/tok 3.2593 (3.4785)	LR 2.875e-03
0: TRAIN [1][450/1291]	Time 0.041 (0.053)	Data 1.40e-04 (5.21e-04)	Tok/s 189017 (200567)	Loss/tok 3.1814 (3.4775)	LR 2.875e-03
0: TRAIN [1][460/1291]	Time 0.024 (0.053)	Data 8.32e-05 (5.11e-04)	Tok/s 165029 (200444)	Loss/tok 2.7652 (3.4742)	LR 2.875e-03
0: TRAIN [1][470/1291]	Time 0.042 (0.053)	Data 8.94e-05 (5.03e-04)	Tok/s 185209 (200539)	Loss/tok 3.2256 (3.4748)	LR 2.875e-03
0: Upscaling, new scale: 1024.0
0: TRAIN [1][480/1291]	Time 0.058 (0.053)	Data 8.63e-05 (4.94e-04)	Tok/s 215317 (200835)	Loss/tok 3.5438 (3.4766)	LR 2.875e-03
0: TRAIN [1][490/1291]	Time 0.041 (0.053)	Data 8.82e-05 (4.86e-04)	Tok/s 188360 (200814)	Loss/tok 3.3052 (3.4752)	LR 2.875e-03
0: TRAIN [1][500/1291]	Time 0.041 (0.053)	Data 1.18e-04 (4.79e-04)	Tok/s 189075 (200794)	Loss/tok 3.2325 (3.4737)	LR 2.875e-03
0: TRAIN [1][510/1291]	Time 0.041 (0.053)	Data 8.99e-05 (4.71e-04)	Tok/s 192913 (200670)	Loss/tok 3.1484 (3.4749)	LR 2.875e-03
0: TRAIN [1][520/1291]	Time 0.076 (0.053)	Data 1.41e-04 (4.64e-04)	Tok/s 233145 (200795)	Loss/tok 3.6648 (3.4746)	LR 2.875e-03
0: TRAIN [1][530/1291]	Time 0.041 (0.053)	Data 1.45e-04 (4.57e-04)	Tok/s 190219 (200791)	Loss/tok 3.2834 (3.4733)	LR 2.875e-03
0: TRAIN [1][540/1291]	Time 0.025 (0.053)	Data 1.39e-04 (4.51e-04)	Tok/s 159352 (200669)	Loss/tok 2.6947 (3.4708)	LR 2.875e-03
0: TRAIN [1][550/1291]	Time 0.025 (0.053)	Data 8.70e-05 (4.44e-04)	Tok/s 155312 (200748)	Loss/tok 2.7584 (3.4712)	LR 2.875e-03
0: TRAIN [1][560/1291]	Time 0.076 (0.053)	Data 8.58e-05 (4.38e-04)	Tok/s 230687 (200688)	Loss/tok 3.6078 (3.4712)	LR 2.875e-03
0: TRAIN [1][570/1291]	Time 0.041 (0.053)	Data 8.34e-05 (4.32e-04)	Tok/s 187818 (200653)	Loss/tok 3.1248 (3.4707)	LR 2.875e-03
0: TRAIN [1][580/1291]	Time 0.058 (0.053)	Data 1.45e-04 (4.27e-04)	Tok/s 216456 (200606)	Loss/tok 3.5776 (3.4693)	LR 2.875e-03
0: TRAIN [1][590/1291]	Time 0.097 (0.053)	Data 8.92e-05 (4.21e-04)	Tok/s 234379 (200790)	Loss/tok 3.7746 (3.4709)	LR 2.875e-03
0: TRAIN [1][600/1291]	Time 0.058 (0.053)	Data 8.51e-05 (4.16e-04)	Tok/s 215239 (200874)	Loss/tok 3.3418 (3.4702)	LR 2.875e-03
0: Upscaling, new scale: 2048.0
0: TRAIN [1][610/1291]	Time 0.041 (0.053)	Data 8.87e-05 (4.11e-04)	Tok/s 186169 (200752)	Loss/tok 3.2704 (3.4691)	LR 2.875e-03
0: TRAIN [1][620/1291]	Time 0.058 (0.053)	Data 8.85e-05 (4.06e-04)	Tok/s 217384 (200992)	Loss/tok 3.4689 (3.4701)	LR 2.875e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 1024.0
0: TRAIN [1][630/1291]	Time 0.059 (0.053)	Data 8.56e-05 (4.01e-04)	Tok/s 213256 (201093)	Loss/tok 3.3038 (3.4721)	LR 2.875e-03
0: TRAIN [1][640/1291]	Time 0.041 (0.053)	Data 8.51e-05 (3.96e-04)	Tok/s 184011 (201057)	Loss/tok 3.3568 (3.4704)	LR 2.875e-03
0: TRAIN [1][650/1291]	Time 0.059 (0.053)	Data 8.68e-05 (3.92e-04)	Tok/s 215778 (201130)	Loss/tok 3.4877 (3.4695)	LR 2.875e-03
0: TRAIN [1][660/1291]	Time 0.097 (0.053)	Data 8.56e-05 (3.87e-04)	Tok/s 226499 (201282)	Loss/tok 3.9665 (3.4711)	LR 2.875e-03
0: TRAIN [1][670/1291]	Time 0.041 (0.053)	Data 8.30e-05 (3.83e-04)	Tok/s 190456 (201220)	Loss/tok 3.3122 (3.4707)	LR 2.875e-03
0: TRAIN [1][680/1291]	Time 0.058 (0.053)	Data 9.16e-05 (3.79e-04)	Tok/s 214674 (201175)	Loss/tok 3.5048 (3.4691)	LR 2.875e-03
0: TRAIN [1][690/1291]	Time 0.076 (0.053)	Data 8.77e-05 (3.74e-04)	Tok/s 227620 (201164)	Loss/tok 3.5781 (3.4671)	LR 2.875e-03
0: TRAIN [1][700/1291]	Time 0.025 (0.053)	Data 1.41e-04 (3.70e-04)	Tok/s 155875 (201063)	Loss/tok 2.7601 (3.4662)	LR 2.875e-03
0: TRAIN [1][710/1291]	Time 0.041 (0.053)	Data 8.46e-05 (3.67e-04)	Tok/s 192390 (201150)	Loss/tok 3.2356 (3.4669)	LR 2.875e-03
0: TRAIN [1][720/1291]	Time 0.042 (0.053)	Data 8.77e-05 (3.63e-04)	Tok/s 184175 (201174)	Loss/tok 3.1658 (3.4663)	LR 2.875e-03
0: TRAIN [1][730/1291]	Time 0.058 (0.053)	Data 1.43e-04 (3.59e-04)	Tok/s 215591 (201380)	Loss/tok 3.4940 (3.4662)	LR 2.875e-03
0: TRAIN [1][740/1291]	Time 0.076 (0.053)	Data 8.87e-05 (3.56e-04)	Tok/s 227950 (201445)	Loss/tok 3.7061 (3.4651)	LR 2.875e-03
0: TRAIN [1][750/1291]	Time 0.097 (0.054)	Data 1.41e-04 (3.52e-04)	Tok/s 234615 (201494)	Loss/tok 3.6206 (3.4651)	LR 2.875e-03
0: Upscaling, new scale: 2048.0
0: TRAIN [1][760/1291]	Time 0.097 (0.054)	Data 8.54e-05 (3.49e-04)	Tok/s 233771 (201420)	Loss/tok 3.7148 (3.4650)	LR 2.875e-03
0: TRAIN [1][770/1291]	Time 0.042 (0.054)	Data 1.42e-04 (3.46e-04)	Tok/s 181933 (201371)	Loss/tok 3.2441 (3.4644)	LR 2.875e-03
0: TRAIN [1][780/1291]	Time 0.041 (0.053)	Data 8.61e-05 (3.43e-04)	Tok/s 186921 (201316)	Loss/tok 3.1045 (3.4634)	LR 2.875e-03
0: TRAIN [1][790/1291]	Time 0.097 (0.053)	Data 8.85e-05 (3.40e-04)	Tok/s 230472 (201364)	Loss/tok 3.8465 (3.4632)	LR 2.875e-03
0: TRAIN [1][800/1291]	Time 0.058 (0.054)	Data 8.77e-05 (3.37e-04)	Tok/s 215836 (201440)	Loss/tok 3.3283 (3.4634)	LR 2.875e-03
0: TRAIN [1][810/1291]	Time 0.042 (0.054)	Data 8.75e-05 (3.34e-04)	Tok/s 185323 (201501)	Loss/tok 3.1754 (3.4637)	LR 2.875e-03
0: TRAIN [1][820/1291]	Time 0.041 (0.054)	Data 8.99e-05 (3.31e-04)	Tok/s 187461 (201562)	Loss/tok 3.2807 (3.4636)	LR 2.875e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 1024.0
0: TRAIN [1][830/1291]	Time 0.042 (0.054)	Data 9.58e-05 (3.28e-04)	Tok/s 183767 (201662)	Loss/tok 3.2527 (3.4645)	LR 2.875e-03
0: TRAIN [1][840/1291]	Time 0.059 (0.054)	Data 8.65e-05 (3.26e-04)	Tok/s 216392 (201765)	Loss/tok 3.3355 (3.4650)	LR 2.875e-03
0: TRAIN [1][850/1291]	Time 0.042 (0.054)	Data 8.92e-05 (3.23e-04)	Tok/s 189940 (201606)	Loss/tok 3.1667 (3.4631)	LR 2.875e-03
0: TRAIN [1][860/1291]	Time 0.059 (0.054)	Data 1.38e-04 (3.20e-04)	Tok/s 212212 (201609)	Loss/tok 3.2323 (3.4617)	LR 2.875e-03
0: TRAIN [1][870/1291]	Time 0.041 (0.054)	Data 1.45e-04 (3.18e-04)	Tok/s 187082 (201528)	Loss/tok 3.3657 (3.4601)	LR 2.875e-03
0: TRAIN [1][880/1291]	Time 0.042 (0.054)	Data 1.43e-04 (3.16e-04)	Tok/s 189232 (201504)	Loss/tok 3.2031 (3.4600)	LR 2.875e-03
0: TRAIN [1][890/1291]	Time 0.042 (0.054)	Data 8.25e-05 (3.13e-04)	Tok/s 190506 (201494)	Loss/tok 3.1280 (3.4588)	LR 2.875e-03
0: TRAIN [1][900/1291]	Time 0.076 (0.053)	Data 8.63e-05 (3.11e-04)	Tok/s 231426 (201388)	Loss/tok 3.6421 (3.4574)	LR 2.875e-03
0: TRAIN [1][910/1291]	Time 0.058 (0.053)	Data 1.46e-04 (3.08e-04)	Tok/s 215358 (201416)	Loss/tok 3.3752 (3.4561)	LR 2.875e-03
0: TRAIN [1][920/1291]	Time 0.058 (0.053)	Data 1.02e-04 (3.06e-04)	Tok/s 218231 (201323)	Loss/tok 3.3886 (3.4545)	LR 2.875e-03
0: TRAIN [1][930/1291]	Time 0.059 (0.053)	Data 8.89e-05 (3.04e-04)	Tok/s 213484 (201360)	Loss/tok 3.3407 (3.4542)	LR 2.875e-03
0: TRAIN [1][940/1291]	Time 0.025 (0.053)	Data 8.46e-05 (3.02e-04)	Tok/s 154428 (201276)	Loss/tok 2.7181 (3.4526)	LR 2.875e-03
0: TRAIN [1][950/1291]	Time 0.077 (0.053)	Data 8.34e-05 (2.99e-04)	Tok/s 226330 (201221)	Loss/tok 3.6651 (3.4516)	LR 2.875e-03
0: Upscaling, new scale: 2048.0
0: TRAIN [1][960/1291]	Time 0.041 (0.053)	Data 8.37e-05 (2.97e-04)	Tok/s 187322 (201175)	Loss/tok 3.0986 (3.4504)	LR 2.875e-03
0: TRAIN [1][970/1291]	Time 0.058 (0.053)	Data 1.46e-04 (2.95e-04)	Tok/s 217433 (201131)	Loss/tok 3.3679 (3.4493)	LR 2.875e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 1024.0
0: TRAIN [1][980/1291]	Time 0.058 (0.053)	Data 1.46e-04 (2.94e-04)	Tok/s 216057 (201260)	Loss/tok 3.3387 (3.4497)	LR 2.875e-03
0: TRAIN [1][990/1291]	Time 0.041 (0.053)	Data 8.73e-05 (2.92e-04)	Tok/s 188672 (201274)	Loss/tok 3.1721 (3.4484)	LR 2.875e-03
0: TRAIN [1][1000/1291]	Time 0.058 (0.053)	Data 8.80e-05 (2.90e-04)	Tok/s 214132 (201299)	Loss/tok 3.4304 (3.4476)	LR 2.875e-03
0: TRAIN [1][1010/1291]	Time 0.025 (0.053)	Data 8.96e-05 (2.88e-04)	Tok/s 157070 (201219)	Loss/tok 2.7467 (3.4462)	LR 2.875e-03
0: TRAIN [1][1020/1291]	Time 0.042 (0.053)	Data 8.70e-05 (2.86e-04)	Tok/s 184997 (201210)	Loss/tok 3.1396 (3.4449)	LR 2.875e-03
0: TRAIN [1][1030/1291]	Time 0.024 (0.053)	Data 8.51e-05 (2.84e-04)	Tok/s 162484 (201156)	Loss/tok 2.7275 (3.4439)	LR 2.875e-03
0: TRAIN [1][1040/1291]	Time 0.024 (0.053)	Data 8.89e-05 (2.82e-04)	Tok/s 162438 (201133)	Loss/tok 2.6075 (3.4433)	LR 2.875e-03
0: TRAIN [1][1050/1291]	Time 0.043 (0.053)	Data 8.56e-05 (2.81e-04)	Tok/s 180718 (201051)	Loss/tok 3.2975 (3.4417)	LR 2.875e-03
0: TRAIN [1][1060/1291]	Time 0.058 (0.053)	Data 8.75e-05 (2.79e-04)	Tok/s 213369 (201108)	Loss/tok 3.4264 (3.4410)	LR 2.875e-03
0: TRAIN [1][1070/1291]	Time 0.076 (0.053)	Data 9.18e-05 (2.77e-04)	Tok/s 233839 (201132)	Loss/tok 3.3748 (3.4399)	LR 2.875e-03
0: TRAIN [1][1080/1291]	Time 0.041 (0.053)	Data 8.73e-05 (2.75e-04)	Tok/s 184241 (201114)	Loss/tok 3.1354 (3.4388)	LR 2.875e-03
0: TRAIN [1][1090/1291]	Time 0.042 (0.053)	Data 8.63e-05 (2.74e-04)	Tok/s 185360 (200986)	Loss/tok 3.1789 (3.4373)	LR 2.875e-03
0: TRAIN [1][1100/1291]	Time 0.042 (0.053)	Data 1.44e-04 (2.72e-04)	Tok/s 189378 (201046)	Loss/tok 3.2611 (3.4374)	LR 2.875e-03
0: Upscaling, new scale: 2048.0
0: TRAIN [1][1110/1291]	Time 0.077 (0.053)	Data 8.96e-05 (2.71e-04)	Tok/s 226362 (201118)	Loss/tok 3.6234 (3.4376)	LR 2.875e-03
0: TRAIN [1][1120/1291]	Time 0.024 (0.053)	Data 1.43e-04 (2.69e-04)	Tok/s 161840 (201030)	Loss/tok 2.7319 (3.4364)	LR 2.875e-03
0: TRAIN [1][1130/1291]	Time 0.042 (0.053)	Data 8.56e-05 (2.68e-04)	Tok/s 183353 (201028)	Loss/tok 3.2542 (3.4359)	LR 2.875e-03
0: TRAIN [1][1140/1291]	Time 0.042 (0.053)	Data 8.68e-05 (2.66e-04)	Tok/s 185497 (200986)	Loss/tok 3.0907 (3.4355)	LR 2.875e-03
0: TRAIN [1][1150/1291]	Time 0.042 (0.053)	Data 8.70e-05 (2.65e-04)	Tok/s 183187 (201085)	Loss/tok 3.2091 (3.4358)	LR 2.875e-03
0: TRAIN [1][1160/1291]	Time 0.025 (0.053)	Data 8.54e-05 (2.63e-04)	Tok/s 160606 (201053)	Loss/tok 2.5964 (3.4346)	LR 2.875e-03
0: TRAIN [1][1170/1291]	Time 0.076 (0.053)	Data 8.87e-05 (2.62e-04)	Tok/s 230475 (201024)	Loss/tok 3.5665 (3.4342)	LR 2.875e-03
0: TRAIN [1][1180/1291]	Time 0.059 (0.053)	Data 8.46e-05 (2.61e-04)	Tok/s 217316 (200989)	Loss/tok 3.4641 (3.4343)	LR 2.875e-03
0: TRAIN [1][1190/1291]	Time 0.042 (0.053)	Data 8.63e-05 (2.59e-04)	Tok/s 188439 (200997)	Loss/tok 3.1245 (3.4340)	LR 2.875e-03
0: TRAIN [1][1200/1291]	Time 0.042 (0.053)	Data 1.45e-04 (2.58e-04)	Tok/s 189721 (201016)	Loss/tok 3.2152 (3.4345)	LR 2.875e-03
0: TRAIN [1][1210/1291]	Time 0.042 (0.053)	Data 8.80e-05 (2.57e-04)	Tok/s 183054 (200890)	Loss/tok 3.0504 (3.4332)	LR 2.875e-03
0: TRAIN [1][1220/1291]	Time 0.076 (0.053)	Data 8.82e-05 (2.55e-04)	Tok/s 229155 (200946)	Loss/tok 3.5644 (3.4327)	LR 2.875e-03
0: Upscaling, new scale: 4096.0
0: TRAIN [1][1230/1291]	Time 0.041 (0.053)	Data 8.25e-05 (2.54e-04)	Tok/s 187838 (201067)	Loss/tok 3.1150 (3.4330)	LR 2.875e-03
0: TRAIN [1][1240/1291]	Time 0.041 (0.053)	Data 8.65e-05 (2.53e-04)	Tok/s 184748 (201062)	Loss/tok 3.1010 (3.4320)	LR 2.875e-03
0: TRAIN [1][1250/1291]	Time 0.077 (0.053)	Data 8.25e-05 (2.52e-04)	Tok/s 229598 (201013)	Loss/tok 3.4079 (3.4312)	LR 2.875e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 2048.0
0: TRAIN [1][1260/1291]	Time 0.076 (0.053)	Data 8.44e-05 (2.50e-04)	Tok/s 233247 (200929)	Loss/tok 3.6086 (3.4305)	LR 2.875e-03
0: TRAIN [1][1270/1291]	Time 0.076 (0.053)	Data 9.16e-05 (2.49e-04)	Tok/s 225813 (201017)	Loss/tok 3.6435 (3.4301)	LR 2.875e-03
0: TRAIN [1][1280/1291]	Time 0.042 (0.053)	Data 8.56e-05 (2.48e-04)	Tok/s 189261 (200991)	Loss/tok 3.0590 (3.4289)	LR 2.875e-03
0: TRAIN [1][1290/1291]	Time 0.041 (0.053)	Data 4.01e-05 (2.48e-04)	Tok/s 190780 (201062)	Loss/tok 3.1418 (3.4286)	LR 2.875e-03
:::MLLOG {"namespace": "", "time_ms": 1593113222933, "event_type": "INTERVAL_END", "key": "epoch_stop", "value": null, "metadata": {"file": "train.py", "lineno": 551, "epoch_num": 2}}
:::MLLOG {"namespace": "", "time_ms": 1593113222934, "event_type": "INTERVAL_START", "key": "eval_start", "value": null, "metadata": {"file": "train.py", "lineno": 556, "epoch_num": 2}}
0: Running evaluation on test set
0: TEST [1][0/3]	Time 0.376 (0.376)	Decoder iters 149.0 (149.0)	Tok/s 23422 (23422)
0: Running moses detokenizer
0: BLEU(score=21.936463280314616, counts=[35851, 17140, 9419, 5421], totals=[65267, 62264, 59262, 56263], precisions=[54.929750103421334, 27.527945522292175, 15.893827410482265, 9.635106553152161], bp=1.0, sys_len=65267, ref_len=64676)
0: Finished evaluation on test set
:::MLLOG {"namespace": "", "time_ms": 1593113224189, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.2194, "metadata": {"file": "train.py", "lineno": 562, "epoch_num": 2}}
:::MLLOG {"namespace": "", "time_ms": 1593113224189, "event_type": "INTERVAL_END", "key": "eval_stop", "value": null, "metadata": {"file": "train.py", "lineno": 565, "epoch_num": 2}}
0: Summary: Epoch: 1	Training Loss: 3.4303	Test BLEU: 21.94
0: Performance: Epoch: 1	Training: 3217312 Tok/s
0: Finished epoch 1
:::MLLOG {"namespace": "", "time_ms": 1593113224189, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "train.py", "lineno": 584, "first_epoch_num": 2}}
:::MLLOG {"namespace": "", "time_ms": 1593113224189, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "train.py", "lineno": 538, "first_epoch_num": 3, "epoch_count": 1}}
:::MLLOG {"namespace": "", "time_ms": 1593113224189, "event_type": "INTERVAL_START", "key": "epoch_start", "value": null, "metadata": {"file": "train.py", "lineno": 541, "epoch_num": 3}}
0: Starting epoch 2
0: Sampler for epoch 2 uses seed 3884269883
0: TRAIN [2][0/1291]	Time 0.271 (0.271)	Data 1.89e-01 (1.89e-01)	Tok/s 14190 (14190)	Loss/tok 2.6223 (2.6223)	LR 2.875e-03
0: TRAIN [2][10/1291]	Time 0.042 (0.072)	Data 8.37e-05 (1.73e-02)	Tok/s 190417 (185606)	Loss/tok 3.0126 (3.2094)	LR 2.875e-03
0: TRAIN [2][20/1291]	Time 0.042 (0.067)	Data 8.18e-05 (9.12e-03)	Tok/s 188426 (196871)	Loss/tok 3.0645 (3.2839)	LR 2.875e-03
0: TRAIN [2][30/1291]	Time 0.098 (0.061)	Data 9.30e-05 (6.20e-03)	Tok/s 229238 (196783)	Loss/tok 3.5661 (3.2660)	LR 2.875e-03
0: TRAIN [2][40/1291]	Time 0.041 (0.059)	Data 8.99e-05 (4.71e-03)	Tok/s 189137 (197699)	Loss/tok 2.9954 (3.2698)	LR 2.875e-03
0: TRAIN [2][50/1291]	Time 0.042 (0.059)	Data 8.27e-05 (3.81e-03)	Tok/s 187893 (199433)	Loss/tok 2.9219 (3.2741)	LR 2.875e-03
0: TRAIN [2][60/1291]	Time 0.041 (0.058)	Data 8.94e-05 (3.20e-03)	Tok/s 192411 (200012)	Loss/tok 3.0593 (3.2644)	LR 2.875e-03
0: TRAIN [2][70/1291]	Time 0.041 (0.057)	Data 8.75e-05 (2.76e-03)	Tok/s 188860 (199765)	Loss/tok 3.0708 (3.2727)	LR 2.875e-03
0: TRAIN [2][80/1291]	Time 0.042 (0.055)	Data 8.65e-05 (2.43e-03)	Tok/s 183153 (198511)	Loss/tok 3.0374 (3.2591)	LR 2.875e-03
0: TRAIN [2][90/1291]	Time 0.024 (0.055)	Data 8.99e-05 (2.18e-03)	Tok/s 159214 (198698)	Loss/tok 2.6890 (3.2650)	LR 2.875e-03
0: Upscaling, new scale: 4096.0
0: TRAIN [2][100/1291]	Time 0.059 (0.055)	Data 9.20e-05 (1.97e-03)	Tok/s 217136 (199891)	Loss/tok 3.1914 (3.2692)	LR 2.875e-03
0: TRAIN [2][110/1291]	Time 0.041 (0.054)	Data 8.34e-05 (1.80e-03)	Tok/s 189411 (198965)	Loss/tok 3.0605 (3.2614)	LR 2.875e-03
0: TRAIN [2][120/1291]	Time 0.041 (0.053)	Data 9.85e-05 (1.66e-03)	Tok/s 188376 (197918)	Loss/tok 3.0435 (3.2548)	LR 2.875e-03
0: TRAIN [2][130/1291]	Time 0.041 (0.053)	Data 9.20e-05 (1.54e-03)	Tok/s 186317 (197733)	Loss/tok 3.1805 (3.2541)	LR 2.875e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 2048.0
0: TRAIN [2][140/1291]	Time 0.041 (0.052)	Data 1.05e-04 (1.44e-03)	Tok/s 192071 (198035)	Loss/tok 3.0274 (3.2508)	LR 2.875e-03
0: TRAIN [2][150/1291]	Time 0.041 (0.052)	Data 1.38e-04 (1.35e-03)	Tok/s 190296 (197649)	Loss/tok 3.0628 (3.2445)	LR 2.875e-03
0: TRAIN [2][160/1291]	Time 0.025 (0.052)	Data 8.85e-05 (1.27e-03)	Tok/s 161175 (198207)	Loss/tok 2.5797 (3.2515)	LR 2.875e-03
0: TRAIN [2][170/1291]	Time 0.041 (0.052)	Data 8.49e-05 (1.20e-03)	Tok/s 188811 (198093)	Loss/tok 3.1695 (3.2568)	LR 2.875e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 1024.0
0: TRAIN [2][180/1291]	Time 0.076 (0.052)	Data 8.25e-05 (1.14e-03)	Tok/s 228431 (198373)	Loss/tok 3.4901 (3.2592)	LR 2.875e-03
0: TRAIN [2][190/1291]	Time 0.058 (0.052)	Data 8.70e-05 (1.09e-03)	Tok/s 215024 (198577)	Loss/tok 3.2633 (3.2626)	LR 2.875e-03
0: TRAIN [2][200/1291]	Time 0.058 (0.052)	Data 8.63e-05 (1.04e-03)	Tok/s 214611 (198697)	Loss/tok 3.3689 (3.2603)	LR 2.875e-03
0: TRAIN [2][210/1291]	Time 0.041 (0.052)	Data 8.92e-05 (9.93e-04)	Tok/s 190616 (199081)	Loss/tok 3.1322 (3.2693)	LR 2.875e-03
0: TRAIN [2][220/1291]	Time 0.077 (0.053)	Data 1.41e-04 (9.53e-04)	Tok/s 230495 (199654)	Loss/tok 3.3854 (3.2730)	LR 2.875e-03
0: TRAIN [2][230/1291]	Time 0.041 (0.052)	Data 8.34e-05 (9.15e-04)	Tok/s 190958 (199208)	Loss/tok 3.0724 (3.2682)	LR 2.875e-03
0: TRAIN [2][240/1291]	Time 0.077 (0.052)	Data 7.89e-05 (8.81e-04)	Tok/s 230927 (199065)	Loss/tok 3.4292 (3.2667)	LR 2.875e-03
0: TRAIN [2][250/1291]	Time 0.059 (0.052)	Data 8.08e-05 (8.50e-04)	Tok/s 208318 (198586)	Loss/tok 3.4697 (3.2670)	LR 2.875e-03
0: TRAIN [2][260/1291]	Time 0.025 (0.052)	Data 8.56e-05 (8.21e-04)	Tok/s 156383 (198139)	Loss/tok 2.7008 (3.2662)	LR 2.875e-03
0: TRAIN [2][270/1291]	Time 0.043 (0.052)	Data 8.03e-05 (7.94e-04)	Tok/s 178547 (198160)	Loss/tok 2.9835 (3.2656)	LR 2.875e-03
0: TRAIN [2][280/1291]	Time 0.076 (0.052)	Data 8.20e-05 (7.69e-04)	Tok/s 229061 (198504)	Loss/tok 3.4908 (3.2692)	LR 2.875e-03
0: TRAIN [2][290/1291]	Time 0.042 (0.052)	Data 8.63e-05 (7.47e-04)	Tok/s 182266 (198926)	Loss/tok 3.0583 (3.2732)	LR 2.875e-03
0: TRAIN [2][300/1291]	Time 0.041 (0.052)	Data 1.64e-04 (7.25e-04)	Tok/s 184453 (198817)	Loss/tok 3.0589 (3.2684)	LR 2.875e-03
0: Upscaling, new scale: 2048.0
0: TRAIN [2][310/1291]	Time 0.041 (0.052)	Data 1.38e-04 (7.05e-04)	Tok/s 192307 (198670)	Loss/tok 3.0210 (3.2678)	LR 2.875e-03
0: TRAIN [2][320/1291]	Time 0.058 (0.052)	Data 8.03e-05 (6.86e-04)	Tok/s 214785 (198467)	Loss/tok 3.2775 (3.2654)	LR 2.875e-03
0: TRAIN [2][330/1291]	Time 0.041 (0.052)	Data 8.15e-05 (6.68e-04)	Tok/s 184726 (198408)	Loss/tok 3.0430 (3.2659)	LR 2.875e-03
0: TRAIN [2][340/1291]	Time 0.059 (0.052)	Data 1.45e-04 (6.51e-04)	Tok/s 211521 (199012)	Loss/tok 3.2227 (3.2677)	LR 2.875e-03
0: TRAIN [2][350/1291]	Time 0.076 (0.052)	Data 8.03e-05 (6.35e-04)	Tok/s 224351 (199126)	Loss/tok 3.5605 (3.2681)	LR 2.875e-03
0: TRAIN [2][360/1291]	Time 0.042 (0.052)	Data 8.54e-05 (6.21e-04)	Tok/s 182612 (199196)	Loss/tok 3.0197 (3.2694)	LR 2.875e-03
0: TRAIN [2][370/1291]	Time 0.076 (0.052)	Data 1.40e-04 (6.07e-04)	Tok/s 230050 (199583)	Loss/tok 3.3549 (3.2732)	LR 2.875e-03
0: TRAIN [2][380/1291]	Time 0.097 (0.052)	Data 8.23e-05 (5.93e-04)	Tok/s 230116 (199518)	Loss/tok 3.6540 (3.2731)	LR 2.875e-03
0: TRAIN [2][390/1291]	Time 0.024 (0.052)	Data 8.56e-05 (5.80e-04)	Tok/s 164767 (199403)	Loss/tok 2.6939 (3.2717)	LR 2.875e-03
0: TRAIN [2][400/1291]	Time 0.041 (0.052)	Data 9.73e-05 (5.68e-04)	Tok/s 186976 (199513)	Loss/tok 3.0865 (3.2714)	LR 2.875e-03
0: TRAIN [2][410/1291]	Time 0.059 (0.053)	Data 1.44e-04 (5.57e-04)	Tok/s 210218 (199782)	Loss/tok 3.2945 (3.2776)	LR 2.875e-03
0: TRAIN [2][420/1291]	Time 0.058 (0.053)	Data 8.25e-05 (5.46e-04)	Tok/s 215358 (199778)	Loss/tok 3.3599 (3.2782)	LR 2.875e-03
0: TRAIN [2][430/1291]	Time 0.041 (0.053)	Data 8.23e-05 (5.36e-04)	Tok/s 185103 (199835)	Loss/tok 3.0698 (3.2785)	LR 2.875e-03
0: Upscaling, new scale: 4096.0
0: TRAIN [2][440/1291]	Time 0.059 (0.053)	Data 8.75e-05 (5.26e-04)	Tok/s 214411 (199795)	Loss/tok 3.3919 (3.2793)	LR 2.875e-03
0: TRAIN [2][450/1291]	Time 0.058 (0.053)	Data 8.65e-05 (5.16e-04)	Tok/s 215498 (199924)	Loss/tok 3.2467 (3.2809)	LR 2.875e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 2048.0
0: TRAIN [2][460/1291]	Time 0.042 (0.053)	Data 8.89e-05 (5.07e-04)	Tok/s 183178 (199961)	Loss/tok 3.0472 (3.2823)	LR 2.875e-03
0: TRAIN [2][470/1291]	Time 0.058 (0.053)	Data 8.70e-05 (4.98e-04)	Tok/s 217446 (200091)	Loss/tok 3.3462 (3.2824)	LR 2.875e-03
0: TRAIN [2][480/1291]	Time 0.041 (0.053)	Data 8.85e-05 (4.90e-04)	Tok/s 187865 (200108)	Loss/tok 3.0617 (3.2839)	LR 2.875e-03
0: TRAIN [2][490/1291]	Time 0.042 (0.053)	Data 8.30e-05 (4.82e-04)	Tok/s 187523 (200082)	Loss/tok 3.0886 (3.2837)	LR 2.875e-03
0: TRAIN [2][500/1291]	Time 0.042 (0.053)	Data 1.42e-04 (4.74e-04)	Tok/s 188732 (200123)	Loss/tok 3.0185 (3.2826)	LR 2.875e-03
0: TRAIN [2][510/1291]	Time 0.076 (0.052)	Data 8.82e-05 (4.67e-04)	Tok/s 230414 (200008)	Loss/tok 3.5313 (3.2808)	LR 2.875e-03
0: TRAIN [2][520/1291]	Time 0.041 (0.052)	Data 8.27e-05 (4.59e-04)	Tok/s 191328 (199902)	Loss/tok 3.1629 (3.2789)	LR 2.875e-03
0: TRAIN [2][530/1291]	Time 0.041 (0.053)	Data 1.39e-04 (4.53e-04)	Tok/s 187027 (200040)	Loss/tok 3.1311 (3.2818)	LR 2.875e-03
0: TRAIN [2][540/1291]	Time 0.097 (0.052)	Data 8.39e-05 (4.46e-04)	Tok/s 233822 (199961)	Loss/tok 3.5507 (3.2816)	LR 2.875e-03
0: TRAIN [2][550/1291]	Time 0.041 (0.052)	Data 8.61e-05 (4.39e-04)	Tok/s 189845 (199929)	Loss/tok 3.1365 (3.2826)	LR 2.875e-03
0: TRAIN [2][560/1291]	Time 0.041 (0.052)	Data 1.40e-04 (4.33e-04)	Tok/s 187897 (199986)	Loss/tok 3.1131 (3.2821)	LR 2.875e-03
0: TRAIN [2][570/1291]	Time 0.042 (0.052)	Data 8.39e-05 (4.27e-04)	Tok/s 185226 (200069)	Loss/tok 3.0991 (3.2827)	LR 2.875e-03
0: Upscaling, new scale: 4096.0
0: TRAIN [2][580/1291]	Time 0.042 (0.053)	Data 8.51e-05 (4.22e-04)	Tok/s 185281 (200112)	Loss/tok 3.0598 (3.2836)	LR 2.875e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 2048.0
0: TRAIN [2][590/1291]	Time 0.043 (0.053)	Data 8.73e-05 (4.16e-04)	Tok/s 183169 (200204)	Loss/tok 3.0848 (3.2854)	LR 2.875e-03
0: TRAIN [2][600/1291]	Time 0.026 (0.053)	Data 8.65e-05 (4.11e-04)	Tok/s 157950 (200016)	Loss/tok 2.6708 (3.2852)	LR 2.875e-03
0: TRAIN [2][610/1291]	Time 0.043 (0.052)	Data 8.75e-05 (4.06e-04)	Tok/s 184021 (199865)	Loss/tok 3.0252 (3.2832)	LR 2.875e-03
0: TRAIN [2][620/1291]	Time 0.059 (0.052)	Data 8.61e-05 (4.00e-04)	Tok/s 211140 (199814)	Loss/tok 3.2197 (3.2830)	LR 2.875e-03
0: TRAIN [2][630/1291]	Time 0.042 (0.052)	Data 8.77e-05 (3.95e-04)	Tok/s 185074 (199676)	Loss/tok 3.1577 (3.2821)	LR 2.875e-03
0: TRAIN [2][640/1291]	Time 0.059 (0.052)	Data 8.82e-05 (3.91e-04)	Tok/s 211457 (199776)	Loss/tok 3.3673 (3.2837)	LR 2.875e-03
0: TRAIN [2][650/1291]	Time 0.077 (0.052)	Data 8.37e-05 (3.86e-04)	Tok/s 227184 (199656)	Loss/tok 3.4834 (3.2827)	LR 2.875e-03
0: TRAIN [2][660/1291]	Time 0.059 (0.052)	Data 8.46e-05 (3.82e-04)	Tok/s 215792 (199665)	Loss/tok 3.2664 (3.2824)	LR 2.875e-03
0: TRAIN [2][670/1291]	Time 0.042 (0.052)	Data 8.61e-05 (3.78e-04)	Tok/s 181765 (199633)	Loss/tok 3.0708 (3.2821)	LR 2.875e-03
0: TRAIN [2][680/1291]	Time 0.042 (0.052)	Data 8.49e-05 (3.73e-04)	Tok/s 185281 (199593)	Loss/tok 3.1028 (3.2822)	LR 2.875e-03
0: TRAIN [2][690/1291]	Time 0.041 (0.052)	Data 8.54e-05 (3.69e-04)	Tok/s 190099 (199704)	Loss/tok 3.1090 (3.2830)	LR 2.875e-03
0: TRAIN [2][700/1291]	Time 0.058 (0.053)	Data 1.44e-04 (3.65e-04)	Tok/s 216761 (199851)	Loss/tok 3.3265 (3.2834)	LR 2.875e-03
0: TRAIN [2][710/1291]	Time 0.097 (0.053)	Data 8.34e-05 (3.61e-04)	Tok/s 230596 (200044)	Loss/tok 3.6074 (3.2848)	LR 2.875e-03
0: Upscaling, new scale: 4096.0
0: TRAIN [2][720/1291]	Time 0.041 (0.053)	Data 8.61e-05 (3.58e-04)	Tok/s 187614 (200092)	Loss/tok 3.1554 (3.2849)	LR 2.875e-03
0: TRAIN [2][730/1291]	Time 0.076 (0.053)	Data 8.44e-05 (3.54e-04)	Tok/s 231575 (200176)	Loss/tok 3.4212 (3.2844)	LR 2.875e-03
0: TRAIN [2][740/1291]	Time 0.058 (0.053)	Data 8.68e-05 (3.51e-04)	Tok/s 217491 (200383)	Loss/tok 3.2755 (3.2849)	LR 2.875e-03
0: TRAIN [2][750/1291]	Time 0.041 (0.053)	Data 8.68e-05 (3.47e-04)	Tok/s 192497 (200354)	Loss/tok 3.0339 (3.2844)	LR 2.875e-03
0: TRAIN [2][760/1291]	Time 0.042 (0.053)	Data 1.41e-04 (3.44e-04)	Tok/s 187634 (200323)	Loss/tok 3.0687 (3.2836)	LR 2.875e-03
0: TRAIN [2][770/1291]	Time 0.060 (0.053)	Data 8.46e-05 (3.41e-04)	Tok/s 212436 (200379)	Loss/tok 3.3532 (3.2841)	LR 2.875e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 2048.0
0: Gradient norm: inf
0: Skipped batch, new scale: 1024.0
0: TRAIN [2][780/1291]	Time 0.060 (0.053)	Data 8.82e-05 (3.38e-04)	Tok/s 210018 (200217)	Loss/tok 3.3113 (3.2839)	LR 2.875e-03
0: TRAIN [2][790/1291]	Time 0.059 (0.053)	Data 8.68e-05 (3.35e-04)	Tok/s 215830 (200199)	Loss/tok 3.2000 (3.2841)	LR 2.875e-03
0: TRAIN [2][800/1291]	Time 0.076 (0.053)	Data 8.85e-05 (3.32e-04)	Tok/s 224157 (200174)	Loss/tok 3.6180 (3.2843)	LR 2.875e-03
0: TRAIN [2][810/1291]	Time 0.025 (0.053)	Data 1.30e-04 (3.29e-04)	Tok/s 158520 (200121)	Loss/tok 2.7592 (3.2854)	LR 2.875e-03
0: TRAIN [2][820/1291]	Time 0.043 (0.053)	Data 8.42e-05 (3.26e-04)	Tok/s 182901 (200117)	Loss/tok 3.0804 (3.2856)	LR 2.875e-03
0: TRAIN [2][830/1291]	Time 0.097 (0.053)	Data 8.68e-05 (3.23e-04)	Tok/s 224817 (200115)	Loss/tok 3.7562 (3.2863)	LR 2.875e-03
0: TRAIN [2][840/1291]	Time 0.042 (0.053)	Data 8.39e-05 (3.20e-04)	Tok/s 186667 (200061)	Loss/tok 3.1289 (3.2858)	LR 2.875e-03
0: TRAIN [2][850/1291]	Time 0.077 (0.053)	Data 8.54e-05 (3.18e-04)	Tok/s 225447 (200021)	Loss/tok 3.4120 (3.2864)	LR 2.875e-03
0: TRAIN [2][860/1291]	Time 0.097 (0.053)	Data 8.61e-05 (3.15e-04)	Tok/s 231869 (200105)	Loss/tok 3.6583 (3.2874)	LR 2.875e-03
0: TRAIN [2][870/1291]	Time 0.060 (0.053)	Data 8.54e-05 (3.12e-04)	Tok/s 212080 (200266)	Loss/tok 3.2418 (3.2889)	LR 2.875e-03
0: TRAIN [2][880/1291]	Time 0.042 (0.053)	Data 1.42e-04 (3.10e-04)	Tok/s 182170 (200006)	Loss/tok 3.0467 (3.2868)	LR 2.875e-03
0: TRAIN [2][890/1291]	Time 0.042 (0.053)	Data 8.15e-05 (3.08e-04)	Tok/s 185318 (199999)	Loss/tok 2.9656 (3.2867)	LR 2.875e-03
0: TRAIN [2][900/1291]	Time 0.043 (0.053)	Data 8.56e-05 (3.05e-04)	Tok/s 182548 (200082)	Loss/tok 3.1209 (3.2880)	LR 2.875e-03
0: Upscaling, new scale: 2048.0
0: TRAIN [2][910/1291]	Time 0.025 (0.053)	Data 8.44e-05 (3.03e-04)	Tok/s 155840 (199954)	Loss/tok 2.7638 (3.2867)	LR 2.875e-03
0: TRAIN [2][920/1291]	Time 0.042 (0.053)	Data 1.41e-04 (3.01e-04)	Tok/s 186758 (199926)	Loss/tok 3.0902 (3.2863)	LR 2.875e-03
0: TRAIN [2][930/1291]	Time 0.042 (0.053)	Data 8.51e-05 (2.98e-04)	Tok/s 186013 (199931)	Loss/tok 3.0023 (3.2862)	LR 2.875e-03
0: TRAIN [2][940/1291]	Time 0.042 (0.053)	Data 8.42e-05 (2.96e-04)	Tok/s 182050 (199931)	Loss/tok 3.0206 (3.2864)	LR 2.875e-03
0: TRAIN [2][950/1291]	Time 0.059 (0.053)	Data 8.44e-05 (2.94e-04)	Tok/s 214448 (199889)	Loss/tok 3.3603 (3.2857)	LR 2.875e-03
0: TRAIN [2][960/1291]	Time 0.077 (0.053)	Data 8.89e-05 (2.92e-04)	Tok/s 228845 (199794)	Loss/tok 3.4666 (3.2859)	LR 2.875e-03
0: TRAIN [2][970/1291]	Time 0.042 (0.053)	Data 1.38e-04 (2.90e-04)	Tok/s 183745 (199772)	Loss/tok 3.0245 (3.2860)	LR 2.875e-03
0: TRAIN [2][980/1291]	Time 0.043 (0.053)	Data 8.13e-05 (2.88e-04)	Tok/s 181727 (199735)	Loss/tok 2.9909 (3.2858)	LR 2.875e-03
0: TRAIN [2][990/1291]	Time 0.078 (0.053)	Data 8.30e-05 (2.86e-04)	Tok/s 224823 (199811)	Loss/tok 3.5285 (3.2873)	LR 2.875e-03
0: TRAIN [2][1000/1291]	Time 0.078 (0.053)	Data 8.06e-05 (2.84e-04)	Tok/s 223857 (199788)	Loss/tok 3.3533 (3.2878)	LR 2.875e-03
0: TRAIN [2][1010/1291]	Time 0.059 (0.053)	Data 8.61e-05 (2.82e-04)	Tok/s 213698 (199810)	Loss/tok 3.4654 (3.2879)	LR 2.875e-03
0: TRAIN [2][1020/1291]	Time 0.043 (0.053)	Data 8.30e-05 (2.81e-04)	Tok/s 179729 (199773)	Loss/tok 3.1439 (3.2874)	LR 2.875e-03
0: TRAIN [2][1030/1291]	Time 0.026 (0.053)	Data 8.73e-05 (2.79e-04)	Tok/s 154688 (199736)	Loss/tok 2.6970 (3.2868)	LR 2.875e-03
0: Upscaling, new scale: 4096.0
0: TRAIN [2][1040/1291]	Time 0.042 (0.053)	Data 8.11e-05 (2.77e-04)	Tok/s 182651 (199789)	Loss/tok 2.9849 (3.2864)	LR 2.875e-03
0: TRAIN [2][1050/1291]	Time 0.043 (0.053)	Data 8.27e-05 (2.75e-04)	Tok/s 186074 (199816)	Loss/tok 2.9706 (3.2865)	LR 2.875e-03
0: TRAIN [2][1060/1291]	Time 0.060 (0.053)	Data 1.41e-04 (2.74e-04)	Tok/s 211007 (199936)	Loss/tok 3.3192 (3.2873)	LR 2.875e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 2048.0
0: TRAIN [2][1070/1291]	Time 0.042 (0.053)	Data 7.92e-05 (2.72e-04)	Tok/s 177572 (199929)	Loss/tok 2.9296 (3.2865)	LR 2.875e-03
0: TRAIN [2][1080/1291]	Time 0.077 (0.053)	Data 8.20e-05 (2.70e-04)	Tok/s 227690 (199825)	Loss/tok 3.3795 (3.2856)	LR 2.875e-03
0: TRAIN [2][1090/1291]	Time 0.042 (0.053)	Data 8.75e-05 (2.68e-04)	Tok/s 188732 (199798)	Loss/tok 3.1169 (3.2846)	LR 2.875e-03
0: TRAIN [2][1100/1291]	Time 0.043 (0.053)	Data 1.41e-04 (2.67e-04)	Tok/s 180515 (199780)	Loss/tok 3.1371 (3.2846)	LR 2.875e-03
0: TRAIN [2][1110/1291]	Time 0.077 (0.053)	Data 1.51e-04 (2.65e-04)	Tok/s 227555 (199862)	Loss/tok 3.4826 (3.2857)	LR 2.875e-03
0: TRAIN [2][1120/1291]	Time 0.076 (0.053)	Data 1.46e-04 (2.64e-04)	Tok/s 231219 (199820)	Loss/tok 3.4305 (3.2853)	LR 2.875e-03
0: TRAIN [2][1130/1291]	Time 0.076 (0.053)	Data 8.61e-05 (2.62e-04)	Tok/s 227738 (199885)	Loss/tok 3.4189 (3.2857)	LR 2.875e-03
0: TRAIN [2][1140/1291]	Time 0.058 (0.053)	Data 7.96e-05 (2.61e-04)	Tok/s 218302 (200001)	Loss/tok 3.2788 (3.2859)	LR 2.875e-03
0: TRAIN [2][1150/1291]	Time 0.058 (0.053)	Data 8.44e-05 (2.59e-04)	Tok/s 216792 (200022)	Loss/tok 3.2017 (3.2850)	LR 2.875e-03
0: TRAIN [2][1160/1291]	Time 0.059 (0.053)	Data 9.25e-05 (2.58e-04)	Tok/s 209654 (200031)	Loss/tok 3.3523 (3.2846)	LR 2.875e-03
0: TRAIN [2][1170/1291]	Time 0.058 (0.053)	Data 8.46e-05 (2.56e-04)	Tok/s 218861 (200076)	Loss/tok 3.2476 (3.2854)	LR 2.875e-03
0: TRAIN [2][1180/1291]	Time 0.042 (0.053)	Data 8.03e-05 (2.55e-04)	Tok/s 189105 (200027)	Loss/tok 3.1872 (3.2846)	LR 2.875e-03
0: TRAIN [2][1190/1291]	Time 0.025 (0.053)	Data 1.10e-04 (2.54e-04)	Tok/s 160983 (200038)	Loss/tok 2.5929 (3.2850)	LR 2.875e-03
0: Upscaling, new scale: 4096.0
0: TRAIN [2][1200/1291]	Time 0.042 (0.053)	Data 7.84e-05 (2.52e-04)	Tok/s 184869 (200121)	Loss/tok 3.0124 (3.2849)	LR 2.875e-03
0: TRAIN [2][1210/1291]	Time 0.097 (0.053)	Data 8.03e-05 (2.51e-04)	Tok/s 228788 (200255)	Loss/tok 3.5337 (3.2859)	LR 2.875e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 2048.0
0: TRAIN [2][1220/1291]	Time 0.025 (0.053)	Data 8.51e-05 (2.50e-04)	Tok/s 159225 (200250)	Loss/tok 2.7007 (3.2861)	LR 2.875e-03
0: TRAIN [2][1230/1291]	Time 0.041 (0.053)	Data 7.99e-05 (2.48e-04)	Tok/s 188673 (200198)	Loss/tok 3.0532 (3.2852)	LR 2.875e-03
0: TRAIN [2][1240/1291]	Time 0.059 (0.053)	Data 8.39e-05 (2.47e-04)	Tok/s 213509 (200184)	Loss/tok 3.2358 (3.2845)	LR 2.875e-03
0: TRAIN [2][1250/1291]	Time 0.025 (0.053)	Data 8.42e-05 (2.46e-04)	Tok/s 161819 (200121)	Loss/tok 2.7285 (3.2837)	LR 2.875e-03
0: TRAIN [2][1260/1291]	Time 0.058 (0.053)	Data 1.40e-04 (2.45e-04)	Tok/s 216658 (200198)	Loss/tok 3.3123 (3.2852)	LR 2.875e-03
0: TRAIN [2][1270/1291]	Time 0.024 (0.053)	Data 8.75e-05 (2.43e-04)	Tok/s 164937 (200072)	Loss/tok 2.7642 (3.2841)	LR 2.875e-03
0: TRAIN [2][1280/1291]	Time 0.076 (0.053)	Data 1.38e-04 (2.42e-04)	Tok/s 232410 (200085)	Loss/tok 3.4056 (3.2840)	LR 2.875e-03
0: TRAIN [2][1290/1291]	Time 0.041 (0.053)	Data 3.96e-05 (2.42e-04)	Tok/s 184244 (200106)	Loss/tok 3.0840 (3.2847)	LR 2.875e-03
:::MLLOG {"namespace": "", "time_ms": 1593113292907, "event_type": "INTERVAL_END", "key": "epoch_stop", "value": null, "metadata": {"file": "train.py", "lineno": 551, "epoch_num": 3}}
:::MLLOG {"namespace": "", "time_ms": 1593113292908, "event_type": "INTERVAL_START", "key": "eval_start", "value": null, "metadata": {"file": "train.py", "lineno": 556, "epoch_num": 3}}
0: Running evaluation on test set
0: TEST [2][0/3]	Time 0.391 (0.391)	Decoder iters 149.0 (149.0)	Tok/s 23647 (23647)
0: Running moses detokenizer
0: BLEU(score=22.109012580592342, counts=[36624, 17757, 9825, 5678], totals=[67017, 64014, 61011, 58012], precisions=[54.64882044854291, 27.73924454025682, 16.103653439543688, 9.78763014548714], bp=1.0, sys_len=67017, ref_len=64676)
0: Finished evaluation on test set
:::MLLOG {"namespace": "", "time_ms": 1593113294227, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.2211, "metadata": {"file": "train.py", "lineno": 562, "epoch_num": 3}}
:::MLLOG {"namespace": "", "time_ms": 1593113294227, "event_type": "INTERVAL_END", "key": "eval_stop", "value": null, "metadata": {"file": "train.py", "lineno": 565, "epoch_num": 3}}
0: Summary: Epoch: 2	Training Loss: 3.2845	Test BLEU: 22.11
0: Performance: Epoch: 2	Training: 3198317 Tok/s
0: Finished epoch 2
:::MLLOG {"namespace": "", "time_ms": 1593113294227, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "train.py", "lineno": 584, "first_epoch_num": 3}}
:::MLLOG {"namespace": "", "time_ms": 1593113294227, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "train.py", "lineno": 538, "first_epoch_num": 4, "epoch_count": 1}}
:::MLLOG {"namespace": "", "time_ms": 1593113294227, "event_type": "INTERVAL_START", "key": "epoch_start", "value": null, "metadata": {"file": "train.py", "lineno": 541, "epoch_num": 4}}
0: Starting epoch 3
0: Sampler for epoch 3 uses seed 1668003241
0: TRAIN [3][0/1291]	Time 0.282 (0.282)	Data 1.97e-01 (1.97e-01)	Tok/s 27636 (27636)	Loss/tok 3.1523 (3.1523)	LR 2.875e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 1024.0
0: TRAIN [3][10/1291]	Time 0.058 (0.068)	Data 9.44e-05 (1.80e-02)	Tok/s 215443 (178687)	Loss/tok 3.1697 (3.1845)	LR 2.875e-03
0: TRAIN [3][20/1291]	Time 0.041 (0.062)	Data 9.58e-05 (9.49e-03)	Tok/s 183143 (190160)	Loss/tok 2.9792 (3.2018)	LR 2.875e-03
0: TRAIN [3][30/1291]	Time 0.042 (0.058)	Data 8.39e-05 (6.46e-03)	Tok/s 183358 (193660)	Loss/tok 2.9823 (3.1715)	LR 2.875e-03
0: TRAIN [3][40/1291]	Time 0.058 (0.055)	Data 8.89e-05 (4.90e-03)	Tok/s 216248 (193474)	Loss/tok 3.2838 (3.1687)	LR 2.875e-03
0: TRAIN [3][50/1291]	Time 0.042 (0.055)	Data 9.16e-05 (3.96e-03)	Tok/s 185278 (195782)	Loss/tok 2.9119 (3.1609)	LR 2.875e-03
0: TRAIN [3][60/1291]	Time 0.041 (0.054)	Data 9.44e-05 (3.33e-03)	Tok/s 188539 (197137)	Loss/tok 3.1208 (3.1618)	LR 2.875e-03
0: TRAIN [3][70/1291]	Time 0.041 (0.055)	Data 9.58e-05 (2.87e-03)	Tok/s 180248 (198348)	Loss/tok 3.0100 (3.1680)	LR 2.875e-03
0: TRAIN [3][80/1291]	Time 0.041 (0.054)	Data 8.89e-05 (2.53e-03)	Tok/s 186935 (198849)	Loss/tok 3.0388 (3.1758)	LR 2.875e-03
0: TRAIN [3][90/1291]	Time 0.059 (0.054)	Data 9.11e-05 (2.26e-03)	Tok/s 211422 (199111)	Loss/tok 3.2687 (3.1718)	LR 2.875e-03
0: TRAIN [3][100/1291]	Time 0.058 (0.054)	Data 8.68e-05 (2.04e-03)	Tok/s 215891 (199553)	Loss/tok 3.1264 (3.1679)	LR 2.875e-03
0: TRAIN [3][110/1291]	Time 0.041 (0.053)	Data 8.85e-05 (1.87e-03)	Tok/s 187344 (198666)	Loss/tok 2.9456 (3.1643)	LR 2.875e-03
0: TRAIN [3][120/1291]	Time 0.076 (0.054)	Data 8.34e-05 (1.72e-03)	Tok/s 230143 (199199)	Loss/tok 3.3762 (3.1708)	LR 2.875e-03
0: TRAIN [3][130/1291]	Time 0.059 (0.054)	Data 9.25e-05 (1.60e-03)	Tok/s 215261 (199577)	Loss/tok 3.2120 (3.1745)	LR 2.875e-03
0: Upscaling, new scale: 2048.0
0: TRAIN [3][140/1291]	Time 0.058 (0.054)	Data 8.82e-05 (1.49e-03)	Tok/s 210981 (199857)	Loss/tok 3.3360 (3.1733)	LR 2.875e-03
0: TRAIN [3][150/1291]	Time 0.059 (0.054)	Data 9.56e-05 (1.40e-03)	Tok/s 220301 (199961)	Loss/tok 3.2762 (3.1789)	LR 2.875e-03
0: TRAIN [3][160/1291]	Time 0.041 (0.053)	Data 8.51e-05 (1.32e-03)	Tok/s 182682 (199756)	Loss/tok 3.0786 (3.1787)	LR 2.875e-03
0: TRAIN [3][170/1291]	Time 0.058 (0.054)	Data 8.65e-05 (1.24e-03)	Tok/s 217586 (200383)	Loss/tok 3.2610 (3.1793)	LR 2.875e-03
0: TRAIN [3][180/1291]	Time 0.042 (0.054)	Data 9.13e-05 (1.18e-03)	Tok/s 186380 (200628)	Loss/tok 3.0114 (3.1788)	LR 2.875e-03
0: TRAIN [3][190/1291]	Time 0.042 (0.054)	Data 8.58e-05 (1.12e-03)	Tok/s 184512 (200690)	Loss/tok 3.0609 (3.1848)	LR 2.875e-03
0: TRAIN [3][200/1291]	Time 0.042 (0.054)	Data 8.70e-05 (1.07e-03)	Tok/s 184147 (200967)	Loss/tok 2.9508 (3.1836)	LR 2.875e-03
0: TRAIN [3][210/1291]	Time 0.041 (0.054)	Data 8.92e-05 (1.02e-03)	Tok/s 189819 (201266)	Loss/tok 2.9358 (3.1909)	LR 1.437e-03
0: TRAIN [3][220/1291]	Time 0.041 (0.053)	Data 8.49e-05 (9.82e-04)	Tok/s 188248 (200208)	Loss/tok 3.0468 (3.1832)	LR 1.437e-03
0: TRAIN [3][230/1291]	Time 0.076 (0.053)	Data 1.02e-04 (9.44e-04)	Tok/s 230745 (200376)	Loss/tok 3.3087 (3.1853)	LR 1.437e-03
0: TRAIN [3][240/1291]	Time 0.076 (0.053)	Data 8.94e-05 (9.08e-04)	Tok/s 227957 (200532)	Loss/tok 3.3009 (3.1876)	LR 1.437e-03
0: TRAIN [3][250/1291]	Time 0.025 (0.053)	Data 8.80e-05 (8.76e-04)	Tok/s 157509 (200474)	Loss/tok 2.4276 (3.1899)	LR 1.437e-03
0: TRAIN [3][260/1291]	Time 0.041 (0.053)	Data 8.56e-05 (8.46e-04)	Tok/s 186377 (200486)	Loss/tok 2.9929 (3.1882)	LR 1.437e-03
0: Upscaling, new scale: 4096.0
0: TRAIN [3][270/1291]	Time 0.059 (0.053)	Data 8.70e-05 (8.18e-04)	Tok/s 213809 (200242)	Loss/tok 3.1427 (3.1867)	LR 1.437e-03
0: TRAIN [3][280/1291]	Time 0.041 (0.053)	Data 1.51e-04 (7.92e-04)	Tok/s 190420 (199969)	Loss/tok 3.0058 (3.1827)	LR 1.437e-03
0: TRAIN [3][290/1291]	Time 0.076 (0.053)	Data 1.58e-04 (7.69e-04)	Tok/s 230736 (200190)	Loss/tok 3.3466 (3.1829)	LR 1.437e-03
0: TRAIN [3][300/1291]	Time 0.062 (0.053)	Data 8.75e-05 (7.46e-04)	Tok/s 124078 (199501)	Loss/tok 2.9128 (3.1788)	LR 1.437e-03
0: TRAIN [3][310/1291]	Time 0.058 (0.053)	Data 8.61e-05 (7.26e-04)	Tok/s 216353 (199936)	Loss/tok 3.1841 (3.1819)	LR 1.437e-03
0: TRAIN [3][320/1291]	Time 0.042 (0.053)	Data 8.63e-05 (7.06e-04)	Tok/s 188983 (199859)	Loss/tok 3.0145 (3.1802)	LR 1.437e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 2048.0
0: TRAIN [3][330/1291]	Time 0.058 (0.053)	Data 8.58e-05 (6.87e-04)	Tok/s 213543 (200470)	Loss/tok 3.1423 (3.1858)	LR 1.437e-03
0: TRAIN [3][340/1291]	Time 0.041 (0.053)	Data 8.63e-05 (6.70e-04)	Tok/s 183469 (200387)	Loss/tok 2.9490 (3.1871)	LR 1.437e-03
0: TRAIN [3][350/1291]	Time 0.041 (0.053)	Data 1.44e-04 (6.54e-04)	Tok/s 190534 (200290)	Loss/tok 2.9830 (3.1840)	LR 1.437e-03
0: TRAIN [3][360/1291]	Time 0.077 (0.053)	Data 8.63e-05 (6.38e-04)	Tok/s 229187 (200113)	Loss/tok 3.2282 (3.1815)	LR 1.437e-03
0: TRAIN [3][370/1291]	Time 0.058 (0.053)	Data 8.54e-05 (6.23e-04)	Tok/s 220059 (200246)	Loss/tok 3.1245 (3.1810)	LR 1.437e-03
0: TRAIN [3][380/1291]	Time 0.041 (0.053)	Data 1.45e-04 (6.10e-04)	Tok/s 185713 (200334)	Loss/tok 2.8497 (3.1812)	LR 1.437e-03
0: TRAIN [3][390/1291]	Time 0.077 (0.053)	Data 8.63e-05 (5.96e-04)	Tok/s 229081 (200286)	Loss/tok 3.3084 (3.1784)	LR 1.437e-03
0: TRAIN [3][400/1291]	Time 0.058 (0.053)	Data 1.39e-04 (5.84e-04)	Tok/s 217302 (200501)	Loss/tok 3.0942 (3.1787)	LR 1.437e-03
0: TRAIN [3][410/1291]	Time 0.096 (0.053)	Data 1.40e-04 (5.72e-04)	Tok/s 232248 (200326)	Loss/tok 3.5065 (3.1779)	LR 1.437e-03
0: TRAIN [3][420/1291]	Time 0.076 (0.053)	Data 8.82e-05 (5.61e-04)	Tok/s 229711 (200512)	Loss/tok 3.1549 (3.1774)	LR 1.437e-03
0: TRAIN [3][430/1291]	Time 0.058 (0.053)	Data 1.41e-04 (5.50e-04)	Tok/s 213694 (200547)	Loss/tok 3.1568 (3.1753)	LR 1.437e-03
0: TRAIN [3][440/1291]	Time 0.076 (0.053)	Data 8.75e-05 (5.40e-04)	Tok/s 230216 (200544)	Loss/tok 3.3318 (3.1736)	LR 1.437e-03
0: TRAIN [3][450/1291]	Time 0.058 (0.053)	Data 8.80e-05 (5.30e-04)	Tok/s 214604 (200452)	Loss/tok 3.2458 (3.1715)	LR 1.437e-03
0: Upscaling, new scale: 4096.0
0: TRAIN [3][460/1291]	Time 0.041 (0.053)	Data 8.61e-05 (5.21e-04)	Tok/s 189571 (200353)	Loss/tok 2.9405 (3.1697)	LR 1.437e-03
0: TRAIN [3][470/1291]	Time 0.097 (0.053)	Data 8.68e-05 (5.11e-04)	Tok/s 227296 (200315)	Loss/tok 3.6982 (3.1716)	LR 1.437e-03
0: TRAIN [3][480/1291]	Time 0.042 (0.053)	Data 8.25e-05 (5.03e-04)	Tok/s 190379 (200206)	Loss/tok 3.0160 (3.1700)	LR 1.437e-03
0: TRAIN [3][490/1291]	Time 0.042 (0.053)	Data 1.47e-04 (4.95e-04)	Tok/s 185939 (200363)	Loss/tok 2.9813 (3.1699)	LR 1.437e-03
0: TRAIN [3][500/1291]	Time 0.059 (0.053)	Data 8.96e-05 (4.87e-04)	Tok/s 216298 (200162)	Loss/tok 3.1371 (3.1680)	LR 1.437e-03
0: TRAIN [3][510/1291]	Time 0.041 (0.053)	Data 8.51e-05 (4.79e-04)	Tok/s 188299 (200261)	Loss/tok 2.9648 (3.1692)	LR 1.437e-03
0: TRAIN [3][520/1291]	Time 0.042 (0.053)	Data 9.11e-05 (4.72e-04)	Tok/s 185584 (200314)	Loss/tok 3.0318 (3.1690)	LR 1.437e-03
0: TRAIN [3][530/1291]	Time 0.042 (0.053)	Data 8.96e-05 (4.65e-04)	Tok/s 187992 (200159)	Loss/tok 3.0104 (3.1677)	LR 1.437e-03
0: TRAIN [3][540/1291]	Time 0.042 (0.053)	Data 8.68e-05 (4.58e-04)	Tok/s 185008 (200096)	Loss/tok 2.9424 (3.1660)	LR 1.437e-03
0: TRAIN [3][550/1291]	Time 0.059 (0.053)	Data 8.70e-05 (4.52e-04)	Tok/s 210091 (200124)	Loss/tok 3.1708 (3.1665)	LR 1.437e-03
0: TRAIN [3][560/1291]	Time 0.042 (0.053)	Data 8.51e-05 (4.46e-04)	Tok/s 183501 (200054)	Loss/tok 2.9796 (3.1655)	LR 1.437e-03
0: TRAIN [3][570/1291]	Time 0.058 (0.053)	Data 8.37e-05 (4.40e-04)	Tok/s 214834 (200035)	Loss/tok 3.1372 (3.1663)	LR 1.437e-03
0: TRAIN [3][580/1291]	Time 0.058 (0.053)	Data 8.06e-05 (4.34e-04)	Tok/s 218593 (199926)	Loss/tok 3.0452 (3.1646)	LR 1.437e-03
0: Upscaling, new scale: 8192.0
0: TRAIN [3][590/1291]	Time 0.097 (0.053)	Data 8.37e-05 (4.28e-04)	Tok/s 235302 (200116)	Loss/tok 3.5372 (3.1660)	LR 1.437e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 4096.0
0: TRAIN [3][600/1291]	Time 0.041 (0.053)	Data 7.70e-05 (4.22e-04)	Tok/s 185353 (200205)	Loss/tok 2.9007 (3.1658)	LR 1.437e-03
0: TRAIN [3][610/1291]	Time 0.041 (0.053)	Data 1.38e-04 (4.17e-04)	Tok/s 185949 (200153)	Loss/tok 2.9356 (3.1642)	LR 1.437e-03
0: TRAIN [3][620/1291]	Time 0.041 (0.052)	Data 8.46e-05 (4.12e-04)	Tok/s 189900 (200044)	Loss/tok 2.9294 (3.1627)	LR 1.437e-03
0: TRAIN [3][630/1291]	Time 0.058 (0.052)	Data 1.36e-04 (4.06e-04)	Tok/s 216765 (200087)	Loss/tok 3.0457 (3.1610)	LR 1.437e-03
0: TRAIN [3][640/1291]	Time 0.058 (0.053)	Data 8.20e-05 (4.02e-04)	Tok/s 217461 (200080)	Loss/tok 3.0698 (3.1616)	LR 1.437e-03
0: TRAIN [3][650/1291]	Time 0.058 (0.053)	Data 1.41e-04 (3.97e-04)	Tok/s 214980 (200176)	Loss/tok 3.2288 (3.1622)	LR 1.437e-03
0: TRAIN [3][660/1291]	Time 0.041 (0.053)	Data 8.75e-05 (3.92e-04)	Tok/s 191978 (200158)	Loss/tok 2.9851 (3.1621)	LR 1.437e-03
0: TRAIN [3][670/1291]	Time 0.058 (0.053)	Data 8.46e-05 (3.88e-04)	Tok/s 215930 (200232)	Loss/tok 3.2924 (3.1626)	LR 1.437e-03
0: TRAIN [3][680/1291]	Time 0.041 (0.053)	Data 8.27e-05 (3.83e-04)	Tok/s 188949 (200102)	Loss/tok 2.8754 (3.1614)	LR 1.437e-03
0: TRAIN [3][690/1291]	Time 0.041 (0.052)	Data 8.20e-05 (3.79e-04)	Tok/s 185078 (200030)	Loss/tok 2.9261 (3.1594)	LR 1.437e-03
0: TRAIN [3][700/1291]	Time 0.058 (0.053)	Data 8.37e-05 (3.75e-04)	Tok/s 215746 (200217)	Loss/tok 3.1232 (3.1602)	LR 1.437e-03
0: TRAIN [3][710/1291]	Time 0.041 (0.053)	Data 8.44e-05 (3.71e-04)	Tok/s 187145 (200254)	Loss/tok 2.9247 (3.1582)	LR 1.437e-03
0: TRAIN [3][720/1291]	Time 0.058 (0.053)	Data 1.56e-04 (3.67e-04)	Tok/s 214408 (200335)	Loss/tok 3.2261 (3.1581)	LR 7.187e-04
0: Upscaling, new scale: 8192.0
0: TRAIN [3][730/1291]	Time 0.025 (0.053)	Data 8.18e-05 (3.64e-04)	Tok/s 155528 (200310)	Loss/tok 2.5537 (3.1575)	LR 7.187e-04
0: TRAIN [3][740/1291]	Time 0.058 (0.053)	Data 9.39e-05 (3.60e-04)	Tok/s 212824 (200377)	Loss/tok 3.2101 (3.1578)	LR 7.187e-04
0: TRAIN [3][750/1291]	Time 0.041 (0.053)	Data 8.85e-05 (3.57e-04)	Tok/s 184521 (200296)	Loss/tok 2.8782 (3.1563)	LR 7.187e-04
0: Gradient norm: inf
0: Skipped batch, new scale: 4096.0
0: TRAIN [3][760/1291]	Time 0.025 (0.053)	Data 8.44e-05 (3.53e-04)	Tok/s 158831 (200256)	Loss/tok 2.5508 (3.1564)	LR 7.187e-04
0: TRAIN [3][770/1291]	Time 0.058 (0.053)	Data 8.06e-05 (3.50e-04)	Tok/s 218551 (200372)	Loss/tok 3.1367 (3.1571)	LR 7.187e-04
0: TRAIN [3][780/1291]	Time 0.041 (0.053)	Data 8.63e-05 (3.47e-04)	Tok/s 185002 (200258)	Loss/tok 2.9579 (3.1565)	LR 7.187e-04
0: TRAIN [3][790/1291]	Time 0.025 (0.053)	Data 8.37e-05 (3.43e-04)	Tok/s 159398 (200251)	Loss/tok 2.5891 (3.1562)	LR 7.187e-04
0: TRAIN [3][800/1291]	Time 0.076 (0.053)	Data 8.25e-05 (3.40e-04)	Tok/s 233284 (200273)	Loss/tok 3.3439 (3.1559)	LR 7.187e-04
0: TRAIN [3][810/1291]	Time 0.058 (0.053)	Data 1.27e-04 (3.37e-04)	Tok/s 216477 (200245)	Loss/tok 3.0127 (3.1545)	LR 7.187e-04
0: TRAIN [3][820/1291]	Time 0.025 (0.053)	Data 8.11e-05 (3.34e-04)	Tok/s 155011 (200313)	Loss/tok 2.5068 (3.1546)	LR 7.187e-04
0: TRAIN [3][830/1291]	Time 0.058 (0.052)	Data 8.58e-05 (3.31e-04)	Tok/s 213327 (200165)	Loss/tok 3.2382 (3.1532)	LR 7.187e-04
0: TRAIN [3][840/1291]	Time 0.076 (0.053)	Data 8.23e-05 (3.28e-04)	Tok/s 227615 (200224)	Loss/tok 3.3209 (3.1535)	LR 7.187e-04
0: TRAIN [3][850/1291]	Time 0.058 (0.053)	Data 1.26e-04 (3.26e-04)	Tok/s 214886 (200302)	Loss/tok 3.1024 (3.1534)	LR 7.187e-04
0: TRAIN [3][860/1291]	Time 0.058 (0.053)	Data 8.51e-05 (3.23e-04)	Tok/s 215680 (200236)	Loss/tok 3.0498 (3.1519)	LR 7.187e-04
0: TRAIN [3][870/1291]	Time 0.058 (0.052)	Data 8.34e-05 (3.20e-04)	Tok/s 216468 (200190)	Loss/tok 3.1781 (3.1506)	LR 7.187e-04
0: Upscaling, new scale: 8192.0
0: TRAIN [3][880/1291]	Time 0.041 (0.053)	Data 8.32e-05 (3.18e-04)	Tok/s 187742 (200312)	Loss/tok 3.0003 (3.1523)	LR 7.187e-04
0: TRAIN [3][890/1291]	Time 0.076 (0.053)	Data 8.23e-05 (3.15e-04)	Tok/s 227812 (200323)	Loss/tok 3.2125 (3.1518)	LR 7.187e-04
0: TRAIN [3][900/1291]	Time 0.042 (0.053)	Data 8.03e-05 (3.13e-04)	Tok/s 181865 (200429)	Loss/tok 2.9912 (3.1529)	LR 7.187e-04
0: TRAIN [3][910/1291]	Time 0.076 (0.053)	Data 8.51e-05 (3.10e-04)	Tok/s 227715 (200383)	Loss/tok 3.4063 (3.1532)	LR 7.187e-04
0: TRAIN [3][920/1291]	Time 0.098 (0.053)	Data 8.30e-05 (3.08e-04)	Tok/s 226044 (200476)	Loss/tok 3.4856 (3.1548)	LR 7.187e-04
0: TRAIN [3][930/1291]	Time 0.042 (0.053)	Data 1.38e-04 (3.06e-04)	Tok/s 185038 (200456)	Loss/tok 2.9387 (3.1543)	LR 7.187e-04
0: TRAIN [3][940/1291]	Time 0.058 (0.053)	Data 8.03e-05 (3.04e-04)	Tok/s 214737 (200512)	Loss/tok 3.2382 (3.1542)	LR 7.187e-04
0: TRAIN [3][950/1291]	Time 0.041 (0.053)	Data 8.32e-05 (3.02e-04)	Tok/s 186274 (200500)	Loss/tok 3.0198 (3.1535)	LR 7.187e-04
0: TRAIN [3][960/1291]	Time 0.097 (0.053)	Data 8.42e-05 (3.00e-04)	Tok/s 230904 (200452)	Loss/tok 3.4375 (3.1533)	LR 7.187e-04
0: TRAIN [3][970/1291]	Time 0.076 (0.053)	Data 8.27e-05 (2.98e-04)	Tok/s 231323 (200446)	Loss/tok 3.2938 (3.1526)	LR 7.187e-04
0: Gradient norm: inf
0: Skipped batch, new scale: 4096.0
0: TRAIN [3][980/1291]	Time 0.041 (0.053)	Data 9.16e-05 (2.96e-04)	Tok/s 182292 (200418)	Loss/tok 3.0577 (3.1520)	LR 7.187e-04
0: TRAIN [3][990/1291]	Time 0.042 (0.053)	Data 1.77e-04 (2.94e-04)	Tok/s 193058 (200475)	Loss/tok 3.0252 (3.1517)	LR 7.187e-04
0: TRAIN [3][1000/1291]	Time 0.058 (0.053)	Data 8.56e-05 (2.92e-04)	Tok/s 218299 (200487)	Loss/tok 3.1088 (3.1510)	LR 7.187e-04
0: TRAIN [3][1010/1291]	Time 0.077 (0.053)	Data 8.42e-05 (2.90e-04)	Tok/s 228931 (200651)	Loss/tok 3.0932 (3.1519)	LR 7.187e-04
0: TRAIN [3][1020/1291]	Time 0.041 (0.053)	Data 1.37e-04 (2.88e-04)	Tok/s 186439 (200569)	Loss/tok 3.0070 (3.1516)	LR 7.187e-04
0: TRAIN [3][1030/1291]	Time 0.041 (0.053)	Data 8.65e-05 (2.86e-04)	Tok/s 191630 (200565)	Loss/tok 2.9631 (3.1514)	LR 7.187e-04
0: TRAIN [3][1040/1291]	Time 0.042 (0.053)	Data 8.63e-05 (2.84e-04)	Tok/s 182811 (200442)	Loss/tok 2.8464 (3.1497)	LR 7.187e-04
0: TRAIN [3][1050/1291]	Time 0.042 (0.053)	Data 8.46e-05 (2.82e-04)	Tok/s 184710 (200354)	Loss/tok 2.9739 (3.1486)	LR 7.187e-04
0: TRAIN [3][1060/1291]	Time 0.059 (0.053)	Data 1.40e-04 (2.81e-04)	Tok/s 217750 (200450)	Loss/tok 3.1079 (3.1491)	LR 7.187e-04
0: TRAIN [3][1070/1291]	Time 0.059 (0.053)	Data 1.41e-04 (2.79e-04)	Tok/s 217035 (200439)	Loss/tok 3.1949 (3.1483)	LR 7.187e-04
0: TRAIN [3][1080/1291]	Time 0.024 (0.053)	Data 1.37e-04 (2.77e-04)	Tok/s 159538 (200338)	Loss/tok 2.6137 (3.1485)	LR 7.187e-04
0: TRAIN [3][1090/1291]	Time 0.025 (0.053)	Data 8.70e-05 (2.75e-04)	Tok/s 161068 (200270)	Loss/tok 2.4987 (3.1477)	LR 7.187e-04
0: TRAIN [3][1100/1291]	Time 0.077 (0.053)	Data 8.54e-05 (2.74e-04)	Tok/s 230989 (200336)	Loss/tok 3.1307 (3.1478)	LR 7.187e-04
0: Upscaling, new scale: 8192.0
0: TRAIN [3][1110/1291]	Time 0.042 (0.053)	Data 8.03e-05 (2.72e-04)	Tok/s 184675 (200387)	Loss/tok 2.8985 (3.1472)	LR 7.187e-04
0: TRAIN [3][1120/1291]	Time 0.059 (0.053)	Data 9.30e-05 (2.71e-04)	Tok/s 212748 (200386)	Loss/tok 3.1125 (3.1466)	LR 7.187e-04
0: TRAIN [3][1130/1291]	Time 0.042 (0.053)	Data 8.30e-05 (2.69e-04)	Tok/s 187503 (200280)	Loss/tok 2.9523 (3.1454)	LR 7.187e-04
0: Gradient norm: inf
0: Skipped batch, new scale: 4096.0
0: TRAIN [3][1140/1291]	Time 0.077 (0.053)	Data 8.39e-05 (2.67e-04)	Tok/s 228328 (200464)	Loss/tok 3.1733 (3.1466)	LR 7.187e-04
0: TRAIN [3][1150/1291]	Time 0.042 (0.053)	Data 1.45e-04 (2.66e-04)	Tok/s 179959 (200539)	Loss/tok 2.9331 (3.1481)	LR 7.187e-04
0: TRAIN [3][1160/1291]	Time 0.042 (0.053)	Data 8.15e-05 (2.64e-04)	Tok/s 186874 (200484)	Loss/tok 2.8998 (3.1477)	LR 7.187e-04
0: TRAIN [3][1170/1291]	Time 0.026 (0.053)	Data 8.18e-05 (2.63e-04)	Tok/s 155056 (200375)	Loss/tok 2.6025 (3.1471)	LR 7.187e-04
0: TRAIN [3][1180/1291]	Time 0.042 (0.053)	Data 1.40e-04 (2.61e-04)	Tok/s 187540 (200466)	Loss/tok 2.9543 (3.1474)	LR 7.187e-04
0: TRAIN [3][1190/1291]	Time 0.077 (0.053)	Data 8.23e-05 (2.60e-04)	Tok/s 230024 (200399)	Loss/tok 3.2066 (3.1466)	LR 7.187e-04
0: TRAIN [3][1200/1291]	Time 0.043 (0.053)	Data 8.13e-05 (2.59e-04)	Tok/s 175763 (200378)	Loss/tok 2.9038 (3.1459)	LR 7.187e-04
0: TRAIN [3][1210/1291]	Time 0.097 (0.053)	Data 8.32e-05 (2.57e-04)	Tok/s 229384 (200420)	Loss/tok 3.4884 (3.1462)	LR 7.187e-04
0: TRAIN [3][1220/1291]	Time 0.097 (0.053)	Data 8.13e-05 (2.56e-04)	Tok/s 228493 (200475)	Loss/tok 3.4436 (3.1464)	LR 3.594e-04
0: TRAIN [3][1230/1291]	Time 0.042 (0.053)	Data 1.30e-04 (2.54e-04)	Tok/s 184675 (200507)	Loss/tok 2.8813 (3.1462)	LR 3.594e-04
0: TRAIN [3][1240/1291]	Time 0.059 (0.053)	Data 8.27e-05 (2.53e-04)	Tok/s 216541 (200461)	Loss/tok 3.0471 (3.1456)	LR 3.594e-04
0: TRAIN [3][1250/1291]	Time 0.059 (0.053)	Data 8.49e-05 (2.52e-04)	Tok/s 213215 (200428)	Loss/tok 3.0991 (3.1449)	LR 3.594e-04
0: TRAIN [3][1260/1291]	Time 0.042 (0.053)	Data 8.27e-05 (2.51e-04)	Tok/s 181944 (200402)	Loss/tok 3.0202 (3.1450)	LR 3.594e-04
0: Upscaling, new scale: 8192.0
0: TRAIN [3][1270/1291]	Time 0.042 (0.053)	Data 8.39e-05 (2.50e-04)	Tok/s 180557 (200383)	Loss/tok 3.0735 (3.1451)	LR 3.594e-04
0: TRAIN [3][1280/1291]	Time 0.059 (0.053)	Data 7.61e-05 (2.48e-04)	Tok/s 211858 (200448)	Loss/tok 3.1746 (3.1454)	LR 3.594e-04
0: TRAIN [3][1290/1291]	Time 0.059 (0.053)	Data 4.29e-05 (2.49e-04)	Tok/s 218192 (200505)	Loss/tok 3.0844 (3.1460)	LR 3.594e-04
:::MLLOG {"namespace": "", "time_ms": 1593113362797, "event_type": "INTERVAL_END", "key": "epoch_stop", "value": null, "metadata": {"file": "train.py", "lineno": 551, "epoch_num": 4}}
:::MLLOG {"namespace": "", "time_ms": 1593113362797, "event_type": "INTERVAL_START", "key": "eval_start", "value": null, "metadata": {"file": "train.py", "lineno": 556, "epoch_num": 4}}
0: Running evaluation on test set
0: TEST [3][0/3]	Time 0.317 (0.317)	Decoder iters 110.0 (110.0)	Tok/s 28258 (28258)
0: Running moses detokenizer
0: BLEU(score=23.77510494223196, counts=[36962, 18432, 10462, 6152], totals=[65461, 62458, 59455, 56456], precisions=[56.46415422923573, 29.51103141310961, 17.596501555798504, 10.896981720277738], bp=1.0, sys_len=65461, ref_len=64676)
0: Finished evaluation on test set
:::MLLOG {"namespace": "", "time_ms": 1593113364018, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.2378, "metadata": {"file": "train.py", "lineno": 562, "epoch_num": 4}}
:::MLLOG {"namespace": "", "time_ms": 1593113364018, "event_type": "INTERVAL_END", "key": "eval_stop", "value": null, "metadata": {"file": "train.py", "lineno": 565, "epoch_num": 4}}
0: Summary: Epoch: 3	Training Loss: 3.1498	Test BLEU: 23.78
0: Performance: Epoch: 3	Training: 3206595 Tok/s
0: Finished epoch 3
:::MLLOG {"namespace": "", "time_ms": 1593113364018, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "train.py", "lineno": 584, "first_epoch_num": 4}}
:::MLLOG {"namespace": "", "time_ms": 1593113364018, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "train.py", "lineno": 538, "first_epoch_num": 5, "epoch_count": 1}}
:::MLLOG {"namespace": "", "time_ms": 1593113364018, "event_type": "INTERVAL_START", "key": "epoch_start", "value": null, "metadata": {"file": "train.py", "lineno": 541, "epoch_num": 5}}
0: Starting epoch 4
0: Sampler for epoch 4 uses seed 729172555
0: TRAIN [4][0/1291]	Time 0.326 (0.326)	Data 1.78e-01 (1.78e-01)	Tok/s 53394 (53394)	Loss/tok 3.2136 (3.2136)	LR 3.594e-04
0: TRAIN [4][10/1291]	Time 0.041 (0.077)	Data 8.49e-05 (1.63e-02)	Tok/s 188011 (188163)	Loss/tok 2.7409 (3.0643)	LR 3.594e-04
0: TRAIN [4][20/1291]	Time 0.025 (0.063)	Data 8.85e-05 (8.60e-03)	Tok/s 155333 (191222)	Loss/tok 2.4940 (3.0464)	LR 3.594e-04
0: TRAIN [4][30/1291]	Time 0.042 (0.059)	Data 8.85e-05 (5.86e-03)	Tok/s 186094 (193735)	Loss/tok 2.8363 (3.0244)	LR 3.594e-04
0: TRAIN [4][40/1291]	Time 0.025 (0.060)	Data 9.92e-05 (4.46e-03)	Tok/s 163318 (197414)	Loss/tok 2.4704 (3.0564)	LR 3.594e-04
0: TRAIN [4][50/1291]	Time 0.024 (0.060)	Data 9.18e-05 (3.60e-03)	Tok/s 161343 (200432)	Loss/tok 2.4903 (3.0655)	LR 3.594e-04
0: TRAIN [4][60/1291]	Time 0.041 (0.058)	Data 1.47e-04 (3.03e-03)	Tok/s 190883 (199892)	Loss/tok 2.8106 (3.0622)	LR 3.594e-04
0: TRAIN [4][70/1291]	Time 0.041 (0.058)	Data 9.23e-05 (2.61e-03)	Tok/s 188711 (199951)	Loss/tok 2.8149 (3.0691)	LR 3.594e-04
0: TRAIN [4][80/1291]	Time 0.041 (0.057)	Data 9.11e-05 (2.30e-03)	Tok/s 189368 (200916)	Loss/tok 2.9018 (3.0691)	LR 3.594e-04
0: TRAIN [4][90/1291]	Time 0.025 (0.056)	Data 9.54e-05 (2.06e-03)	Tok/s 163375 (199736)	Loss/tok 2.5759 (3.0592)	LR 3.594e-04
0: TRAIN [4][100/1291]	Time 0.042 (0.056)	Data 1.67e-04 (1.87e-03)	Tok/s 184653 (200343)	Loss/tok 2.9520 (3.0622)	LR 3.594e-04
0: Upscaling, new scale: 8192.0
0: TRAIN [4][110/1291]	Time 0.041 (0.057)	Data 1.44e-04 (1.71e-03)	Tok/s 183935 (200638)	Loss/tok 2.8918 (3.0728)	LR 3.594e-04
0: TRAIN [4][120/1291]	Time 0.042 (0.056)	Data 9.04e-05 (1.58e-03)	Tok/s 184685 (200736)	Loss/tok 2.9153 (3.0770)	LR 3.594e-04
0: TRAIN [4][130/1291]	Time 0.041 (0.056)	Data 1.39e-04 (1.46e-03)	Tok/s 185163 (200735)	Loss/tok 2.8184 (3.0723)	LR 3.594e-04
0: TRAIN [4][140/1291]	Time 0.058 (0.056)	Data 1.47e-04 (1.37e-03)	Tok/s 212280 (200497)	Loss/tok 3.0833 (3.0697)	LR 3.594e-04
0: TRAIN [4][150/1291]	Time 0.059 (0.056)	Data 8.92e-05 (1.28e-03)	Tok/s 216852 (201154)	Loss/tok 3.1743 (3.0705)	LR 3.594e-04
0: TRAIN [4][160/1291]	Time 0.043 (0.056)	Data 9.58e-05 (1.21e-03)	Tok/s 179177 (201528)	Loss/tok 2.8850 (3.0728)	LR 3.594e-04
0: TRAIN [4][170/1291]	Time 0.077 (0.056)	Data 9.94e-05 (1.15e-03)	Tok/s 227179 (202099)	Loss/tok 3.3457 (3.0756)	LR 3.594e-04
0: TRAIN [4][180/1291]	Time 0.076 (0.056)	Data 9.06e-05 (1.09e-03)	Tok/s 231395 (202101)	Loss/tok 3.2009 (3.0786)	LR 3.594e-04
0: TRAIN [4][190/1291]	Time 0.076 (0.056)	Data 8.63e-05 (1.04e-03)	Tok/s 224742 (202087)	Loss/tok 3.2005 (3.0788)	LR 3.594e-04
0: TRAIN [4][200/1291]	Time 0.025 (0.056)	Data 9.06e-05 (9.89e-04)	Tok/s 161950 (201657)	Loss/tok 2.3793 (3.0762)	LR 3.594e-04
0: TRAIN [4][210/1291]	Time 0.098 (0.055)	Data 8.94e-05 (9.47e-04)	Tok/s 231681 (201525)	Loss/tok 3.2952 (3.0730)	LR 3.594e-04
0: TRAIN [4][220/1291]	Time 0.041 (0.055)	Data 1.40e-04 (9.09e-04)	Tok/s 188531 (201052)	Loss/tok 2.8825 (3.0724)	LR 3.594e-04
0: Upscaling, new scale: 8192.0
0: TRAIN [4][230/1291]	Time 0.042 (0.055)	Data 8.82e-05 (8.74e-04)	Tok/s 188144 (201188)	Loss/tok 2.7516 (3.0722)	LR 3.594e-04
0: TRAIN [4][240/1291]	Time 0.042 (0.055)	Data 8.92e-05 (8.41e-04)	Tok/s 183625 (201184)	Loss/tok 2.7812 (3.0699)	LR 3.594e-04
0: TRAIN [4][250/1291]	Time 0.076 (0.055)	Data 8.82e-05 (8.12e-04)	Tok/s 229283 (201616)	Loss/tok 3.3318 (3.0725)	LR 3.594e-04
0: TRAIN [4][260/1291]	Time 0.042 (0.055)	Data 1.35e-04 (7.84e-04)	Tok/s 182871 (201675)	Loss/tok 2.8115 (3.0702)	LR 3.594e-04
0: TRAIN [4][270/1291]	Time 0.059 (0.055)	Data 9.13e-05 (7.59e-04)	Tok/s 216262 (201880)	Loss/tok 3.1416 (3.0716)	LR 3.594e-04
0: TRAIN [4][280/1291]	Time 0.058 (0.055)	Data 8.82e-05 (7.36e-04)	Tok/s 216715 (201701)	Loss/tok 3.2077 (3.0715)	LR 3.594e-04
0: TRAIN [4][290/1291]	Time 0.041 (0.055)	Data 8.30e-05 (7.14e-04)	Tok/s 190976 (201425)	Loss/tok 2.9201 (3.0667)	LR 3.594e-04
0: TRAIN [4][300/1291]	Time 0.042 (0.055)	Data 8.42e-05 (6.94e-04)	Tok/s 185390 (201478)	Loss/tok 2.8195 (3.0673)	LR 3.594e-04
0: TRAIN [4][310/1291]	Time 0.076 (0.055)	Data 1.41e-04 (6.75e-04)	Tok/s 229824 (201476)	Loss/tok 3.2309 (3.0658)	LR 3.594e-04
0: TRAIN [4][320/1291]	Time 0.041 (0.055)	Data 1.49e-04 (6.57e-04)	Tok/s 188131 (201796)	Loss/tok 2.7934 (3.0663)	LR 3.594e-04
0: TRAIN [4][330/1291]	Time 0.076 (0.055)	Data 8.89e-05 (6.40e-04)	Tok/s 226248 (201864)	Loss/tok 3.3627 (3.0667)	LR 3.594e-04
0: TRAIN [4][340/1291]	Time 0.041 (0.055)	Data 8.70e-05 (6.24e-04)	Tok/s 188984 (201589)	Loss/tok 2.7696 (3.0647)	LR 3.594e-04
0: TRAIN [4][350/1291]	Time 0.042 (0.054)	Data 8.54e-05 (6.09e-04)	Tok/s 188029 (201280)	Loss/tok 2.8364 (3.0621)	LR 3.594e-04
0: Upscaling, new scale: 8192.0
0: TRAIN [4][360/1291]	Time 0.024 (0.054)	Data 1.38e-04 (5.95e-04)	Tok/s 156710 (201275)	Loss/tok 2.6008 (3.0614)	LR 3.594e-04
0: TRAIN [4][370/1291]	Time 0.025 (0.054)	Data 8.75e-05 (5.81e-04)	Tok/s 160456 (201212)	Loss/tok 2.5452 (3.0610)	LR 3.594e-04
0: TRAIN [4][380/1291]	Time 0.041 (0.054)	Data 1.47e-04 (5.68e-04)	Tok/s 187262 (200923)	Loss/tok 2.7902 (3.0598)	LR 3.594e-04
0: TRAIN [4][390/1291]	Time 0.058 (0.054)	Data 8.80e-05 (5.56e-04)	Tok/s 219234 (200935)	Loss/tok 3.0140 (3.0590)	LR 3.594e-04
0: TRAIN [4][400/1291]	Time 0.025 (0.053)	Data 8.70e-05 (5.45e-04)	Tok/s 163016 (200495)	Loss/tok 2.4859 (3.0557)	LR 3.594e-04
0: TRAIN [4][410/1291]	Time 0.076 (0.054)	Data 8.77e-05 (5.34e-04)	Tok/s 232549 (200771)	Loss/tok 3.1033 (3.0568)	LR 3.594e-04
0: TRAIN [4][420/1291]	Time 0.059 (0.054)	Data 8.61e-05 (5.23e-04)	Tok/s 215335 (200973)	Loss/tok 3.1047 (3.0584)	LR 3.594e-04
0: TRAIN [4][430/1291]	Time 0.058 (0.054)	Data 1.43e-04 (5.14e-04)	Tok/s 214114 (200998)	Loss/tok 3.1601 (3.0578)	LR 3.594e-04
0: TRAIN [4][440/1291]	Time 0.059 (0.054)	Data 8.94e-05 (5.04e-04)	Tok/s 213538 (201120)	Loss/tok 3.0935 (3.0586)	LR 1.797e-04
0: TRAIN [4][450/1291]	Time 0.042 (0.053)	Data 1.39e-04 (4.95e-04)	Tok/s 188594 (200773)	Loss/tok 2.8211 (3.0558)	LR 1.797e-04
0: TRAIN [4][460/1291]	Time 0.042 (0.054)	Data 8.27e-05 (4.87e-04)	Tok/s 183939 (200910)	Loss/tok 2.9087 (3.0565)	LR 1.797e-04
0: TRAIN [4][470/1291]	Time 0.058 (0.054)	Data 8.39e-05 (4.78e-04)	Tok/s 214308 (200913)	Loss/tok 3.0524 (3.0558)	LR 1.797e-04
0: TRAIN [4][480/1291]	Time 0.041 (0.054)	Data 9.11e-05 (4.70e-04)	Tok/s 184361 (201047)	Loss/tok 2.8462 (3.0571)	LR 1.797e-04
0: Upscaling, new scale: 8192.0
0: TRAIN [4][490/1291]	Time 0.059 (0.054)	Data 8.73e-05 (4.63e-04)	Tok/s 215552 (201029)	Loss/tok 3.0309 (3.0558)	LR 1.797e-04
0: TRAIN [4][500/1291]	Time 0.076 (0.054)	Data 8.99e-05 (4.55e-04)	Tok/s 224207 (200985)	Loss/tok 3.4305 (3.0581)	LR 1.797e-04
0: TRAIN [4][510/1291]	Time 0.058 (0.053)	Data 8.58e-05 (4.48e-04)	Tok/s 218354 (200757)	Loss/tok 2.9382 (3.0563)	LR 1.797e-04
0: TRAIN [4][520/1291]	Time 0.042 (0.054)	Data 9.11e-05 (4.41e-04)	Tok/s 183778 (200841)	Loss/tok 2.7693 (3.0565)	LR 1.797e-04
0: TRAIN [4][530/1291]	Time 0.097 (0.053)	Data 8.49e-05 (4.35e-04)	Tok/s 228038 (200535)	Loss/tok 3.4858 (3.0554)	LR 1.797e-04
0: TRAIN [4][540/1291]	Time 0.058 (0.053)	Data 1.41e-04 (4.29e-04)	Tok/s 219016 (200591)	Loss/tok 3.0143 (3.0544)	LR 1.797e-04
0: TRAIN [4][550/1291]	Time 0.058 (0.053)	Data 8.73e-05 (4.23e-04)	Tok/s 215777 (200673)	Loss/tok 3.1138 (3.0555)	LR 1.797e-04
0: TRAIN [4][560/1291]	Time 0.042 (0.053)	Data 8.94e-05 (4.17e-04)	Tok/s 183769 (200730)	Loss/tok 2.8871 (3.0560)	LR 1.797e-04
0: TRAIN [4][570/1291]	Time 0.058 (0.053)	Data 1.44e-04 (4.11e-04)	Tok/s 215902 (200741)	Loss/tok 3.0714 (3.0573)	LR 1.797e-04
0: TRAIN [4][580/1291]	Time 0.077 (0.054)	Data 9.08e-05 (4.06e-04)	Tok/s 229376 (200878)	Loss/tok 3.1934 (3.0575)	LR 1.797e-04
0: TRAIN [4][590/1291]	Time 0.041 (0.053)	Data 1.46e-04 (4.01e-04)	Tok/s 192719 (200770)	Loss/tok 2.7542 (3.0557)	LR 1.797e-04
0: TRAIN [4][600/1291]	Time 0.041 (0.053)	Data 1.55e-04 (3.96e-04)	Tok/s 185356 (200585)	Loss/tok 2.8983 (3.0540)	LR 1.797e-04
0: TRAIN [4][610/1291]	Time 0.024 (0.053)	Data 8.89e-05 (3.91e-04)	Tok/s 164765 (200556)	Loss/tok 2.5960 (3.0534)	LR 1.797e-04
0: Upscaling, new scale: 8192.0
0: TRAIN [4][620/1291]	Time 0.042 (0.053)	Data 8.96e-05 (3.86e-04)	Tok/s 180867 (200543)	Loss/tok 2.8406 (3.0534)	LR 1.797e-04
0: TRAIN [4][630/1291]	Time 0.076 (0.053)	Data 9.16e-05 (3.82e-04)	Tok/s 232089 (200646)	Loss/tok 3.1942 (3.0537)	LR 1.797e-04
0: TRAIN [4][640/1291]	Time 0.041 (0.053)	Data 1.40e-04 (3.77e-04)	Tok/s 189059 (200851)	Loss/tok 2.8126 (3.0555)	LR 1.797e-04
0: TRAIN [4][650/1291]	Time 0.024 (0.053)	Data 8.75e-05 (3.73e-04)	Tok/s 161760 (200795)	Loss/tok 2.4996 (3.0548)	LR 1.797e-04
0: TRAIN [4][660/1291]	Time 0.024 (0.053)	Data 1.43e-04 (3.69e-04)	Tok/s 163248 (200907)	Loss/tok 2.6084 (3.0548)	LR 1.797e-04
0: TRAIN [4][670/1291]	Time 0.025 (0.053)	Data 8.51e-05 (3.65e-04)	Tok/s 162670 (200933)	Loss/tok 2.4509 (3.0549)	LR 1.797e-04
0: TRAIN [4][680/1291]	Time 0.024 (0.053)	Data 1.63e-04 (3.61e-04)	Tok/s 161661 (200847)	Loss/tok 2.5060 (3.0534)	LR 1.797e-04
0: TRAIN [4][690/1291]	Time 0.059 (0.053)	Data 8.87e-05 (3.57e-04)	Tok/s 213321 (200947)	Loss/tok 3.0755 (3.0535)	LR 1.797e-04
0: TRAIN [4][700/1291]	Time 0.058 (0.053)	Data 8.65e-05 (3.54e-04)	Tok/s 217684 (200954)	Loss/tok 3.0302 (3.0526)	LR 1.797e-04
0: TRAIN [4][710/1291]	Time 0.077 (0.053)	Data 8.99e-05 (3.50e-04)	Tok/s 227336 (200821)	Loss/tok 3.2598 (3.0512)	LR 1.797e-04
0: TRAIN [4][720/1291]	Time 0.097 (0.053)	Data 8.65e-05 (3.46e-04)	Tok/s 232400 (200991)	Loss/tok 3.3929 (3.0540)	LR 1.797e-04
0: TRAIN [4][730/1291]	Time 0.076 (0.054)	Data 8.92e-05 (3.43e-04)	Tok/s 228976 (201115)	Loss/tok 3.2701 (3.0542)	LR 1.797e-04
0: TRAIN [4][740/1291]	Time 0.058 (0.054)	Data 1.39e-04 (3.40e-04)	Tok/s 216318 (201225)	Loss/tok 3.0769 (3.0542)	LR 1.797e-04
0: Upscaling, new scale: 8192.0
0: TRAIN [4][750/1291]	Time 0.042 (0.054)	Data 8.80e-05 (3.36e-04)	Tok/s 188283 (201324)	Loss/tok 2.8480 (3.0545)	LR 1.797e-04
0: TRAIN [4][760/1291]	Time 0.042 (0.054)	Data 8.75e-05 (3.33e-04)	Tok/s 186079 (201296)	Loss/tok 2.8964 (3.0536)	LR 1.797e-04
0: TRAIN [4][770/1291]	Time 0.042 (0.053)	Data 8.73e-05 (3.30e-04)	Tok/s 187148 (201194)	Loss/tok 2.8746 (3.0523)	LR 1.797e-04
0: TRAIN [4][780/1291]	Time 0.058 (0.053)	Data 1.39e-04 (3.27e-04)	Tok/s 216604 (201119)	Loss/tok 3.0260 (3.0511)	LR 1.797e-04
0: TRAIN [4][790/1291]	Time 0.059 (0.053)	Data 1.44e-04 (3.25e-04)	Tok/s 216229 (201276)	Loss/tok 3.0488 (3.0522)	LR 1.797e-04
0: TRAIN [4][800/1291]	Time 0.042 (0.054)	Data 8.82e-05 (3.22e-04)	Tok/s 191585 (201434)	Loss/tok 2.8345 (3.0542)	LR 1.797e-04
0: TRAIN [4][810/1291]	Time 0.042 (0.054)	Data 8.65e-05 (3.19e-04)	Tok/s 185655 (201345)	Loss/tok 2.8730 (3.0545)	LR 1.797e-04
0: TRAIN [4][820/1291]	Time 0.042 (0.053)	Data 8.70e-05 (3.16e-04)	Tok/s 178568 (201269)	Loss/tok 2.7839 (3.0530)	LR 1.797e-04
0: TRAIN [4][830/1291]	Time 0.041 (0.053)	Data 8.65e-05 (3.14e-04)	Tok/s 187332 (201236)	Loss/tok 2.9357 (3.0524)	LR 1.797e-04
0: TRAIN [4][840/1291]	Time 0.042 (0.053)	Data 8.27e-05 (3.11e-04)	Tok/s 185382 (201139)	Loss/tok 2.9148 (3.0511)	LR 1.797e-04
0: TRAIN [4][850/1291]	Time 0.059 (0.053)	Data 8.89e-05 (3.09e-04)	Tok/s 221245 (201204)	Loss/tok 2.9790 (3.0512)	LR 1.797e-04
0: TRAIN [4][860/1291]	Time 0.059 (0.053)	Data 8.94e-05 (3.06e-04)	Tok/s 213731 (201184)	Loss/tok 2.9627 (3.0506)	LR 1.797e-04
0: Upscaling, new scale: 8192.0
0: TRAIN [4][870/1291]	Time 0.058 (0.053)	Data 1.46e-04 (3.04e-04)	Tok/s 213708 (201175)	Loss/tok 3.0635 (3.0501)	LR 1.797e-04
0: TRAIN [4][880/1291]	Time 0.025 (0.053)	Data 1.41e-04 (3.02e-04)	Tok/s 158961 (201200)	Loss/tok 2.4799 (3.0493)	LR 1.797e-04
0: TRAIN [4][890/1291]	Time 0.059 (0.053)	Data 1.47e-04 (3.00e-04)	Tok/s 216067 (201225)	Loss/tok 3.0023 (3.0497)	LR 1.797e-04
0: TRAIN [4][900/1291]	Time 0.058 (0.053)	Data 1.45e-04 (2.97e-04)	Tok/s 217671 (201145)	Loss/tok 2.9565 (3.0489)	LR 1.797e-04
0: TRAIN [4][910/1291]	Time 0.058 (0.053)	Data 8.70e-05 (2.95e-04)	Tok/s 212264 (201172)	Loss/tok 3.1413 (3.0483)	LR 1.797e-04
0: TRAIN [4][920/1291]	Time 0.097 (0.053)	Data 8.85e-05 (2.93e-04)	Tok/s 226816 (201204)	Loss/tok 3.3773 (3.0488)	LR 1.797e-04
0: TRAIN [4][930/1291]	Time 0.041 (0.053)	Data 1.42e-04 (2.91e-04)	Tok/s 185562 (201162)	Loss/tok 2.9158 (3.0478)	LR 1.797e-04
0: TRAIN [4][940/1291]	Time 0.076 (0.053)	Data 8.42e-05 (2.89e-04)	Tok/s 225946 (201127)	Loss/tok 3.2620 (3.0476)	LR 1.797e-04
0: TRAIN [4][950/1291]	Time 0.058 (0.053)	Data 8.32e-05 (2.87e-04)	Tok/s 214984 (201159)	Loss/tok 2.9700 (3.0476)	LR 1.797e-04
0: TRAIN [4][960/1291]	Time 0.076 (0.053)	Data 1.49e-04 (2.85e-04)	Tok/s 232280 (201097)	Loss/tok 3.1454 (3.0468)	LR 1.797e-04
0: TRAIN [4][970/1291]	Time 0.041 (0.053)	Data 8.96e-05 (2.83e-04)	Tok/s 188615 (201061)	Loss/tok 2.8914 (3.0466)	LR 1.797e-04
0: TRAIN [4][980/1291]	Time 0.097 (0.053)	Data 8.73e-05 (2.81e-04)	Tok/s 230282 (201030)	Loss/tok 3.3605 (3.0464)	LR 1.797e-04
0: TRAIN [4][990/1291]	Time 0.041 (0.053)	Data 8.39e-05 (2.79e-04)	Tok/s 184979 (201013)	Loss/tok 3.0416 (3.0463)	LR 1.797e-04
0: Upscaling, new scale: 8192.0
0: TRAIN [4][1000/1291]	Time 0.041 (0.053)	Data 8.92e-05 (2.77e-04)	Tok/s 186932 (200990)	Loss/tok 2.9044 (3.0459)	LR 1.797e-04
0: TRAIN [4][1010/1291]	Time 0.041 (0.053)	Data 1.40e-04 (2.76e-04)	Tok/s 187765 (200821)	Loss/tok 2.8999 (3.0449)	LR 1.797e-04
0: TRAIN [4][1020/1291]	Time 0.097 (0.053)	Data 8.82e-05 (2.74e-04)	Tok/s 232446 (200836)	Loss/tok 3.3722 (3.0448)	LR 1.797e-04
0: TRAIN [4][1030/1291]	Time 0.025 (0.053)	Data 1.17e-04 (2.72e-04)	Tok/s 160363 (200765)	Loss/tok 2.4675 (3.0438)	LR 1.797e-04
0: TRAIN [4][1040/1291]	Time 0.042 (0.053)	Data 8.58e-05 (2.70e-04)	Tok/s 188274 (200770)	Loss/tok 2.8713 (3.0437)	LR 1.797e-04
0: TRAIN [4][1050/1291]	Time 0.041 (0.053)	Data 8.77e-05 (2.69e-04)	Tok/s 182252 (200839)	Loss/tok 2.8539 (3.0437)	LR 1.797e-04
0: TRAIN [4][1060/1291]	Time 0.042 (0.053)	Data 8.68e-05 (2.67e-04)	Tok/s 179165 (200924)	Loss/tok 3.0555 (3.0443)	LR 1.797e-04
0: TRAIN [4][1070/1291]	Time 0.097 (0.053)	Data 8.87e-05 (2.66e-04)	Tok/s 225291 (200907)	Loss/tok 3.4780 (3.0448)	LR 1.797e-04
0: TRAIN [4][1080/1291]	Time 0.076 (0.053)	Data 8.77e-05 (2.64e-04)	Tok/s 226138 (200917)	Loss/tok 3.2718 (3.0453)	LR 1.797e-04
0: TRAIN [4][1090/1291]	Time 0.058 (0.053)	Data 8.94e-05 (2.63e-04)	Tok/s 213958 (201015)	Loss/tok 2.9675 (3.0453)	LR 1.797e-04
0: TRAIN [4][1100/1291]	Time 0.042 (0.053)	Data 8.23e-05 (2.62e-04)	Tok/s 184262 (200995)	Loss/tok 2.8891 (3.0450)	LR 1.797e-04
0: TRAIN [4][1110/1291]	Time 0.077 (0.053)	Data 8.70e-05 (2.60e-04)	Tok/s 227778 (201086)	Loss/tok 3.3251 (3.0470)	LR 1.797e-04
0: TRAIN [4][1120/1291]	Time 0.076 (0.053)	Data 8.99e-05 (2.59e-04)	Tok/s 230551 (201034)	Loss/tok 3.2671 (3.0468)	LR 1.797e-04
0: Upscaling, new scale: 8192.0
0: TRAIN [4][1130/1291]	Time 0.041 (0.053)	Data 9.01e-05 (2.57e-04)	Tok/s 186882 (201019)	Loss/tok 2.8570 (3.0470)	LR 1.797e-04
0: TRAIN [4][1140/1291]	Time 0.041 (0.053)	Data 8.85e-05 (2.56e-04)	Tok/s 187136 (200977)	Loss/tok 3.0020 (3.0465)	LR 1.797e-04
0: TRAIN [4][1150/1291]	Time 0.058 (0.053)	Data 1.42e-04 (2.55e-04)	Tok/s 220210 (201047)	Loss/tok 2.9558 (3.0466)	LR 1.797e-04
0: TRAIN [4][1160/1291]	Time 0.042 (0.053)	Data 8.42e-05 (2.53e-04)	Tok/s 183192 (201038)	Loss/tok 2.8414 (3.0464)	LR 1.797e-04
0: TRAIN [4][1170/1291]	Time 0.042 (0.053)	Data 9.01e-05 (2.52e-04)	Tok/s 185220 (201059)	Loss/tok 2.8909 (3.0461)	LR 1.797e-04
0: TRAIN [4][1180/1291]	Time 0.042 (0.053)	Data 9.32e-05 (2.51e-04)	Tok/s 183078 (201039)	Loss/tok 2.9440 (3.0457)	LR 1.797e-04
0: TRAIN [4][1190/1291]	Time 0.076 (0.053)	Data 8.77e-05 (2.49e-04)	Tok/s 232689 (200972)	Loss/tok 3.1204 (3.0457)	LR 1.797e-04
0: TRAIN [4][1200/1291]	Time 0.025 (0.053)	Data 8.46e-05 (2.48e-04)	Tok/s 157483 (200968)	Loss/tok 2.5364 (3.0452)	LR 1.797e-04
0: TRAIN [4][1210/1291]	Time 0.041 (0.053)	Data 9.92e-05 (2.47e-04)	Tok/s 185595 (200939)	Loss/tok 2.8974 (3.0455)	LR 1.797e-04
0: TRAIN [4][1220/1291]	Time 0.041 (0.053)	Data 8.96e-05 (2.46e-04)	Tok/s 185193 (200883)	Loss/tok 2.8808 (3.0454)	LR 1.797e-04
0: TRAIN [4][1230/1291]	Time 0.059 (0.053)	Data 8.94e-05 (2.44e-04)	Tok/s 214170 (200870)	Loss/tok 2.9640 (3.0448)	LR 1.797e-04
0: TRAIN [4][1240/1291]	Time 0.041 (0.053)	Data 1.37e-04 (2.43e-04)	Tok/s 184914 (200879)	Loss/tok 2.7952 (3.0444)	LR 1.797e-04
0: TRAIN [4][1250/1291]	Time 0.076 (0.053)	Data 9.04e-05 (2.42e-04)	Tok/s 229853 (200928)	Loss/tok 3.0973 (3.0447)	LR 1.797e-04
0: Upscaling, new scale: 8192.0
0: TRAIN [4][1260/1291]	Time 0.097 (0.053)	Data 1.42e-04 (2.41e-04)	Tok/s 228776 (200889)	Loss/tok 3.4477 (3.0455)	LR 1.797e-04
0: TRAIN [4][1270/1291]	Time 0.025 (0.053)	Data 1.44e-04 (2.40e-04)	Tok/s 158251 (200901)	Loss/tok 2.5693 (3.0457)	LR 1.797e-04
0: TRAIN [4][1280/1291]	Time 0.041 (0.053)	Data 8.68e-05 (2.39e-04)	Tok/s 189143 (200878)	Loss/tok 2.8008 (3.0454)	LR 1.797e-04
0: TRAIN [4][1290/1291]	Time 0.042 (0.053)	Data 4.20e-05 (2.39e-04)	Tok/s 184327 (200892)	Loss/tok 2.8422 (3.0456)	LR 1.797e-04
:::MLLOG {"namespace": "", "time_ms": 1593113432450, "event_type": "INTERVAL_END", "key": "epoch_stop", "value": null, "metadata": {"file": "train.py", "lineno": 551, "epoch_num": 5}}
:::MLLOG {"namespace": "", "time_ms": 1593113432450, "event_type": "INTERVAL_START", "key": "eval_start", "value": null, "metadata": {"file": "train.py", "lineno": 556, "epoch_num": 5}}
0: Running evaluation on test set
0: TEST [4][0/3]	Time 0.326 (0.326)	Decoder iters 122.0 (122.0)	Tok/s 27635 (27635)
0: Running moses detokenizer
0: BLEU(score=23.925093850236117, counts=[37136, 18586, 10578, 6256], totals=[65699, 62696, 59693, 56695], precisions=[56.52445242697758, 29.64463442643869, 17.72067076541638, 11.03448275862069], bp=1.0, sys_len=65699, ref_len=64676)
0: Finished evaluation on test set
:::MLLOG {"namespace": "", "time_ms": 1593113433654, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.23929999999999998, "metadata": {"file": "train.py", "lineno": 562, "epoch_num": 5}}
:::MLLOG {"namespace": "", "time_ms": 1593113433654, "event_type": "INTERVAL_END", "key": "eval_stop", "value": null, "metadata": {"file": "train.py", "lineno": 565, "epoch_num": 5}}
0: Summary: Epoch: 4	Training Loss: 3.0457	Test BLEU: 23.93
0: Performance: Epoch: 4	Training: 3212474 Tok/s
0: Finished epoch 4
:::MLLOG {"namespace": "", "time_ms": 1593113433654, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "train.py", "lineno": 584, "first_epoch_num": 5}}
:::MLLOG {"namespace": "", "time_ms": 1593113433655, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "train.py", "lineno": 538, "first_epoch_num": 6, "epoch_count": 1}}
:::MLLOG {"namespace": "", "time_ms": 1593113433655, "event_type": "INTERVAL_START", "key": "epoch_start", "value": null, "metadata": {"file": "train.py", "lineno": 541, "epoch_num": 6}}
0: Starting epoch 5
0: Sampler for epoch 5 uses seed 25313827
0: TRAIN [5][0/1291]	Time 0.298 (0.298)	Data 1.82e-01 (1.82e-01)	Tok/s 58373 (58373)	Loss/tok 3.1153 (3.1153)	LR 1.797e-04
0: TRAIN [5][10/1291]	Time 0.077 (0.077)	Data 8.77e-05 (1.66e-02)	Tok/s 228736 (193143)	Loss/tok 3.1768 (3.0302)	LR 1.797e-04
0: TRAIN [5][20/1291]	Time 0.025 (0.066)	Data 8.32e-05 (8.77e-03)	Tok/s 158910 (196922)	Loss/tok 2.4901 (3.0275)	LR 1.797e-04
0: TRAIN [5][30/1291]	Time 0.076 (0.064)	Data 8.34e-05 (5.97e-03)	Tok/s 225237 (200475)	Loss/tok 3.2427 (3.0484)	LR 1.797e-04
0: TRAIN [5][40/1291]	Time 0.042 (0.063)	Data 8.46e-05 (4.54e-03)	Tok/s 186332 (201743)	Loss/tok 2.9128 (3.0630)	LR 1.797e-04
0: TRAIN [5][50/1291]	Time 0.097 (0.061)	Data 8.34e-05 (3.67e-03)	Tok/s 230029 (200633)	Loss/tok 3.3765 (3.0687)	LR 1.797e-04
0: TRAIN [5][60/1291]	Time 0.077 (0.061)	Data 8.08e-05 (3.08e-03)	Tok/s 231947 (202880)	Loss/tok 3.1861 (3.0685)	LR 1.797e-04
0: TRAIN [5][70/1291]	Time 0.097 (0.062)	Data 9.35e-05 (2.66e-03)	Tok/s 228973 (203982)	Loss/tok 3.4045 (3.0862)	LR 1.797e-04
0: TRAIN [5][80/1291]	Time 0.042 (0.061)	Data 8.89e-05 (2.34e-03)	Tok/s 182513 (203669)	Loss/tok 2.8647 (3.0793)	LR 1.797e-04
0: TRAIN [5][90/1291]	Time 0.041 (0.059)	Data 1.44e-04 (2.10e-03)	Tok/s 189455 (202920)	Loss/tok 2.8710 (3.0723)	LR 1.797e-04
0: Upscaling, new scale: 8192.0
0: TRAIN [5][100/1291]	Time 0.058 (0.058)	Data 8.11e-05 (1.90e-03)	Tok/s 211257 (202210)	Loss/tok 2.9526 (3.0606)	LR 1.797e-04
0: TRAIN [5][110/1291]	Time 0.041 (0.057)	Data 8.25e-05 (1.74e-03)	Tok/s 184564 (200971)	Loss/tok 2.8048 (3.0490)	LR 1.797e-04
0: TRAIN [5][120/1291]	Time 0.077 (0.057)	Data 7.94e-05 (1.60e-03)	Tok/s 228504 (200815)	Loss/tok 3.2799 (3.0486)	LR 1.797e-04
0: TRAIN [5][130/1291]	Time 0.058 (0.056)	Data 1.35e-04 (1.49e-03)	Tok/s 215143 (200373)	Loss/tok 2.9391 (3.0464)	LR 1.797e-04
0: TRAIN [5][140/1291]	Time 0.041 (0.056)	Data 7.53e-05 (1.39e-03)	Tok/s 191061 (200498)	Loss/tok 2.8227 (3.0421)	LR 1.797e-04
0: TRAIN [5][150/1291]	Time 0.076 (0.055)	Data 9.35e-05 (1.30e-03)	Tok/s 227450 (199747)	Loss/tok 3.3912 (3.0369)	LR 1.797e-04
0: TRAIN [5][160/1291]	Time 0.042 (0.055)	Data 8.25e-05 (1.23e-03)	Tok/s 184233 (200209)	Loss/tok 2.7520 (3.0441)	LR 1.797e-04
0: TRAIN [5][170/1291]	Time 0.041 (0.055)	Data 8.18e-05 (1.16e-03)	Tok/s 185895 (199301)	Loss/tok 2.8793 (3.0408)	LR 1.797e-04
0: TRAIN [5][180/1291]	Time 0.041 (0.055)	Data 8.20e-05 (1.10e-03)	Tok/s 187462 (199297)	Loss/tok 2.9485 (3.0411)	LR 1.797e-04
0: TRAIN [5][190/1291]	Time 0.059 (0.054)	Data 8.39e-05 (1.05e-03)	Tok/s 217159 (199065)	Loss/tok 3.0534 (3.0359)	LR 1.797e-04
0: TRAIN [5][200/1291]	Time 0.042 (0.055)	Data 8.30e-05 (1.00e-03)	Tok/s 187013 (199541)	Loss/tok 2.8030 (3.0432)	LR 1.797e-04
0: TRAIN [5][210/1291]	Time 0.041 (0.054)	Data 8.39e-05 (9.58e-04)	Tok/s 185688 (199591)	Loss/tok 2.8850 (3.0410)	LR 1.797e-04
0: Upscaling, new scale: 8192.0
0: TRAIN [5][220/1291]	Time 0.059 (0.054)	Data 1.62e-04 (9.19e-04)	Tok/s 210181 (199657)	Loss/tok 3.1203 (3.0421)	LR 1.797e-04
0: TRAIN [5][230/1291]	Time 0.041 (0.054)	Data 1.34e-04 (8.83e-04)	Tok/s 188516 (199621)	Loss/tok 3.1260 (3.0407)	LR 1.797e-04
0: TRAIN [5][240/1291]	Time 0.024 (0.054)	Data 8.34e-05 (8.51e-04)	Tok/s 163392 (199315)	Loss/tok 2.4866 (3.0363)	LR 1.797e-04
0: TRAIN [5][250/1291]	Time 0.058 (0.054)	Data 1.33e-04 (8.20e-04)	Tok/s 217254 (199370)	Loss/tok 3.1102 (3.0376)	LR 1.797e-04
0: TRAIN [5][260/1291]	Time 0.041 (0.054)	Data 7.89e-05 (7.93e-04)	Tok/s 187984 (199056)	Loss/tok 2.8108 (3.0340)	LR 1.797e-04
0: TRAIN [5][270/1291]	Time 0.059 (0.053)	Data 7.82e-05 (7.67e-04)	Tok/s 212906 (198907)	Loss/tok 3.1339 (3.0322)	LR 1.797e-04
0: TRAIN [5][280/1291]	Time 0.077 (0.053)	Data 8.03e-05 (7.43e-04)	Tok/s 224821 (199009)	Loss/tok 3.2192 (3.0340)	LR 1.797e-04
0: TRAIN [5][290/1291]	Time 0.059 (0.054)	Data 7.82e-05 (7.21e-04)	Tok/s 215023 (199474)	Loss/tok 3.0301 (3.0346)	LR 1.797e-04
0: TRAIN [5][300/1291]	Time 0.041 (0.053)	Data 1.37e-04 (7.00e-04)	Tok/s 195283 (199234)	Loss/tok 2.8490 (3.0317)	LR 1.797e-04
0: TRAIN [5][310/1291]	Time 0.041 (0.053)	Data 8.23e-05 (6.80e-04)	Tok/s 186418 (199405)	Loss/tok 2.8596 (3.0312)	LR 1.797e-04
0: TRAIN [5][320/1291]	Time 0.059 (0.053)	Data 8.06e-05 (6.62e-04)	Tok/s 215222 (199532)	Loss/tok 3.0130 (3.0294)	LR 1.797e-04
0: TRAIN [5][330/1291]	Time 0.076 (0.053)	Data 8.11e-05 (6.44e-04)	Tok/s 229469 (199587)	Loss/tok 3.1401 (3.0295)	LR 1.797e-04
0: TRAIN [5][340/1291]	Time 0.058 (0.053)	Data 8.15e-05 (6.28e-04)	Tok/s 212531 (199606)	Loss/tok 2.9916 (3.0280)	LR 1.797e-04
0: Upscaling, new scale: 8192.0
0: TRAIN [5][350/1291]	Time 0.059 (0.053)	Data 8.01e-05 (6.13e-04)	Tok/s 211995 (199551)	Loss/tok 3.0220 (3.0259)	LR 1.797e-04
0: TRAIN [5][360/1291]	Time 0.058 (0.053)	Data 1.34e-04 (5.99e-04)	Tok/s 215465 (199753)	Loss/tok 3.0762 (3.0278)	LR 1.797e-04
0: TRAIN [5][370/1291]	Time 0.076 (0.053)	Data 9.13e-05 (5.85e-04)	Tok/s 226289 (199404)	Loss/tok 3.1244 (3.0250)	LR 1.797e-04
0: TRAIN [5][380/1291]	Time 0.042 (0.053)	Data 8.32e-05 (5.72e-04)	Tok/s 188608 (199338)	Loss/tok 2.9000 (3.0236)	LR 1.797e-04
0: TRAIN [5][390/1291]	Time 0.042 (0.053)	Data 8.18e-05 (5.60e-04)	Tok/s 184943 (199313)	Loss/tok 2.8923 (3.0231)	LR 1.797e-04
0: TRAIN [5][400/1291]	Time 0.041 (0.053)	Data 1.35e-04 (5.48e-04)	Tok/s 190161 (199256)	Loss/tok 2.8476 (3.0211)	LR 1.797e-04
0: TRAIN [5][410/1291]	Time 0.042 (0.052)	Data 1.34e-04 (5.37e-04)	Tok/s 187083 (199147)	Loss/tok 2.8426 (3.0193)	LR 1.797e-04
0: TRAIN [5][420/1291]	Time 0.097 (0.052)	Data 1.11e-04 (5.26e-04)	Tok/s 234574 (199106)	Loss/tok 3.2989 (3.0203)	LR 1.797e-04
0: TRAIN [5][430/1291]	Time 0.059 (0.052)	Data 8.37e-05 (5.16e-04)	Tok/s 207977 (199157)	Loss/tok 3.1122 (3.0194)	LR 1.797e-04
0: TRAIN [5][440/1291]	Time 0.076 (0.052)	Data 8.01e-05 (5.07e-04)	Tok/s 232504 (199265)	Loss/tok 3.1285 (3.0189)	LR 1.797e-04
0: TRAIN [5][450/1291]	Time 0.076 (0.052)	Data 1.61e-04 (4.98e-04)	Tok/s 231510 (199233)	Loss/tok 3.2207 (3.0197)	LR 1.797e-04
0: TRAIN [5][460/1291]	Time 0.025 (0.053)	Data 8.01e-05 (4.89e-04)	Tok/s 160913 (199322)	Loss/tok 2.5274 (3.0216)	LR 1.797e-04
0: TRAIN [5][470/1291]	Time 0.060 (0.053)	Data 1.14e-04 (4.81e-04)	Tok/s 205444 (199423)	Loss/tok 3.2107 (3.0220)	LR 1.797e-04
0: Upscaling, new scale: 8192.0
0: TRAIN [5][480/1291]	Time 0.042 (0.053)	Data 1.34e-04 (4.73e-04)	Tok/s 182729 (199427)	Loss/tok 2.8705 (3.0240)	LR 1.797e-04
0: TRAIN [5][490/1291]	Time 0.060 (0.053)	Data 7.89e-05 (4.65e-04)	Tok/s 211573 (199416)	Loss/tok 3.0604 (3.0251)	LR 1.797e-04
0: TRAIN [5][500/1291]	Time 0.042 (0.053)	Data 7.75e-05 (4.57e-04)	Tok/s 178185 (199180)	Loss/tok 2.8507 (3.0243)	LR 1.797e-04
0: TRAIN [5][510/1291]	Time 0.060 (0.053)	Data 8.06e-05 (4.50e-04)	Tok/s 210759 (199091)	Loss/tok 3.0578 (3.0230)	LR 1.797e-04
0: TRAIN [5][520/1291]	Time 0.025 (0.052)	Data 7.75e-05 (4.43e-04)	Tok/s 155850 (198833)	Loss/tok 2.3994 (3.0222)	LR 1.797e-04
0: TRAIN [5][530/1291]	Time 0.043 (0.052)	Data 1.36e-04 (4.37e-04)	Tok/s 181839 (198693)	Loss/tok 2.8591 (3.0212)	LR 1.797e-04
0: TRAIN [5][540/1291]	Time 0.025 (0.052)	Data 1.37e-04 (4.30e-04)	Tok/s 156587 (198346)	Loss/tok 2.4917 (3.0189)	LR 1.797e-04
0: TRAIN [5][550/1291]	Time 0.060 (0.052)	Data 7.82e-05 (4.24e-04)	Tok/s 208182 (198333)	Loss/tok 3.0202 (3.0186)	LR 1.797e-04
0: TRAIN [5][560/1291]	Time 0.042 (0.052)	Data 1.39e-04 (4.18e-04)	Tok/s 187816 (198316)	Loss/tok 2.8658 (3.0184)	LR 1.797e-04
0: TRAIN [5][570/1291]	Time 0.043 (0.052)	Data 8.11e-05 (4.12e-04)	Tok/s 178297 (198391)	Loss/tok 2.8275 (3.0188)	LR 1.797e-04
0: TRAIN [5][580/1291]	Time 0.060 (0.052)	Data 8.03e-05 (4.07e-04)	Tok/s 210687 (198391)	Loss/tok 2.9098 (3.0199)	LR 1.797e-04
0: TRAIN [5][590/1291]	Time 0.026 (0.052)	Data 8.03e-05 (4.02e-04)	Tok/s 152471 (198288)	Loss/tok 2.5932 (3.0202)	LR 1.797e-04
0: TRAIN [5][600/1291]	Time 0.025 (0.052)	Data 8.20e-05 (3.97e-04)	Tok/s 155817 (198188)	Loss/tok 2.4651 (3.0199)	LR 1.797e-04
0: Upscaling, new scale: 8192.0
0: TRAIN [5][610/1291]	Time 0.059 (0.052)	Data 7.82e-05 (3.92e-04)	Tok/s 210261 (198216)	Loss/tok 2.9707 (3.0202)	LR 1.797e-04
0: TRAIN [5][620/1291]	Time 0.043 (0.052)	Data 8.44e-05 (3.87e-04)	Tok/s 184458 (198357)	Loss/tok 2.9523 (3.0213)	LR 1.797e-04
0: TRAIN [5][630/1291]	Time 0.098 (0.053)	Data 8.56e-05 (3.82e-04)	Tok/s 228398 (198337)	Loss/tok 3.3086 (3.0223)	LR 1.797e-04
0: TRAIN [5][640/1291]	Time 0.043 (0.053)	Data 7.77e-05 (3.77e-04)	Tok/s 180849 (198397)	Loss/tok 2.7968 (3.0239)	LR 1.797e-04
0: TRAIN [5][650/1291]	Time 0.043 (0.053)	Data 8.63e-05 (3.73e-04)	Tok/s 181181 (198298)	Loss/tok 2.8246 (3.0228)	LR 1.797e-04
0: TRAIN [5][660/1291]	Time 0.077 (0.053)	Data 1.16e-04 (3.69e-04)	Tok/s 225488 (198432)	Loss/tok 3.2478 (3.0239)	LR 1.797e-04
0: TRAIN [5][670/1291]	Time 0.078 (0.053)	Data 8.46e-05 (3.65e-04)	Tok/s 221332 (198505)	Loss/tok 3.1925 (3.0249)	LR 1.797e-04
0: TRAIN [5][680/1291]	Time 0.042 (0.053)	Data 7.96e-05 (3.61e-04)	Tok/s 186174 (198523)	Loss/tok 2.7925 (3.0249)	LR 1.797e-04
0: TRAIN [5][690/1291]	Time 0.042 (0.053)	Data 1.03e-04 (3.57e-04)	Tok/s 185019 (198609)	Loss/tok 2.8942 (3.0244)	LR 1.797e-04
0: TRAIN [5][700/1291]	Time 0.058 (0.053)	Data 8.51e-05 (3.53e-04)	Tok/s 218841 (198536)	Loss/tok 3.0077 (3.0244)	LR 1.797e-04
0: TRAIN [5][710/1291]	Time 0.042 (0.053)	Data 9.92e-05 (3.49e-04)	Tok/s 176848 (198509)	Loss/tok 2.9254 (3.0239)	LR 1.797e-04
0: TRAIN [5][720/1291]	Time 0.042 (0.053)	Data 1.29e-04 (3.45e-04)	Tok/s 184887 (198687)	Loss/tok 2.9562 (3.0258)	LR 1.797e-04
0: TRAIN [5][730/1291]	Time 0.076 (0.053)	Data 8.20e-05 (3.42e-04)	Tok/s 230320 (198838)	Loss/tok 3.1112 (3.0262)	LR 1.797e-04
0: Upscaling, new scale: 8192.0
0: TRAIN [5][740/1291]	Time 0.076 (0.053)	Data 7.80e-05 (3.38e-04)	Tok/s 229905 (198974)	Loss/tok 3.2555 (3.0265)	LR 1.797e-04
0: TRAIN [5][750/1291]	Time 0.076 (0.053)	Data 8.06e-05 (3.35e-04)	Tok/s 229873 (198966)	Loss/tok 3.1962 (3.0260)	LR 1.797e-04
0: TRAIN [5][760/1291]	Time 0.041 (0.053)	Data 8.34e-05 (3.32e-04)	Tok/s 188805 (198949)	Loss/tok 2.9645 (3.0252)	LR 1.797e-04
0: TRAIN [5][770/1291]	Time 0.058 (0.053)	Data 8.44e-05 (3.29e-04)	Tok/s 213697 (198972)	Loss/tok 3.0605 (3.0251)	LR 1.797e-04
0: TRAIN [5][780/1291]	Time 0.041 (0.053)	Data 8.32e-05 (3.26e-04)	Tok/s 187401 (198991)	Loss/tok 2.8773 (3.0244)	LR 1.797e-04
0: TRAIN [5][790/1291]	Time 0.058 (0.053)	Data 8.15e-05 (3.23e-04)	Tok/s 215931 (198988)	Loss/tok 2.9726 (3.0232)	LR 1.797e-04
0: TRAIN [5][800/1291]	Time 0.058 (0.053)	Data 8.44e-05 (3.20e-04)	Tok/s 218516 (199000)	Loss/tok 3.0402 (3.0226)	LR 1.797e-04
0: TRAIN [5][810/1291]	Time 0.058 (0.053)	Data 7.58e-05 (3.17e-04)	Tok/s 217949 (199033)	Loss/tok 2.8498 (3.0225)	LR 1.797e-04
0: TRAIN [5][820/1291]	Time 0.076 (0.053)	Data 7.96e-05 (3.14e-04)	Tok/s 228926 (199208)	Loss/tok 3.1436 (3.0246)	LR 1.797e-04
0: TRAIN [5][830/1291]	Time 0.041 (0.053)	Data 8.11e-05 (3.12e-04)	Tok/s 187910 (199231)	Loss/tok 2.8971 (3.0242)	LR 1.797e-04
0: TRAIN [5][840/1291]	Time 0.025 (0.053)	Data 7.84e-05 (3.09e-04)	Tok/s 162524 (199325)	Loss/tok 2.6610 (3.0244)	LR 1.797e-04
0: TRAIN [5][850/1291]	Time 0.097 (0.053)	Data 8.18e-05 (3.07e-04)	Tok/s 229452 (199400)	Loss/tok 3.3220 (3.0247)	LR 1.797e-04
0: Upscaling, new scale: 8192.0
0: TRAIN [5][860/1291]	Time 0.041 (0.053)	Data 1.35e-04 (3.04e-04)	Tok/s 183061 (199495)	Loss/tok 2.8113 (3.0259)	LR 1.797e-04
0: TRAIN [5][870/1291]	Time 0.077 (0.053)	Data 8.18e-05 (3.02e-04)	Tok/s 229117 (199534)	Loss/tok 3.1093 (3.0260)	LR 1.797e-04
0: TRAIN [5][880/1291]	Time 0.041 (0.053)	Data 7.87e-05 (3.00e-04)	Tok/s 186229 (199555)	Loss/tok 2.7501 (3.0258)	LR 1.797e-04
0: TRAIN [5][890/1291]	Time 0.042 (0.053)	Data 1.33e-04 (2.97e-04)	Tok/s 186827 (199657)	Loss/tok 2.8384 (3.0262)	LR 1.797e-04
0: TRAIN [5][900/1291]	Time 0.058 (0.053)	Data 8.23e-05 (2.95e-04)	Tok/s 219996 (199735)	Loss/tok 2.9779 (3.0266)	LR 1.797e-04
0: TRAIN [5][910/1291]	Time 0.076 (0.053)	Data 1.34e-04 (2.93e-04)	Tok/s 228556 (199705)	Loss/tok 3.1572 (3.0265)	LR 1.797e-04
0: TRAIN [5][920/1291]	Time 0.042 (0.053)	Data 8.11e-05 (2.91e-04)	Tok/s 186849 (199631)	Loss/tok 2.8497 (3.0261)	LR 1.797e-04
0: TRAIN [5][930/1291]	Time 0.041 (0.053)	Data 8.06e-05 (2.89e-04)	Tok/s 187059 (199509)	Loss/tok 2.7999 (3.0251)	LR 1.797e-04
0: TRAIN [5][940/1291]	Time 0.076 (0.053)	Data 1.02e-04 (2.87e-04)	Tok/s 227825 (199531)	Loss/tok 3.2867 (3.0253)	LR 1.797e-04
0: TRAIN [5][950/1291]	Time 0.041 (0.053)	Data 8.20e-05 (2.85e-04)	Tok/s 186180 (199636)	Loss/tok 2.7816 (3.0265)	LR 1.797e-04
0: TRAIN [5][960/1291]	Time 0.059 (0.053)	Data 8.56e-05 (2.83e-04)	Tok/s 213375 (199708)	Loss/tok 3.1030 (3.0277)	LR 1.797e-04
0: TRAIN [5][970/1291]	Time 0.076 (0.053)	Data 9.82e-05 (2.81e-04)	Tok/s 232139 (199743)	Loss/tok 3.0966 (3.0271)	LR 1.797e-04
0: TRAIN [5][980/1291]	Time 0.059 (0.053)	Data 8.15e-05 (2.79e-04)	Tok/s 213960 (199908)	Loss/tok 2.9404 (3.0285)	LR 1.797e-04
0: Upscaling, new scale: 8192.0
0: TRAIN [5][990/1291]	Time 0.058 (0.053)	Data 8.30e-05 (2.77e-04)	Tok/s 213867 (200088)	Loss/tok 3.0808 (3.0292)	LR 1.797e-04
0: TRAIN [5][1000/1291]	Time 0.042 (0.053)	Data 1.54e-04 (2.75e-04)	Tok/s 188398 (200064)	Loss/tok 2.6996 (3.0289)	LR 1.797e-04
0: TRAIN [5][1010/1291]	Time 0.041 (0.053)	Data 1.54e-04 (2.74e-04)	Tok/s 183658 (200050)	Loss/tok 2.8113 (3.0288)	LR 1.797e-04
0: TRAIN [5][1020/1291]	Time 0.041 (0.053)	Data 7.94e-05 (2.72e-04)	Tok/s 187437 (199987)	Loss/tok 2.9147 (3.0279)	LR 1.797e-04
0: TRAIN [5][1030/1291]	Time 0.042 (0.053)	Data 7.87e-05 (2.70e-04)	Tok/s 185996 (199956)	Loss/tok 2.8337 (3.0273)	LR 1.797e-04
0: TRAIN [5][1040/1291]	Time 0.076 (0.053)	Data 9.89e-05 (2.68e-04)	Tok/s 228399 (199974)	Loss/tok 3.2356 (3.0266)	LR 1.797e-04
0: TRAIN [5][1050/1291]	Time 0.025 (0.053)	Data 8.01e-05 (2.67e-04)	Tok/s 164526 (200014)	Loss/tok 2.6125 (3.0269)	LR 1.797e-04
0: TRAIN [5][1060/1291]	Time 0.058 (0.053)	Data 1.39e-04 (2.65e-04)	Tok/s 217057 (200097)	Loss/tok 3.0039 (3.0268)	LR 1.797e-04
0: TRAIN [5][1070/1291]	Time 0.076 (0.053)	Data 8.11e-05 (2.63e-04)	Tok/s 229808 (200051)	Loss/tok 3.1575 (3.0262)	LR 1.797e-04
0: TRAIN [5][1080/1291]	Time 0.058 (0.053)	Data 1.31e-04 (2.62e-04)	Tok/s 220397 (200003)	Loss/tok 3.0943 (3.0255)	LR 1.797e-04
0: TRAIN [5][1090/1291]	Time 0.041 (0.053)	Data 1.34e-04 (2.60e-04)	Tok/s 186678 (199935)	Loss/tok 2.8936 (3.0256)	LR 1.797e-04
0: TRAIN [5][1100/1291]	Time 0.025 (0.053)	Data 8.03e-05 (2.59e-04)	Tok/s 157013 (199944)	Loss/tok 2.5030 (3.0256)	LR 1.797e-04
0: TRAIN [5][1110/1291]	Time 0.042 (0.053)	Data 8.32e-05 (2.57e-04)	Tok/s 190674 (199890)	Loss/tok 2.8816 (3.0249)	LR 1.797e-04
0: Upscaling, new scale: 8192.0
0: TRAIN [5][1120/1291]	Time 0.041 (0.053)	Data 7.94e-05 (2.56e-04)	Tok/s 185522 (199823)	Loss/tok 2.8597 (3.0244)	LR 1.797e-04
0: TRAIN [5][1130/1291]	Time 0.058 (0.053)	Data 8.27e-05 (2.55e-04)	Tok/s 217511 (199839)	Loss/tok 2.9619 (3.0251)	LR 1.797e-04
0: TRAIN [5][1140/1291]	Time 0.041 (0.053)	Data 1.40e-04 (2.53e-04)	Tok/s 185741 (199889)	Loss/tok 2.9318 (3.0260)	LR 1.797e-04
0: TRAIN [5][1150/1291]	Time 0.041 (0.053)	Data 1.35e-04 (2.52e-04)	Tok/s 185548 (199946)	Loss/tok 2.8527 (3.0266)	LR 1.797e-04
0: TRAIN [5][1160/1291]	Time 0.058 (0.053)	Data 1.34e-04 (2.50e-04)	Tok/s 218080 (200004)	Loss/tok 2.9424 (3.0267)	LR 1.797e-04
0: TRAIN [5][1170/1291]	Time 0.042 (0.053)	Data 8.03e-05 (2.49e-04)	Tok/s 186759 (200032)	Loss/tok 2.7945 (3.0271)	LR 1.797e-04
0: TRAIN [5][1180/1291]	Time 0.042 (0.053)	Data 1.60e-04 (2.48e-04)	Tok/s 185600 (200027)	Loss/tok 2.8661 (3.0267)	LR 1.797e-04
0: TRAIN [5][1190/1291]	Time 0.076 (0.053)	Data 8.30e-05 (2.47e-04)	Tok/s 227024 (200092)	Loss/tok 3.2180 (3.0271)	LR 1.797e-04
0: TRAIN [5][1200/1291]	Time 0.097 (0.053)	Data 8.23e-05 (2.45e-04)	Tok/s 233089 (200159)	Loss/tok 3.3473 (3.0279)	LR 1.797e-04
0: TRAIN [5][1210/1291]	Time 0.059 (0.053)	Data 8.03e-05 (2.44e-04)	Tok/s 218240 (200124)	Loss/tok 3.0160 (3.0270)	LR 1.797e-04
0: TRAIN [5][1220/1291]	Time 0.041 (0.053)	Data 8.42e-05 (2.43e-04)	Tok/s 184379 (200101)	Loss/tok 2.9009 (3.0272)	LR 1.797e-04
0: TRAIN [5][1230/1291]	Time 0.076 (0.053)	Data 8.27e-05 (2.42e-04)	Tok/s 231836 (200143)	Loss/tok 3.1642 (3.0271)	LR 1.797e-04
0: TRAIN [5][1240/1291]	Time 0.059 (0.053)	Data 8.15e-05 (2.40e-04)	Tok/s 213378 (200112)	Loss/tok 3.0331 (3.0271)	LR 1.797e-04
0: Upscaling, new scale: 8192.0
0: TRAIN [5][1250/1291]	Time 0.025 (0.053)	Data 1.06e-04 (2.39e-04)	Tok/s 155222 (200124)	Loss/tok 2.4962 (3.0271)	LR 1.797e-04
0: TRAIN [5][1260/1291]	Time 0.058 (0.053)	Data 7.99e-05 (2.38e-04)	Tok/s 217723 (200131)	Loss/tok 3.0788 (3.0275)	LR 1.797e-04
0: TRAIN [5][1270/1291]	Time 0.024 (0.053)	Data 8.11e-05 (2.37e-04)	Tok/s 159043 (200106)	Loss/tok 2.4761 (3.0271)	LR 1.797e-04
0: TRAIN [5][1280/1291]	Time 0.060 (0.053)	Data 7.96e-05 (2.36e-04)	Tok/s 213593 (200056)	Loss/tok 2.9725 (3.0272)	LR 1.797e-04
0: TRAIN [5][1290/1291]	Time 0.042 (0.053)	Data 4.10e-05 (2.36e-04)	Tok/s 189914 (200038)	Loss/tok 2.8189 (3.0273)	LR 1.797e-04
:::MLLOG {"namespace": "", "time_ms": 1593113502295, "event_type": "INTERVAL_END", "key": "epoch_stop", "value": null, "metadata": {"file": "train.py", "lineno": 551, "epoch_num": 6}}
:::MLLOG {"namespace": "", "time_ms": 1593113502295, "event_type": "INTERVAL_START", "key": "eval_start", "value": null, "metadata": {"file": "train.py", "lineno": 556, "epoch_num": 6}}
0: Running evaluation on test set
0: TEST [5][0/3]	Time 0.299 (0.299)	Decoder iters 109.0 (109.0)	Tok/s 30102 (30102)
0: Running moses detokenizer
0: BLEU(score=23.952384227686117, counts=[37081, 18554, 10513, 6203], totals=[65358, 62355, 59352, 56353], precisions=[56.735212215796075, 29.755432603640447, 17.712966707103384, 11.007399783507532], bp=1.0, sys_len=65358, ref_len=64676)
0: Finished evaluation on test set
:::MLLOG {"namespace": "", "time_ms": 1593113503466, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.2395, "metadata": {"file": "train.py", "lineno": 562, "epoch_num": 6}}
:::MLLOG {"namespace": "", "time_ms": 1593113503467, "event_type": "INTERVAL_END", "key": "eval_stop", "value": null, "metadata": {"file": "train.py", "lineno": 565, "epoch_num": 6}}
0: Summary: Epoch: 5	Training Loss: 3.0255	Test BLEU: 23.95
0: Performance: Epoch: 5	Training: 3200906 Tok/s
0: Finished epoch 5
:::MLLOG {"namespace": "", "time_ms": 1593113503467, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "train.py", "lineno": 584, "first_epoch_num": 6}}
:::MLLOG {"namespace": "", "time_ms": 1593113503467, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "train.py", "lineno": 538, "first_epoch_num": 7, "epoch_count": 1}}
:::MLLOG {"namespace": "", "time_ms": 1593113503467, "event_type": "INTERVAL_START", "key": "epoch_start", "value": null, "metadata": {"file": "train.py", "lineno": 541, "epoch_num": 7}}
0: Starting epoch 6
0: Sampler for epoch 6 uses seed 1949428919
0: TRAIN [6][0/1291]	Time 0.291 (0.291)	Data 1.93e-01 (1.93e-01)	Tok/s 26491 (26491)	Loss/tok 2.7113 (2.7113)	LR 1.797e-04
0: TRAIN [6][10/1291]	Time 0.076 (0.069)	Data 8.70e-05 (1.77e-02)	Tok/s 232462 (179046)	Loss/tok 3.0677 (2.8728)	LR 1.797e-04
0: TRAIN [6][20/1291]	Time 0.042 (0.062)	Data 1.55e-04 (9.30e-03)	Tok/s 185998 (187752)	Loss/tok 2.9982 (2.9859)	LR 1.797e-04
0: TRAIN [6][30/1291]	Time 0.059 (0.061)	Data 9.04e-05 (6.33e-03)	Tok/s 211247 (195993)	Loss/tok 3.0303 (2.9913)	LR 1.797e-04
0: TRAIN [6][40/1291]	Time 0.077 (0.060)	Data 9.01e-05 (4.81e-03)	Tok/s 229539 (199513)	Loss/tok 3.1076 (2.9915)	LR 1.797e-04
0: TRAIN [6][50/1291]	Time 0.059 (0.058)	Data 9.54e-05 (3.88e-03)	Tok/s 216843 (199931)	Loss/tok 2.9938 (2.9877)	LR 1.797e-04
0: TRAIN [6][60/1291]	Time 0.042 (0.057)	Data 9.58e-05 (3.27e-03)	Tok/s 182477 (199940)	Loss/tok 2.8072 (2.9933)	LR 1.797e-04
0: TRAIN [6][70/1291]	Time 0.041 (0.057)	Data 1.01e-04 (2.82e-03)	Tok/s 181359 (200482)	Loss/tok 2.8793 (3.0009)	LR 1.797e-04
0: Upscaling, new scale: 8192.0
0: TRAIN [6][80/1291]	Time 0.041 (0.056)	Data 8.27e-05 (2.48e-03)	Tok/s 183185 (199993)	Loss/tok 2.8785 (2.9953)	LR 1.797e-04
0: TRAIN [6][90/1291]	Time 0.042 (0.056)	Data 8.82e-05 (2.22e-03)	Tok/s 185243 (200316)	Loss/tok 2.8577 (3.0038)	LR 1.797e-04
0: TRAIN [6][100/1291]	Time 0.059 (0.055)	Data 8.58e-05 (2.01e-03)	Tok/s 213866 (200803)	Loss/tok 3.0100 (3.0049)	LR 1.797e-04
0: TRAIN [6][110/1291]	Time 0.042 (0.055)	Data 8.23e-05 (1.84e-03)	Tok/s 187049 (200662)	Loss/tok 2.8415 (3.0029)	LR 1.797e-04
0: TRAIN [6][120/1291]	Time 0.059 (0.054)	Data 8.54e-05 (1.69e-03)	Tok/s 217128 (200146)	Loss/tok 3.0392 (2.9972)	LR 1.797e-04
0: TRAIN [6][130/1291]	Time 0.041 (0.054)	Data 8.73e-05 (1.57e-03)	Tok/s 189392 (199644)	Loss/tok 2.7108 (2.9896)	LR 1.797e-04
0: TRAIN [6][140/1291]	Time 0.076 (0.054)	Data 8.42e-05 (1.46e-03)	Tok/s 226233 (200200)	Loss/tok 3.2168 (2.9952)	LR 1.797e-04
0: TRAIN [6][150/1291]	Time 0.041 (0.054)	Data 8.92e-05 (1.37e-03)	Tok/s 187283 (199989)	Loss/tok 2.8845 (2.9942)	LR 1.797e-04
0: TRAIN [6][160/1291]	Time 0.058 (0.054)	Data 8.39e-05 (1.29e-03)	Tok/s 212221 (200169)	Loss/tok 3.0097 (2.9941)	LR 1.797e-04
0: TRAIN [6][170/1291]	Time 0.059 (0.054)	Data 8.15e-05 (1.22e-03)	Tok/s 215182 (200514)	Loss/tok 2.8681 (3.0001)	LR 1.797e-04
0: TRAIN [6][180/1291]	Time 0.043 (0.053)	Data 8.68e-05 (1.16e-03)	Tok/s 178073 (199979)	Loss/tok 2.7968 (2.9972)	LR 1.797e-04
0: TRAIN [6][190/1291]	Time 0.059 (0.053)	Data 8.85e-05 (1.10e-03)	Tok/s 212521 (199844)	Loss/tok 3.0059 (2.9948)	LR 1.797e-04
0: TRAIN [6][200/1291]	Time 0.060 (0.053)	Data 8.54e-05 (1.05e-03)	Tok/s 210299 (199658)	Loss/tok 2.9322 (2.9923)	LR 1.797e-04
0: Upscaling, new scale: 8192.0
0: TRAIN [6][210/1291]	Time 0.043 (0.053)	Data 8.89e-05 (1.01e-03)	Tok/s 179321 (199355)	Loss/tok 2.7724 (2.9911)	LR 1.797e-04
0: TRAIN [6][220/1291]	Time 0.043 (0.053)	Data 8.65e-05 (9.65e-04)	Tok/s 182193 (198966)	Loss/tok 2.8658 (2.9918)	LR 1.797e-04
0: TRAIN [6][230/1291]	Time 0.042 (0.053)	Data 8.82e-05 (9.27e-04)	Tok/s 182520 (198763)	Loss/tok 3.0257 (2.9935)	LR 1.797e-04
0: TRAIN [6][240/1291]	Time 0.059 (0.053)	Data 8.51e-05 (8.92e-04)	Tok/s 217375 (199201)	Loss/tok 3.0791 (2.9995)	LR 1.797e-04
0: TRAIN [6][250/1291]	Time 0.043 (0.053)	Data 8.65e-05 (8.60e-04)	Tok/s 177227 (199053)	Loss/tok 2.8448 (2.9990)	LR 1.797e-04
0: TRAIN [6][260/1291]	Time 0.042 (0.053)	Data 8.42e-05 (8.31e-04)	Tok/s 179130 (198804)	Loss/tok 2.6945 (2.9981)	LR 1.797e-04
0: TRAIN [6][270/1291]	Time 0.059 (0.053)	Data 8.56e-05 (8.03e-04)	Tok/s 209030 (198744)	Loss/tok 3.0387 (2.9995)	LR 1.797e-04
0: TRAIN [6][280/1291]	Time 0.026 (0.053)	Data 8.58e-05 (7.78e-04)	Tok/s 153942 (198659)	Loss/tok 2.4039 (3.0015)	LR 1.797e-04
0: TRAIN [6][290/1291]	Time 0.042 (0.053)	Data 8.20e-05 (7.54e-04)	Tok/s 186872 (198444)	Loss/tok 2.8868 (3.0017)	LR 1.797e-04
0: TRAIN [6][300/1291]	Time 0.043 (0.054)	Data 8.73e-05 (7.32e-04)	Tok/s 178328 (198807)	Loss/tok 2.8247 (3.0084)	LR 1.797e-04
0: TRAIN [6][310/1291]	Time 0.098 (0.054)	Data 8.85e-05 (7.11e-04)	Tok/s 227820 (199012)	Loss/tok 3.3042 (3.0140)	LR 1.797e-04
0: TRAIN [6][320/1291]	Time 0.060 (0.054)	Data 8.49e-05 (6.92e-04)	Tok/s 210968 (198642)	Loss/tok 2.9705 (3.0126)	LR 1.797e-04
0: TRAIN [6][330/1291]	Time 0.025 (0.054)	Data 8.94e-05 (6.73e-04)	Tok/s 154987 (198474)	Loss/tok 2.5020 (3.0121)	LR 1.797e-04
0: Upscaling, new scale: 8192.0
0: TRAIN [6][340/1291]	Time 0.099 (0.054)	Data 8.82e-05 (6.56e-04)	Tok/s 223620 (198497)	Loss/tok 3.3931 (3.0122)	LR 1.797e-04
0: TRAIN [6][350/1291]	Time 0.043 (0.054)	Data 8.51e-05 (6.40e-04)	Tok/s 178470 (198322)	Loss/tok 2.8907 (3.0122)	LR 1.797e-04
0: TRAIN [6][360/1291]	Time 0.043 (0.054)	Data 8.61e-05 (6.24e-04)	Tok/s 176702 (198206)	Loss/tok 2.9000 (3.0127)	LR 1.797e-04
0: TRAIN [6][370/1291]	Time 0.043 (0.054)	Data 8.73e-05 (6.10e-04)	Tok/s 183712 (198146)	Loss/tok 2.7959 (3.0114)	LR 1.797e-04
0: TRAIN [6][380/1291]	Time 0.059 (0.054)	Data 9.37e-05 (5.96e-04)	Tok/s 209830 (198010)	Loss/tok 3.0057 (3.0114)	LR 1.797e-04
0: TRAIN [6][390/1291]	Time 0.042 (0.053)	Data 8.56e-05 (5.83e-04)	Tok/s 184701 (197630)	Loss/tok 2.8497 (3.0085)	LR 1.797e-04
0: TRAIN [6][400/1291]	Time 0.043 (0.053)	Data 8.85e-05 (5.71e-04)	Tok/s 179474 (197530)	Loss/tok 2.8850 (3.0086)	LR 1.797e-04
0: TRAIN [6][410/1291]	Time 0.059 (0.053)	Data 8.46e-05 (5.59e-04)	Tok/s 213573 (197220)	Loss/tok 3.0960 (3.0072)	LR 1.797e-04
0: TRAIN [6][420/1291]	Time 0.042 (0.053)	Data 8.32e-05 (5.48e-04)	Tok/s 178682 (197103)	Loss/tok 2.8584 (3.0079)	LR 1.797e-04
0: TRAIN [6][430/1291]	Time 0.042 (0.053)	Data 8.25e-05 (5.37e-04)	Tok/s 179968 (197263)	Loss/tok 2.8293 (3.0090)	LR 1.797e-04
0: TRAIN [6][440/1291]	Time 0.078 (0.053)	Data 8.85e-05 (5.27e-04)	Tok/s 227435 (197386)	Loss/tok 3.1596 (3.0095)	LR 1.797e-04
0: TRAIN [6][450/1291]	Time 0.042 (0.053)	Data 8.89e-05 (5.17e-04)	Tok/s 184780 (197294)	Loss/tok 2.7988 (3.0093)	LR 1.797e-04
0: TRAIN [6][460/1291]	Time 0.060 (0.053)	Data 8.68e-05 (5.08e-04)	Tok/s 210825 (197318)	Loss/tok 3.0506 (3.0122)	LR 1.797e-04
0: Upscaling, new scale: 8192.0
0: TRAIN [6][470/1291]	Time 0.042 (0.053)	Data 8.30e-05 (4.99e-04)	Tok/s 180853 (197229)	Loss/tok 2.9014 (3.0118)	LR 1.797e-04
0: TRAIN [6][480/1291]	Time 0.043 (0.054)	Data 8.70e-05 (4.90e-04)	Tok/s 182836 (197456)	Loss/tok 2.7600 (3.0136)	LR 1.797e-04
0: TRAIN [6][490/1291]	Time 0.060 (0.054)	Data 8.49e-05 (4.82e-04)	Tok/s 212625 (197613)	Loss/tok 3.0631 (3.0156)	LR 1.797e-04
0: TRAIN [6][500/1291]	Time 0.060 (0.054)	Data 8.94e-05 (4.74e-04)	Tok/s 212673 (197479)	Loss/tok 2.9796 (3.0146)	LR 1.797e-04
0: TRAIN [6][510/1291]	Time 0.077 (0.054)	Data 8.51e-05 (4.67e-04)	Tok/s 225678 (197592)	Loss/tok 3.1923 (3.0166)	LR 1.797e-04
0: TRAIN [6][520/1291]	Time 0.043 (0.054)	Data 8.68e-05 (4.59e-04)	Tok/s 175780 (197728)	Loss/tok 2.8704 (3.0196)	LR 1.797e-04
0: TRAIN [6][530/1291]	Time 0.076 (0.054)	Data 8.30e-05 (4.52e-04)	Tok/s 227179 (198033)	Loss/tok 3.2829 (3.0234)	LR 1.797e-04
0: TRAIN [6][540/1291]	Time 0.025 (0.054)	Data 8.87e-05 (4.45e-04)	Tok/s 157283 (197899)	Loss/tok 2.4278 (3.0217)	LR 1.797e-04
0: TRAIN [6][550/1291]	Time 0.076 (0.054)	Data 8.46e-05 (4.39e-04)	Tok/s 229293 (198001)	Loss/tok 3.0914 (3.0205)	LR 1.797e-04
0: TRAIN [6][560/1291]	Time 0.097 (0.054)	Data 8.37e-05 (4.33e-04)	Tok/s 225615 (198149)	Loss/tok 3.3494 (3.0217)	LR 1.797e-04
0: TRAIN [6][570/1291]	Time 0.058 (0.054)	Data 8.46e-05 (4.27e-04)	Tok/s 215646 (198197)	Loss/tok 3.0879 (3.0217)	LR 1.797e-04
0: TRAIN [6][580/1291]	Time 0.077 (0.054)	Data 8.65e-05 (4.21e-04)	Tok/s 227578 (198229)	Loss/tok 3.0904 (3.0209)	LR 1.797e-04
0: TRAIN [6][590/1291]	Time 0.058 (0.054)	Data 8.70e-05 (4.15e-04)	Tok/s 217854 (198331)	Loss/tok 3.0506 (3.0220)	LR 1.797e-04
0: Upscaling, new scale: 8192.0
0: TRAIN [6][600/1291]	Time 0.024 (0.054)	Data 9.11e-05 (4.10e-04)	Tok/s 158955 (198343)	Loss/tok 2.3892 (3.0212)	LR 1.797e-04
0: TRAIN [6][610/1291]	Time 0.041 (0.054)	Data 8.49e-05 (4.04e-04)	Tok/s 185239 (198394)	Loss/tok 2.8497 (3.0208)	LR 1.797e-04
0: TRAIN [6][620/1291]	Time 0.058 (0.054)	Data 8.44e-05 (3.99e-04)	Tok/s 214645 (198457)	Loss/tok 3.1245 (3.0206)	LR 1.797e-04
0: TRAIN [6][630/1291]	Time 0.041 (0.054)	Data 8.15e-05 (3.94e-04)	Tok/s 187240 (198589)	Loss/tok 2.7467 (3.0231)	LR 1.797e-04
0: TRAIN [6][640/1291]	Time 0.041 (0.054)	Data 8.75e-05 (3.89e-04)	Tok/s 189263 (198430)	Loss/tok 2.8491 (3.0213)	LR 1.797e-04
0: TRAIN [6][650/1291]	Time 0.058 (0.054)	Data 8.73e-05 (3.85e-04)	Tok/s 210713 (198500)	Loss/tok 3.0266 (3.0211)	LR 1.797e-04
0: TRAIN [6][660/1291]	Time 0.059 (0.054)	Data 9.75e-05 (3.80e-04)	Tok/s 217389 (198629)	Loss/tok 2.9033 (3.0204)	LR 1.797e-04
0: TRAIN [6][670/1291]	Time 0.042 (0.054)	Data 8.77e-05 (3.76e-04)	Tok/s 185730 (198784)	Loss/tok 2.8030 (3.0215)	LR 1.797e-04
0: TRAIN [6][680/1291]	Time 0.041 (0.054)	Data 8.56e-05 (3.72e-04)	Tok/s 186169 (198782)	Loss/tok 2.9798 (3.0219)	LR 1.797e-04
0: TRAIN [6][690/1291]	Time 0.041 (0.054)	Data 8.82e-05 (3.67e-04)	Tok/s 182373 (198734)	Loss/tok 2.9306 (3.0217)	LR 1.797e-04
0: TRAIN [6][700/1291]	Time 0.097 (0.054)	Data 8.56e-05 (3.63e-04)	Tok/s 233695 (198891)	Loss/tok 3.2836 (3.0230)	LR 1.797e-04
0: TRAIN [6][710/1291]	Time 0.041 (0.054)	Data 8.75e-05 (3.60e-04)	Tok/s 186488 (198957)	Loss/tok 2.8124 (3.0234)	LR 1.797e-04
0: Upscaling, new scale: 8192.0
0: TRAIN [6][720/1291]	Time 0.058 (0.054)	Data 8.42e-05 (3.56e-04)	Tok/s 211926 (199050)	Loss/tok 2.9579 (3.0228)	LR 1.797e-04
0: TRAIN [6][730/1291]	Time 0.041 (0.054)	Data 8.99e-05 (3.52e-04)	Tok/s 186609 (199014)	Loss/tok 2.9123 (3.0215)	LR 1.797e-04
0: TRAIN [6][740/1291]	Time 0.077 (0.054)	Data 8.65e-05 (3.48e-04)	Tok/s 230034 (199069)	Loss/tok 3.1121 (3.0211)	LR 1.797e-04
0: TRAIN [6][750/1291]	Time 0.042 (0.054)	Data 8.51e-05 (3.45e-04)	Tok/s 187380 (199048)	Loss/tok 2.8542 (3.0209)	LR 1.797e-04
0: TRAIN [6][760/1291]	Time 0.042 (0.054)	Data 8.54e-05 (3.42e-04)	Tok/s 183796 (199120)	Loss/tok 2.8630 (3.0221)	LR 1.797e-04
0: TRAIN [6][770/1291]	Time 0.058 (0.054)	Data 8.89e-05 (3.38e-04)	Tok/s 219090 (199121)	Loss/tok 3.0874 (3.0209)	LR 1.797e-04
0: TRAIN [6][780/1291]	Time 0.042 (0.054)	Data 8.46e-05 (3.35e-04)	Tok/s 183063 (199159)	Loss/tok 2.8788 (3.0215)	LR 1.797e-04
0: TRAIN [6][790/1291]	Time 0.058 (0.054)	Data 8.65e-05 (3.32e-04)	Tok/s 215020 (199213)	Loss/tok 3.0737 (3.0207)	LR 1.797e-04
0: TRAIN [6][800/1291]	Time 0.058 (0.054)	Data 8.63e-05 (3.29e-04)	Tok/s 218701 (199187)	Loss/tok 3.0806 (3.0206)	LR 1.797e-04
0: TRAIN [6][810/1291]	Time 0.076 (0.054)	Data 8.68e-05 (3.26e-04)	Tok/s 233579 (199264)	Loss/tok 3.0901 (3.0204)	LR 1.797e-04
0: TRAIN [6][820/1291]	Time 0.025 (0.054)	Data 8.42e-05 (3.23e-04)	Tok/s 160985 (199184)	Loss/tok 2.4351 (3.0202)	LR 1.797e-04
0: TRAIN [6][830/1291]	Time 0.076 (0.054)	Data 8.61e-05 (3.20e-04)	Tok/s 229432 (199275)	Loss/tok 3.1569 (3.0208)	LR 1.797e-04
0: TRAIN [6][840/1291]	Time 0.042 (0.054)	Data 8.61e-05 (3.17e-04)	Tok/s 182841 (199407)	Loss/tok 2.8513 (3.0211)	LR 1.797e-04
0: Upscaling, new scale: 8192.0
0: TRAIN [6][850/1291]	Time 0.041 (0.054)	Data 8.70e-05 (3.15e-04)	Tok/s 188614 (199409)	Loss/tok 2.8010 (3.0203)	LR 1.797e-04
0: TRAIN [6][860/1291]	Time 0.097 (0.054)	Data 8.51e-05 (3.12e-04)	Tok/s 231273 (199398)	Loss/tok 3.4472 (3.0204)	LR 1.797e-04
0: TRAIN [6][870/1291]	Time 0.041 (0.054)	Data 8.06e-05 (3.09e-04)	Tok/s 188601 (199461)	Loss/tok 2.7817 (3.0198)	LR 1.797e-04
0: TRAIN [6][880/1291]	Time 0.041 (0.054)	Data 9.25e-05 (3.07e-04)	Tok/s 185317 (199528)	Loss/tok 2.8664 (3.0205)	LR 1.797e-04
0: TRAIN [6][890/1291]	Time 0.025 (0.054)	Data 8.46e-05 (3.04e-04)	Tok/s 160358 (199527)	Loss/tok 2.6247 (3.0200)	LR 1.797e-04
0: TRAIN [6][900/1291]	Time 0.041 (0.054)	Data 8.99e-05 (3.02e-04)	Tok/s 192653 (199460)	Loss/tok 2.8301 (3.0194)	LR 1.797e-04
0: TRAIN [6][910/1291]	Time 0.042 (0.054)	Data 8.42e-05 (3.00e-04)	Tok/s 186103 (199550)	Loss/tok 2.8017 (3.0195)	LR 1.797e-04
0: TRAIN [6][920/1291]	Time 0.076 (0.054)	Data 8.77e-05 (2.97e-04)	Tok/s 224283 (199504)	Loss/tok 3.3177 (3.0194)	LR 1.797e-04
0: TRAIN [6][930/1291]	Time 0.058 (0.054)	Data 8.92e-05 (2.95e-04)	Tok/s 214246 (199538)	Loss/tok 3.0526 (3.0194)	LR 1.797e-04
0: TRAIN [6][940/1291]	Time 0.043 (0.054)	Data 8.56e-05 (2.93e-04)	Tok/s 177235 (199513)	Loss/tok 2.7879 (3.0191)	LR 1.797e-04
0: TRAIN [6][950/1291]	Time 0.043 (0.054)	Data 8.42e-05 (2.91e-04)	Tok/s 176909 (199396)	Loss/tok 2.9144 (3.0183)	LR 1.797e-04
0: TRAIN [6][960/1291]	Time 0.059 (0.054)	Data 8.70e-05 (2.89e-04)	Tok/s 212124 (199398)	Loss/tok 2.9462 (3.0188)	LR 1.797e-04
0: TRAIN [6][970/1291]	Time 0.043 (0.053)	Data 8.96e-05 (2.86e-04)	Tok/s 179217 (199266)	Loss/tok 2.8752 (3.0179)	LR 1.797e-04
0: Upscaling, new scale: 8192.0
0: TRAIN [6][980/1291]	Time 0.042 (0.053)	Data 8.56e-05 (2.84e-04)	Tok/s 182464 (199228)	Loss/tok 2.7715 (3.0173)	LR 1.797e-04
0: TRAIN [6][990/1291]	Time 0.043 (0.053)	Data 8.65e-05 (2.82e-04)	Tok/s 181085 (199140)	Loss/tok 2.9543 (3.0172)	LR 1.797e-04
0: TRAIN [6][1000/1291]	Time 0.042 (0.053)	Data 8.42e-05 (2.80e-04)	Tok/s 187035 (199060)	Loss/tok 2.7867 (3.0159)	LR 1.797e-04
0: TRAIN [6][1010/1291]	Time 0.059 (0.053)	Data 8.96e-05 (2.79e-04)	Tok/s 212271 (199014)	Loss/tok 3.0453 (3.0153)	LR 1.797e-04
0: TRAIN [6][1020/1291]	Time 0.026 (0.053)	Data 8.46e-05 (2.77e-04)	Tok/s 155290 (198909)	Loss/tok 2.4664 (3.0157)	LR 1.797e-04
0: TRAIN [6][1030/1291]	Time 0.043 (0.053)	Data 8.61e-05 (2.75e-04)	Tok/s 181826 (198897)	Loss/tok 2.7818 (3.0153)	LR 1.797e-04
0: TRAIN [6][1040/1291]	Time 0.077 (0.053)	Data 8.42e-05 (2.73e-04)	Tok/s 222867 (198917)	Loss/tok 3.2265 (3.0155)	LR 1.797e-04
0: TRAIN [6][1050/1291]	Time 0.043 (0.053)	Data 8.56e-05 (2.71e-04)	Tok/s 182690 (198886)	Loss/tok 2.7093 (3.0155)	LR 1.797e-04
0: TRAIN [6][1060/1291]	Time 0.059 (0.053)	Data 8.68e-05 (2.70e-04)	Tok/s 211388 (198923)	Loss/tok 3.0138 (3.0155)	LR 1.797e-04
0: TRAIN [6][1070/1291]	Time 0.043 (0.053)	Data 8.54e-05 (2.68e-04)	Tok/s 181829 (198973)	Loss/tok 2.7602 (3.0158)	LR 1.797e-04
0: TRAIN [6][1080/1291]	Time 0.042 (0.053)	Data 8.54e-05 (2.66e-04)	Tok/s 182289 (198889)	Loss/tok 2.8027 (3.0151)	LR 1.797e-04
0: TRAIN [6][1090/1291]	Time 0.042 (0.053)	Data 8.80e-05 (2.64e-04)	Tok/s 186516 (198847)	Loss/tok 2.7760 (3.0145)	LR 1.797e-04
0: TRAIN [6][1100/1291]	Time 0.041 (0.053)	Data 8.70e-05 (2.63e-04)	Tok/s 183735 (198848)	Loss/tok 2.8249 (3.0145)	LR 1.797e-04
0: Upscaling, new scale: 8192.0
0: TRAIN [6][1110/1291]	Time 0.076 (0.053)	Data 8.37e-05 (2.61e-04)	Tok/s 228742 (198873)	Loss/tok 3.1754 (3.0151)	LR 1.797e-04
0: TRAIN [6][1120/1291]	Time 0.059 (0.053)	Data 8.63e-05 (2.60e-04)	Tok/s 215319 (198878)	Loss/tok 2.9097 (3.0146)	LR 1.797e-04
0: TRAIN [6][1130/1291]	Time 0.076 (0.053)	Data 9.08e-05 (2.58e-04)	Tok/s 228070 (198914)	Loss/tok 3.3180 (3.0146)	LR 1.797e-04
0: TRAIN [6][1140/1291]	Time 0.042 (0.053)	Data 8.46e-05 (2.57e-04)	Tok/s 187631 (198926)	Loss/tok 2.8121 (3.0147)	LR 1.797e-04
0: TRAIN [6][1150/1291]	Time 0.059 (0.053)	Data 8.61e-05 (2.55e-04)	Tok/s 210106 (199001)	Loss/tok 2.9447 (3.0151)	LR 1.797e-04
0: TRAIN [6][1160/1291]	Time 0.077 (0.053)	Data 8.68e-05 (2.54e-04)	Tok/s 226070 (199019)	Loss/tok 3.2369 (3.0151)	LR 1.797e-04
0: TRAIN [6][1170/1291]	Time 0.076 (0.053)	Data 8.80e-05 (2.52e-04)	Tok/s 232180 (199041)	Loss/tok 3.1633 (3.0146)	LR 1.797e-04
0: TRAIN [6][1180/1291]	Time 0.058 (0.053)	Data 8.58e-05 (2.51e-04)	Tok/s 216544 (199175)	Loss/tok 3.0469 (3.0155)	LR 1.797e-04
0: TRAIN [6][1190/1291]	Time 0.042 (0.053)	Data 9.44e-05 (2.50e-04)	Tok/s 186460 (199195)	Loss/tok 2.8767 (3.0157)	LR 1.797e-04
0: TRAIN [6][1200/1291]	Time 0.042 (0.053)	Data 9.23e-05 (2.48e-04)	Tok/s 186742 (199102)	Loss/tok 2.8552 (3.0147)	LR 1.797e-04
0: TRAIN [6][1210/1291]	Time 0.097 (0.053)	Data 8.92e-05 (2.47e-04)	Tok/s 230217 (199188)	Loss/tok 3.4102 (3.0164)	LR 1.797e-04
0: TRAIN [6][1220/1291]	Time 0.058 (0.054)	Data 8.51e-05 (2.46e-04)	Tok/s 215331 (199303)	Loss/tok 2.9498 (3.0174)	LR 1.797e-04
0: TRAIN [6][1230/1291]	Time 0.059 (0.054)	Data 8.27e-05 (2.44e-04)	Tok/s 217632 (199322)	Loss/tok 3.1539 (3.0181)	LR 1.797e-04
0: Upscaling, new scale: 8192.0
0: TRAIN [6][1240/1291]	Time 0.058 (0.053)	Data 8.92e-05 (2.43e-04)	Tok/s 214703 (199270)	Loss/tok 3.1266 (3.0175)	LR 1.797e-04
0: TRAIN [6][1250/1291]	Time 0.058 (0.053)	Data 8.87e-05 (2.42e-04)	Tok/s 221337 (199205)	Loss/tok 2.9491 (3.0168)	LR 1.797e-04
0: TRAIN [6][1260/1291]	Time 0.059 (0.053)	Data 8.70e-05 (2.41e-04)	Tok/s 213599 (199233)	Loss/tok 2.9572 (3.0164)	LR 1.797e-04
0: TRAIN [6][1270/1291]	Time 0.076 (0.053)	Data 8.68e-05 (2.39e-04)	Tok/s 232530 (199229)	Loss/tok 3.1263 (3.0161)	LR 1.797e-04
0: TRAIN [6][1280/1291]	Time 0.077 (0.053)	Data 8.58e-05 (2.38e-04)	Tok/s 228724 (199293)	Loss/tok 2.9588 (3.0160)	LR 1.797e-04
0: TRAIN [6][1290/1291]	Time 0.076 (0.053)	Data 3.91e-05 (2.38e-04)	Tok/s 231418 (199287)	Loss/tok 3.1147 (3.0157)	LR 1.797e-04
:::MLLOG {"namespace": "", "time_ms": 1593113572388, "event_type": "INTERVAL_END", "key": "epoch_stop", "value": null, "metadata": {"file": "train.py", "lineno": 551, "epoch_num": 7}}
:::MLLOG {"namespace": "", "time_ms": 1593113572389, "event_type": "INTERVAL_START", "key": "eval_start", "value": null, "metadata": {"file": "train.py", "lineno": 556, "epoch_num": 7}}
0: Running evaluation on test set
0: TEST [6][0/3]	Time 0.290 (0.290)	Decoder iters 101.0 (101.0)	Tok/s 30881 (30881)
0: Running moses detokenizer
0: BLEU(score=24.053373603015498, counts=[37025, 18557, 10538, 6218], totals=[65155, 62152, 59149, 56151], precisions=[56.82603023559205, 29.857446260780023, 17.81602393954251, 11.073711955263486], bp=1.0, sys_len=65155, ref_len=64676)
0: Target accuracy reached
0: Finished evaluation on test set
:::MLLOG {"namespace": "", "time_ms": 1593113573562, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.24050000000000002, "metadata": {"file": "train.py", "lineno": 562, "epoch_num": 7}}
:::MLLOG {"namespace": "", "time_ms": 1593113573563, "event_type": "INTERVAL_END", "key": "eval_stop", "value": null, "metadata": {"file": "train.py", "lineno": 565, "epoch_num": 7}}
0: Summary: Epoch: 6	Training Loss: 3.0145	Test BLEU: 24.05
0: Performance: Epoch: 6	Training: 3188761 Tok/s
0: Finished epoch 6
:::MLLOG {"namespace": "", "time_ms": 1593113573563, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "train.py", "lineno": 584, "first_epoch_num": 7}}
0: Closing preprocessed data file
:::MLLOG {"namespace": "", "time_ms": 1593113573563, "event_type": "INTERVAL_END", "key": "run_stop", "value": null, "metadata": {"file": "train.py", "lineno": 595, "status": "success"}}
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
ENDING TIMING RUN AT 2020-06-25 12:33:00 PM
RESULT,RNN_TRANSLATOR,,521,nvidia,2020-06-25 12:24:19 PM
ENDING TIMING RUN AT 2020-06-25 12:33:00 PM
RESULT,RNN_TRANSLATOR,,521,nvidia,2020-06-25 12:24:19 PM
ENDING TIMING RUN AT 2020-06-25 12:33:01 PM
RESULT,RNN_TRANSLATOR,,522,nvidia,2020-06-25 12:24:19 PM
ENDING TIMING RUN AT 2020-06-25 12:33:01 PM
RESULT,RNN_TRANSLATOR,,522,nvidia,2020-06-25 12:24:19 PM
ENDING TIMING RUN AT 2020-06-25 12:33:01 PM
RESULT,RNN_TRANSLATOR,,522,nvidia,2020-06-25 12:24:19 PM
ENDING TIMING RUN AT 2020-06-25 12:33:01 PM
ENDING TIMING RUN AT 2020-06-25 12:33:01 PM
RESULT,RNN_TRANSLATOR,,522,nvidia,2020-06-25 12:24:19 PM
RESULT,RNN_TRANSLATOR,,522,nvidia,2020-06-25 12:24:19 PM
ENDING TIMING RUN AT 2020-06-25 12:33:01 PM
RESULT,RNN_TRANSLATOR,,522,nvidia,2020-06-25 12:24:19 PM
ENDING TIMING RUN AT 2020-06-25 12:33:01 PM
RESULT,RNN_TRANSLATOR,,522,nvidia,2020-06-25 12:24:19 PM
ENDING TIMING RUN AT 2020-06-25 12:33:01 PM
RESULT,RNN_TRANSLATOR,,522,nvidia,2020-06-25 12:24:19 PM
ENDING TIMING RUN AT 2020-06-25 12:33:01 PM
ENDING TIMING RUN AT 2020-06-25 12:33:01 PM
RESULT,RNN_TRANSLATOR,,522,nvidia,2020-06-25 12:24:19 PM
RESULT,RNN_TRANSLATOR,,522,nvidia,2020-06-25 12:24:19 PM
ENDING TIMING RUN AT 2020-06-25 12:33:01 PM
RESULT,RNN_TRANSLATOR,,522,nvidia,2020-06-25 12:24:19 PM
ENDING TIMING RUN AT 2020-06-25 12:33:01 PM
RESULT,RNN_TRANSLATOR,,522,nvidia,2020-06-25 12:24:19 PM
ENDING TIMING RUN AT 2020-06-25 12:33:01 PM
RESULT,RNN_TRANSLATOR,,522,nvidia,2020-06-25 12:24:19 PM
ENDING TIMING RUN AT 2020-06-25 12:33:01 PM
RESULT,RNN_TRANSLATOR,,522,nvidia,2020-06-25 12:24:19 PM
