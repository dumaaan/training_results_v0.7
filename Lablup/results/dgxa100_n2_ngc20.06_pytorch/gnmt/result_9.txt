+ echo 'Beginning trial 5 of 5'
Beginning trial 5 of 5
+ srun --ntasks=2 --container-name=rnn_translator python -c '
import mlperf_log_utils
from mlperf_logging.mllog import constants
mlperf_log_utils.mlperf_submission_log(constants.GNMT)'
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
:::MLLOG {"namespace": "", "time_ms": 1593114332478, "event_type": "POINT_IN_TIME", "key": "submission_benchmark", "value": "gnmt", "metadata": {"file": "/workspace/rnn_translator/mlperf_log_utils.py", "lineno": 19}}
:::MLLOG {"namespace": "", "time_ms": 1593114332516, "event_type": "POINT_IN_TIME", "key": "submission_org", "value": "NVIDIA", "metadata": {"file": "/workspace/rnn_translator/mlperf_log_utils.py", "lineno": 24}}
:::MLLOG {"namespace": "", "time_ms": 1593114332516, "event_type": "POINT_IN_TIME", "key": "submission_division", "value": "closed", "metadata": {"file": "/workspace/rnn_translator/mlperf_log_utils.py", "lineno": 28}}
:::MLLOG {"namespace": "", "time_ms": 1593114332516, "event_type": "POINT_IN_TIME", "key": "submission_status", "value": "onprem", "metadata": {"file": "/workspace/rnn_translator/mlperf_log_utils.py", "lineno": 32}}
:::MLLOG {"namespace": "", "time_ms": 1593114332516, "event_type": "POINT_IN_TIME", "key": "submission_platform", "value": "2xNVIDIA DGX A100", "metadata": {"file": "/workspace/rnn_translator/mlperf_log_utils.py", "lineno": 36}}
+ '[' 1 -eq 1 ']'
+ srun --ntasks=2 bash -c 'echo -n '\''Clearing cache on '\'' && hostname && sync && sudo /sbin/sysctl vm.drop_caches=3'
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
Clearing cache on luna-0101
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
Clearing cache on luna-0102
vm.drop_caches = 3
vm.drop_caches = 3
+ srun --ntasks=2 --container-name=rnn_translator python -c '
from mlperf_logging.mllog import constants
from seq2seq.utils import log_event
log_event(key=constants.CACHE_CLEAR, value=True)'
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
:::MLLOG {"namespace": "", "time_ms": 1593114337533, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
:::MLLOG {"namespace": "", "time_ms": 1593114337594, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
+ srun --mpi=none --ntasks=16 --ntasks-per-node=8 --container-name=rnn_translator --container-mounts=/lustre/fsr/datasets/wmt16_de_en:/data,/raid/scratch/svcnvdlfw/gnmt:/preproc_data,/lustre/fsw/mlperf-ci/14251653/results:/results ./run_and_time.sh
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
STARTING TIMING RUN AT 2020-06-25 12:45:39 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=2.875e-3
+ TRAIN_BATCH_SIZE=192
+ TEST_BATCH_SIZE=64
+ WARMUP_STEPS=200
+ REMAIN_STEPS=4054
+ DECAY_INTERVAL=506
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=8
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 3    --dwu-num-chunks 2    --dwu-num-rs-pg 1    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-overlap-reductions    --dwu-grad-norm    '
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 7 ']'
+ '[' 16 -gt 2 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGXA100_multi_2x8x192_dist == *\D\G\X\A\1\0\0* ]]
+ echo 'Using TCMalloc'
Using TCMalloc
+ export LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ echo 'running benchmark'
running benchmark
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 192 --test-batch-size 64 --optimizer FusedAdam --lr 2.875e-3 --warmup-steps 200 --remain-steps 4054 --decay-interval 506 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
STARTING TIMING RUN AT 2020-06-25 12:45:39 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=2.875e-3
+ TRAIN_BATCH_SIZE=192
+ TEST_BATCH_SIZE=64
+ WARMUP_STEPS=200
+ REMAIN_STEPS=4054
+ DECAY_INTERVAL=506
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=8
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 3    --dwu-num-chunks 2    --dwu-num-rs-pg 1    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-overlap-reductions    --dwu-grad-norm    '
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 0 ']'
+ '[' 16 -gt 2 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGXA100_multi_2x8x192_dist == *\D\G\X\A\1\0\0* ]]
+ echo 'Using TCMalloc'
Using TCMalloc
+ export LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
running benchmark
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 192 --test-batch-size 64 --optimizer FusedAdam --lr 2.875e-3 --warmup-steps 200 --remain-steps 4054 --decay-interval 506 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
STARTING TIMING RUN AT 2020-06-25 12:45:39 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=2.875e-3
+ TRAIN_BATCH_SIZE=192
+ TEST_BATCH_SIZE=64
+ WARMUP_STEPS=200
+ REMAIN_STEPS=4054
+ DECAY_INTERVAL=506
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=8
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 3    --dwu-num-chunks 2    --dwu-num-rs-pg 1    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-overlap-reductions    --dwu-grad-norm    '
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 6 ']'
+ '[' 16 -gt 2 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGXA100_multi_2x8x192_dist == *\D\G\X\A\1\0\0* ]]
Using TCMalloc
+ echo 'Using TCMalloc'
+ export LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
running benchmark
+ echo 'running benchmark'
STARTING TIMING RUN AT 2020-06-25 12:45:39 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=2.875e-3
+ TRAIN_BATCH_SIZE=192
+ TEST_BATCH_SIZE=64
+ WARMUP_STEPS=200
+ REMAIN_STEPS=4054
+ DECAY_INTERVAL=506
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=8
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 192 --test-batch-size 64 --optimizer FusedAdam --lr 2.875e-3 --warmup-steps 200 --remain-steps 4054 --decay-interval 506 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 3    --dwu-num-chunks 2    --dwu-num-rs-pg 1    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-overlap-reductions    --dwu-grad-norm    '
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 2 ']'
+ '[' 16 -gt 2 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGXA100_multi_2x8x192_dist == *\D\G\X\A\1\0\0* ]]
Using TCMalloc
+ echo 'Using TCMalloc'
+ export LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ echo 'running benchmark'
running benchmark
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 192 --test-batch-size 64 --optimizer FusedAdam --lr 2.875e-3 --warmup-steps 200 --remain-steps 4054 --decay-interval 506 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
STARTING TIMING RUN AT 2020-06-25 12:45:39 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=2.875e-3
+ TRAIN_BATCH_SIZE=192
+ TEST_BATCH_SIZE=64
+ WARMUP_STEPS=200
+ REMAIN_STEPS=4054
+ DECAY_INTERVAL=506
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=8
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 3    --dwu-num-chunks 2    --dwu-num-rs-pg 1    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-overlap-reductions    --dwu-grad-norm    '
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 3 ']'
+ '[' 16 -gt 2 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGXA100_multi_2x8x192_dist == *\D\G\X\A\1\0\0* ]]
Using TCMalloc
+ echo 'Using TCMalloc'
+ export LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
running benchmark
+ echo 'running benchmark'
STARTING TIMING RUN AT 2020-06-25 12:45:39 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=2.875e-3
+ TRAIN_BATCH_SIZE=192
+ TEST_BATCH_SIZE=64
+ WARMUP_STEPS=200
+ REMAIN_STEPS=4054
+ DECAY_INTERVAL=506
+ TARGET=24.0
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 192 --test-batch-size 64 --optimizer FusedAdam --lr 2.875e-3 --warmup-steps 200 --remain-steps 4054 --decay-interval 506 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=8
STARTING TIMING RUN AT 2020-06-25 12:45:39 PM
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 3    --dwu-num-chunks 2    --dwu-num-rs-pg 1    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-overlap-reductions    --dwu-grad-norm    '
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    '
+ MATH=fp16
+ declare -a CMD
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=2.875e-3
+ '[' -n 1 ']'
+ '[' 16 -gt 2 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ TRAIN_BATCH_SIZE=192
+ TEST_BATCH_SIZE=64
+ WARMUP_STEPS=200
+ REMAIN_STEPS=4054
+ DECAY_INTERVAL=506
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=8
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGXA100_multi_2x8x192_dist == *\D\G\X\A\1\0\0* ]]
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 3    --dwu-num-chunks 2    --dwu-num-rs-pg 1    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-overlap-reductions    --dwu-grad-norm    '
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    '
Using TCMalloc
+ echo 'Using TCMalloc'
+ export LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ MATH=fp16
+ declare -a CMD
running benchmark
+ echo 'running benchmark'
+ '[' -n 5 ']'
+ '[' 16 -gt 2 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 192 --test-batch-size 64 --optimizer FusedAdam --lr 2.875e-3 --warmup-steps 200 --remain-steps 4054 --decay-interval 506 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
+ [[ DGXA100_multi_2x8x192_dist == *\D\G\X\A\1\0\0* ]]
+ echo 'Using TCMalloc'
Using TCMalloc
+ export LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
running benchmark
+ echo 'running benchmark'
STARTING TIMING RUN AT 2020-06-25 12:45:39 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=2.875e-3
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 192 --test-batch-size 64 --optimizer FusedAdam --lr 2.875e-3 --warmup-steps 200 --remain-steps 4054 --decay-interval 506 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
+ TRAIN_BATCH_SIZE=192
+ TEST_BATCH_SIZE=64
+ WARMUP_STEPS=200
+ REMAIN_STEPS=4054
+ DECAY_INTERVAL=506
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=8
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 3    --dwu-num-chunks 2    --dwu-num-rs-pg 1    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-overlap-reductions    --dwu-grad-norm    '
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 4 ']'
+ '[' 16 -gt 2 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGXA100_multi_2x8x192_dist == *\D\G\X\A\1\0\0* ]]
+ echo 'Using TCMalloc'
Using TCMalloc
+ export LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
running benchmark
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 192 --test-batch-size 64 --optimizer FusedAdam --lr 2.875e-3 --warmup-steps 200 --remain-steps 4054 --decay-interval 506 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
STARTING TIMING RUN AT 2020-06-25 12:45:39 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=2.875e-3
+ TRAIN_BATCH_SIZE=192
+ TEST_BATCH_SIZE=64
+ WARMUP_STEPS=200
+ REMAIN_STEPS=4054
+ DECAY_INTERVAL=506
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=8
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 3    --dwu-num-chunks 2    --dwu-num-rs-pg 1    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-overlap-reductions    --dwu-grad-norm    '
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    '
+ MATH=fp16
+ declare -a CMD
STARTING TIMING RUN AT 2020-06-25 12:45:39 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=2.875e-3
+ TRAIN_BATCH_SIZE=192
+ '[' -n 0 ']'
+ '[' 16 -gt 2 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ TEST_BATCH_SIZE=64
+ WARMUP_STEPS=200
+ REMAIN_STEPS=4054
+ DECAY_INTERVAL=506
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=8
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 3    --dwu-num-chunks 2    --dwu-num-rs-pg 1    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-overlap-reductions    --dwu-grad-norm    '
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    '
+ MATH=fp16
Using TCMalloc
running benchmark
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGXA100_multi_2x8x192_dist == *\D\G\X\A\1\0\0* ]]
+ echo 'Using TCMalloc'
+ export LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ echo 'running benchmark'
+ declare -a CMD
+ '[' -n 7 ']'
+ '[' 16 -gt 2 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
Using TCMalloc
running benchmark
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGXA100_multi_2x8x192_dist == *\D\G\X\A\1\0\0* ]]
+ echo 'Using TCMalloc'
+ export LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 192 --test-batch-size 64 --optimizer FusedAdam --lr 2.875e-3 --warmup-steps 200 --remain-steps 4054 --decay-interval 506 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 192 --test-batch-size 64 --optimizer FusedAdam --lr 2.875e-3 --warmup-steps 200 --remain-steps 4054 --decay-interval 506 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
STARTING TIMING RUN AT 2020-06-25 12:45:39 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=2.875e-3
+ TRAIN_BATCH_SIZE=192
+ TEST_BATCH_SIZE=64
+ WARMUP_STEPS=200
+ REMAIN_STEPS=4054
+ DECAY_INTERVAL=506
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=8
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 3    --dwu-num-chunks 2    --dwu-num-rs-pg 1    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-overlap-reductions    --dwu-grad-norm    '
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 1 ']'
+ '[' 16 -gt 2 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGXA100_multi_2x8x192_dist == *\D\G\X\A\1\0\0* ]]
Using TCMalloc
+ echo 'Using TCMalloc'
+ export LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
running benchmark
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 192 --test-batch-size 64 --optimizer FusedAdam --lr 2.875e-3 --warmup-steps 200 --remain-steps 4054 --decay-interval 506 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
STARTING TIMING RUN AT 2020-06-25 12:45:39 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=2.875e-3
+ TRAIN_BATCH_SIZE=192
+ TEST_BATCH_SIZE=64
+ WARMUP_STEPS=200
+ REMAIN_STEPS=4054
+ DECAY_INTERVAL=506
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=8
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 3    --dwu-num-chunks 2    --dwu-num-rs-pg 1    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-overlap-reductions    --dwu-grad-norm    '
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 5 ']'
+ '[' 16 -gt 2 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGXA100_multi_2x8x192_dist == *\D\G\X\A\1\0\0* ]]
Using TCMalloc
+ echo 'Using TCMalloc'
+ export LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ echo 'running benchmark'
running benchmark
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 192 --test-batch-size 64 --optimizer FusedAdam --lr 2.875e-3 --warmup-steps 200 --remain-steps 4054 --decay-interval 506 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
STARTING TIMING RUN AT 2020-06-25 12:45:39 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=2.875e-3
+ TRAIN_BATCH_SIZE=192
+ TEST_BATCH_SIZE=64
+ WARMUP_STEPS=200
+ REMAIN_STEPS=4054
+ DECAY_INTERVAL=506
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=8
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 3    --dwu-num-chunks 2    --dwu-num-rs-pg 1    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-overlap-reductions    --dwu-grad-norm    '
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 6 ']'
STARTING TIMING RUN AT 2020-06-25 12:45:39 PM
+ '[' 16 -gt 2 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ DATASET_DIR=/data
+ [[ DGXA100_multi_2x8x192_dist == *\D\G\X\A\1\0\0* ]]
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=2.875e-3
Using TCMalloc
+ echo 'Using TCMalloc'
+ export LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ TRAIN_BATCH_SIZE=192
+ echo 'running benchmark'
+ TEST_BATCH_SIZE=64
running benchmark
+ WARMUP_STEPS=200
+ REMAIN_STEPS=4054
+ DECAY_INTERVAL=506
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=8
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 3    --dwu-num-chunks 2    --dwu-num-rs-pg 1    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-overlap-reductions    --dwu-grad-norm    '
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    '
+ MATH=fp16
+ declare -a CMD
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 192 --test-batch-size 64 --optimizer FusedAdam --lr 2.875e-3 --warmup-steps 200 --remain-steps 4054 --decay-interval 506 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
+ '[' -n 3 ']'
+ '[' 16 -gt 2 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGXA100_multi_2x8x192_dist == *\D\G\X\A\1\0\0* ]]
Using TCMalloc
+ echo 'Using TCMalloc'
+ export LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
running benchmark
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 192 --test-batch-size 64 --optimizer FusedAdam --lr 2.875e-3 --warmup-steps 200 --remain-steps 4054 --decay-interval 506 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
STARTING TIMING RUN AT 2020-06-25 12:45:39 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=2.875e-3
+ TRAIN_BATCH_SIZE=192
+ TEST_BATCH_SIZE=64
+ WARMUP_STEPS=200
+ REMAIN_STEPS=4054
+ DECAY_INTERVAL=506
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=8
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 3    --dwu-num-chunks 2    --dwu-num-rs-pg 1    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-overlap-reductions    --dwu-grad-norm    '
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    '
+ MATH=fp16
+ declare -a CMD
STARTING TIMING RUN AT 2020-06-25 12:45:39 PM
+ '[' -n 2 ']'
+ '[' 16 -gt 2 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=2.875e-3
+ TRAIN_BATCH_SIZE=192
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ TEST_BATCH_SIZE=64
+ WARMUP_STEPS=200
+ [[ DGXA100_multi_2x8x192_dist == *\D\G\X\A\1\0\0* ]]
+ REMAIN_STEPS=4054
+ DECAY_INTERVAL=506
Using TCMalloc
+ echo 'Using TCMalloc'
+ export LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=8
running benchmark
+ echo 'running benchmark'
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 3    --dwu-num-chunks 2    --dwu-num-rs-pg 1    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-overlap-reductions    --dwu-grad-norm    '
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 4 ']'
+ '[' 16 -gt 2 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 192 --test-batch-size 64 --optimizer FusedAdam --lr 2.875e-3 --warmup-steps 200 --remain-steps 4054 --decay-interval 506 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGXA100_multi_2x8x192_dist == *\D\G\X\A\1\0\0* ]]
Using TCMalloc
+ echo 'Using TCMalloc'
+ export LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ echo 'running benchmark'
running benchmark
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 192 --test-batch-size 64 --optimizer FusedAdam --lr 2.875e-3 --warmup-steps 200 --remain-steps 4054 --decay-interval 506 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
num_sockets = 2 num_nodes=8 cores_per_socket=64
num_sockets = 2 num_nodes=8 cores_per_socket=64
+ exec numactl --physcpubind=32-47,160-175 --membind=2 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 192 --test-batch-size 64 --optimizer FusedAdam --lr 2.875e-3 --warmup-steps 200 --remain-steps 4054 --decay-interval 506 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
+ exec numactl --physcpubind=0-15,128-143 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 192 --test-batch-size 64 --optimizer FusedAdam --lr 2.875e-3 --warmup-steps 200 --remain-steps 4054 --decay-interval 506 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
num_sockets = 2 num_nodes=8 cores_per_socket=64
+ exec numactl --physcpubind=96-111,224-239 --membind=6 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 192 --test-batch-size 64 --optimizer FusedAdam --lr 2.875e-3 --warmup-steps 200 --remain-steps 4054 --decay-interval 506 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
num_sockets = 2 num_nodes=8 cores_per_socket=64
+ exec numactl --physcpubind=80-95,208-223 --membind=5 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 192 --test-batch-size 64 --optimizer FusedAdam --lr 2.875e-3 --warmup-steps 200 --remain-steps 4054 --decay-interval 506 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
num_sockets = 2 num_nodes=8 cores_per_socket=64
+ exec numactl --physcpubind=48-63,176-191 --membind=3 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 192 --test-batch-size 64 --optimizer FusedAdam --lr 2.875e-3 --warmup-steps 200 --remain-steps 4054 --decay-interval 506 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
num_sockets = 2 num_nodes=8 cores_per_socket=64
num_sockets = 2 num_nodes=8 cores_per_socket=64
num_sockets = 2 num_nodes=8 cores_per_socket=64
num_sockets = 2 num_nodes=8 cores_per_socket=64
num_sockets = 2 num_nodes=8 cores_per_socket=64
+ exec numactl --physcpubind=64-79,192-207 --membind=4 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 192 --test-batch-size 64 --optimizer FusedAdam --lr 2.875e-3 --warmup-steps 200 --remain-steps 4054 --decay-interval 506 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
+ exec numactl --physcpubind=112-127,240-255 --membind=7 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 192 --test-batch-size 64 --optimizer FusedAdam --lr 2.875e-3 --warmup-steps 200 --remain-steps 4054 --decay-interval 506 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
+ exec numactl --physcpubind=112-127,240-255 --membind=7 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 192 --test-batch-size 64 --optimizer FusedAdam --lr 2.875e-3 --warmup-steps 200 --remain-steps 4054 --decay-interval 506 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
+ exec numactl --physcpubind=16-31,144-159 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 192 --test-batch-size 64 --optimizer FusedAdam --lr 2.875e-3 --warmup-steps 200 --remain-steps 4054 --decay-interval 506 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
+ exec numactl --physcpubind=48-63,176-191 --membind=3 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 192 --test-batch-size 64 --optimizer FusedAdam --lr 2.875e-3 --warmup-steps 200 --remain-steps 4054 --decay-interval 506 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
num_sockets = 2 num_nodes=8 cores_per_socket=64
+ exec numactl --physcpubind=16-31,144-159 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 192 --test-batch-size 64 --optimizer FusedAdam --lr 2.875e-3 --warmup-steps 200 --remain-steps 4054 --decay-interval 506 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
num_sockets = 2 num_nodes=8 cores_per_socket=64
+ exec numactl --physcpubind=80-95,208-223 --membind=5 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 192 --test-batch-size 64 --optimizer FusedAdam --lr 2.875e-3 --warmup-steps 200 --remain-steps 4054 --decay-interval 506 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
num_sockets = 2 num_nodes=8 cores_per_socket=64
num_sockets = 2 num_nodes=8 cores_per_socket=64
num_sockets = 2 num_nodes=8 cores_per_socket=64
num_sockets = 2 num_nodes=8 cores_per_socket=64
+ exec numactl --physcpubind=96-111,224-239 --membind=6 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 192 --test-batch-size 64 --optimizer FusedAdam --lr 2.875e-3 --warmup-steps 200 --remain-steps 4054 --decay-interval 506 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
+ exec numactl --physcpubind=0-15,128-143 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 192 --test-batch-size 64 --optimizer FusedAdam --lr 2.875e-3 --warmup-steps 200 --remain-steps 4054 --decay-interval 506 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
+ exec numactl --physcpubind=32-47,160-175 --membind=2 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 192 --test-batch-size 64 --optimizer FusedAdam --lr 2.875e-3 --warmup-steps 200 --remain-steps 4054 --decay-interval 506 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
+ exec numactl --physcpubind=64-79,192-207 --membind=4 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 192 --test-batch-size 64 --optimizer FusedAdam --lr 2.875e-3 --warmup-steps 200 --remain-steps 4054 --decay-interval 506 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
:::MLLOG {"namespace": "", "time_ms": 1593114341501, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593114341515, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593114341540, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593114341581, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593114341606, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593114341608, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593114341609, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593114341624, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593114341634, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593114341635, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593114341639, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593114341683, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593114341710, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593114341741, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593114341760, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593114341763, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
NCCL version 2.7.5+cuda11.0
0: Saving results to: results/gnmt_wmt16
0: Run arguments: Namespace(apex_message_size=10000000.0, apex_num_allreduce_streams=1, batching='bucketing', beam_size=5, cov_penalty_factor=0.1, cuda=True, cudnn=True, dataset_dir='/data', decay_factor=0.5, decay_interval=506, decay_steps=4, distributed_weight_update=2, dropout=0.2, dwu_e5m2_allgather=False, dwu_full_pipeline=False, dwu_grad_norm=True, dwu_group_size=0, dwu_num_ag_pg=0, dwu_num_ar_pg=2, dwu_num_blocks=3, dwu_num_chunks=2, dwu_num_rs_pg=1, dwu_overlap_reductions=True, enable_apex_allreduce_overlap=False, env=False, epochs=8, eval=True, fused_attention=True, fused_xentropy=True, grad_clip=5.0, hidden_size=1024, init_scale=1024, intra_epoch_eval=0, keep_checkpoints=0, len_norm_const=5.0, len_norm_factor=0.6, local_rank=0, log_all_ranks=False, lr=0.002875, math='fp16', max_length_test=150, max_length_train=75, max_size=None, min_length_test=0, min_length_train=0, num_buckets=5, num_layers=4, optimizer='FusedAdam', optimizer_extra='{}', prealloc_mode='once', preproc_data_dir='/preproc_data/75', print_freq=10, rank=0, remain_steps=4054, results_dir='results', resume=None, save='gnmt_wmt16', save_all=False, save_freq=5000, save_path='results/gnmt_wmt16', seed=None, shard_size=80, share_embedding=True, smoothing=0.1, start_epoch=0, target_bleu=24.0, test_batch_size=64, test_loader_workers=0, train_batch_size=192, train_global_batch_size=None, train_iter_size=1, train_loader_workers=1, upscale_interval=128, use_preproc_data=True, warmup_steps=200)
0: L2 promotion: 128B
0: Using random master seed: 4047552978
:::MLLOG {"namespace": "", "time_ms": 1593114351539, "event_type": "POINT_IN_TIME", "key": "seed", "value": 4047552978, "metadata": {"file": "/workspace/rnn_translator/seq2seq/utils.py", "lineno": 144}}
0: Worker 0 is using worker seed: 1632503595
0: Building vocabulary from /data/vocab.bpe.32000
0: Size of vocabulary: 32320
0: Fused attention flag set to True
0: GNMT(
  (embedder): Embedding(32320, 1024, padding_idx=0)
  (encoder): ResidualRecurrentEncoder(
    (rnn_layers): ModuleList(
      (0): EmuBidirLSTM(
        (bidir): LSTM(1024, 1024, bidirectional=True)
        (layer1): LSTM(1024, 1024)
        (layer2): LSTM(1024, 1024)
      )
      (1): LSTM(2048, 1024)
      (2): LSTM(1024, 1024)
      (3): LSTM(1024, 1024)
    )
    (dropout): Dropout(p=0.2, inplace=False)
    (embedder): Embedding(32320, 1024, padding_idx=0)
  )
  (decoder): ResidualRecurrentDecoder(
    (att_rnn): RecurrentAttention(
      (rnn): LSTM(1024, 1024)
      (attn): BahdanauAttention(
        (linear_q): Linear(in_features=1024, out_features=1024, bias=False)
        (linear_k): Linear(in_features=1024, out_features=1024, bias=False)
      )
      (dropout): Dropout(p=0.2, inplace=False)
    )
    (rnn_layers): ModuleList(
      (0): LSTM(2048, 1024)
      (1): LSTM(2048, 1024)
      (2): LSTM(2048, 1024)
    )
    (embedder): Embedding(32320, 1024, padding_idx=0)
    (classifier): Classifier(
      (classifier): Linear(in_features=1024, out_features=32320, bias=True)
    )
    (dropout): Dropout(p=0.2, inplace=False)
  )
)
0: Building LabelSmoothingLoss (smoothing: 0.1)
0: Fused xentropy flag set to True
0: Training optimizer config: {'optimizer': 'FusedAdam', 'lr': 0.002875}
0: Number of parameters: 160671297
0: Saving state of the tokenizer
self._net_total_param_size=160671360, self._total_param_size=160677888, dwu_min_page_size=12288, self._block_size=53559296, self._chunk_size=26779648, self._shard_size=3347456
[0, 9, 34]
self._net_total_param_size=160671360, self._total_param_size=160677888, dwu_min_page_size=12288, self._block_size=53559296, self._chunk_size=26779648, self._shard_size=3347456
[0, 9, 34]
self._net_total_param_size=160671360, self._total_param_size=160677888, dwu_min_page_size=12288, self._block_size=53559296, self._chunk_size=26779648, self._shard_size=3347456
[0, 9, 34]
self._net_total_param_size=160671360, self._total_param_size=160677888, dwu_min_page_size=12288, self._block_size=53559296, self._chunk_size=26779648, self._shard_size=3347456
[0, 9, 34]
self._net_total_param_size=160671360, self._total_param_size=160677888, dwu_min_page_size=12288, self._block_size=53559296, self._chunk_size=26779648, self._shard_size=3347456
[0, 9, 34]
self._net_total_param_size=160671360, self._total_param_size=160677888, dwu_min_page_size=12288, self._block_size=53559296, self._chunk_size=26779648, self._shard_size=3347456
[0, 9, 34]
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3016704]), new_param_packed_fragment.size()=torch.Size([3016704]), master_param_fragment.size()=torch.Size([3016704])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([322560]), new_param_packed_fragment.size()=torch.Size([322560]), master_param_fragment.size()=torch.Size([322560])
model_param_fragment.size()=torch.Size([1427456]), new_param_packed_fragment.size()=torch.Size([1427456]), master_param_fragment.size()=torch.Size([1427456])
model_param_fragment.size()=torch.Size([1920000]), new_param_packed_fragment.size()=torch.Size([1920000]), master_param_fragment.size()=torch.Size([1920000])
model_param_fragment.size()=torch.Size([1929280]), new_param_packed_fragment.size()=torch.Size([1929280]), master_param_fragment.size()=torch.Size([1929280])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([1409984]), new_param_packed_fragment.size()=torch.Size([1409984]), master_param_fragment.size()=torch.Size([1409984])
model_param_fragment.size()=torch.Size([331840]), new_param_packed_fragment.size()=torch.Size([331840]), master_param_fragment.size()=torch.Size([331840])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([3007424]), new_param_packed_fragment.size()=torch.Size([3007424]), master_param_fragment.size()=torch.Size([3007424])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
self._net_total_param_size=160671360, self._total_param_size=160677888, dwu_min_page_size=12288, self._block_size=53559296, self._chunk_size=26779648, self._shard_size=3347456
[0, 9, 34]
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([1314816]), new_param_packed_fragment.size()=torch.Size([1314816]), master_param_fragment.size()=torch.Size([1314816])
self._net_total_param_size=160671360, self._total_param_size=160677888, dwu_min_page_size=12288, self._block_size=53559296, self._chunk_size=26779648, self._shard_size=3347456
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
[0, 9, 34]
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([2024448]), new_param_packed_fragment.size()=torch.Size([2024448]), master_param_fragment.size()=torch.Size([2024448])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([2832448]), new_param_packed_fragment.size()=torch.Size([2832448]), master_param_fragment.size()=torch.Size([2832448])
model_param_fragment.size()=torch.Size([515008]), new_param_packed_fragment.size()=torch.Size([515008]), master_param_fragment.size()=torch.Size([515008])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([2169856]), new_param_packed_fragment.size()=torch.Size([2169856]), master_param_fragment.size()=torch.Size([2169856])
model_param_fragment.size()=torch.Size([1177600]), new_param_packed_fragment.size()=torch.Size([1177600]), master_param_fragment.size()=torch.Size([1177600])
model_param_fragment.size()=torch.Size([572416]), new_param_packed_fragment.size()=torch.Size([572416]), master_param_fragment.size()=torch.Size([572416])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([2766848]), new_param_packed_fragment.size()=torch.Size([2766848]), master_param_fragment.size()=torch.Size([2766848])
model_param_fragment.size()=torch.Size([1082432]), new_param_packed_fragment.size()=torch.Size([1082432]), master_param_fragment.size()=torch.Size([1082432])
model_param_fragment.size()=torch.Size([2265024]), new_param_packed_fragment.size()=torch.Size([2265024]), master_param_fragment.size()=torch.Size([2265024])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
self._net_total_param_size=160671360, self._total_param_size=160677888, dwu_min_page_size=12288, self._block_size=53559296, self._chunk_size=26779648, self._shard_size=3347456
self._net_total_param_size=160671360, self._total_param_size=160677888, dwu_min_page_size=12288, self._block_size=53559296, self._chunk_size=26779648, self._shard_size=3347456
[0, 9, 34]
[0, 9, 34]
self._net_total_param_size=160671360, self._total_param_size=160677888, dwu_min_page_size=12288, self._block_size=53559296, self._chunk_size=26779648, self._shard_size=3347456
[0, 9, 34]
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3129344]), new_param_packed_fragment.size()=torch.Size([3129344]), master_param_fragment.size()=torch.Size([3129344])
model_param_fragment.size()=torch.Size([218112]), new_param_packed_fragment.size()=torch.Size([218112]), master_param_fragment.size()=torch.Size([218112])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([2217984]), new_param_packed_fragment.size()=torch.Size([2217984]), master_param_fragment.size()=torch.Size([2217984])
model_param_fragment.size()=torch.Size([2169856]), new_param_packed_fragment.size()=torch.Size([2169856]), master_param_fragment.size()=torch.Size([2169856])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([1177600]), new_param_packed_fragment.size()=torch.Size([1177600]), master_param_fragment.size()=torch.Size([1177600])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([1121280]), new_param_packed_fragment.size()=torch.Size([1121280]), master_param_fragment.size()=torch.Size([1121280])
model_param_fragment.size()=torch.Size([572416]), new_param_packed_fragment.size()=torch.Size([572416]), master_param_fragment.size()=torch.Size([572416])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([628736]), new_param_packed_fragment.size()=torch.Size([628736]), master_param_fragment.size()=torch.Size([628736])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([2766848]), new_param_packed_fragment.size()=torch.Size([2766848]), master_param_fragment.size()=torch.Size([2766848])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([1024]), new_param_packed_fragment.size()=torch.Size([1024]), master_param_fragment.size()=torch.Size([1024])
model_param_fragment.size()=torch.Size([1082432]), new_param_packed_fragment.size()=torch.Size([1082432]), master_param_fragment.size()=torch.Size([1082432])
model_param_fragment.size()=torch.Size([1]), new_param_packed_fragment.size()=torch.Size([1]), master_param_fragment.size()=torch.Size([1])
model_param_fragment.size()=torch.Size([2265024]), new_param_packed_fragment.size()=torch.Size([2265024]), master_param_fragment.size()=torch.Size([2265024])
model_param_fragment.size()=torch.Size([1024]), new_param_packed_fragment.size()=torch.Size([1024]), master_param_fragment.size()=torch.Size([1024])
0: Initializing dwu fp16 optimizer
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([1048576]), new_param_packed_fragment.size()=torch.Size([1048576]), master_param_fragment.size()=torch.Size([1048576])
model_param_fragment.size()=torch.Size([1048576]), new_param_packed_fragment.size()=torch.Size([1048576]), master_param_fragment.size()=torch.Size([1048576])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([611264]), new_param_packed_fragment.size()=torch.Size([611264]), master_param_fragment.size()=torch.Size([611264])
model_param_fragment.size()=torch.Size([1130560]), new_param_packed_fragment.size()=torch.Size([1130560]), master_param_fragment.size()=torch.Size([1130560])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([2208704]), new_param_packed_fragment.size()=torch.Size([2208704]), master_param_fragment.size()=torch.Size([2208704])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
NCCL version 2.7.5+cuda11.0
self._net_total_param_size=160671360, self._total_param_size=160677888, dwu_min_page_size=12288, self._block_size=53559296, self._chunk_size=26779648, self._shard_size=3347456
[0, 9, 34]
self._net_total_param_size=160671360, self._total_param_size=160677888, dwu_min_page_size=12288, self._block_size=53559296, self._chunk_size=26779648, self._shard_size=3347456
[0, 9, 34]
self._net_total_param_size=160671360, self._total_param_size=160677888, dwu_min_page_size=12288, self._block_size=53559296, self._chunk_size=26779648, self._shard_size=3347456
[0, 9, 34]
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([2274304]), new_param_packed_fragment.size()=torch.Size([2274304]), master_param_fragment.size()=torch.Size([2274304])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([1064960]), new_param_packed_fragment.size()=torch.Size([1064960]), master_param_fragment.size()=torch.Size([1064960])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3308608]), new_param_packed_fragment.size()=torch.Size([3308608]), master_param_fragment.size()=torch.Size([3308608])
model_param_fragment.size()=torch.Size([32320]), new_param_packed_fragment.size()=torch.Size([32320]), master_param_fragment.size()=torch.Size([32320])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([2968576]), new_param_packed_fragment.size()=torch.Size([2968576]), master_param_fragment.size()=torch.Size([2968576])
model_param_fragment.size()=torch.Size([378880]), new_param_packed_fragment.size()=torch.Size([378880]), master_param_fragment.size()=torch.Size([378880])
model_param_fragment.size()=torch.Size([1371136]), new_param_packed_fragment.size()=torch.Size([1371136]), master_param_fragment.size()=torch.Size([1371136])
model_param_fragment.size()=torch.Size([1976320]), new_param_packed_fragment.size()=torch.Size([1976320]), master_param_fragment.size()=torch.Size([1976320])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([283712]), new_param_packed_fragment.size()=torch.Size([283712]), master_param_fragment.size()=torch.Size([283712])
model_param_fragment.size()=torch.Size([3063744]), new_param_packed_fragment.size()=torch.Size([3063744]), master_param_fragment.size()=torch.Size([3063744])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
self._net_total_param_size=160671360, self._total_param_size=160677888, dwu_min_page_size=12288, self._block_size=53559296, self._chunk_size=26779648, self._shard_size=3347456
[0, 9, 34]
self._net_total_param_size=160671360, self._total_param_size=160677888, dwu_min_page_size=12288, self._block_size=53559296, self._chunk_size=26779648, self._shard_size=3347456
[0, 9, 34]
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([1314816]), new_param_packed_fragment.size()=torch.Size([1314816]), master_param_fragment.size()=torch.Size([1314816])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([2217984]), new_param_packed_fragment.size()=torch.Size([2217984]), master_param_fragment.size()=torch.Size([2217984])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([2024448]), new_param_packed_fragment.size()=torch.Size([2024448]), master_param_fragment.size()=torch.Size([2024448])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([1121280]), new_param_packed_fragment.size()=torch.Size([1121280]), master_param_fragment.size()=torch.Size([1121280])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([628736]), new_param_packed_fragment.size()=torch.Size([628736]), master_param_fragment.size()=torch.Size([628736])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([2832448]), new_param_packed_fragment.size()=torch.Size([2832448]), master_param_fragment.size()=torch.Size([2832448])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([515008]), new_param_packed_fragment.size()=torch.Size([515008]), master_param_fragment.size()=torch.Size([515008])
model_param_fragment.size()=torch.Size([1024]), new_param_packed_fragment.size()=torch.Size([1024]), master_param_fragment.size()=torch.Size([1024])
model_param_fragment.size()=torch.Size([1]), new_param_packed_fragment.size()=torch.Size([1]), master_param_fragment.size()=torch.Size([1])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([1024]), new_param_packed_fragment.size()=torch.Size([1024]), master_param_fragment.size()=torch.Size([1024])
model_param_fragment.size()=torch.Size([1048576]), new_param_packed_fragment.size()=torch.Size([1048576]), master_param_fragment.size()=torch.Size([1048576])
model_param_fragment.size()=torch.Size([1048576]), new_param_packed_fragment.size()=torch.Size([1048576]), master_param_fragment.size()=torch.Size([1048576])
model_param_fragment.size()=torch.Size([611264]), new_param_packed_fragment.size()=torch.Size([611264]), master_param_fragment.size()=torch.Size([611264])
model_param_fragment.size()=torch.Size([1130560]), new_param_packed_fragment.size()=torch.Size([1130560]), master_param_fragment.size()=torch.Size([1130560])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([2208704]), new_param_packed_fragment.size()=torch.Size([2208704]), master_param_fragment.size()=torch.Size([2208704])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3016704]), new_param_packed_fragment.size()=torch.Size([3016704]), master_param_fragment.size()=torch.Size([3016704])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([322560]), new_param_packed_fragment.size()=torch.Size([322560]), master_param_fragment.size()=torch.Size([322560])
model_param_fragment.size()=torch.Size([1427456]), new_param_packed_fragment.size()=torch.Size([1427456]), master_param_fragment.size()=torch.Size([1427456])
model_param_fragment.size()=torch.Size([1920000]), new_param_packed_fragment.size()=torch.Size([1920000]), master_param_fragment.size()=torch.Size([1920000])
model_param_fragment.size()=torch.Size([1929280]), new_param_packed_fragment.size()=torch.Size([1929280]), master_param_fragment.size()=torch.Size([1929280])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([1409984]), new_param_packed_fragment.size()=torch.Size([1409984]), master_param_fragment.size()=torch.Size([1409984])
model_param_fragment.size()=torch.Size([331840]), new_param_packed_fragment.size()=torch.Size([331840]), master_param_fragment.size()=torch.Size([331840])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([3007424]), new_param_packed_fragment.size()=torch.Size([3007424]), master_param_fragment.size()=torch.Size([3007424])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
NCCL version 2.7.5+cuda11.0
NCCL version 2.7.5+cuda11.0
NCCL version 2.7.5+cuda11.0
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3129344]), new_param_packed_fragment.size()=torch.Size([3129344]), master_param_fragment.size()=torch.Size([3129344])
model_param_fragment.size()=torch.Size([218112]), new_param_packed_fragment.size()=torch.Size([218112]), master_param_fragment.size()=torch.Size([218112])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([2968576]), new_param_packed_fragment.size()=torch.Size([2968576]), master_param_fragment.size()=torch.Size([2968576])
model_param_fragment.size()=torch.Size([378880]), new_param_packed_fragment.size()=torch.Size([378880]), master_param_fragment.size()=torch.Size([378880])
model_param_fragment.size()=torch.Size([1371136]), new_param_packed_fragment.size()=torch.Size([1371136]), master_param_fragment.size()=torch.Size([1371136])
model_param_fragment.size()=torch.Size([1976320]), new_param_packed_fragment.size()=torch.Size([1976320]), master_param_fragment.size()=torch.Size([1976320])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([283712]), new_param_packed_fragment.size()=torch.Size([283712]), master_param_fragment.size()=torch.Size([283712])
model_param_fragment.size()=torch.Size([3063744]), new_param_packed_fragment.size()=torch.Size([3063744]), master_param_fragment.size()=torch.Size([3063744])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
NCCL version 2.7.5+cuda11.0
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([467968]), new_param_packed_fragment.size()=torch.Size([467968]), master_param_fragment.size()=torch.Size([467968])
model_param_fragment.size()=torch.Size([2879488]), new_param_packed_fragment.size()=torch.Size([2879488]), master_param_fragment.size()=torch.Size([2879488])
model_param_fragment.size()=torch.Size([3073024]), new_param_packed_fragment.size()=torch.Size([3073024]), master_param_fragment.size()=torch.Size([3073024])
model_param_fragment.size()=torch.Size([274432]), new_param_packed_fragment.size()=torch.Size([274432]), master_param_fragment.size()=torch.Size([274432])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([467968]), new_param_packed_fragment.size()=torch.Size([467968]), master_param_fragment.size()=torch.Size([467968])
model_param_fragment.size()=torch.Size([2879488]), new_param_packed_fragment.size()=torch.Size([2879488]), master_param_fragment.size()=torch.Size([2879488])
model_param_fragment.size()=torch.Size([3073024]), new_param_packed_fragment.size()=torch.Size([3073024]), master_param_fragment.size()=torch.Size([3073024])
model_param_fragment.size()=torch.Size([274432]), new_param_packed_fragment.size()=torch.Size([274432]), master_param_fragment.size()=torch.Size([274432])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
NCCL version 2.7.5+cuda11.0
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([2274304]), new_param_packed_fragment.size()=torch.Size([2274304]), master_param_fragment.size()=torch.Size([2274304])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([1064960]), new_param_packed_fragment.size()=torch.Size([1064960]), master_param_fragment.size()=torch.Size([1064960])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3308608]), new_param_packed_fragment.size()=torch.Size([3308608]), master_param_fragment.size()=torch.Size([3308608])
model_param_fragment.size()=torch.Size([32320]), new_param_packed_fragment.size()=torch.Size([32320]), master_param_fragment.size()=torch.Size([32320])
NCCL version 2.7.5+cuda11.0
NCCL version 2.7.5+cuda11.0
0: Using optimizer: DistributedFusedAdam (
Parameter Group 0
    betas: (0.9, 0.999)
    bias_correction: True
    eps: 1e-08
    lr: 0.002875
    max_grad_norm: 5.0
    weight_decay: 0.0
)
:::MLLOG {"namespace": "", "time_ms": 1593114363549, "event_type": "POINT_IN_TIME", "key": "opt_name", "value": "adam", "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/trainer.py", "lineno": 176}}
:::MLLOG {"namespace": "", "time_ms": 1593114363550, "event_type": "POINT_IN_TIME", "key": "opt_base_learning_rate", "value": 0.002875, "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/trainer.py", "lineno": 178}}
:::MLLOG {"namespace": "", "time_ms": 1593114363550, "event_type": "POINT_IN_TIME", "key": "opt_adam_beta_1", "value": 0.9, "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/trainer.py", "lineno": 180}}
:::MLLOG {"namespace": "", "time_ms": 1593114363550, "event_type": "POINT_IN_TIME", "key": "opt_adam_beta_2", "value": 0.999, "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/trainer.py", "lineno": 182}}
:::MLLOG {"namespace": "", "time_ms": 1593114363550, "event_type": "POINT_IN_TIME", "key": "opt_adam_epsilon", "value": 1e-08, "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/trainer.py", "lineno": 184}}
0: Executing preallocation
:::MLLOG {"namespace": "", "time_ms": 1593114365006, "event_type": "INTERVAL_END", "key": "init_stop", "value": null, "metadata": {"file": "train.py", "lineno": 429}}
:::MLLOG {"namespace": "", "time_ms": 1593114365007, "event_type": "INTERVAL_START", "key": "run_start", "value": null, "metadata": {"file": "train.py", "lineno": 430}}
:::MLLOG {"namespace": "", "time_ms": 1593114365007, "event_type": "POINT_IN_TIME", "key": "max_sequence_length", "value": 75, "metadata": {"file": "train.py", "lineno": 435, "method": "discard"}}
0: Reading preprocessed data file from /preproc_data/75/training.bin
0: Preprocessed data: length: 3975116 min length: 0 max length: 75
0: Opening preprocessed data /preproc_data/75/training.bin for reading
0: Processing data from /data/newstest2014.tok.bpe.32000.en
0: Filtering data, min len: 0, max len: 150
0: Pairs before: 3003, after: 3003
:::MLLOG {"namespace": "", "time_ms": 1593114365277, "event_type": "POINT_IN_TIME", "key": "global_batch_size", "value": 3072, "metadata": {"file": "train.py", "lineno": 480}}
:::MLLOG {"namespace": "", "time_ms": 1593114365278, "event_type": "POINT_IN_TIME", "key": "train_samples", "value": 3965952, "metadata": {"file": "train.py", "lineno": 488}}
:::MLLOG {"namespace": "", "time_ms": 1593114365278, "event_type": "POINT_IN_TIME", "key": "eval_samples", "value": 3003, "metadata": {"file": "train.py", "lineno": 490}}
0: Training LR schedule config: {'warmup_steps': 200, 'remain_steps': 4054, 'decay_interval': 506, 'decay_steps': 4, 'decay_factor': 0.5}
0: Scheduler warmup steps: 200
0: Scheduler remain steps: 4054
0: Scheduler decay interval: 506
0: Scheduler decay factor: 0.5
0: Scheduler max decay steps: 4
:::MLLOG {"namespace": "", "time_ms": 1593114365279, "event_type": "POINT_IN_TIME", "key": "opt_learning_rate_alt_decay_func", "value": true, "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/lr_scheduler.py", "lineno": 78}}
:::MLLOG {"namespace": "", "time_ms": 1593114365279, "event_type": "POINT_IN_TIME", "key": "opt_learning_rate_alt_warmup_func", "value": true, "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/lr_scheduler.py", "lineno": 80}}
:::MLLOG {"namespace": "", "time_ms": 1593114365279, "event_type": "POINT_IN_TIME", "key": "opt_learning_rate_decay_interval", "value": 506, "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/lr_scheduler.py", "lineno": 83}}
:::MLLOG {"namespace": "", "time_ms": 1593114365279, "event_type": "POINT_IN_TIME", "key": "opt_learning_rate_decay_factor", "value": 0.5, "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/lr_scheduler.py", "lineno": 85}}
:::MLLOG {"namespace": "", "time_ms": 1593114365279, "event_type": "POINT_IN_TIME", "key": "opt_learning_rate_decay_steps", "value": 4, "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/lr_scheduler.py", "lineno": 87}}
:::MLLOG {"namespace": "", "time_ms": 1593114365279, "event_type": "POINT_IN_TIME", "key": "opt_learning_rate_remain_steps", "value": 4054, "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/lr_scheduler.py", "lineno": 89}}
:::MLLOG {"namespace": "", "time_ms": 1593114365279, "event_type": "POINT_IN_TIME", "key": "opt_learning_rate_warmup_steps", "value": 200, "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/lr_scheduler.py", "lineno": 91}}
:::MLLOG {"namespace": "", "time_ms": 1593114365279, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "train.py", "lineno": 538, "first_epoch_num": 1, "epoch_count": 1}}
:::MLLOG {"namespace": "", "time_ms": 1593114365280, "event_type": "INTERVAL_START", "key": "epoch_start", "value": null, "metadata": {"file": "train.py", "lineno": 541, "epoch_num": 1}}
0: Starting epoch 0
0: Sampler for epoch 0 uses seed 4176291807
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
0: TRAIN [0][0/1291]	Time 0.308 (0.308)	Data 1.87e-01 (1.87e-01)	Tok/s 25307 (25307)	Loss/tok 10.7127 (10.7127)	LR 2.942e-05
0: TRAIN [0][10/1291]	Time 0.042 (0.072)	Data 1.03e-04 (1.71e-02)	Tok/s 182221 (180294)	Loss/tok 9.4553 (10.0129)	LR 3.704e-05
0: TRAIN [0][20/1291]	Time 0.059 (0.062)	Data 8.63e-05 (8.99e-03)	Tok/s 213908 (186700)	Loss/tok 9.2233 (9.6840)	LR 4.663e-05
0: Gradient norm: inf
0: Skipped batch, new scale: 512.0
0: TRAIN [0][30/1291]	Time 0.058 (0.058)	Data 1.09e-04 (6.12e-03)	Tok/s 216948 (190078)	Loss/tok 9.0566 (9.4753)	LR 5.736e-05
0: TRAIN [0][40/1291]	Time 0.042 (0.059)	Data 9.23e-05 (4.65e-03)	Tok/s 184947 (194831)	Loss/tok 8.5481 (9.2805)	LR 7.222e-05
0: Gradient norm: inf
0: Skipped batch, new scale: 256.0
0: Gradient norm: inf
0: Skipped batch, new scale: 128.0
0: TRAIN [0][50/1291]	Time 0.075 (0.057)	Data 8.56e-05 (3.76e-03)	Tok/s 231544 (195843)	Loss/tok 8.8454 (9.1616)	LR 8.682e-05
0: TRAIN [0][60/1291]	Time 0.058 (0.055)	Data 8.77e-05 (3.16e-03)	Tok/s 216205 (196084)	Loss/tok 8.2591 (9.0338)	LR 1.093e-04
0: TRAIN [0][70/1291]	Time 0.025 (0.055)	Data 8.32e-05 (2.73e-03)	Tok/s 153467 (197011)	Loss/tok 7.5727 (8.9131)	LR 1.376e-04
0: TRAIN [0][80/1291]	Time 0.076 (0.055)	Data 7.49e-05 (2.40e-03)	Tok/s 227668 (197799)	Loss/tok 8.1012 (8.8053)	LR 1.732e-04
0: TRAIN [0][90/1291]	Time 0.075 (0.054)	Data 7.58e-05 (2.15e-03)	Tok/s 230313 (198212)	Loss/tok 8.1012 (8.7179)	LR 2.181e-04
0: TRAIN [0][100/1291]	Time 0.041 (0.054)	Data 7.72e-05 (1.94e-03)	Tok/s 187001 (198766)	Loss/tok 7.7814 (8.6394)	LR 2.746e-04
0: Gradient norm: inf
0: Skipped batch, new scale: 64.0
0: TRAIN [0][110/1291]	Time 0.058 (0.054)	Data 7.82e-05 (1.78e-03)	Tok/s 217573 (199286)	Loss/tok 7.9640 (8.5744)	LR 3.378e-04
0: TRAIN [0][120/1291]	Time 0.042 (0.055)	Data 1.17e-04 (1.64e-03)	Tok/s 183884 (199846)	Loss/tok 7.7597 (8.5233)	LR 4.252e-04
0: TRAIN [0][130/1291]	Time 0.042 (0.054)	Data 7.30e-05 (1.52e-03)	Tok/s 182604 (199513)	Loss/tok 7.6423 (8.4731)	LR 5.354e-04
0: TRAIN [0][140/1291]	Time 0.096 (0.054)	Data 7.63e-05 (1.42e-03)	Tok/s 234300 (200231)	Loss/tok 7.9648 (8.4202)	LR 6.740e-04
0: TRAIN [0][150/1291]	Time 0.076 (0.055)	Data 7.75e-05 (1.33e-03)	Tok/s 227300 (200542)	Loss/tok 7.7757 (8.3700)	LR 8.485e-04
0: TRAIN [0][160/1291]	Time 0.041 (0.054)	Data 7.61e-05 (1.25e-03)	Tok/s 189337 (199803)	Loss/tok 7.1704 (8.3232)	LR 1.068e-03
0: TRAIN [0][170/1291]	Time 0.025 (0.054)	Data 8.11e-05 (1.18e-03)	Tok/s 160277 (200074)	Loss/tok 6.6724 (8.2609)	LR 1.345e-03
0: TRAIN [0][180/1291]	Time 0.076 (0.054)	Data 7.44e-05 (1.12e-03)	Tok/s 234902 (199878)	Loss/tok 7.2377 (8.2088)	LR 1.693e-03
0: TRAIN [0][190/1291]	Time 0.097 (0.054)	Data 7.77e-05 (1.07e-03)	Tok/s 227459 (200106)	Loss/tok 7.1980 (8.1448)	LR 2.131e-03
0: TRAIN [0][200/1291]	Time 0.058 (0.054)	Data 7.39e-05 (1.02e-03)	Tok/s 218023 (200222)	Loss/tok 7.0299 (8.0841)	LR 2.683e-03
0: TRAIN [0][210/1291]	Time 0.041 (0.054)	Data 1.13e-04 (9.76e-04)	Tok/s 192152 (200016)	Loss/tok 6.4517 (8.0277)	LR 2.875e-03
0: TRAIN [0][220/1291]	Time 0.058 (0.054)	Data 7.96e-05 (9.36e-04)	Tok/s 217202 (200464)	Loss/tok 6.4932 (7.9582)	LR 2.875e-03
0: TRAIN [0][230/1291]	Time 0.041 (0.054)	Data 7.72e-05 (8.99e-04)	Tok/s 183191 (200514)	Loss/tok 6.2268 (7.8897)	LR 2.875e-03
0: Upscaling, new scale: 128.0
0: TRAIN [0][240/1291]	Time 0.058 (0.054)	Data 7.15e-05 (8.65e-04)	Tok/s 215015 (200916)	Loss/tok 6.2639 (7.8244)	LR 2.875e-03
0: TRAIN [0][250/1291]	Time 0.041 (0.054)	Data 8.70e-05 (8.34e-04)	Tok/s 190149 (200632)	Loss/tok 5.7987 (7.7650)	LR 2.875e-03
0: TRAIN [0][260/1291]	Time 0.058 (0.054)	Data 7.53e-05 (8.05e-04)	Tok/s 218280 (200565)	Loss/tok 6.0276 (7.7001)	LR 2.875e-03
0: TRAIN [0][270/1291]	Time 0.041 (0.054)	Data 7.58e-05 (7.79e-04)	Tok/s 184143 (200720)	Loss/tok 5.7168 (7.6313)	LR 2.875e-03
0: TRAIN [0][280/1291]	Time 0.058 (0.054)	Data 7.77e-05 (7.54e-04)	Tok/s 213167 (201120)	Loss/tok 5.7380 (7.5556)	LR 2.875e-03
0: TRAIN [0][290/1291]	Time 0.098 (0.054)	Data 9.44e-05 (7.31e-04)	Tok/s 227865 (201590)	Loss/tok 5.9547 (7.4756)	LR 2.875e-03
0: TRAIN [0][300/1291]	Time 0.041 (0.054)	Data 7.65e-05 (7.09e-04)	Tok/s 189196 (201399)	Loss/tok 5.3748 (7.4198)	LR 2.875e-03
0: TRAIN [0][310/1291]	Time 0.076 (0.054)	Data 7.82e-05 (6.89e-04)	Tok/s 232296 (201637)	Loss/tok 5.5746 (7.3508)	LR 2.875e-03
0: TRAIN [0][320/1291]	Time 0.076 (0.054)	Data 7.70e-05 (6.70e-04)	Tok/s 232294 (201417)	Loss/tok 5.5932 (7.2912)	LR 2.875e-03
0: TRAIN [0][330/1291]	Time 0.042 (0.054)	Data 7.53e-05 (6.52e-04)	Tok/s 183631 (201461)	Loss/tok 4.9030 (7.2273)	LR 2.875e-03
0: TRAIN [0][340/1291]	Time 0.041 (0.054)	Data 7.65e-05 (6.36e-04)	Tok/s 185346 (201534)	Loss/tok 4.7672 (7.1621)	LR 2.875e-03
0: TRAIN [0][350/1291]	Time 0.076 (0.055)	Data 7.84e-05 (6.20e-04)	Tok/s 230012 (201995)	Loss/tok 5.2324 (7.0861)	LR 2.875e-03
0: TRAIN [0][360/1291]	Time 0.025 (0.054)	Data 7.82e-05 (6.05e-04)	Tok/s 161785 (201744)	Loss/tok 3.8625 (7.0334)	LR 2.875e-03
0: Upscaling, new scale: 256.0
0: TRAIN [0][370/1291]	Time 0.058 (0.054)	Data 7.82e-05 (5.91e-04)	Tok/s 216503 (201828)	Loss/tok 4.6209 (6.9709)	LR 2.875e-03
0: TRAIN [0][380/1291]	Time 0.058 (0.054)	Data 7.80e-05 (5.78e-04)	Tok/s 218689 (201911)	Loss/tok 4.7716 (6.9125)	LR 2.875e-03
0: TRAIN [0][390/1291]	Time 0.041 (0.054)	Data 1.29e-04 (5.65e-04)	Tok/s 195990 (201753)	Loss/tok 4.1791 (6.8622)	LR 2.875e-03
0: TRAIN [0][400/1291]	Time 0.058 (0.054)	Data 7.65e-05 (5.54e-04)	Tok/s 212169 (201763)	Loss/tok 4.6023 (6.8053)	LR 2.875e-03
0: TRAIN [0][410/1291]	Time 0.058 (0.054)	Data 7.46e-05 (5.42e-04)	Tok/s 218338 (201653)	Loss/tok 4.4523 (6.7552)	LR 2.875e-03
0: TRAIN [0][420/1291]	Time 0.041 (0.054)	Data 8.15e-05 (5.31e-04)	Tok/s 186903 (201639)	Loss/tok 4.2322 (6.7042)	LR 2.875e-03
0: TRAIN [0][430/1291]	Time 0.025 (0.054)	Data 7.61e-05 (5.21e-04)	Tok/s 157789 (201734)	Loss/tok 3.4194 (6.6486)	LR 2.875e-03
0: TRAIN [0][440/1291]	Time 0.041 (0.054)	Data 7.87e-05 (5.11e-04)	Tok/s 187191 (201653)	Loss/tok 4.0506 (6.6010)	LR 2.875e-03
0: TRAIN [0][450/1291]	Time 0.041 (0.054)	Data 7.80e-05 (5.01e-04)	Tok/s 179737 (201702)	Loss/tok 3.9585 (6.5489)	LR 2.875e-03
0: TRAIN [0][460/1291]	Time 0.042 (0.054)	Data 7.65e-05 (4.92e-04)	Tok/s 193112 (201687)	Loss/tok 3.9853 (6.5021)	LR 2.875e-03
0: TRAIN [0][470/1291]	Time 0.025 (0.054)	Data 7.58e-05 (4.83e-04)	Tok/s 156133 (201697)	Loss/tok 3.3698 (6.4536)	LR 2.875e-03
0: TRAIN [0][480/1291]	Time 0.041 (0.054)	Data 7.94e-05 (4.75e-04)	Tok/s 185473 (201563)	Loss/tok 3.9705 (6.4131)	LR 2.875e-03
0: TRAIN [0][490/1291]	Time 0.059 (0.054)	Data 7.80e-05 (4.67e-04)	Tok/s 213251 (201516)	Loss/tok 4.1526 (6.3719)	LR 2.875e-03
0: Upscaling, new scale: 512.0
0: TRAIN [0][500/1291]	Time 0.059 (0.054)	Data 7.53e-05 (4.59e-04)	Tok/s 215375 (201664)	Loss/tok 4.1153 (6.3258)	LR 2.875e-03
0: TRAIN [0][510/1291]	Time 0.041 (0.054)	Data 7.53e-05 (4.52e-04)	Tok/s 181287 (201628)	Loss/tok 3.8330 (6.2858)	LR 2.875e-03
0: TRAIN [0][520/1291]	Time 0.077 (0.054)	Data 7.77e-05 (4.45e-04)	Tok/s 228119 (201746)	Loss/tok 4.2669 (6.2409)	LR 2.875e-03
0: TRAIN [0][530/1291]	Time 0.025 (0.053)	Data 1.34e-04 (4.38e-04)	Tok/s 161946 (201600)	Loss/tok 3.3044 (6.2066)	LR 2.875e-03
0: TRAIN [0][540/1291]	Time 0.058 (0.054)	Data 7.65e-05 (4.32e-04)	Tok/s 218846 (201724)	Loss/tok 4.1336 (6.1662)	LR 2.875e-03
0: TRAIN [0][550/1291]	Time 0.041 (0.054)	Data 8.73e-05 (4.26e-04)	Tok/s 191473 (201749)	Loss/tok 3.9085 (6.1274)	LR 2.875e-03
0: TRAIN [0][560/1291]	Time 0.042 (0.053)	Data 7.87e-05 (4.19e-04)	Tok/s 185831 (201742)	Loss/tok 3.8036 (6.0921)	LR 2.875e-03
0: TRAIN [0][570/1291]	Time 0.042 (0.053)	Data 7.56e-05 (4.13e-04)	Tok/s 183755 (201620)	Loss/tok 3.6695 (6.0602)	LR 2.875e-03
0: TRAIN [0][580/1291]	Time 0.059 (0.053)	Data 7.68e-05 (4.08e-04)	Tok/s 212289 (201534)	Loss/tok 3.9072 (6.0281)	LR 2.875e-03
0: TRAIN [0][590/1291]	Time 0.076 (0.053)	Data 7.72e-05 (4.02e-04)	Tok/s 229629 (201509)	Loss/tok 4.3119 (5.9944)	LR 2.875e-03
0: TRAIN [0][600/1291]	Time 0.041 (0.053)	Data 1.30e-04 (3.97e-04)	Tok/s 188560 (201486)	Loss/tok 3.7898 (5.9615)	LR 2.875e-03
0: TRAIN [0][610/1291]	Time 0.041 (0.053)	Data 7.77e-05 (3.92e-04)	Tok/s 188596 (201304)	Loss/tok 3.7486 (5.9334)	LR 2.875e-03
0: Upscaling, new scale: 1024.0
0: TRAIN [0][620/1291]	Time 0.042 (0.053)	Data 7.63e-05 (3.87e-04)	Tok/s 188491 (201129)	Loss/tok 3.7784 (5.9056)	LR 2.875e-03
0: TRAIN [0][630/1291]	Time 0.058 (0.053)	Data 7.63e-05 (3.82e-04)	Tok/s 215403 (201205)	Loss/tok 3.9392 (5.8733)	LR 2.875e-03
0: TRAIN [0][640/1291]	Time 0.043 (0.053)	Data 7.65e-05 (3.77e-04)	Tok/s 182704 (201143)	Loss/tok 3.6357 (5.8446)	LR 2.875e-03
0: TRAIN [0][650/1291]	Time 0.043 (0.053)	Data 7.75e-05 (3.73e-04)	Tok/s 183591 (201266)	Loss/tok 3.6707 (5.8125)	LR 2.875e-03
0: TRAIN [0][660/1291]	Time 0.043 (0.053)	Data 7.92e-05 (3.68e-04)	Tok/s 182508 (201159)	Loss/tok 3.6125 (5.7855)	LR 2.875e-03
0: TRAIN [0][670/1291]	Time 0.042 (0.053)	Data 7.39e-05 (3.64e-04)	Tok/s 182649 (201042)	Loss/tok 3.5791 (5.7592)	LR 2.875e-03
0: TRAIN [0][680/1291]	Time 0.041 (0.053)	Data 1.20e-04 (3.60e-04)	Tok/s 183995 (200923)	Loss/tok 3.7012 (5.7339)	LR 2.875e-03
0: TRAIN [0][690/1291]	Time 0.042 (0.053)	Data 7.65e-05 (3.56e-04)	Tok/s 182860 (200773)	Loss/tok 3.6025 (5.7094)	LR 2.875e-03
0: TRAIN [0][700/1291]	Time 0.042 (0.053)	Data 1.35e-04 (3.52e-04)	Tok/s 185703 (200901)	Loss/tok 3.6415 (5.6804)	LR 2.875e-03
0: TRAIN [0][710/1291]	Time 0.076 (0.053)	Data 1.34e-04 (3.48e-04)	Tok/s 235038 (200825)	Loss/tok 4.0921 (5.6568)	LR 2.875e-03
0: TRAIN [0][720/1291]	Time 0.041 (0.053)	Data 7.70e-05 (3.45e-04)	Tok/s 185852 (200906)	Loss/tok 3.6576 (5.6314)	LR 2.875e-03
0: TRAIN [0][730/1291]	Time 0.059 (0.053)	Data 7.89e-05 (3.41e-04)	Tok/s 213925 (201039)	Loss/tok 3.8120 (5.6033)	LR 2.875e-03
0: TRAIN [0][740/1291]	Time 0.058 (0.053)	Data 7.70e-05 (3.37e-04)	Tok/s 216174 (201092)	Loss/tok 3.9048 (5.5791)	LR 2.875e-03
0: Upscaling, new scale: 2048.0
0: TRAIN [0][750/1291]	Time 0.076 (0.053)	Data 7.39e-05 (3.34e-04)	Tok/s 231557 (201279)	Loss/tok 4.0493 (5.5507)	LR 2.875e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 1024.0
0: TRAIN [0][760/1291]	Time 0.042 (0.053)	Data 7.87e-05 (3.31e-04)	Tok/s 187476 (201296)	Loss/tok 3.6692 (5.5276)	LR 2.875e-03
0: TRAIN [0][770/1291]	Time 0.041 (0.053)	Data 7.63e-05 (3.28e-04)	Tok/s 189210 (201165)	Loss/tok 3.5440 (5.5085)	LR 2.875e-03
0: TRAIN [0][780/1291]	Time 0.042 (0.053)	Data 1.32e-04 (3.24e-04)	Tok/s 191726 (201256)	Loss/tok 3.4824 (5.4855)	LR 2.875e-03
0: TRAIN [0][790/1291]	Time 0.041 (0.053)	Data 7.70e-05 (3.21e-04)	Tok/s 187633 (201170)	Loss/tok 3.5888 (5.4664)	LR 2.875e-03
0: TRAIN [0][800/1291]	Time 0.058 (0.053)	Data 7.75e-05 (3.18e-04)	Tok/s 213784 (201159)	Loss/tok 3.8403 (5.4466)	LR 2.875e-03
0: TRAIN [0][810/1291]	Time 0.041 (0.053)	Data 1.03e-04 (3.15e-04)	Tok/s 191987 (201261)	Loss/tok 3.5411 (5.4242)	LR 2.875e-03
0: TRAIN [0][820/1291]	Time 0.058 (0.053)	Data 7.75e-05 (3.13e-04)	Tok/s 212265 (201222)	Loss/tok 3.7593 (5.4049)	LR 2.875e-03
0: TRAIN [0][830/1291]	Time 0.041 (0.053)	Data 8.01e-05 (3.10e-04)	Tok/s 187080 (201359)	Loss/tok 3.5116 (5.3824)	LR 2.875e-03
0: TRAIN [0][840/1291]	Time 0.058 (0.053)	Data 7.56e-05 (3.07e-04)	Tok/s 218120 (201305)	Loss/tok 3.7207 (5.3639)	LR 2.875e-03
0: TRAIN [0][850/1291]	Time 0.042 (0.053)	Data 7.34e-05 (3.05e-04)	Tok/s 188291 (201442)	Loss/tok 3.5250 (5.3423)	LR 2.875e-03
0: TRAIN [0][860/1291]	Time 0.059 (0.053)	Data 7.77e-05 (3.02e-04)	Tok/s 214436 (201489)	Loss/tok 3.6777 (5.3229)	LR 2.875e-03
0: TRAIN [0][870/1291]	Time 0.042 (0.053)	Data 8.08e-05 (2.99e-04)	Tok/s 185828 (201528)	Loss/tok 3.5527 (5.3042)	LR 2.875e-03
0: TRAIN [0][880/1291]	Time 0.024 (0.053)	Data 1.36e-04 (2.97e-04)	Tok/s 159164 (201501)	Loss/tok 2.9816 (5.2873)	LR 2.875e-03
0: Upscaling, new scale: 2048.0
0: TRAIN [0][890/1291]	Time 0.041 (0.053)	Data 7.72e-05 (2.95e-04)	Tok/s 187926 (201570)	Loss/tok 3.4465 (5.2694)	LR 2.875e-03
0: TRAIN [0][900/1291]	Time 0.027 (0.053)	Data 7.94e-05 (2.92e-04)	Tok/s 153633 (201491)	Loss/tok 2.9065 (5.2528)	LR 2.875e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 1024.0
0: TRAIN [0][910/1291]	Time 0.078 (0.053)	Data 7.75e-05 (2.90e-04)	Tok/s 226925 (201510)	Loss/tok 4.0051 (5.2340)	LR 2.875e-03
0: TRAIN [0][920/1291]	Time 0.059 (0.054)	Data 1.05e-04 (2.88e-04)	Tok/s 218640 (201644)	Loss/tok 3.7203 (5.2139)	LR 2.875e-03
0: TRAIN [0][930/1291]	Time 0.042 (0.054)	Data 7.51e-05 (2.86e-04)	Tok/s 184950 (201601)	Loss/tok 3.6192 (5.1979)	LR 2.875e-03
0: TRAIN [0][940/1291]	Time 0.043 (0.054)	Data 7.65e-05 (2.84e-04)	Tok/s 174084 (201549)	Loss/tok 3.5190 (5.1821)	LR 2.875e-03
0: TRAIN [0][950/1291]	Time 0.060 (0.054)	Data 1.50e-04 (2.82e-04)	Tok/s 205686 (201529)	Loss/tok 3.8397 (5.1662)	LR 2.875e-03
0: TRAIN [0][960/1291]	Time 0.060 (0.054)	Data 7.82e-05 (2.80e-04)	Tok/s 209902 (201386)	Loss/tok 3.6605 (5.1529)	LR 2.875e-03
0: TRAIN [0][970/1291]	Time 0.026 (0.054)	Data 1.39e-04 (2.78e-04)	Tok/s 156084 (201343)	Loss/tok 2.8470 (5.1378)	LR 2.875e-03
0: TRAIN [0][980/1291]	Time 0.043 (0.053)	Data 7.87e-05 (2.76e-04)	Tok/s 180571 (201254)	Loss/tok 3.6746 (5.1238)	LR 2.875e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 512.0
0: TRAIN [0][990/1291]	Time 0.042 (0.054)	Data 7.68e-05 (2.74e-04)	Tok/s 183368 (201273)	Loss/tok 3.3636 (5.1078)	LR 2.875e-03
0: TRAIN [0][1000/1291]	Time 0.061 (0.054)	Data 8.70e-05 (2.72e-04)	Tok/s 205839 (201234)	Loss/tok 3.6907 (5.0932)	LR 2.875e-03
0: TRAIN [0][1010/1291]	Time 0.041 (0.054)	Data 7.56e-05 (2.70e-04)	Tok/s 186784 (201173)	Loss/tok 3.3973 (5.0804)	LR 2.875e-03
0: TRAIN [0][1020/1291]	Time 0.058 (0.053)	Data 7.39e-05 (2.68e-04)	Tok/s 218994 (201170)	Loss/tok 3.5658 (5.0669)	LR 2.875e-03
0: TRAIN [0][1030/1291]	Time 0.041 (0.053)	Data 1.34e-04 (2.66e-04)	Tok/s 190016 (201128)	Loss/tok 3.3752 (5.0545)	LR 2.875e-03
0: TRAIN [0][1040/1291]	Time 0.058 (0.053)	Data 7.82e-05 (2.65e-04)	Tok/s 213521 (201068)	Loss/tok 3.7933 (5.0422)	LR 2.875e-03
0: TRAIN [0][1050/1291]	Time 0.097 (0.053)	Data 7.49e-05 (2.63e-04)	Tok/s 227476 (201187)	Loss/tok 3.9706 (5.0266)	LR 2.875e-03
0: TRAIN [0][1060/1291]	Time 0.058 (0.053)	Data 7.63e-05 (2.61e-04)	Tok/s 213915 (201229)	Loss/tok 3.5965 (5.0132)	LR 2.875e-03
0: TRAIN [0][1070/1291]	Time 0.025 (0.053)	Data 7.87e-05 (2.60e-04)	Tok/s 157965 (201161)	Loss/tok 2.8957 (5.0011)	LR 2.875e-03
0: TRAIN [0][1080/1291]	Time 0.058 (0.053)	Data 7.72e-05 (2.58e-04)	Tok/s 216883 (201191)	Loss/tok 3.5585 (4.9879)	LR 2.875e-03
0: TRAIN [0][1090/1291]	Time 0.058 (0.053)	Data 7.65e-05 (2.56e-04)	Tok/s 218951 (201144)	Loss/tok 3.6702 (4.9764)	LR 2.875e-03
0: TRAIN [0][1100/1291]	Time 0.041 (0.053)	Data 7.58e-05 (2.55e-04)	Tok/s 189902 (201109)	Loss/tok 3.2707 (4.9645)	LR 2.875e-03
0: Upscaling, new scale: 1024.0
0: TRAIN [0][1110/1291]	Time 0.041 (0.053)	Data 7.70e-05 (2.53e-04)	Tok/s 191473 (201062)	Loss/tok 3.3526 (4.9536)	LR 2.875e-03
0: TRAIN [0][1120/1291]	Time 0.059 (0.053)	Data 8.20e-05 (2.52e-04)	Tok/s 212278 (201126)	Loss/tok 3.6269 (4.9406)	LR 2.875e-03
0: TRAIN [0][1130/1291]	Time 0.059 (0.053)	Data 9.06e-05 (2.50e-04)	Tok/s 213461 (201118)	Loss/tok 3.5995 (4.9285)	LR 2.875e-03
0: TRAIN [0][1140/1291]	Time 0.059 (0.053)	Data 1.34e-04 (2.49e-04)	Tok/s 215552 (201082)	Loss/tok 3.5447 (4.9173)	LR 2.875e-03
0: TRAIN [0][1150/1291]	Time 0.043 (0.053)	Data 7.77e-05 (2.48e-04)	Tok/s 177250 (200964)	Loss/tok 3.3723 (4.9076)	LR 2.875e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 512.0
0: TRAIN [0][1160/1291]	Time 0.043 (0.053)	Data 8.01e-05 (2.46e-04)	Tok/s 181855 (200935)	Loss/tok 3.2751 (4.8962)	LR 2.875e-03
0: TRAIN [0][1170/1291]	Time 0.097 (0.053)	Data 8.13e-05 (2.45e-04)	Tok/s 229106 (201022)	Loss/tok 3.9921 (4.8837)	LR 2.875e-03
0: TRAIN [0][1180/1291]	Time 0.042 (0.053)	Data 1.33e-04 (2.44e-04)	Tok/s 181083 (200987)	Loss/tok 3.3104 (4.8729)	LR 2.875e-03
0: TRAIN [0][1190/1291]	Time 0.042 (0.053)	Data 7.72e-05 (2.42e-04)	Tok/s 181723 (200916)	Loss/tok 3.3936 (4.8626)	LR 2.875e-03
0: TRAIN [0][1200/1291]	Time 0.025 (0.053)	Data 7.96e-05 (2.41e-04)	Tok/s 160351 (200887)	Loss/tok 3.0437 (4.8526)	LR 2.875e-03
0: TRAIN [0][1210/1291]	Time 0.059 (0.053)	Data 7.84e-05 (2.40e-04)	Tok/s 211476 (200768)	Loss/tok 3.6548 (4.8436)	LR 2.875e-03
0: TRAIN [0][1220/1291]	Time 0.043 (0.053)	Data 1.31e-04 (2.39e-04)	Tok/s 185832 (200734)	Loss/tok 3.4336 (4.8334)	LR 2.875e-03
0: TRAIN [0][1230/1291]	Time 0.042 (0.053)	Data 1.39e-04 (2.37e-04)	Tok/s 189105 (200663)	Loss/tok 3.3497 (4.8242)	LR 2.875e-03
0: TRAIN [0][1240/1291]	Time 0.059 (0.053)	Data 8.46e-05 (2.36e-04)	Tok/s 213652 (200653)	Loss/tok 3.7551 (4.8141)	LR 2.875e-03
0: TRAIN [0][1250/1291]	Time 0.042 (0.053)	Data 8.77e-05 (2.35e-04)	Tok/s 179837 (200578)	Loss/tok 3.3780 (4.8052)	LR 2.875e-03
0: TRAIN [0][1260/1291]	Time 0.042 (0.053)	Data 8.13e-05 (2.34e-04)	Tok/s 177849 (200529)	Loss/tok 3.2600 (4.7957)	LR 2.875e-03
0: TRAIN [0][1270/1291]	Time 0.042 (0.053)	Data 9.08e-05 (2.33e-04)	Tok/s 185219 (200486)	Loss/tok 3.3329 (4.7864)	LR 2.875e-03
0: TRAIN [0][1280/1291]	Time 0.059 (0.053)	Data 8.46e-05 (2.32e-04)	Tok/s 209999 (200492)	Loss/tok 3.5780 (4.7765)	LR 2.875e-03
0: Upscaling, new scale: 1024.0
0: TRAIN [0][1290/1291]	Time 0.058 (0.053)	Data 4.27e-05 (2.32e-04)	Tok/s 214660 (200380)	Loss/tok 3.5114 (4.7684)	LR 2.875e-03
:::MLLOG {"namespace": "", "time_ms": 1593114433831, "event_type": "INTERVAL_END", "key": "epoch_stop", "value": null, "metadata": {"file": "train.py", "lineno": 551, "epoch_num": 1}}
:::MLLOG {"namespace": "", "time_ms": 1593114433832, "event_type": "INTERVAL_START", "key": "eval_start", "value": null, "metadata": {"file": "train.py", "lineno": 556, "epoch_num": 1}}
0: Running evaluation on test set
0: TEST [0][0/3]	Time 0.403 (0.403)	Decoder iters 149.0 (149.0)	Tok/s 23266 (23266)
0: Running moses detokenizer
0: BLEU(score=19.13747489456636, counts=[34784, 15799, 8325, 4579], totals=[67458, 64455, 61452, 58452], precisions=[51.56393607874529, 24.51167481188426, 13.547158758055067, 7.8337781427496065], bp=1.0, sys_len=67458, ref_len=64676)
0: Finished evaluation on test set
:::MLLOG {"namespace": "", "time_ms": 1593114435172, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.19140000000000001, "metadata": {"file": "train.py", "lineno": 562, "epoch_num": 1}}
:::MLLOG {"namespace": "", "time_ms": 1593114435172, "event_type": "INTERVAL_END", "key": "eval_stop", "value": null, "metadata": {"file": "train.py", "lineno": 565, "epoch_num": 1}}
0: Summary: Epoch: 0	Training Loss: 4.7676	Test BLEU: 19.14
0: Performance: Epoch: 0	Training: 3205402 Tok/s
0: Finished epoch 0
:::MLLOG {"namespace": "", "time_ms": 1593114435173, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "train.py", "lineno": 584, "first_epoch_num": 1}}
:::MLLOG {"namespace": "", "time_ms": 1593114435173, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "train.py", "lineno": 538, "first_epoch_num": 2, "epoch_count": 1}}
:::MLLOG {"namespace": "", "time_ms": 1593114435173, "event_type": "INTERVAL_START", "key": "epoch_start", "value": null, "metadata": {"file": "train.py", "lineno": 541, "epoch_num": 2}}
0: Starting epoch 1
0: Sampler for epoch 1 uses seed 3606557997
0: TRAIN [1][0/1291]	Time 0.282 (0.282)	Data 1.86e-01 (1.86e-01)	Tok/s 44417 (44417)	Loss/tok 3.4979 (3.4979)	LR 2.875e-03
0: TRAIN [1][10/1291]	Time 0.097 (0.079)	Data 8.34e-05 (1.70e-02)	Tok/s 232859 (194554)	Loss/tok 3.8149 (3.5444)	LR 2.875e-03
0: TRAIN [1][20/1291]	Time 0.058 (0.067)	Data 1.47e-04 (8.97e-03)	Tok/s 218850 (199584)	Loss/tok 3.4783 (3.5028)	LR 2.875e-03
0: TRAIN [1][30/1291]	Time 0.041 (0.064)	Data 9.30e-05 (6.10e-03)	Tok/s 186698 (202923)	Loss/tok 3.1727 (3.5249)	LR 2.875e-03
0: TRAIN [1][40/1291]	Time 0.076 (0.065)	Data 8.87e-05 (4.64e-03)	Tok/s 230307 (206120)	Loss/tok 3.5888 (3.5516)	LR 2.875e-03
0: TRAIN [1][50/1291]	Time 0.042 (0.062)	Data 9.18e-05 (3.75e-03)	Tok/s 184474 (204877)	Loss/tok 3.3369 (3.5408)	LR 2.875e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 512.0
0: TRAIN [1][60/1291]	Time 0.076 (0.062)	Data 8.94e-05 (3.15e-03)	Tok/s 227122 (205723)	Loss/tok 3.6908 (3.5621)	LR 2.875e-03
0: TRAIN [1][70/1291]	Time 0.076 (0.061)	Data 8.27e-05 (2.72e-03)	Tok/s 227447 (205189)	Loss/tok 3.7547 (3.5535)	LR 2.875e-03
0: TRAIN [1][80/1291]	Time 0.058 (0.059)	Data 8.68e-05 (2.39e-03)	Tok/s 219247 (203915)	Loss/tok 3.5722 (3.5392)	LR 2.875e-03
0: TRAIN [1][90/1291]	Time 0.042 (0.058)	Data 9.37e-05 (2.14e-03)	Tok/s 185266 (203809)	Loss/tok 3.1438 (3.5279)	LR 2.875e-03
0: TRAIN [1][100/1291]	Time 0.042 (0.058)	Data 7.68e-05 (1.94e-03)	Tok/s 186721 (204192)	Loss/tok 3.2046 (3.5349)	LR 2.875e-03
0: TRAIN [1][110/1291]	Time 0.059 (0.059)	Data 8.70e-05 (1.77e-03)	Tok/s 213313 (205128)	Loss/tok 3.3942 (3.5358)	LR 2.875e-03
0: TRAIN [1][120/1291]	Time 0.042 (0.057)	Data 8.58e-05 (1.63e-03)	Tok/s 186580 (203671)	Loss/tok 3.2089 (3.5192)	LR 2.875e-03
0: TRAIN [1][130/1291]	Time 0.096 (0.057)	Data 8.63e-05 (1.52e-03)	Tok/s 233249 (203793)	Loss/tok 3.9535 (3.5256)	LR 2.875e-03
0: TRAIN [1][140/1291]	Time 0.025 (0.057)	Data 7.89e-05 (1.42e-03)	Tok/s 157095 (203452)	Loss/tok 2.8220 (3.5149)	LR 2.875e-03
0: TRAIN [1][150/1291]	Time 0.058 (0.057)	Data 1.36e-04 (1.33e-03)	Tok/s 218665 (203710)	Loss/tok 3.4506 (3.5212)	LR 2.875e-03
0: TRAIN [1][160/1291]	Time 0.042 (0.056)	Data 8.73e-05 (1.25e-03)	Tok/s 189566 (203393)	Loss/tok 3.2874 (3.5203)	LR 2.875e-03
0: TRAIN [1][170/1291]	Time 0.098 (0.057)	Data 8.30e-05 (1.18e-03)	Tok/s 225974 (204211)	Loss/tok 3.9169 (3.5267)	LR 2.875e-03
0: TRAIN [1][180/1291]	Time 0.025 (0.057)	Data 1.59e-04 (1.12e-03)	Tok/s 160426 (204038)	Loss/tok 2.6923 (3.5298)	LR 2.875e-03
0: Upscaling, new scale: 1024.0
0: TRAIN [1][190/1291]	Time 0.041 (0.057)	Data 7.96e-05 (1.07e-03)	Tok/s 188571 (203731)	Loss/tok 3.1473 (3.5261)	LR 2.875e-03
0: TRAIN [1][200/1291]	Time 0.025 (0.057)	Data 7.96e-05 (1.02e-03)	Tok/s 158620 (203513)	Loss/tok 2.8104 (3.5211)	LR 2.875e-03
0: TRAIN [1][210/1291]	Time 0.058 (0.057)	Data 7.96e-05 (9.78e-04)	Tok/s 217568 (203972)	Loss/tok 3.3317 (3.5181)	LR 2.875e-03
0: TRAIN [1][220/1291]	Time 0.076 (0.057)	Data 8.77e-05 (9.37e-04)	Tok/s 234256 (204456)	Loss/tok 3.6601 (3.5224)	LR 2.875e-03
0: TRAIN [1][230/1291]	Time 0.097 (0.057)	Data 7.94e-05 (9.01e-04)	Tok/s 228732 (203945)	Loss/tok 3.8388 (3.5232)	LR 2.875e-03
0: TRAIN [1][240/1291]	Time 0.042 (0.057)	Data 1.47e-04 (8.67e-04)	Tok/s 188378 (203880)	Loss/tok 3.2090 (3.5220)	LR 2.875e-03
0: TRAIN [1][250/1291]	Time 0.059 (0.056)	Data 7.94e-05 (8.36e-04)	Tok/s 212756 (203572)	Loss/tok 3.6238 (3.5224)	LR 2.875e-03
0: TRAIN [1][260/1291]	Time 0.058 (0.056)	Data 8.11e-05 (8.08e-04)	Tok/s 215456 (203129)	Loss/tok 3.6294 (3.5163)	LR 2.875e-03
0: TRAIN [1][270/1291]	Time 0.042 (0.056)	Data 8.01e-05 (7.81e-04)	Tok/s 183819 (202965)	Loss/tok 3.3704 (3.5156)	LR 2.875e-03
0: TRAIN [1][280/1291]	Time 0.042 (0.056)	Data 7.87e-05 (7.57e-04)	Tok/s 187245 (202925)	Loss/tok 3.2379 (3.5158)	LR 2.875e-03
0: TRAIN [1][290/1291]	Time 0.076 (0.056)	Data 1.37e-04 (7.34e-04)	Tok/s 228517 (203171)	Loss/tok 3.5982 (3.5146)	LR 2.875e-03
0: TRAIN [1][300/1291]	Time 0.041 (0.055)	Data 9.16e-05 (7.13e-04)	Tok/s 185390 (202875)	Loss/tok 3.3464 (3.5101)	LR 2.875e-03
0: TRAIN [1][310/1291]	Time 0.041 (0.055)	Data 1.65e-04 (6.93e-04)	Tok/s 183277 (202997)	Loss/tok 3.2814 (3.5081)	LR 2.875e-03
0: Upscaling, new scale: 2048.0
0: TRAIN [1][320/1291]	Time 0.025 (0.055)	Data 8.15e-05 (6.74e-04)	Tok/s 158372 (202061)	Loss/tok 2.6847 (3.5019)	LR 2.875e-03
0: TRAIN [1][330/1291]	Time 0.058 (0.055)	Data 8.06e-05 (6.56e-04)	Tok/s 211769 (201971)	Loss/tok 3.5074 (3.4999)	LR 2.875e-03
0: TRAIN [1][340/1291]	Time 0.059 (0.054)	Data 7.99e-05 (6.40e-04)	Tok/s 216676 (202019)	Loss/tok 3.3526 (3.4979)	LR 2.875e-03
0: TRAIN [1][350/1291]	Time 0.042 (0.054)	Data 1.36e-04 (6.24e-04)	Tok/s 185226 (202016)	Loss/tok 3.2624 (3.4980)	LR 2.875e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 1024.0
0: TRAIN [1][360/1291]	Time 0.058 (0.054)	Data 8.42e-05 (6.10e-04)	Tok/s 214511 (201928)	Loss/tok 3.4996 (3.5009)	LR 2.875e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 512.0
0: TRAIN [1][370/1291]	Time 0.025 (0.054)	Data 8.32e-05 (5.96e-04)	Tok/s 157976 (201335)	Loss/tok 2.7954 (3.4983)	LR 2.875e-03
0: TRAIN [1][380/1291]	Time 0.058 (0.054)	Data 8.39e-05 (5.82e-04)	Tok/s 217982 (201670)	Loss/tok 3.3867 (3.4987)	LR 2.875e-03
0: TRAIN [1][390/1291]	Time 0.076 (0.054)	Data 7.82e-05 (5.70e-04)	Tok/s 227935 (201666)	Loss/tok 3.7231 (3.4974)	LR 2.875e-03
0: TRAIN [1][400/1291]	Time 0.041 (0.054)	Data 1.03e-04 (5.58e-04)	Tok/s 186261 (201597)	Loss/tok 3.1803 (3.4948)	LR 2.875e-03
0: TRAIN [1][410/1291]	Time 0.042 (0.054)	Data 8.25e-05 (5.46e-04)	Tok/s 182805 (201378)	Loss/tok 3.1813 (3.4917)	LR 2.875e-03
0: TRAIN [1][420/1291]	Time 0.042 (0.054)	Data 8.18e-05 (5.36e-04)	Tok/s 185989 (201095)	Loss/tok 3.2184 (3.4903)	LR 2.875e-03
0: TRAIN [1][430/1291]	Time 0.059 (0.054)	Data 8.15e-05 (5.25e-04)	Tok/s 217712 (201198)	Loss/tok 3.3377 (3.4916)	LR 2.875e-03
0: TRAIN [1][440/1291]	Time 0.059 (0.054)	Data 9.68e-05 (5.15e-04)	Tok/s 214435 (201560)	Loss/tok 3.4200 (3.4930)	LR 2.875e-03
0: TRAIN [1][450/1291]	Time 0.058 (0.054)	Data 8.30e-05 (5.06e-04)	Tok/s 214702 (201601)	Loss/tok 3.5265 (3.4915)	LR 2.875e-03
0: TRAIN [1][460/1291]	Time 0.025 (0.054)	Data 1.43e-04 (4.97e-04)	Tok/s 163859 (201602)	Loss/tok 2.7980 (3.4907)	LR 2.875e-03
0: TRAIN [1][470/1291]	Time 0.041 (0.054)	Data 8.08e-05 (4.88e-04)	Tok/s 185682 (201692)	Loss/tok 3.2522 (3.4898)	LR 2.875e-03
0: TRAIN [1][480/1291]	Time 0.076 (0.054)	Data 8.25e-05 (4.80e-04)	Tok/s 229920 (201702)	Loss/tok 3.5038 (3.4897)	LR 2.875e-03
0: Upscaling, new scale: 1024.0
0: TRAIN [1][490/1291]	Time 0.042 (0.054)	Data 7.92e-05 (4.72e-04)	Tok/s 189645 (201987)	Loss/tok 3.2155 (3.4913)	LR 2.875e-03
0: TRAIN [1][500/1291]	Time 0.025 (0.054)	Data 8.13e-05 (4.65e-04)	Tok/s 161335 (201865)	Loss/tok 2.7333 (3.4901)	LR 2.875e-03
0: TRAIN [1][510/1291]	Time 0.058 (0.054)	Data 8.30e-05 (4.57e-04)	Tok/s 217217 (201664)	Loss/tok 3.4507 (3.4873)	LR 2.875e-03
0: TRAIN [1][520/1291]	Time 0.076 (0.054)	Data 8.13e-05 (4.50e-04)	Tok/s 232651 (201823)	Loss/tok 3.6147 (3.4884)	LR 2.875e-03
0: TRAIN [1][530/1291]	Time 0.058 (0.054)	Data 1.38e-04 (4.43e-04)	Tok/s 218073 (201840)	Loss/tok 3.3576 (3.4868)	LR 2.875e-03
0: TRAIN [1][540/1291]	Time 0.042 (0.054)	Data 8.01e-05 (4.37e-04)	Tok/s 186585 (201805)	Loss/tok 3.2354 (3.4865)	LR 2.875e-03
0: TRAIN [1][550/1291]	Time 0.058 (0.054)	Data 8.23e-05 (4.30e-04)	Tok/s 214397 (201784)	Loss/tok 3.4708 (3.4847)	LR 2.875e-03
0: TRAIN [1][560/1291]	Time 0.025 (0.054)	Data 8.30e-05 (4.24e-04)	Tok/s 162628 (201668)	Loss/tok 2.7401 (3.4822)	LR 2.875e-03
0: TRAIN [1][570/1291]	Time 0.025 (0.054)	Data 8.03e-05 (4.18e-04)	Tok/s 156609 (201509)	Loss/tok 2.8149 (3.4796)	LR 2.875e-03
0: TRAIN [1][580/1291]	Time 0.041 (0.054)	Data 7.87e-05 (4.12e-04)	Tok/s 189215 (201442)	Loss/tok 3.1656 (3.4789)	LR 2.875e-03
0: TRAIN [1][590/1291]	Time 0.025 (0.054)	Data 8.25e-05 (4.07e-04)	Tok/s 157600 (201470)	Loss/tok 2.6212 (3.4779)	LR 2.875e-03
0: TRAIN [1][600/1291]	Time 0.025 (0.053)	Data 8.11e-05 (4.01e-04)	Tok/s 157205 (201353)	Loss/tok 2.8326 (3.4757)	LR 2.875e-03
0: TRAIN [1][610/1291]	Time 0.042 (0.054)	Data 8.13e-05 (3.96e-04)	Tok/s 188307 (201493)	Loss/tok 3.1040 (3.4759)	LR 2.875e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 512.0
0: TRAIN [1][620/1291]	Time 0.041 (0.054)	Data 8.11e-05 (3.91e-04)	Tok/s 190544 (201483)	Loss/tok 3.1831 (3.4753)	LR 2.875e-03
0: TRAIN [1][630/1291]	Time 0.042 (0.053)	Data 8.32e-05 (3.86e-04)	Tok/s 184304 (201346)	Loss/tok 3.2380 (3.4730)	LR 2.875e-03
0: TRAIN [1][640/1291]	Time 0.058 (0.053)	Data 8.25e-05 (3.82e-04)	Tok/s 218863 (201338)	Loss/tok 3.4488 (3.4723)	LR 2.875e-03
0: TRAIN [1][650/1291]	Time 0.042 (0.053)	Data 9.23e-05 (3.77e-04)	Tok/s 188529 (201200)	Loss/tok 3.2221 (3.4707)	LR 2.875e-03
0: TRAIN [1][660/1291]	Time 0.059 (0.053)	Data 7.96e-05 (3.73e-04)	Tok/s 214028 (201287)	Loss/tok 3.3691 (3.4710)	LR 2.875e-03
0: TRAIN [1][670/1291]	Time 0.041 (0.053)	Data 1.39e-04 (3.69e-04)	Tok/s 182179 (201329)	Loss/tok 3.1350 (3.4727)	LR 2.875e-03
0: TRAIN [1][680/1291]	Time 0.041 (0.053)	Data 7.87e-05 (3.65e-04)	Tok/s 189226 (201260)	Loss/tok 3.2633 (3.4711)	LR 2.875e-03
0: TRAIN [1][690/1291]	Time 0.025 (0.053)	Data 8.03e-05 (3.61e-04)	Tok/s 161128 (201240)	Loss/tok 2.6970 (3.4707)	LR 2.875e-03
0: TRAIN [1][700/1291]	Time 0.058 (0.053)	Data 8.06e-05 (3.57e-04)	Tok/s 214995 (201196)	Loss/tok 3.5204 (3.4694)	LR 2.875e-03
0: TRAIN [1][710/1291]	Time 0.041 (0.053)	Data 1.43e-04 (3.53e-04)	Tok/s 188480 (201080)	Loss/tok 3.1567 (3.4693)	LR 2.875e-03
0: TRAIN [1][720/1291]	Time 0.041 (0.053)	Data 7.96e-05 (3.49e-04)	Tok/s 187789 (201183)	Loss/tok 3.3427 (3.4691)	LR 2.875e-03
0: TRAIN [1][730/1291]	Time 0.025 (0.053)	Data 8.11e-05 (3.45e-04)	Tok/s 152143 (201171)	Loss/tok 2.6707 (3.4711)	LR 2.875e-03
0: Upscaling, new scale: 1024.0
0: TRAIN [1][740/1291]	Time 0.041 (0.053)	Data 8.13e-05 (3.42e-04)	Tok/s 195009 (201136)	Loss/tok 3.2409 (3.4693)	LR 2.875e-03
0: TRAIN [1][750/1291]	Time 0.042 (0.053)	Data 8.08e-05 (3.39e-04)	Tok/s 187444 (201106)	Loss/tok 3.2265 (3.4676)	LR 2.875e-03
0: TRAIN [1][760/1291]	Time 0.077 (0.053)	Data 8.06e-05 (3.35e-04)	Tok/s 224887 (201241)	Loss/tok 3.5721 (3.4673)	LR 2.875e-03
0: TRAIN [1][770/1291]	Time 0.058 (0.053)	Data 7.99e-05 (3.32e-04)	Tok/s 216110 (201253)	Loss/tok 3.4659 (3.4671)	LR 2.875e-03
0: TRAIN [1][780/1291]	Time 0.041 (0.053)	Data 9.49e-05 (3.29e-04)	Tok/s 189061 (201140)	Loss/tok 3.3556 (3.4652)	LR 2.875e-03
0: TRAIN [1][790/1291]	Time 0.058 (0.053)	Data 8.18e-05 (3.26e-04)	Tok/s 214709 (201143)	Loss/tok 3.4460 (3.4642)	LR 2.875e-03
0: TRAIN [1][800/1291]	Time 0.059 (0.053)	Data 7.96e-05 (3.23e-04)	Tok/s 215327 (201193)	Loss/tok 3.4474 (3.4644)	LR 2.875e-03
0: TRAIN [1][810/1291]	Time 0.042 (0.053)	Data 8.08e-05 (3.20e-04)	Tok/s 180644 (201139)	Loss/tok 3.1557 (3.4635)	LR 2.875e-03
0: TRAIN [1][820/1291]	Time 0.025 (0.053)	Data 7.96e-05 (3.17e-04)	Tok/s 161259 (201154)	Loss/tok 2.7839 (3.4635)	LR 2.875e-03
0: TRAIN [1][830/1291]	Time 0.058 (0.053)	Data 7.99e-05 (3.14e-04)	Tok/s 216945 (201147)	Loss/tok 3.4717 (3.4625)	LR 2.875e-03
0: TRAIN [1][840/1291]	Time 0.041 (0.053)	Data 8.18e-05 (3.12e-04)	Tok/s 186309 (201108)	Loss/tok 3.2070 (3.4609)	LR 2.875e-03
0: TRAIN [1][850/1291]	Time 0.076 (0.053)	Data 8.11e-05 (3.09e-04)	Tok/s 233883 (201185)	Loss/tok 3.4145 (3.4616)	LR 2.875e-03
0: TRAIN [1][860/1291]	Time 0.042 (0.053)	Data 7.82e-05 (3.07e-04)	Tok/s 191269 (201135)	Loss/tok 3.0238 (3.4599)	LR 2.875e-03
0: Upscaling, new scale: 2048.0
0: TRAIN [1][870/1291]	Time 0.041 (0.053)	Data 8.06e-05 (3.04e-04)	Tok/s 189459 (201060)	Loss/tok 3.3009 (3.4589)	LR 2.875e-03
0: TRAIN [1][880/1291]	Time 0.042 (0.053)	Data 8.58e-05 (3.02e-04)	Tok/s 188307 (201054)	Loss/tok 3.1816 (3.4594)	LR 2.875e-03
0: TRAIN [1][890/1291]	Time 0.058 (0.053)	Data 8.13e-05 (3.00e-04)	Tok/s 217158 (200959)	Loss/tok 3.3455 (3.4575)	LR 2.875e-03
0: TRAIN [1][900/1291]	Time 0.076 (0.053)	Data 1.08e-04 (2.97e-04)	Tok/s 228967 (201013)	Loss/tok 3.4632 (3.4574)	LR 2.875e-03
0: TRAIN [1][910/1291]	Time 0.059 (0.053)	Data 8.23e-05 (2.95e-04)	Tok/s 209181 (201043)	Loss/tok 3.6462 (3.4569)	LR 2.875e-03
0: TRAIN [1][920/1291]	Time 0.025 (0.053)	Data 7.65e-05 (2.93e-04)	Tok/s 157772 (201062)	Loss/tok 2.7802 (3.4565)	LR 2.875e-03
0: TRAIN [1][930/1291]	Time 0.042 (0.053)	Data 7.72e-05 (2.91e-04)	Tok/s 189713 (200957)	Loss/tok 2.9669 (3.4548)	LR 2.875e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 1024.0
0: TRAIN [1][940/1291]	Time 0.024 (0.053)	Data 7.61e-05 (2.88e-04)	Tok/s 161164 (200857)	Loss/tok 2.7732 (3.4537)	LR 2.875e-03
0: TRAIN [1][950/1291]	Time 0.058 (0.053)	Data 8.61e-05 (2.86e-04)	Tok/s 216630 (200901)	Loss/tok 3.4476 (3.4528)	LR 2.875e-03
0: TRAIN [1][960/1291]	Time 0.042 (0.053)	Data 7.49e-05 (2.84e-04)	Tok/s 187395 (200933)	Loss/tok 3.2799 (3.4525)	LR 2.875e-03
0: TRAIN [1][970/1291]	Time 0.058 (0.053)	Data 7.61e-05 (2.82e-04)	Tok/s 218010 (200879)	Loss/tok 3.3570 (3.4506)	LR 2.875e-03
0: TRAIN [1][980/1291]	Time 0.058 (0.053)	Data 8.46e-05 (2.80e-04)	Tok/s 216803 (201013)	Loss/tok 3.4139 (3.4509)	LR 2.875e-03
0: TRAIN [1][990/1291]	Time 0.042 (0.053)	Data 8.44e-05 (2.78e-04)	Tok/s 184996 (201035)	Loss/tok 3.2510 (3.4493)	LR 2.875e-03
0: TRAIN [1][1000/1291]	Time 0.042 (0.053)	Data 9.06e-05 (2.76e-04)	Tok/s 187054 (201077)	Loss/tok 3.1455 (3.4492)	LR 2.875e-03
0: TRAIN [1][1010/1291]	Time 0.041 (0.053)	Data 1.41e-04 (2.74e-04)	Tok/s 185690 (200997)	Loss/tok 3.0800 (3.4476)	LR 2.875e-03
0: TRAIN [1][1020/1291]	Time 0.076 (0.053)	Data 8.30e-05 (2.72e-04)	Tok/s 228312 (200915)	Loss/tok 3.5802 (3.4473)	LR 2.875e-03
0: TRAIN [1][1030/1291]	Time 0.058 (0.053)	Data 8.08e-05 (2.71e-04)	Tok/s 217309 (200866)	Loss/tok 3.3516 (3.4460)	LR 2.875e-03
0: TRAIN [1][1040/1291]	Time 0.041 (0.053)	Data 1.27e-04 (2.69e-04)	Tok/s 188902 (200938)	Loss/tok 3.0558 (3.4453)	LR 2.875e-03
0: TRAIN [1][1050/1291]	Time 0.058 (0.053)	Data 8.25e-05 (2.67e-04)	Tok/s 212686 (200954)	Loss/tok 3.3295 (3.4445)	LR 2.875e-03
0: TRAIN [1][1060/1291]	Time 0.042 (0.053)	Data 8.06e-05 (2.65e-04)	Tok/s 181485 (200937)	Loss/tok 3.2706 (3.4445)	LR 2.875e-03
0: Upscaling, new scale: 2048.0
0: TRAIN [1][1070/1291]	Time 0.042 (0.053)	Data 8.08e-05 (2.64e-04)	Tok/s 187456 (201053)	Loss/tok 3.1412 (3.4448)	LR 2.875e-03
0: TRAIN [1][1080/1291]	Time 0.024 (0.053)	Data 7.94e-05 (2.62e-04)	Tok/s 163160 (201077)	Loss/tok 2.6938 (3.4440)	LR 2.875e-03
0: TRAIN [1][1090/1291]	Time 0.058 (0.053)	Data 8.08e-05 (2.60e-04)	Tok/s 218487 (201017)	Loss/tok 3.3414 (3.4425)	LR 2.875e-03
0: TRAIN [1][1100/1291]	Time 0.076 (0.053)	Data 8.06e-05 (2.59e-04)	Tok/s 232680 (201111)	Loss/tok 3.4853 (3.4426)	LR 2.875e-03
0: TRAIN [1][1110/1291]	Time 0.042 (0.053)	Data 8.08e-05 (2.57e-04)	Tok/s 184462 (201137)	Loss/tok 3.2149 (3.4421)	LR 2.875e-03
0: TRAIN [1][1120/1291]	Time 0.077 (0.053)	Data 8.03e-05 (2.56e-04)	Tok/s 229616 (201186)	Loss/tok 3.4605 (3.4423)	LR 2.875e-03
0: TRAIN [1][1130/1291]	Time 0.058 (0.053)	Data 8.65e-05 (2.54e-04)	Tok/s 219207 (201172)	Loss/tok 3.3760 (3.4408)	LR 2.875e-03
0: TRAIN [1][1140/1291]	Time 0.041 (0.053)	Data 8.30e-05 (2.53e-04)	Tok/s 186612 (201110)	Loss/tok 3.1994 (3.4396)	LR 2.875e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 1024.0
0: TRAIN [1][1150/1291]	Time 0.041 (0.053)	Data 8.15e-05 (2.51e-04)	Tok/s 184670 (201056)	Loss/tok 3.1810 (3.4383)	LR 2.875e-03
0: TRAIN [1][1160/1291]	Time 0.076 (0.053)	Data 8.18e-05 (2.50e-04)	Tok/s 232864 (201170)	Loss/tok 3.3992 (3.4386)	LR 2.875e-03
0: TRAIN [1][1170/1291]	Time 0.042 (0.053)	Data 8.13e-05 (2.48e-04)	Tok/s 185643 (201189)	Loss/tok 3.0143 (3.4376)	LR 2.875e-03
0: TRAIN [1][1180/1291]	Time 0.042 (0.053)	Data 8.08e-05 (2.47e-04)	Tok/s 188947 (201189)	Loss/tok 3.0779 (3.4374)	LR 2.875e-03
0: TRAIN [1][1190/1291]	Time 0.058 (0.053)	Data 8.11e-05 (2.45e-04)	Tok/s 221636 (201171)	Loss/tok 3.3314 (3.4365)	LR 2.875e-03
0: TRAIN [1][1200/1291]	Time 0.076 (0.053)	Data 7.96e-05 (2.44e-04)	Tok/s 225533 (201255)	Loss/tok 3.7089 (3.4365)	LR 2.875e-03
0: TRAIN [1][1210/1291]	Time 0.025 (0.053)	Data 8.01e-05 (2.43e-04)	Tok/s 159262 (201140)	Loss/tok 2.7478 (3.4349)	LR 2.875e-03
0: TRAIN [1][1220/1291]	Time 0.058 (0.053)	Data 8.06e-05 (2.41e-04)	Tok/s 217246 (201089)	Loss/tok 3.2745 (3.4338)	LR 2.875e-03
0: TRAIN [1][1230/1291]	Time 0.041 (0.053)	Data 8.01e-05 (2.40e-04)	Tok/s 182604 (201103)	Loss/tok 3.2347 (3.4329)	LR 2.875e-03
0: TRAIN [1][1240/1291]	Time 0.058 (0.053)	Data 8.15e-05 (2.39e-04)	Tok/s 217839 (201159)	Loss/tok 3.2843 (3.4324)	LR 2.875e-03
0: TRAIN [1][1250/1291]	Time 0.041 (0.053)	Data 1.33e-04 (2.38e-04)	Tok/s 185569 (201141)	Loss/tok 3.0770 (3.4313)	LR 2.875e-03
0: TRAIN [1][1260/1291]	Time 0.042 (0.053)	Data 8.23e-05 (2.37e-04)	Tok/s 183191 (201097)	Loss/tok 3.0730 (3.4306)	LR 2.875e-03
0: Upscaling, new scale: 2048.0
0: TRAIN [1][1270/1291]	Time 0.097 (0.053)	Data 8.06e-05 (2.36e-04)	Tok/s 228154 (201104)	Loss/tok 3.7158 (3.4308)	LR 2.875e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 1024.0
0: TRAIN [1][1280/1291]	Time 0.025 (0.053)	Data 8.03e-05 (2.34e-04)	Tok/s 160907 (201052)	Loss/tok 2.7172 (3.4301)	LR 2.875e-03
0: TRAIN [1][1290/1291]	Time 0.041 (0.053)	Data 4.39e-05 (2.35e-04)	Tok/s 186800 (201023)	Loss/tok 3.1112 (3.4293)	LR 2.875e-03
:::MLLOG {"namespace": "", "time_ms": 1593114503459, "event_type": "INTERVAL_END", "key": "epoch_stop", "value": null, "metadata": {"file": "train.py", "lineno": 551, "epoch_num": 2}}
:::MLLOG {"namespace": "", "time_ms": 1593114503459, "event_type": "INTERVAL_START", "key": "eval_start", "value": null, "metadata": {"file": "train.py", "lineno": 556, "epoch_num": 2}}
0: Running evaluation on test set
0: TEST [1][0/3]	Time 0.397 (0.397)	Decoder iters 149.0 (149.0)	Tok/s 23291 (23291)
0: Running moses detokenizer
0: BLEU(score=21.7611688468561, counts=[36057, 17333, 9471, 5370], totals=[65954, 62951, 59948, 56949], precisions=[54.6699214604118, 27.534113834569744, 15.798692199906586, 9.429489543275562], bp=1.0, sys_len=65954, ref_len=64676)
0: Finished evaluation on test set
:::MLLOG {"namespace": "", "time_ms": 1593114504839, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.21760000000000002, "metadata": {"file": "train.py", "lineno": 562, "epoch_num": 2}}
:::MLLOG {"namespace": "", "time_ms": 1593114504839, "event_type": "INTERVAL_END", "key": "eval_stop", "value": null, "metadata": {"file": "train.py", "lineno": 565, "epoch_num": 2}}
0: Summary: Epoch: 1	Training Loss: 3.4258	Test BLEU: 21.76
0: Performance: Epoch: 1	Training: 3217305 Tok/s
0: Finished epoch 1
:::MLLOG {"namespace": "", "time_ms": 1593114504839, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "train.py", "lineno": 584, "first_epoch_num": 2}}
:::MLLOG {"namespace": "", "time_ms": 1593114504839, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "train.py", "lineno": 538, "first_epoch_num": 3, "epoch_count": 1}}
:::MLLOG {"namespace": "", "time_ms": 1593114504839, "event_type": "INTERVAL_START", "key": "epoch_start", "value": null, "metadata": {"file": "train.py", "lineno": 541, "epoch_num": 3}}
0: Starting epoch 2
0: Sampler for epoch 2 uses seed 3726883404
0: TRAIN [2][0/1291]	Time 0.291 (0.291)	Data 1.78e-01 (1.78e-01)	Tok/s 26344 (26344)	Loss/tok 3.0800 (3.0800)	LR 2.875e-03
0: TRAIN [2][10/1291]	Time 0.042 (0.072)	Data 8.61e-05 (1.63e-02)	Tok/s 186346 (185064)	Loss/tok 3.1695 (3.2324)	LR 2.875e-03
0: TRAIN [2][20/1291]	Time 0.042 (0.061)	Data 8.65e-05 (8.56e-03)	Tok/s 180912 (190441)	Loss/tok 3.0220 (3.2193)	LR 2.875e-03
0: TRAIN [2][30/1291]	Time 0.097 (0.057)	Data 1.49e-04 (5.84e-03)	Tok/s 229493 (192542)	Loss/tok 3.7026 (3.2452)	LR 2.875e-03
0: TRAIN [2][40/1291]	Time 0.042 (0.057)	Data 1.34e-04 (4.44e-03)	Tok/s 182788 (195894)	Loss/tok 2.9795 (3.2679)	LR 2.875e-03
0: TRAIN [2][50/1291]	Time 0.058 (0.057)	Data 8.30e-05 (3.59e-03)	Tok/s 214078 (197212)	Loss/tok 3.4430 (3.2989)	LR 2.875e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 512.0
0: TRAIN [2][60/1291]	Time 0.076 (0.056)	Data 9.97e-05 (3.01e-03)	Tok/s 229908 (197431)	Loss/tok 3.5699 (3.3132)	LR 2.875e-03
0: TRAIN [2][70/1291]	Time 0.097 (0.056)	Data 8.46e-05 (2.60e-03)	Tok/s 229098 (197689)	Loss/tok 3.6702 (3.3130)	LR 2.875e-03
0: TRAIN [2][80/1291]	Time 0.025 (0.056)	Data 9.75e-05 (2.29e-03)	Tok/s 161623 (198084)	Loss/tok 2.5865 (3.3123)	LR 2.875e-03
0: TRAIN [2][90/1291]	Time 0.076 (0.056)	Data 8.70e-05 (2.05e-03)	Tok/s 230085 (199495)	Loss/tok 3.5412 (3.3157)	LR 2.875e-03
0: TRAIN [2][100/1291]	Time 0.025 (0.056)	Data 9.63e-05 (1.86e-03)	Tok/s 155321 (199333)	Loss/tok 2.6966 (3.3156)	LR 2.875e-03
0: TRAIN [2][110/1291]	Time 0.025 (0.055)	Data 8.27e-05 (1.70e-03)	Tok/s 161246 (199280)	Loss/tok 2.7268 (3.3104)	LR 2.875e-03
0: TRAIN [2][120/1291]	Time 0.059 (0.055)	Data 9.49e-05 (1.57e-03)	Tok/s 210971 (199676)	Loss/tok 3.2295 (3.3018)	LR 2.875e-03
0: TRAIN [2][130/1291]	Time 0.076 (0.054)	Data 9.63e-05 (1.46e-03)	Tok/s 231725 (199496)	Loss/tok 3.3575 (3.3021)	LR 2.875e-03
0: TRAIN [2][140/1291]	Time 0.042 (0.054)	Data 1.02e-04 (1.36e-03)	Tok/s 182331 (199613)	Loss/tok 3.1685 (3.3050)	LR 2.875e-03
0: TRAIN [2][150/1291]	Time 0.058 (0.054)	Data 8.27e-05 (1.28e-03)	Tok/s 218197 (199393)	Loss/tok 3.1795 (3.2976)	LR 2.875e-03
0: TRAIN [2][160/1291]	Time 0.042 (0.053)	Data 8.20e-05 (1.20e-03)	Tok/s 187501 (198959)	Loss/tok 3.0403 (3.2901)	LR 2.875e-03
0: TRAIN [2][170/1291]	Time 0.097 (0.054)	Data 1.20e-04 (1.14e-03)	Tok/s 232904 (199096)	Loss/tok 3.5835 (3.2964)	LR 2.875e-03
0: Upscaling, new scale: 1024.0
0: TRAIN [2][180/1291]	Time 0.076 (0.055)	Data 8.80e-05 (1.08e-03)	Tok/s 228510 (199796)	Loss/tok 3.6055 (3.3058)	LR 2.875e-03
0: TRAIN [2][190/1291]	Time 0.058 (0.054)	Data 8.32e-05 (1.03e-03)	Tok/s 215757 (200081)	Loss/tok 3.1980 (3.3032)	LR 2.875e-03
0: TRAIN [2][200/1291]	Time 0.077 (0.054)	Data 8.42e-05 (9.83e-04)	Tok/s 229929 (200215)	Loss/tok 3.3068 (3.2999)	LR 2.875e-03
0: TRAIN [2][210/1291]	Time 0.041 (0.054)	Data 1.40e-04 (9.41e-04)	Tok/s 190152 (200275)	Loss/tok 3.0808 (3.3003)	LR 2.875e-03
0: TRAIN [2][220/1291]	Time 0.058 (0.054)	Data 8.15e-05 (9.02e-04)	Tok/s 212041 (200333)	Loss/tok 3.3795 (3.2986)	LR 2.875e-03
0: TRAIN [2][230/1291]	Time 0.059 (0.054)	Data 8.34e-05 (8.67e-04)	Tok/s 215724 (200542)	Loss/tok 3.2573 (3.2935)	LR 2.875e-03
0: TRAIN [2][240/1291]	Time 0.041 (0.054)	Data 1.36e-04 (8.35e-04)	Tok/s 185132 (200544)	Loss/tok 2.9856 (3.2969)	LR 2.875e-03
0: TRAIN [2][250/1291]	Time 0.025 (0.054)	Data 8.23e-05 (8.05e-04)	Tok/s 156703 (200474)	Loss/tok 2.5524 (3.2976)	LR 2.875e-03
0: TRAIN [2][260/1291]	Time 0.058 (0.054)	Data 1.42e-04 (7.79e-04)	Tok/s 217908 (200661)	Loss/tok 3.3391 (3.2981)	LR 2.875e-03
0: TRAIN [2][270/1291]	Time 0.042 (0.054)	Data 8.44e-05 (7.53e-04)	Tok/s 181444 (200419)	Loss/tok 3.0478 (3.3000)	LR 2.875e-03
0: TRAIN [2][280/1291]	Time 0.025 (0.054)	Data 8.20e-05 (7.30e-04)	Tok/s 154488 (200400)	Loss/tok 2.5573 (3.2984)	LR 2.875e-03
0: TRAIN [2][290/1291]	Time 0.059 (0.054)	Data 8.42e-05 (7.08e-04)	Tok/s 212722 (200595)	Loss/tok 3.3262 (3.2968)	LR 2.875e-03
0: TRAIN [2][300/1291]	Time 0.042 (0.054)	Data 8.96e-05 (6.88e-04)	Tok/s 187011 (200561)	Loss/tok 3.1149 (3.2966)	LR 2.875e-03
0: Upscaling, new scale: 2048.0
0: TRAIN [2][310/1291]	Time 0.042 (0.054)	Data 8.42e-05 (6.69e-04)	Tok/s 180626 (200677)	Loss/tok 3.1435 (3.2983)	LR 2.875e-03
0: TRAIN [2][320/1291]	Time 0.058 (0.054)	Data 8.11e-05 (6.51e-04)	Tok/s 218637 (200553)	Loss/tok 3.2332 (3.2974)	LR 2.875e-03
0: TRAIN [2][330/1291]	Time 0.058 (0.054)	Data 1.38e-04 (6.34e-04)	Tok/s 213914 (200415)	Loss/tok 3.2847 (3.2965)	LR 2.875e-03
0: TRAIN [2][340/1291]	Time 0.059 (0.054)	Data 8.46e-05 (6.18e-04)	Tok/s 213239 (200711)	Loss/tok 3.3816 (3.2968)	LR 2.875e-03
0: TRAIN [2][350/1291]	Time 0.042 (0.054)	Data 8.44e-05 (6.03e-04)	Tok/s 182957 (200989)	Loss/tok 2.9965 (3.2972)	LR 2.875e-03
0: TRAIN [2][360/1291]	Time 0.041 (0.054)	Data 1.40e-04 (5.89e-04)	Tok/s 190569 (201154)	Loss/tok 3.0328 (3.2987)	LR 2.875e-03
0: TRAIN [2][370/1291]	Time 0.025 (0.054)	Data 1.37e-04 (5.76e-04)	Tok/s 151982 (201014)	Loss/tok 2.4978 (3.2956)	LR 2.875e-03
0: TRAIN [2][380/1291]	Time 0.042 (0.054)	Data 1.41e-04 (5.64e-04)	Tok/s 186549 (200983)	Loss/tok 3.0211 (3.2952)	LR 2.875e-03
0: TRAIN [2][390/1291]	Time 0.042 (0.054)	Data 1.36e-04 (5.52e-04)	Tok/s 183601 (201028)	Loss/tok 3.2256 (3.2960)	LR 2.875e-03
0: TRAIN [2][400/1291]	Time 0.058 (0.054)	Data 8.63e-05 (5.40e-04)	Tok/s 221375 (201227)	Loss/tok 3.2022 (3.2986)	LR 2.875e-03
0: TRAIN [2][410/1291]	Time 0.041 (0.054)	Data 1.36e-04 (5.30e-04)	Tok/s 189984 (201276)	Loss/tok 3.1140 (3.2988)	LR 2.875e-03
0: TRAIN [2][420/1291]	Time 0.041 (0.053)	Data 1.23e-04 (5.19e-04)	Tok/s 184249 (200812)	Loss/tok 3.0080 (3.2947)	LR 2.875e-03
0: TRAIN [2][430/1291]	Time 0.041 (0.053)	Data 8.82e-05 (5.09e-04)	Tok/s 189285 (200867)	Loss/tok 3.1218 (3.2934)	LR 2.875e-03
0: Upscaling, new scale: 4096.0
0: TRAIN [2][440/1291]	Time 0.058 (0.053)	Data 8.03e-05 (5.00e-04)	Tok/s 216902 (200819)	Loss/tok 3.4559 (3.2917)	LR 2.875e-03
0: TRAIN [2][450/1291]	Time 0.076 (0.053)	Data 8.13e-05 (4.91e-04)	Tok/s 231656 (200772)	Loss/tok 3.4888 (3.2917)	LR 2.875e-03
0: TRAIN [2][460/1291]	Time 0.058 (0.053)	Data 9.20e-05 (4.82e-04)	Tok/s 219017 (200753)	Loss/tok 3.3102 (3.2912)	LR 2.875e-03
0: TRAIN [2][470/1291]	Time 0.042 (0.053)	Data 8.51e-05 (4.74e-04)	Tok/s 185641 (200703)	Loss/tok 2.9912 (3.2917)	LR 2.875e-03
0: TRAIN [2][480/1291]	Time 0.042 (0.053)	Data 8.15e-05 (4.67e-04)	Tok/s 185028 (200506)	Loss/tok 3.1214 (3.2894)	LR 2.875e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 2048.0
0: TRAIN [2][490/1291]	Time 0.041 (0.053)	Data 8.89e-05 (4.59e-04)	Tok/s 185595 (200493)	Loss/tok 3.0886 (3.2900)	LR 2.875e-03
0: TRAIN [2][500/1291]	Time 0.042 (0.053)	Data 8.37e-05 (4.52e-04)	Tok/s 190872 (200573)	Loss/tok 3.0795 (3.2913)	LR 2.875e-03
0: TRAIN [2][510/1291]	Time 0.041 (0.053)	Data 1.01e-04 (4.45e-04)	Tok/s 192450 (200625)	Loss/tok 3.1972 (3.2906)	LR 2.875e-03
0: TRAIN [2][520/1291]	Time 0.025 (0.053)	Data 8.87e-05 (4.38e-04)	Tok/s 155482 (200357)	Loss/tok 2.5613 (3.2882)	LR 2.875e-03
0: TRAIN [2][530/1291]	Time 0.076 (0.053)	Data 1.36e-04 (4.32e-04)	Tok/s 232615 (200463)	Loss/tok 3.4234 (3.2884)	LR 2.875e-03
0: TRAIN [2][540/1291]	Time 0.076 (0.053)	Data 1.40e-04 (4.25e-04)	Tok/s 229069 (200451)	Loss/tok 3.3728 (3.2893)	LR 2.875e-03
0: TRAIN [2][550/1291]	Time 0.076 (0.053)	Data 8.25e-05 (4.19e-04)	Tok/s 230242 (200477)	Loss/tok 3.4976 (3.2895)	LR 2.875e-03
0: TRAIN [2][560/1291]	Time 0.076 (0.053)	Data 8.11e-05 (4.13e-04)	Tok/s 230403 (200650)	Loss/tok 3.3756 (3.2912)	LR 2.875e-03
0: TRAIN [2][570/1291]	Time 0.025 (0.053)	Data 8.06e-05 (4.08e-04)	Tok/s 156793 (200467)	Loss/tok 2.5675 (3.2901)	LR 2.875e-03
0: TRAIN [2][580/1291]	Time 0.076 (0.053)	Data 8.63e-05 (4.02e-04)	Tok/s 233695 (200512)	Loss/tok 3.3629 (3.2901)	LR 2.875e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 1024.0
0: TRAIN [2][590/1291]	Time 0.042 (0.053)	Data 8.23e-05 (3.97e-04)	Tok/s 183353 (200532)	Loss/tok 3.0641 (3.2907)	LR 2.875e-03
0: TRAIN [2][600/1291]	Time 0.076 (0.053)	Data 8.32e-05 (3.92e-04)	Tok/s 228145 (200601)	Loss/tok 3.5747 (3.2911)	LR 2.875e-03
0: TRAIN [2][610/1291]	Time 0.058 (0.053)	Data 8.54e-05 (3.87e-04)	Tok/s 210356 (200722)	Loss/tok 3.2715 (3.2927)	LR 2.875e-03
0: TRAIN [2][620/1291]	Time 0.025 (0.053)	Data 1.05e-04 (3.82e-04)	Tok/s 157054 (200697)	Loss/tok 2.6120 (3.2929)	LR 2.875e-03
0: TRAIN [2][630/1291]	Time 0.041 (0.053)	Data 8.27e-05 (3.78e-04)	Tok/s 186747 (200588)	Loss/tok 3.0034 (3.2925)	LR 2.875e-03
0: TRAIN [2][640/1291]	Time 0.058 (0.053)	Data 1.38e-04 (3.73e-04)	Tok/s 217169 (200642)	Loss/tok 3.3257 (3.2928)	LR 2.875e-03
0: TRAIN [2][650/1291]	Time 0.041 (0.053)	Data 8.56e-05 (3.69e-04)	Tok/s 191577 (200612)	Loss/tok 3.0843 (3.2920)	LR 2.875e-03
0: TRAIN [2][660/1291]	Time 0.058 (0.053)	Data 9.23e-05 (3.65e-04)	Tok/s 220532 (200484)	Loss/tok 3.3599 (3.2908)	LR 2.875e-03
0: TRAIN [2][670/1291]	Time 0.058 (0.053)	Data 1.39e-04 (3.61e-04)	Tok/s 216033 (200441)	Loss/tok 3.1908 (3.2892)	LR 2.875e-03
0: TRAIN [2][680/1291]	Time 0.025 (0.053)	Data 8.49e-05 (3.57e-04)	Tok/s 160808 (200357)	Loss/tok 2.6621 (3.2880)	LR 2.875e-03
0: TRAIN [2][690/1291]	Time 0.041 (0.053)	Data 1.37e-04 (3.53e-04)	Tok/s 189618 (200278)	Loss/tok 3.1520 (3.2863)	LR 2.875e-03
0: TRAIN [2][700/1291]	Time 0.058 (0.053)	Data 1.34e-04 (3.50e-04)	Tok/s 214139 (200444)	Loss/tok 3.2892 (3.2882)	LR 2.875e-03
0: TRAIN [2][710/1291]	Time 0.025 (0.053)	Data 8.25e-05 (3.46e-04)	Tok/s 161146 (200372)	Loss/tok 2.7793 (3.2876)	LR 2.875e-03
0: Upscaling, new scale: 2048.0
0: TRAIN [2][720/1291]	Time 0.077 (0.053)	Data 9.94e-05 (3.43e-04)	Tok/s 229926 (200400)	Loss/tok 3.3484 (3.2869)	LR 2.875e-03
0: TRAIN [2][730/1291]	Time 0.058 (0.053)	Data 1.04e-04 (3.39e-04)	Tok/s 213033 (200374)	Loss/tok 3.2688 (3.2867)	LR 2.875e-03
0: TRAIN [2][740/1291]	Time 0.076 (0.053)	Data 7.96e-05 (3.36e-04)	Tok/s 229118 (200348)	Loss/tok 3.4754 (3.2865)	LR 2.875e-03
0: TRAIN [2][750/1291]	Time 0.058 (0.053)	Data 8.32e-05 (3.33e-04)	Tok/s 217818 (200292)	Loss/tok 3.2126 (3.2849)	LR 2.875e-03
0: TRAIN [2][760/1291]	Time 0.041 (0.052)	Data 8.44e-05 (3.29e-04)	Tok/s 183218 (200099)	Loss/tok 3.0830 (3.2829)	LR 2.875e-03
0: TRAIN [2][770/1291]	Time 0.097 (0.052)	Data 8.20e-05 (3.26e-04)	Tok/s 231633 (200151)	Loss/tok 3.6787 (3.2833)	LR 2.875e-03
0: TRAIN [2][780/1291]	Time 0.058 (0.052)	Data 8.37e-05 (3.23e-04)	Tok/s 215961 (200175)	Loss/tok 3.3596 (3.2831)	LR 2.875e-03
0: TRAIN [2][790/1291]	Time 0.041 (0.052)	Data 8.15e-05 (3.20e-04)	Tok/s 184815 (200176)	Loss/tok 2.9347 (3.2822)	LR 2.875e-03
0: TRAIN [2][800/1291]	Time 0.097 (0.053)	Data 9.30e-05 (3.17e-04)	Tok/s 231491 (200316)	Loss/tok 3.7013 (3.2850)	LR 2.875e-03
0: TRAIN [2][810/1291]	Time 0.059 (0.053)	Data 9.85e-05 (3.14e-04)	Tok/s 214727 (200368)	Loss/tok 3.2225 (3.2861)	LR 2.875e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 1024.0
0: TRAIN [2][820/1291]	Time 0.076 (0.053)	Data 8.32e-05 (3.12e-04)	Tok/s 229838 (200466)	Loss/tok 3.4038 (3.2884)	LR 2.875e-03
0: TRAIN [2][830/1291]	Time 0.058 (0.053)	Data 8.68e-05 (3.09e-04)	Tok/s 216092 (200473)	Loss/tok 3.3819 (3.2882)	LR 2.875e-03
0: TRAIN [2][840/1291]	Time 0.076 (0.053)	Data 8.34e-05 (3.07e-04)	Tok/s 232824 (200519)	Loss/tok 3.3255 (3.2881)	LR 2.875e-03
0: TRAIN [2][850/1291]	Time 0.097 (0.053)	Data 8.61e-05 (3.04e-04)	Tok/s 231609 (200446)	Loss/tok 3.5203 (3.2878)	LR 2.875e-03
0: TRAIN [2][860/1291]	Time 0.058 (0.053)	Data 7.94e-05 (3.02e-04)	Tok/s 218860 (200416)	Loss/tok 3.3113 (3.2872)	LR 2.875e-03
0: TRAIN [2][870/1291]	Time 0.097 (0.053)	Data 8.30e-05 (2.99e-04)	Tok/s 230324 (200510)	Loss/tok 3.7402 (3.2891)	LR 2.875e-03
0: TRAIN [2][880/1291]	Time 0.041 (0.053)	Data 8.44e-05 (2.97e-04)	Tok/s 185709 (200400)	Loss/tok 3.2158 (3.2876)	LR 2.875e-03
0: TRAIN [2][890/1291]	Time 0.076 (0.053)	Data 8.25e-05 (2.95e-04)	Tok/s 233550 (200471)	Loss/tok 3.2924 (3.2890)	LR 2.875e-03
0: TRAIN [2][900/1291]	Time 0.025 (0.053)	Data 8.03e-05 (2.92e-04)	Tok/s 159144 (200480)	Loss/tok 2.6354 (3.2890)	LR 2.875e-03
0: TRAIN [2][910/1291]	Time 0.058 (0.053)	Data 8.18e-05 (2.90e-04)	Tok/s 220717 (200589)	Loss/tok 3.1600 (3.2887)	LR 2.875e-03
0: TRAIN [2][920/1291]	Time 0.058 (0.053)	Data 9.54e-05 (2.88e-04)	Tok/s 214636 (200675)	Loss/tok 3.3355 (3.2880)	LR 2.875e-03
0: TRAIN [2][930/1291]	Time 0.041 (0.053)	Data 8.44e-05 (2.86e-04)	Tok/s 188374 (200658)	Loss/tok 3.0421 (3.2871)	LR 2.875e-03
0: TRAIN [2][940/1291]	Time 0.058 (0.053)	Data 1.36e-04 (2.84e-04)	Tok/s 217142 (200701)	Loss/tok 3.2975 (3.2874)	LR 2.875e-03
0: Upscaling, new scale: 2048.0
0: TRAIN [2][950/1291]	Time 0.025 (0.053)	Data 1.21e-04 (2.82e-04)	Tok/s 157266 (200801)	Loss/tok 2.6439 (3.2891)	LR 2.875e-03
0: TRAIN [2][960/1291]	Time 0.076 (0.053)	Data 8.58e-05 (2.80e-04)	Tok/s 227931 (200770)	Loss/tok 3.5062 (3.2888)	LR 2.875e-03
0: TRAIN [2][970/1291]	Time 0.042 (0.053)	Data 8.70e-05 (2.78e-04)	Tok/s 188381 (200851)	Loss/tok 3.0592 (3.2889)	LR 2.875e-03
0: TRAIN [2][980/1291]	Time 0.059 (0.053)	Data 9.92e-05 (2.76e-04)	Tok/s 211217 (200827)	Loss/tok 3.1929 (3.2881)	LR 2.875e-03
0: TRAIN [2][990/1291]	Time 0.041 (0.053)	Data 9.42e-05 (2.74e-04)	Tok/s 186909 (200848)	Loss/tok 3.1071 (3.2873)	LR 2.875e-03
0: TRAIN [2][1000/1291]	Time 0.059 (0.053)	Data 8.34e-05 (2.72e-04)	Tok/s 210306 (200803)	Loss/tok 3.3329 (3.2871)	LR 2.875e-03
0: TRAIN [2][1010/1291]	Time 0.076 (0.053)	Data 8.51e-05 (2.71e-04)	Tok/s 230094 (200745)	Loss/tok 3.4744 (3.2870)	LR 2.875e-03
0: TRAIN [2][1020/1291]	Time 0.042 (0.053)	Data 8.87e-05 (2.69e-04)	Tok/s 182493 (200674)	Loss/tok 3.1285 (3.2855)	LR 2.875e-03
0: TRAIN [2][1030/1291]	Time 0.097 (0.053)	Data 7.99e-05 (2.67e-04)	Tok/s 230221 (200698)	Loss/tok 3.6153 (3.2859)	LR 2.875e-03
0: TRAIN [2][1040/1291]	Time 0.076 (0.053)	Data 8.44e-05 (2.66e-04)	Tok/s 232646 (200692)	Loss/tok 3.4497 (3.2864)	LR 2.875e-03
0: TRAIN [2][1050/1291]	Time 0.059 (0.053)	Data 8.15e-05 (2.64e-04)	Tok/s 216331 (200848)	Loss/tok 3.3296 (3.2876)	LR 2.875e-03
0: TRAIN [2][1060/1291]	Time 0.042 (0.053)	Data 7.80e-05 (2.62e-04)	Tok/s 183790 (200853)	Loss/tok 3.1118 (3.2875)	LR 2.875e-03
0: TRAIN [2][1070/1291]	Time 0.058 (0.053)	Data 7.84e-05 (2.60e-04)	Tok/s 217409 (200736)	Loss/tok 3.1812 (3.2870)	LR 2.875e-03
0: Upscaling, new scale: 4096.0
0: TRAIN [2][1080/1291]	Time 0.042 (0.053)	Data 7.70e-05 (2.59e-04)	Tok/s 183832 (200770)	Loss/tok 3.0165 (3.2872)	LR 2.875e-03
0: TRAIN [2][1090/1291]	Time 0.058 (0.053)	Data 7.84e-05 (2.57e-04)	Tok/s 212885 (200789)	Loss/tok 3.2574 (3.2865)	LR 2.875e-03
0: TRAIN [2][1100/1291]	Time 0.059 (0.053)	Data 7.87e-05 (2.56e-04)	Tok/s 217611 (200793)	Loss/tok 3.3061 (3.2855)	LR 2.875e-03
0: TRAIN [2][1110/1291]	Time 0.041 (0.053)	Data 7.72e-05 (2.54e-04)	Tok/s 188801 (200818)	Loss/tok 3.0371 (3.2851)	LR 2.875e-03
0: TRAIN [2][1120/1291]	Time 0.058 (0.053)	Data 7.70e-05 (2.52e-04)	Tok/s 214275 (200712)	Loss/tok 3.2862 (3.2839)	LR 2.875e-03
0: TRAIN [2][1130/1291]	Time 0.041 (0.053)	Data 7.80e-05 (2.51e-04)	Tok/s 186338 (200673)	Loss/tok 2.9635 (3.2831)	LR 2.875e-03
0: TRAIN [2][1140/1291]	Time 0.041 (0.053)	Data 7.84e-05 (2.49e-04)	Tok/s 187608 (200653)	Loss/tok 3.0445 (3.2818)	LR 2.875e-03
0: TRAIN [2][1150/1291]	Time 0.058 (0.053)	Data 7.80e-05 (2.48e-04)	Tok/s 214617 (200624)	Loss/tok 3.2636 (3.2818)	LR 2.875e-03
0: TRAIN [2][1160/1291]	Time 0.098 (0.053)	Data 7.68e-05 (2.46e-04)	Tok/s 228077 (200580)	Loss/tok 3.6466 (3.2820)	LR 2.875e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 2048.0
0: TRAIN [2][1170/1291]	Time 0.042 (0.053)	Data 7.87e-05 (2.45e-04)	Tok/s 186131 (200615)	Loss/tok 3.1098 (3.2825)	LR 2.875e-03
0: TRAIN [2][1180/1291]	Time 0.059 (0.053)	Data 7.82e-05 (2.44e-04)	Tok/s 214892 (200741)	Loss/tok 3.1909 (3.2829)	LR 2.875e-03
0: TRAIN [2][1190/1291]	Time 0.041 (0.053)	Data 7.65e-05 (2.42e-04)	Tok/s 182733 (200689)	Loss/tok 2.9991 (3.2827)	LR 2.875e-03
0: TRAIN [2][1200/1291]	Time 0.076 (0.053)	Data 7.65e-05 (2.41e-04)	Tok/s 227753 (200842)	Loss/tok 3.3423 (3.2828)	LR 2.875e-03
0: TRAIN [2][1210/1291]	Time 0.077 (0.053)	Data 7.70e-05 (2.39e-04)	Tok/s 227122 (200908)	Loss/tok 3.4586 (3.2834)	LR 2.875e-03
0: TRAIN [2][1220/1291]	Time 0.076 (0.053)	Data 7.77e-05 (2.38e-04)	Tok/s 228954 (200804)	Loss/tok 3.4566 (3.2826)	LR 2.875e-03
0: TRAIN [2][1230/1291]	Time 0.025 (0.053)	Data 7.63e-05 (2.37e-04)	Tok/s 157932 (200808)	Loss/tok 2.7192 (3.2826)	LR 2.875e-03
0: TRAIN [2][1240/1291]	Time 0.077 (0.053)	Data 7.77e-05 (2.35e-04)	Tok/s 231535 (200779)	Loss/tok 3.3638 (3.2821)	LR 2.875e-03
0: TRAIN [2][1250/1291]	Time 0.058 (0.053)	Data 7.65e-05 (2.34e-04)	Tok/s 216510 (200813)	Loss/tok 3.1773 (3.2820)	LR 2.875e-03
0: TRAIN [2][1260/1291]	Time 0.097 (0.053)	Data 7.89e-05 (2.33e-04)	Tok/s 230117 (200846)	Loss/tok 3.6031 (3.2822)	LR 2.875e-03
0: TRAIN [2][1270/1291]	Time 0.041 (0.053)	Data 7.70e-05 (2.32e-04)	Tok/s 185342 (200838)	Loss/tok 2.9906 (3.2814)	LR 2.875e-03
0: TRAIN [2][1280/1291]	Time 0.076 (0.053)	Data 7.80e-05 (2.31e-04)	Tok/s 227588 (200908)	Loss/tok 3.4802 (3.2816)	LR 2.875e-03
0: TRAIN [2][1290/1291]	Time 0.076 (0.053)	Data 4.36e-05 (2.31e-04)	Tok/s 231894 (200938)	Loss/tok 3.5441 (3.2817)	LR 2.875e-03
:::MLLOG {"namespace": "", "time_ms": 1593114573181, "event_type": "INTERVAL_END", "key": "epoch_stop", "value": null, "metadata": {"file": "train.py", "lineno": 551, "epoch_num": 3}}
:::MLLOG {"namespace": "", "time_ms": 1593114573181, "event_type": "INTERVAL_START", "key": "eval_start", "value": null, "metadata": {"file": "train.py", "lineno": 556, "epoch_num": 3}}
0: Running evaluation on test set
0: TEST [2][0/3]	Time 0.394 (0.394)	Decoder iters 149.0 (149.0)	Tok/s 23039 (23039)
0: Running moses detokenizer
0: BLEU(score=22.53700001775114, counts=[36526, 17779, 9889, 5737], totals=[66069, 63066, 60063, 57064], precisions=[55.28462667816979, 28.191101385849745, 16.46437906864459, 10.053624001121548], bp=1.0, sys_len=66069, ref_len=64676)
0: Finished evaluation on test set
:::MLLOG {"namespace": "", "time_ms": 1593114574635, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.2254, "metadata": {"file": "train.py", "lineno": 562, "epoch_num": 3}}
:::MLLOG {"namespace": "", "time_ms": 1593114574635, "event_type": "INTERVAL_END", "key": "eval_stop", "value": null, "metadata": {"file": "train.py", "lineno": 565, "epoch_num": 3}}
0: Summary: Epoch: 2	Training Loss: 3.2811	Test BLEU: 22.54
0: Performance: Epoch: 2	Training: 3216250 Tok/s
0: Finished epoch 2
:::MLLOG {"namespace": "", "time_ms": 1593114574636, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "train.py", "lineno": 584, "first_epoch_num": 3}}
:::MLLOG {"namespace": "", "time_ms": 1593114574636, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "train.py", "lineno": 538, "first_epoch_num": 4, "epoch_count": 1}}
:::MLLOG {"namespace": "", "time_ms": 1593114574636, "event_type": "INTERVAL_START", "key": "epoch_start", "value": null, "metadata": {"file": "train.py", "lineno": 541, "epoch_num": 4}}
0: Starting epoch 3
0: Sampler for epoch 3 uses seed 3071763742
0: TRAIN [3][0/1291]	Time 0.280 (0.280)	Data 2.06e-01 (2.06e-01)	Tok/s 27390 (27390)	Loss/tok 3.0100 (3.0100)	LR 2.875e-03
0: Upscaling, new scale: 4096.0
0: TRAIN [3][10/1291]	Time 0.042 (0.068)	Data 8.96e-05 (1.88e-02)	Tok/s 183763 (180421)	Loss/tok 2.9741 (3.1013)	LR 2.875e-03
0: TRAIN [3][20/1291]	Time 0.025 (0.059)	Data 9.04e-05 (9.88e-03)	Tok/s 164420 (188078)	Loss/tok 2.7013 (3.1191)	LR 2.875e-03
0: TRAIN [3][30/1291]	Time 0.076 (0.057)	Data 1.65e-04 (6.72e-03)	Tok/s 231135 (193603)	Loss/tok 3.2671 (3.1385)	LR 2.875e-03
0: TRAIN [3][40/1291]	Time 0.041 (0.055)	Data 7.99e-05 (5.10e-03)	Tok/s 192752 (194808)	Loss/tok 3.0066 (3.1374)	LR 2.875e-03
0: TRAIN [3][50/1291]	Time 0.042 (0.053)	Data 8.87e-05 (4.12e-03)	Tok/s 183636 (194199)	Loss/tok 3.0799 (3.1262)	LR 2.875e-03
0: TRAIN [3][60/1291]	Time 0.076 (0.052)	Data 9.99e-05 (3.46e-03)	Tok/s 231401 (194463)	Loss/tok 3.3038 (3.1348)	LR 2.875e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 2048.0
0: TRAIN [3][70/1291]	Time 0.097 (0.052)	Data 1.52e-04 (2.99e-03)	Tok/s 232094 (194187)	Loss/tok 3.3755 (3.1344)	LR 2.875e-03
0: TRAIN [3][80/1291]	Time 0.058 (0.052)	Data 1.11e-04 (2.63e-03)	Tok/s 211203 (195556)	Loss/tok 3.2482 (3.1415)	LR 2.875e-03
0: TRAIN [3][90/1291]	Time 0.076 (0.052)	Data 8.85e-05 (2.36e-03)	Tok/s 231127 (195709)	Loss/tok 3.3292 (3.1433)	LR 2.875e-03
0: TRAIN [3][100/1291]	Time 0.076 (0.053)	Data 9.23e-05 (2.13e-03)	Tok/s 227952 (196830)	Loss/tok 3.3270 (3.1474)	LR 2.875e-03
0: TRAIN [3][110/1291]	Time 0.097 (0.053)	Data 8.44e-05 (1.95e-03)	Tok/s 232524 (196924)	Loss/tok 3.5369 (3.1509)	LR 2.875e-03
0: TRAIN [3][120/1291]	Time 0.076 (0.052)	Data 8.25e-05 (1.80e-03)	Tok/s 229125 (196557)	Loss/tok 3.4837 (3.1486)	LR 2.875e-03
0: TRAIN [3][130/1291]	Time 0.058 (0.051)	Data 8.54e-05 (1.67e-03)	Tok/s 214241 (195850)	Loss/tok 3.2125 (3.1449)	LR 2.875e-03
0: TRAIN [3][140/1291]	Time 0.041 (0.052)	Data 8.13e-05 (1.55e-03)	Tok/s 187763 (196461)	Loss/tok 3.0914 (3.1483)	LR 2.875e-03
0: TRAIN [3][150/1291]	Time 0.058 (0.053)	Data 1.37e-04 (1.46e-03)	Tok/s 209538 (197844)	Loss/tok 3.1715 (3.1670)	LR 2.875e-03
0: TRAIN [3][160/1291]	Time 0.058 (0.053)	Data 1.36e-04 (1.37e-03)	Tok/s 214727 (198010)	Loss/tok 3.2726 (3.1712)	LR 2.875e-03
0: TRAIN [3][170/1291]	Time 0.041 (0.053)	Data 9.61e-05 (1.30e-03)	Tok/s 188185 (198318)	Loss/tok 3.0088 (3.1717)	LR 2.875e-03
0: TRAIN [3][180/1291]	Time 0.041 (0.052)	Data 8.46e-05 (1.23e-03)	Tok/s 187618 (197670)	Loss/tok 3.1699 (3.1658)	LR 2.875e-03
0: TRAIN [3][190/1291]	Time 0.077 (0.053)	Data 8.11e-05 (1.17e-03)	Tok/s 224693 (198242)	Loss/tok 3.3903 (3.1728)	LR 2.875e-03
0: Upscaling, new scale: 4096.0
0: TRAIN [3][200/1291]	Time 0.059 (0.053)	Data 8.54e-05 (1.12e-03)	Tok/s 212766 (198774)	Loss/tok 3.2888 (3.1750)	LR 2.875e-03
0: TRAIN [3][210/1291]	Time 0.058 (0.053)	Data 1.37e-04 (1.07e-03)	Tok/s 215620 (198894)	Loss/tok 3.2108 (3.1739)	LR 1.437e-03
0: TRAIN [3][220/1291]	Time 0.059 (0.053)	Data 8.03e-05 (1.03e-03)	Tok/s 213139 (199053)	Loss/tok 3.0900 (3.1697)	LR 1.437e-03
0: TRAIN [3][230/1291]	Time 0.041 (0.052)	Data 8.96e-05 (9.86e-04)	Tok/s 187962 (198696)	Loss/tok 3.1240 (3.1673)	LR 1.437e-03
0: TRAIN [3][240/1291]	Time 0.041 (0.052)	Data 8.15e-05 (9.49e-04)	Tok/s 186385 (198697)	Loss/tok 3.0670 (3.1679)	LR 1.437e-03
0: TRAIN [3][250/1291]	Time 0.041 (0.052)	Data 7.87e-05 (9.15e-04)	Tok/s 183886 (198777)	Loss/tok 2.9392 (3.1689)	LR 1.437e-03
0: TRAIN [3][260/1291]	Time 0.076 (0.053)	Data 1.64e-04 (8.84e-04)	Tok/s 226599 (199230)	Loss/tok 3.4878 (3.1728)	LR 1.437e-03
0: TRAIN [3][270/1291]	Time 0.042 (0.053)	Data 8.34e-05 (8.55e-04)	Tok/s 181180 (199113)	Loss/tok 3.0153 (3.1697)	LR 1.437e-03
0: TRAIN [3][280/1291]	Time 0.077 (0.053)	Data 8.65e-05 (8.28e-04)	Tok/s 226331 (199361)	Loss/tok 3.3710 (3.1716)	LR 1.437e-03
0: TRAIN [3][290/1291]	Time 0.041 (0.052)	Data 1.38e-04 (8.03e-04)	Tok/s 189677 (199093)	Loss/tok 3.0211 (3.1676)	LR 1.437e-03
0: TRAIN [3][300/1291]	Time 0.059 (0.053)	Data 9.30e-05 (7.79e-04)	Tok/s 214271 (199291)	Loss/tok 3.1626 (3.1703)	LR 1.437e-03
0: TRAIN [3][310/1291]	Time 0.058 (0.053)	Data 8.03e-05 (7.57e-04)	Tok/s 215730 (199711)	Loss/tok 3.2395 (3.1727)	LR 1.437e-03
0: TRAIN [3][320/1291]	Time 0.059 (0.053)	Data 1.36e-04 (7.37e-04)	Tok/s 214360 (200043)	Loss/tok 3.1143 (3.1749)	LR 1.437e-03
0: Upscaling, new scale: 8192.0
0: TRAIN [3][330/1291]	Time 0.059 (0.053)	Data 8.01e-05 (7.17e-04)	Tok/s 214408 (200220)	Loss/tok 3.2174 (3.1751)	LR 1.437e-03
0: TRAIN [3][340/1291]	Time 0.041 (0.053)	Data 1.03e-04 (6.99e-04)	Tok/s 192902 (200293)	Loss/tok 2.9940 (3.1761)	LR 1.437e-03
0: TRAIN [3][350/1291]	Time 0.076 (0.054)	Data 1.34e-04 (6.82e-04)	Tok/s 233101 (200671)	Loss/tok 3.3032 (3.1798)	LR 1.437e-03
0: TRAIN [3][360/1291]	Time 0.042 (0.054)	Data 1.52e-04 (6.65e-04)	Tok/s 184484 (200841)	Loss/tok 2.8852 (3.1801)	LR 1.437e-03
0: TRAIN [3][370/1291]	Time 0.041 (0.053)	Data 8.01e-05 (6.50e-04)	Tok/s 188380 (200685)	Loss/tok 3.0098 (3.1769)	LR 1.437e-03
0: TRAIN [3][380/1291]	Time 0.041 (0.053)	Data 9.68e-05 (6.35e-04)	Tok/s 190627 (200388)	Loss/tok 2.8823 (3.1746)	LR 1.437e-03
0: TRAIN [3][390/1291]	Time 0.058 (0.053)	Data 8.06e-05 (6.21e-04)	Tok/s 217100 (200386)	Loss/tok 3.1491 (3.1734)	LR 1.437e-03
0: TRAIN [3][400/1291]	Time 0.041 (0.053)	Data 7.89e-05 (6.08e-04)	Tok/s 190934 (200099)	Loss/tok 2.9602 (3.1697)	LR 1.437e-03
0: TRAIN [3][410/1291]	Time 0.042 (0.053)	Data 8.03e-05 (5.96e-04)	Tok/s 185122 (199935)	Loss/tok 3.0688 (3.1676)	LR 1.437e-03
0: TRAIN [3][420/1291]	Time 0.059 (0.053)	Data 8.01e-05 (5.84e-04)	Tok/s 213967 (199970)	Loss/tok 3.1610 (3.1656)	LR 1.437e-03
0: TRAIN [3][430/1291]	Time 0.042 (0.053)	Data 8.11e-05 (5.72e-04)	Tok/s 183978 (199905)	Loss/tok 3.0819 (3.1650)	LR 1.437e-03
0: TRAIN [3][440/1291]	Time 0.025 (0.053)	Data 8.80e-05 (5.61e-04)	Tok/s 159581 (199727)	Loss/tok 2.4611 (3.1636)	LR 1.437e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 4096.0
0: TRAIN [3][450/1291]	Time 0.041 (0.053)	Data 7.75e-05 (5.51e-04)	Tok/s 191417 (199798)	Loss/tok 3.0053 (3.1647)	LR 1.437e-03
0: TRAIN [3][460/1291]	Time 0.042 (0.052)	Data 8.23e-05 (5.41e-04)	Tok/s 187339 (199459)	Loss/tok 3.2609 (3.1626)	LR 1.437e-03
0: TRAIN [3][470/1291]	Time 0.041 (0.052)	Data 1.40e-04 (5.32e-04)	Tok/s 187748 (199339)	Loss/tok 2.9401 (3.1608)	LR 1.437e-03
0: TRAIN [3][480/1291]	Time 0.097 (0.052)	Data 7.99e-05 (5.22e-04)	Tok/s 229880 (199267)	Loss/tok 3.5279 (3.1602)	LR 1.437e-03
0: TRAIN [3][490/1291]	Time 0.058 (0.052)	Data 9.63e-05 (5.14e-04)	Tok/s 219004 (199570)	Loss/tok 3.2457 (3.1631)	LR 1.437e-03
0: TRAIN [3][500/1291]	Time 0.025 (0.052)	Data 8.34e-05 (5.05e-04)	Tok/s 160649 (199328)	Loss/tok 2.6748 (3.1615)	LR 1.437e-03
0: TRAIN [3][510/1291]	Time 0.059 (0.052)	Data 8.46e-05 (4.97e-04)	Tok/s 219552 (199395)	Loss/tok 3.1080 (3.1619)	LR 1.437e-03
0: TRAIN [3][520/1291]	Time 0.041 (0.052)	Data 8.30e-05 (4.90e-04)	Tok/s 187785 (199382)	Loss/tok 2.9183 (3.1606)	LR 1.437e-03
0: TRAIN [3][530/1291]	Time 0.041 (0.052)	Data 9.16e-05 (4.82e-04)	Tok/s 185304 (199348)	Loss/tok 2.9640 (3.1588)	LR 1.437e-03
0: TRAIN [3][540/1291]	Time 0.041 (0.052)	Data 8.65e-05 (4.75e-04)	Tok/s 184173 (199400)	Loss/tok 3.0220 (3.1583)	LR 1.437e-03
0: TRAIN [3][550/1291]	Time 0.058 (0.052)	Data 8.96e-05 (4.68e-04)	Tok/s 218873 (199471)	Loss/tok 3.1353 (3.1584)	LR 1.437e-03
0: TRAIN [3][560/1291]	Time 0.076 (0.052)	Data 8.94e-05 (4.61e-04)	Tok/s 231175 (199853)	Loss/tok 3.1726 (3.1602)	LR 1.437e-03
0: TRAIN [3][570/1291]	Time 0.058 (0.052)	Data 8.32e-05 (4.55e-04)	Tok/s 215879 (199784)	Loss/tok 3.1113 (3.1580)	LR 1.437e-03
0: Upscaling, new scale: 8192.0
0: TRAIN [3][580/1291]	Time 0.041 (0.052)	Data 8.01e-05 (4.49e-04)	Tok/s 187984 (199774)	Loss/tok 3.0262 (3.1574)	LR 1.437e-03
0: TRAIN [3][590/1291]	Time 0.041 (0.052)	Data 8.23e-05 (4.43e-04)	Tok/s 187218 (199937)	Loss/tok 2.9961 (3.1589)	LR 1.437e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 4096.0
0: TRAIN [3][600/1291]	Time 0.025 (0.052)	Data 7.92e-05 (4.37e-04)	Tok/s 166143 (199843)	Loss/tok 2.4934 (3.1594)	LR 1.437e-03
0: TRAIN [3][610/1291]	Time 0.059 (0.052)	Data 8.06e-05 (4.31e-04)	Tok/s 211723 (199833)	Loss/tok 3.1536 (3.1596)	LR 1.437e-03
0: TRAIN [3][620/1291]	Time 0.076 (0.053)	Data 8.87e-05 (4.26e-04)	Tok/s 232111 (200082)	Loss/tok 3.3917 (3.1629)	LR 1.437e-03
0: TRAIN [3][630/1291]	Time 0.059 (0.053)	Data 8.11e-05 (4.21e-04)	Tok/s 213242 (200110)	Loss/tok 3.1424 (3.1629)	LR 1.437e-03
0: TRAIN [3][640/1291]	Time 0.059 (0.053)	Data 7.89e-05 (4.15e-04)	Tok/s 213250 (200140)	Loss/tok 3.2321 (3.1626)	LR 1.437e-03
0: TRAIN [3][650/1291]	Time 0.025 (0.053)	Data 8.56e-05 (4.10e-04)	Tok/s 155830 (200023)	Loss/tok 2.6150 (3.1609)	LR 1.437e-03
0: TRAIN [3][660/1291]	Time 0.041 (0.053)	Data 8.01e-05 (4.06e-04)	Tok/s 183236 (200014)	Loss/tok 3.0056 (3.1612)	LR 1.437e-03
0: TRAIN [3][670/1291]	Time 0.042 (0.053)	Data 1.34e-04 (4.01e-04)	Tok/s 183601 (199968)	Loss/tok 2.9989 (3.1616)	LR 1.437e-03
0: TRAIN [3][680/1291]	Time 0.025 (0.053)	Data 8.11e-05 (3.96e-04)	Tok/s 158628 (199994)	Loss/tok 2.6114 (3.1617)	LR 1.437e-03
0: TRAIN [3][690/1291]	Time 0.041 (0.053)	Data 8.03e-05 (3.92e-04)	Tok/s 185706 (200060)	Loss/tok 2.9583 (3.1620)	LR 1.437e-03
0: TRAIN [3][700/1291]	Time 0.041 (0.053)	Data 1.36e-04 (3.88e-04)	Tok/s 184594 (200099)	Loss/tok 3.0345 (3.1612)	LR 1.437e-03
0: TRAIN [3][710/1291]	Time 0.058 (0.053)	Data 8.01e-05 (3.84e-04)	Tok/s 215690 (200190)	Loss/tok 3.1533 (3.1629)	LR 7.187e-04
0: TRAIN [3][720/1291]	Time 0.025 (0.053)	Data 8.13e-05 (3.80e-04)	Tok/s 157580 (200137)	Loss/tok 2.5107 (3.1621)	LR 7.187e-04
0: Upscaling, new scale: 8192.0
0: TRAIN [3][730/1291]	Time 0.041 (0.053)	Data 8.18e-05 (3.76e-04)	Tok/s 185155 (200007)	Loss/tok 3.0518 (3.1608)	LR 7.187e-04
0: TRAIN [3][740/1291]	Time 0.042 (0.053)	Data 8.82e-05 (3.72e-04)	Tok/s 188733 (200185)	Loss/tok 3.0046 (3.1618)	LR 7.187e-04
0: TRAIN [3][750/1291]	Time 0.025 (0.053)	Data 9.30e-05 (3.68e-04)	Tok/s 160164 (200266)	Loss/tok 2.5060 (3.1626)	LR 7.187e-04
0: TRAIN [3][760/1291]	Time 0.059 (0.053)	Data 1.09e-04 (3.65e-04)	Tok/s 211849 (200407)	Loss/tok 3.0018 (3.1633)	LR 7.187e-04
0: TRAIN [3][770/1291]	Time 0.059 (0.053)	Data 8.11e-05 (3.61e-04)	Tok/s 213326 (200435)	Loss/tok 3.2762 (3.1628)	LR 7.187e-04
0: TRAIN [3][780/1291]	Time 0.077 (0.053)	Data 8.96e-05 (3.58e-04)	Tok/s 225498 (200380)	Loss/tok 3.3532 (3.1618)	LR 7.187e-04
0: TRAIN [3][790/1291]	Time 0.042 (0.053)	Data 8.18e-05 (3.55e-04)	Tok/s 180277 (200194)	Loss/tok 2.8160 (3.1611)	LR 7.187e-04
0: TRAIN [3][800/1291]	Time 0.097 (0.053)	Data 9.30e-05 (3.51e-04)	Tok/s 229581 (200257)	Loss/tok 3.4374 (3.1607)	LR 7.187e-04
0: TRAIN [3][810/1291]	Time 0.042 (0.053)	Data 1.28e-04 (3.48e-04)	Tok/s 186352 (200345)	Loss/tok 2.9481 (3.1613)	LR 7.187e-04
0: TRAIN [3][820/1291]	Time 0.042 (0.053)	Data 7.92e-05 (3.45e-04)	Tok/s 193780 (200334)	Loss/tok 3.0013 (3.1603)	LR 7.187e-04
0: TRAIN [3][830/1291]	Time 0.041 (0.053)	Data 8.54e-05 (3.43e-04)	Tok/s 189028 (200227)	Loss/tok 2.8458 (3.1583)	LR 7.187e-04
0: TRAIN [3][840/1291]	Time 0.042 (0.053)	Data 9.11e-05 (3.40e-04)	Tok/s 182322 (200372)	Loss/tok 2.9141 (3.1585)	LR 7.187e-04
0: TRAIN [3][850/1291]	Time 0.058 (0.053)	Data 7.99e-05 (3.37e-04)	Tok/s 217362 (200367)	Loss/tok 3.2980 (3.1579)	LR 7.187e-04
0: Upscaling, new scale: 8192.0
0: TRAIN [3][860/1291]	Time 0.042 (0.053)	Data 8.34e-05 (3.34e-04)	Tok/s 185843 (200315)	Loss/tok 2.9257 (3.1564)	LR 7.187e-04
0: TRAIN [3][870/1291]	Time 0.076 (0.053)	Data 7.92e-05 (3.31e-04)	Tok/s 233866 (200373)	Loss/tok 3.1698 (3.1572)	LR 7.187e-04
0: TRAIN [3][880/1291]	Time 0.025 (0.053)	Data 1.37e-04 (3.28e-04)	Tok/s 156415 (200242)	Loss/tok 2.4914 (3.1561)	LR 7.187e-04
0: TRAIN [3][890/1291]	Time 0.059 (0.053)	Data 1.38e-04 (3.26e-04)	Tok/s 218790 (200270)	Loss/tok 3.1734 (3.1549)	LR 7.187e-04
0: TRAIN [3][900/1291]	Time 0.025 (0.053)	Data 7.84e-05 (3.23e-04)	Tok/s 159416 (200199)	Loss/tok 2.4711 (3.1540)	LR 7.187e-04
0: TRAIN [3][910/1291]	Time 0.058 (0.053)	Data 1.33e-04 (3.21e-04)	Tok/s 215673 (200131)	Loss/tok 3.1635 (3.1532)	LR 7.187e-04
0: TRAIN [3][920/1291]	Time 0.059 (0.053)	Data 8.08e-05 (3.18e-04)	Tok/s 214720 (200189)	Loss/tok 3.1085 (3.1527)	LR 7.187e-04
0: TRAIN [3][930/1291]	Time 0.097 (0.053)	Data 9.04e-05 (3.16e-04)	Tok/s 229314 (200295)	Loss/tok 3.3765 (3.1533)	LR 7.187e-04
0: TRAIN [3][940/1291]	Time 0.042 (0.053)	Data 1.34e-04 (3.13e-04)	Tok/s 186444 (200189)	Loss/tok 2.9803 (3.1520)	LR 7.187e-04
0: TRAIN [3][950/1291]	Time 0.058 (0.052)	Data 1.74e-04 (3.11e-04)	Tok/s 218636 (200114)	Loss/tok 3.1443 (3.1505)	LR 7.187e-04
0: TRAIN [3][960/1291]	Time 0.059 (0.053)	Data 8.06e-05 (3.09e-04)	Tok/s 213786 (200246)	Loss/tok 3.0614 (3.1512)	LR 7.187e-04
0: TRAIN [3][970/1291]	Time 0.059 (0.053)	Data 8.20e-05 (3.07e-04)	Tok/s 217711 (200278)	Loss/tok 3.0393 (3.1503)	LR 7.187e-04
0: TRAIN [3][980/1291]	Time 0.025 (0.053)	Data 9.35e-05 (3.04e-04)	Tok/s 160565 (200316)	Loss/tok 2.5894 (3.1501)	LR 7.187e-04
0: Upscaling, new scale: 8192.0
0: TRAIN [3][990/1291]	Time 0.042 (0.053)	Data 9.06e-05 (3.02e-04)	Tok/s 182007 (200260)	Loss/tok 2.9389 (3.1509)	LR 7.187e-04
0: TRAIN [3][1000/1291]	Time 0.058 (0.053)	Data 8.20e-05 (3.00e-04)	Tok/s 215765 (200246)	Loss/tok 3.0173 (3.1508)	LR 7.187e-04
0: TRAIN [3][1010/1291]	Time 0.024 (0.053)	Data 8.15e-05 (2.98e-04)	Tok/s 162060 (200277)	Loss/tok 2.6535 (3.1505)	LR 7.187e-04
0: TRAIN [3][1020/1291]	Time 0.059 (0.053)	Data 9.58e-05 (2.96e-04)	Tok/s 214289 (200462)	Loss/tok 2.9904 (3.1523)	LR 7.187e-04
0: Gradient norm: inf
0: Skipped batch, new scale: 4096.0
0: TRAIN [3][1030/1291]	Time 0.058 (0.053)	Data 8.13e-05 (2.94e-04)	Tok/s 216298 (200500)	Loss/tok 3.0794 (3.1522)	LR 7.187e-04
0: TRAIN [3][1040/1291]	Time 0.042 (0.053)	Data 1.35e-04 (2.92e-04)	Tok/s 187220 (200525)	Loss/tok 2.8394 (3.1513)	LR 7.187e-04
0: TRAIN [3][1050/1291]	Time 0.097 (0.053)	Data 7.84e-05 (2.90e-04)	Tok/s 232023 (200617)	Loss/tok 3.3651 (3.1522)	LR 7.187e-04
0: TRAIN [3][1060/1291]	Time 0.025 (0.053)	Data 8.11e-05 (2.88e-04)	Tok/s 163253 (200491)	Loss/tok 2.5279 (3.1508)	LR 7.187e-04
0: TRAIN [3][1070/1291]	Time 0.041 (0.053)	Data 7.75e-05 (2.87e-04)	Tok/s 183489 (200414)	Loss/tok 2.9445 (3.1496)	LR 7.187e-04
0: TRAIN [3][1080/1291]	Time 0.042 (0.053)	Data 7.58e-05 (2.85e-04)	Tok/s 179229 (200482)	Loss/tok 2.9932 (3.1497)	LR 7.187e-04
0: TRAIN [3][1090/1291]	Time 0.076 (0.053)	Data 8.06e-05 (2.83e-04)	Tok/s 232645 (200442)	Loss/tok 3.2330 (3.1487)	LR 7.187e-04
0: TRAIN [3][1100/1291]	Time 0.043 (0.053)	Data 7.94e-05 (2.81e-04)	Tok/s 173096 (200404)	Loss/tok 2.9463 (3.1482)	LR 7.187e-04
0: TRAIN [3][1110/1291]	Time 0.059 (0.053)	Data 7.87e-05 (2.80e-04)	Tok/s 211763 (200366)	Loss/tok 3.0308 (3.1477)	LR 7.187e-04
0: TRAIN [3][1120/1291]	Time 0.043 (0.053)	Data 1.33e-04 (2.78e-04)	Tok/s 180264 (200304)	Loss/tok 2.9765 (3.1478)	LR 7.187e-04
0: TRAIN [3][1130/1291]	Time 0.076 (0.053)	Data 8.51e-05 (2.76e-04)	Tok/s 229437 (200389)	Loss/tok 3.2338 (3.1481)	LR 7.187e-04
0: TRAIN [3][1140/1291]	Time 0.058 (0.053)	Data 8.11e-05 (2.75e-04)	Tok/s 218048 (200419)	Loss/tok 3.2077 (3.1488)	LR 7.187e-04
0: TRAIN [3][1150/1291]	Time 0.025 (0.053)	Data 8.20e-05 (2.73e-04)	Tok/s 153700 (200341)	Loss/tok 2.4985 (3.1479)	LR 7.187e-04
0: Upscaling, new scale: 8192.0
0: TRAIN [3][1160/1291]	Time 0.097 (0.053)	Data 7.72e-05 (2.71e-04)	Tok/s 231046 (200395)	Loss/tok 3.4851 (3.1488)	LR 7.187e-04
0: TRAIN [3][1170/1291]	Time 0.042 (0.053)	Data 7.96e-05 (2.70e-04)	Tok/s 187292 (200384)	Loss/tok 2.8824 (3.1486)	LR 7.187e-04
0: TRAIN [3][1180/1291]	Time 0.077 (0.053)	Data 8.01e-05 (2.68e-04)	Tok/s 229069 (200403)	Loss/tok 3.2193 (3.1481)	LR 7.187e-04
0: TRAIN [3][1190/1291]	Time 0.058 (0.053)	Data 7.72e-05 (2.67e-04)	Tok/s 214981 (200370)	Loss/tok 3.2308 (3.1478)	LR 7.187e-04
0: TRAIN [3][1200/1291]	Time 0.041 (0.053)	Data 7.89e-05 (2.65e-04)	Tok/s 186294 (200443)	Loss/tok 2.8926 (3.1476)	LR 7.187e-04
0: TRAIN [3][1210/1291]	Time 0.041 (0.053)	Data 8.96e-05 (2.64e-04)	Tok/s 185284 (200524)	Loss/tok 2.7646 (3.1475)	LR 7.187e-04
0: TRAIN [3][1220/1291]	Time 0.041 (0.053)	Data 8.85e-05 (2.63e-04)	Tok/s 189572 (200547)	Loss/tok 2.9126 (3.1474)	LR 3.594e-04
0: TRAIN [3][1230/1291]	Time 0.097 (0.053)	Data 1.70e-04 (2.61e-04)	Tok/s 234702 (200577)	Loss/tok 3.4810 (3.1481)	LR 3.594e-04
0: TRAIN [3][1240/1291]	Time 0.097 (0.053)	Data 9.82e-05 (2.60e-04)	Tok/s 227092 (200598)	Loss/tok 3.4198 (3.1488)	LR 3.594e-04
0: TRAIN [3][1250/1291]	Time 0.041 (0.053)	Data 2.60e-04 (2.59e-04)	Tok/s 185587 (200626)	Loss/tok 2.8764 (3.1487)	LR 3.594e-04
0: TRAIN [3][1260/1291]	Time 0.042 (0.053)	Data 1.37e-04 (2.57e-04)	Tok/s 186196 (200638)	Loss/tok 2.9288 (3.1484)	LR 3.594e-04
0: TRAIN [3][1270/1291]	Time 0.058 (0.053)	Data 8.11e-05 (2.56e-04)	Tok/s 217319 (200657)	Loss/tok 3.1979 (3.1485)	LR 3.594e-04
0: TRAIN [3][1280/1291]	Time 0.076 (0.053)	Data 8.08e-05 (2.55e-04)	Tok/s 229156 (200665)	Loss/tok 3.3537 (3.1484)	LR 3.594e-04
0: Upscaling, new scale: 8192.0
0: TRAIN [3][1290/1291]	Time 0.058 (0.053)	Data 4.27e-05 (2.55e-04)	Tok/s 217986 (200719)	Loss/tok 3.1027 (3.1489)	LR 3.594e-04
:::MLLOG {"namespace": "", "time_ms": 1593114643091, "event_type": "INTERVAL_END", "key": "epoch_stop", "value": null, "metadata": {"file": "train.py", "lineno": 551, "epoch_num": 4}}
:::MLLOG {"namespace": "", "time_ms": 1593114643091, "event_type": "INTERVAL_START", "key": "eval_start", "value": null, "metadata": {"file": "train.py", "lineno": 556, "epoch_num": 4}}
0: Running evaluation on test set
0: TEST [3][0/3]	Time 0.279 (0.279)	Decoder iters 100.0 (100.0)	Tok/s 32175 (32175)
0: Running moses detokenizer
0: BLEU(score=24.002970592433083, counts=[37135, 18596, 10562, 6260], totals=[65495, 62492, 59490, 56492], precisions=[56.6989846553172, 29.75740894834539, 17.75424441082535, 11.0812150392976], bp=1.0, sys_len=65495, ref_len=64676)
0: Target accuracy reached
0: Finished evaluation on test set
:::MLLOG {"namespace": "", "time_ms": 1593114644253, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.24, "metadata": {"file": "train.py", "lineno": 562, "epoch_num": 4}}
:::MLLOG {"namespace": "", "time_ms": 1593114644253, "event_type": "INTERVAL_END", "key": "eval_stop", "value": null, "metadata": {"file": "train.py", "lineno": 565, "epoch_num": 4}}
0: Summary: Epoch: 3	Training Loss: 3.1459	Test BLEU: 24.00
0: Performance: Epoch: 3	Training: 3210328 Tok/s
0: Finished epoch 3
:::MLLOG {"namespace": "", "time_ms": 1593114644253, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "train.py", "lineno": 584, "first_epoch_num": 4}}
0: Closing preprocessed data file
:::MLLOG {"namespace": "", "time_ms": 1593114644253, "event_type": "INTERVAL_END", "key": "run_stop", "value": null, "metadata": {"file": "train.py", "lineno": 595, "status": "success"}}
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
ENDING TIMING RUN AT 2020-06-25 12:50:50 PM
RESULT,RNN_TRANSLATOR,,311,nvidia,2020-06-25 12:45:39 PM
ENDING TIMING RUN AT 2020-06-25 12:50:50 PM
RESULT,RNN_TRANSLATOR,,311,nvidia,2020-06-25 12:45:39 PM
ENDING TIMING RUN AT 2020-06-25 12:50:51 PM
RESULT,RNN_TRANSLATOR,,312,nvidia,2020-06-25 12:45:39 PM
ENDING TIMING RUN AT 2020-06-25 12:50:51 PM
RESULT,RNN_TRANSLATOR,,312,nvidia,2020-06-25 12:45:39 PM
ENDING TIMING RUN AT 2020-06-25 12:50:51 PM
RESULT,RNN_TRANSLATOR,,312,nvidia,2020-06-25 12:45:39 PM
ENDING TIMING RUN AT 2020-06-25 12:50:51 PM
RESULT,RNN_TRANSLATOR,,312,nvidia,2020-06-25 12:45:39 PM
ENDING TIMING RUN AT 2020-06-25 12:50:51 PM
RESULT,RNN_TRANSLATOR,,312,nvidia,2020-06-25 12:45:39 PM
ENDING TIMING RUN AT 2020-06-25 12:50:51 PM
RESULT,RNN_TRANSLATOR,,312,nvidia,2020-06-25 12:45:39 PM
ENDING TIMING RUN AT 2020-06-25 12:50:51 PM
ENDING TIMING RUN AT 2020-06-25 12:50:51 PM
RESULT,RNN_TRANSLATOR,,312,nvidia,2020-06-25 12:45:39 PM
ENDING TIMING RUN AT 2020-06-25 12:50:51 PM
RESULT,RNN_TRANSLATOR,,312,nvidia,2020-06-25 12:45:39 PM
RESULT,RNN_TRANSLATOR,,312,nvidia,2020-06-25 12:45:39 PM
ENDING TIMING RUN AT 2020-06-25 12:50:51 PM
RESULT,RNN_TRANSLATOR,,312,nvidia,2020-06-25 12:45:39 PM
ENDING TIMING RUN AT 2020-06-25 12:50:51 PM
RESULT,RNN_TRANSLATOR,,312,nvidia,2020-06-25 12:45:39 PM
ENDING TIMING RUN AT 2020-06-25 12:50:51 PM
RESULT,RNN_TRANSLATOR,,312,nvidia,2020-06-25 12:45:39 PM
ENDING TIMING RUN AT 2020-06-25 12:50:52 PM
RESULT,RNN_TRANSLATOR,,313,nvidia,2020-06-25 12:45:39 PM
ENDING TIMING RUN AT 2020-06-25 12:50:52 PM
RESULT,RNN_TRANSLATOR,,313,nvidia,2020-06-25 12:45:39 PM
