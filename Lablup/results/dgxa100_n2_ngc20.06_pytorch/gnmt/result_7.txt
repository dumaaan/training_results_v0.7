+ echo 'Beginning trial 3 of 5'
Beginning trial 3 of 5
+ srun --ntasks=2 --container-name=rnn_translator python -c '
import mlperf_log_utils
from mlperf_logging.mllog import constants
mlperf_log_utils.mlperf_submission_log(constants.GNMT)'
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
:::MLLOG {"namespace": "", "time_ms": 1593113685236, "event_type": "POINT_IN_TIME", "key": "submission_benchmark", "value": "gnmt", "metadata": {"file": "/workspace/rnn_translator/mlperf_log_utils.py", "lineno": 19}}
:::MLLOG {"namespace": "", "time_ms": 1593113685275, "event_type": "POINT_IN_TIME", "key": "submission_org", "value": "NVIDIA", "metadata": {"file": "/workspace/rnn_translator/mlperf_log_utils.py", "lineno": 24}}
:::MLLOG {"namespace": "", "time_ms": 1593113685275, "event_type": "POINT_IN_TIME", "key": "submission_division", "value": "closed", "metadata": {"file": "/workspace/rnn_translator/mlperf_log_utils.py", "lineno": 28}}
:::MLLOG {"namespace": "", "time_ms": 1593113685275, "event_type": "POINT_IN_TIME", "key": "submission_status", "value": "onprem", "metadata": {"file": "/workspace/rnn_translator/mlperf_log_utils.py", "lineno": 32}}
:::MLLOG {"namespace": "", "time_ms": 1593113685275, "event_type": "POINT_IN_TIME", "key": "submission_platform", "value": "2xNVIDIA DGX A100", "metadata": {"file": "/workspace/rnn_translator/mlperf_log_utils.py", "lineno": 36}}
+ '[' 1 -eq 1 ']'
+ srun --ntasks=2 bash -c 'echo -n '\''Clearing cache on '\'' && hostname && sync && sudo /sbin/sysctl vm.drop_caches=3'
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
Clearing cache on luna-0102
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
Clearing cache on luna-0101
vm.drop_caches = 3
vm.drop_caches = 3
+ srun --ntasks=2 --container-name=rnn_translator python -c '
from mlperf_logging.mllog import constants
from seq2seq.utils import log_event
log_event(key=constants.CACHE_CLEAR, value=True)'
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
:::MLLOG {"namespace": "", "time_ms": 1593113690323, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
:::MLLOG {"namespace": "", "time_ms": 1593113690440, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
+ srun --mpi=none --ntasks=16 --ntasks-per-node=8 --container-name=rnn_translator --container-mounts=/lustre/fsr/datasets/wmt16_de_en:/data,/raid/scratch/svcnvdlfw/gnmt:/preproc_data,/lustre/fsw/mlperf-ci/14251653/results:/results ./run_and_time.sh
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
STARTING TIMING RUN AT 2020-06-25 12:34:52 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=2.875e-3
+ TRAIN_BATCH_SIZE=192
+ TEST_BATCH_SIZE=64
+ WARMUP_STEPS=200
+ REMAIN_STEPS=4054
+ DECAY_INTERVAL=506
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=8
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 3    --dwu-num-chunks 2    --dwu-num-rs-pg 1    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-overlap-reductions    --dwu-grad-norm    '
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 5 ']'
+ '[' 16 -gt 2 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGXA100_multi_2x8x192_dist == *\D\G\X\A\1\0\0* ]]
Using TCMalloc
+ echo 'Using TCMalloc'
+ export LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ echo 'running benchmark'
running benchmark
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 192 --test-batch-size 64 --optimizer FusedAdam --lr 2.875e-3 --warmup-steps 200 --remain-steps 4054 --decay-interval 506 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
STARTING TIMING RUN AT 2020-06-25 12:34:52 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=2.875e-3
+ TRAIN_BATCH_SIZE=192
+ TEST_BATCH_SIZE=64
+ WARMUP_STEPS=200
+ REMAIN_STEPS=4054
+ DECAY_INTERVAL=506
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=8
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 3    --dwu-num-chunks 2    --dwu-num-rs-pg 1    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-overlap-reductions    --dwu-grad-norm    '
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 0 ']'
+ '[' 16 -gt 2 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGXA100_multi_2x8x192_dist == *\D\G\X\A\1\0\0* ]]
Using TCMalloc
+ echo 'Using TCMalloc'
+ export LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
running benchmark
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 192 --test-batch-size 64 --optimizer FusedAdam --lr 2.875e-3 --warmup-steps 200 --remain-steps 4054 --decay-interval 506 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
STARTING TIMING RUN AT 2020-06-25 12:34:52 PM
STARTING TIMING RUN AT 2020-06-25 12:34:52 PM
+ DATASET_DIR=/data
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ PREPROC_DATADIR=/preproc_data
+ LR=2.875e-3
+ RESULTS_DIR=gnmt_wmt16
+ TRAIN_BATCH_SIZE=192
+ LR=2.875e-3
+ TEST_BATCH_SIZE=64
+ WARMUP_STEPS=200
+ TRAIN_BATCH_SIZE=192
+ REMAIN_STEPS=4054
+ DECAY_INTERVAL=506
+ TARGET=24.0
+ TEST_BATCH_SIZE=64
+ MAX_SEQ_LEN=75
+ WARMUP_STEPS=200
+ NUMEPOCHS=8
+ REMAIN_STEPS=4054
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 3    --dwu-num-chunks 2    --dwu-num-rs-pg 1    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-overlap-reductions    --dwu-grad-norm    '
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    '
+ DECAY_INTERVAL=506
+ MATH=fp16
+ TARGET=24.0
+ declare -a CMD
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=8
+ '[' -n 2 ']'
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 3    --dwu-num-chunks 2    --dwu-num-rs-pg 1    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-overlap-reductions    --dwu-grad-norm    '
+ '[' 16 -gt 2 ']'
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    '
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ MATH=fp16
+ declare -a CMD
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ '[' -n 4 ']'
+ [[ DGXA100_multi_2x8x192_dist == *\D\G\X\A\1\0\0* ]]
Using TCMalloc
+ echo 'Using TCMalloc'
+ '[' 16 -gt 2 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ export LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
running benchmark
+ echo 'running benchmark'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGXA100_multi_2x8x192_dist == *\D\G\X\A\1\0\0* ]]
Using TCMalloc
+ echo 'Using TCMalloc'
+ export LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
running benchmark
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 192 --test-batch-size 64 --optimizer FusedAdam --lr 2.875e-3 --warmup-steps 200 --remain-steps 4054 --decay-interval 506 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 192 --test-batch-size 64 --optimizer FusedAdam --lr 2.875e-3 --warmup-steps 200 --remain-steps 4054 --decay-interval 506 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
STARTING TIMING RUN AT 2020-06-25 12:34:52 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=2.875e-3
+ TRAIN_BATCH_SIZE=192
+ TEST_BATCH_SIZE=64
+ WARMUP_STEPS=200
+ REMAIN_STEPS=4054
+ DECAY_INTERVAL=506
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=8
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 3    --dwu-num-chunks 2    --dwu-num-rs-pg 1    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-overlap-reductions    --dwu-grad-norm    '
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 6 ']'
+ '[' 16 -gt 2 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGXA100_multi_2x8x192_dist == *\D\G\X\A\1\0\0* ]]
Using TCMalloc
+ echo 'Using TCMalloc'
+ export LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
running benchmark
+ echo 'running benchmark'
STARTING TIMING RUN AT 2020-06-25 12:34:52 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=2.875e-3
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 192 --test-batch-size 64 --optimizer FusedAdam --lr 2.875e-3 --warmup-steps 200 --remain-steps 4054 --decay-interval 506 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
+ TRAIN_BATCH_SIZE=192
+ TEST_BATCH_SIZE=64
+ WARMUP_STEPS=200
+ REMAIN_STEPS=4054
+ DECAY_INTERVAL=506
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=8
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 3    --dwu-num-chunks 2    --dwu-num-rs-pg 1    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-overlap-reductions    --dwu-grad-norm    '
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 3 ']'
+ '[' 16 -gt 2 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGXA100_multi_2x8x192_dist == *\D\G\X\A\1\0\0* ]]
Using TCMalloc
+ echo 'Using TCMalloc'
+ export LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ echo 'running benchmark'
running benchmark
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 192 --test-batch-size 64 --optimizer FusedAdam --lr 2.875e-3 --warmup-steps 200 --remain-steps 4054 --decay-interval 506 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
STARTING TIMING RUN AT 2020-06-25 12:34:52 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=2.875e-3
+ TRAIN_BATCH_SIZE=192
+ TEST_BATCH_SIZE=64
+ WARMUP_STEPS=200
+ REMAIN_STEPS=4054
+ DECAY_INTERVAL=506
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=8
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 3    --dwu-num-chunks 2    --dwu-num-rs-pg 1    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-overlap-reductions    --dwu-grad-norm    '
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 1 ']'
+ '[' 16 -gt 2 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGXA100_multi_2x8x192_dist == *\D\G\X\A\1\0\0* ]]
Using TCMalloc
+ echo 'Using TCMalloc'
+ export LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
running benchmark
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 192 --test-batch-size 64 --optimizer FusedAdam --lr 2.875e-3 --warmup-steps 200 --remain-steps 4054 --decay-interval 506 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
STARTING TIMING RUN AT 2020-06-25 12:34:52 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=2.875e-3
+ TRAIN_BATCH_SIZE=192
+ TEST_BATCH_SIZE=64
+ WARMUP_STEPS=200
+ REMAIN_STEPS=4054
+ DECAY_INTERVAL=506
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=8
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 3    --dwu-num-chunks 2    --dwu-num-rs-pg 1    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-overlap-reductions    --dwu-grad-norm    '
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 7 ']'
+ '[' 16 -gt 2 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGXA100_multi_2x8x192_dist == *\D\G\X\A\1\0\0* ]]
Using TCMalloc
+ echo 'Using TCMalloc'
+ export LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
running benchmark
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 192 --test-batch-size 64 --optimizer FusedAdam --lr 2.875e-3 --warmup-steps 200 --remain-steps 4054 --decay-interval 506 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
STARTING TIMING RUN AT 2020-06-25 12:34:52 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=2.875e-3
+ TRAIN_BATCH_SIZE=192
+ TEST_BATCH_SIZE=64
+ WARMUP_STEPS=200
+ REMAIN_STEPS=4054
+ DECAY_INTERVAL=506
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=8
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 3    --dwu-num-chunks 2    --dwu-num-rs-pg 1    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-overlap-reductions    --dwu-grad-norm    '
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 0 ']'
+ '[' 16 -gt 2 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGXA100_multi_2x8x192_dist == *\D\G\X\A\1\0\0* ]]
Using TCMalloc
+ echo 'Using TCMalloc'
+ export LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
running benchmark
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 192 --test-batch-size 64 --optimizer FusedAdam --lr 2.875e-3 --warmup-steps 200 --remain-steps 4054 --decay-interval 506 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
STARTING TIMING RUN AT 2020-06-25 12:34:52 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=2.875e-3
+ TRAIN_BATCH_SIZE=192
+ TEST_BATCH_SIZE=64
+ WARMUP_STEPS=200
+ REMAIN_STEPS=4054
+ DECAY_INTERVAL=506
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=8
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 3    --dwu-num-chunks 2    --dwu-num-rs-pg 1    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-overlap-reductions    --dwu-grad-norm    '
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 6 ']'
+ '[' 16 -gt 2 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGXA100_multi_2x8x192_dist == *\D\G\X\A\1\0\0* ]]
+ echo 'Using TCMalloc'
Using TCMalloc
+ export LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
running benchmark
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 192 --test-batch-size 64 --optimizer FusedAdam --lr 2.875e-3 --warmup-steps 200 --remain-steps 4054 --decay-interval 506 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
STARTING TIMING RUN AT 2020-06-25 12:34:52 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=2.875e-3
+ TRAIN_BATCH_SIZE=192
+ TEST_BATCH_SIZE=64
+ WARMUP_STEPS=200
+ REMAIN_STEPS=4054
+ DECAY_INTERVAL=506
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=8
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 3    --dwu-num-chunks 2    --dwu-num-rs-pg 1    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-overlap-reductions    --dwu-grad-norm    '
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 4 ']'
+ '[' 16 -gt 2 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGXA100_multi_2x8x192_dist == *\D\G\X\A\1\0\0* ]]
Using TCMalloc
+ echo 'Using TCMalloc'
+ export LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
running benchmark
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 192 --test-batch-size 64 --optimizer FusedAdam --lr 2.875e-3 --warmup-steps 200 --remain-steps 4054 --decay-interval 506 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
STARTING TIMING RUN AT 2020-06-25 12:34:52 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=2.875e-3
+ TRAIN_BATCH_SIZE=192
+ TEST_BATCH_SIZE=64
+ WARMUP_STEPS=200
+ REMAIN_STEPS=4054
+ DECAY_INTERVAL=506
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=8
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 3    --dwu-num-chunks 2    --dwu-num-rs-pg 1    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-overlap-reductions    --dwu-grad-norm    '
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 3 ']'
+ '[' 16 -gt 2 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGXA100_multi_2x8x192_dist == *\D\G\X\A\1\0\0* ]]
Using TCMalloc
+ echo 'Using TCMalloc'
+ export LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
running benchmark
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 192 --test-batch-size 64 --optimizer FusedAdam --lr 2.875e-3 --warmup-steps 200 --remain-steps 4054 --decay-interval 506 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
STARTING TIMING RUN AT 2020-06-25 12:34:52 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=2.875e-3
+ TRAIN_BATCH_SIZE=192
+ TEST_BATCH_SIZE=64
+ WARMUP_STEPS=200
+ REMAIN_STEPS=4054
+ DECAY_INTERVAL=506
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=8
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 3    --dwu-num-chunks 2    --dwu-num-rs-pg 1    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-overlap-reductions    --dwu-grad-norm    '
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 1 ']'
+ '[' 16 -gt 2 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGXA100_multi_2x8x192_dist == *\D\G\X\A\1\0\0* ]]
+ echo 'Using TCMalloc'
Using TCMalloc
+ export LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
running benchmark
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 192 --test-batch-size 64 --optimizer FusedAdam --lr 2.875e-3 --warmup-steps 200 --remain-steps 4054 --decay-interval 506 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
STARTING TIMING RUN AT 2020-06-25 12:34:52 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=2.875e-3
+ TRAIN_BATCH_SIZE=192
+ TEST_BATCH_SIZE=64
+ WARMUP_STEPS=200
+ REMAIN_STEPS=4054
+ DECAY_INTERVAL=506
+ TARGET=24.0
+ MAX_SEQ_LEN=75
STARTING TIMING RUN AT 2020-06-25 12:34:52 PM
+ NUMEPOCHS=8
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 3    --dwu-num-chunks 2    --dwu-num-rs-pg 1    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-overlap-reductions    --dwu-grad-norm    '
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    '
+ MATH=fp16
+ DATASET_DIR=/data
+ declare -a CMD
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=2.875e-3
+ TRAIN_BATCH_SIZE=192
+ TEST_BATCH_SIZE=64
+ WARMUP_STEPS=200
+ REMAIN_STEPS=4054
+ DECAY_INTERVAL=506
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ '[' -n 5 ']'
+ NUMEPOCHS=8
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 3    --dwu-num-chunks 2    --dwu-num-rs-pg 1    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-overlap-reductions    --dwu-grad-norm    '
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    '
+ '[' 16 -gt 2 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ MATH=fp16
+ declare -a CMD
+ '[' -n 2 ']'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGXA100_multi_2x8x192_dist == *\D\G\X\A\1\0\0* ]]
+ '[' 16 -gt 2 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
STARTING TIMING RUN AT 2020-06-25 12:34:52 PM
Using TCMalloc
+ echo 'Using TCMalloc'
+ export LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
running benchmark
+ echo 'running benchmark'
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ RESULTS_DIR=gnmt_wmt16
+ LR=2.875e-3
+ [[ DGXA100_multi_2x8x192_dist == *\D\G\X\A\1\0\0* ]]
+ TRAIN_BATCH_SIZE=192
Using TCMalloc
+ echo 'Using TCMalloc'
+ export LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ TEST_BATCH_SIZE=64
+ WARMUP_STEPS=200
+ REMAIN_STEPS=4054
+ DECAY_INTERVAL=506
+ TARGET=24.0
running benchmark
+ echo 'running benchmark'
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=8
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 3    --dwu-num-chunks 2    --dwu-num-rs-pg 1    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-overlap-reductions    --dwu-grad-norm    '
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 7 ']'
+ '[' 16 -gt 2 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 192 --test-batch-size 64 --optimizer FusedAdam --lr 2.875e-3 --warmup-steps 200 --remain-steps 4054 --decay-interval 506 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 192 --test-batch-size 64 --optimizer FusedAdam --lr 2.875e-3 --warmup-steps 200 --remain-steps 4054 --decay-interval 506 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGXA100_multi_2x8x192_dist == *\D\G\X\A\1\0\0* ]]
+ echo 'Using TCMalloc'
Using TCMalloc
+ export LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
running benchmark
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 192 --test-batch-size 64 --optimizer FusedAdam --lr 2.875e-3 --warmup-steps 200 --remain-steps 4054 --decay-interval 506 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
num_sockets = 2 num_nodes=8 cores_per_socket=64
+ exec numactl --physcpubind=80-95,208-223 --membind=5 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 192 --test-batch-size 64 --optimizer FusedAdam --lr 2.875e-3 --warmup-steps 200 --remain-steps 4054 --decay-interval 506 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
num_sockets = 2 num_nodes=8 cores_per_socket=64
+ exec numactl --physcpubind=0-15,128-143 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 192 --test-batch-size 64 --optimizer FusedAdam --lr 2.875e-3 --warmup-steps 200 --remain-steps 4054 --decay-interval 506 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
num_sockets = 2 num_nodes=8 cores_per_socket=64
+ exec numactl --physcpubind=32-47,160-175 --membind=2 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 192 --test-batch-size 64 --optimizer FusedAdam --lr 2.875e-3 --warmup-steps 200 --remain-steps 4054 --decay-interval 506 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
num_sockets = 2 num_nodes=8 cores_per_socket=64
num_sockets = 2 num_nodes=8 cores_per_socket=64
+ exec numactl --physcpubind=48-63,176-191 --membind=3 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 192 --test-batch-size 64 --optimizer FusedAdam --lr 2.875e-3 --warmup-steps 200 --remain-steps 4054 --decay-interval 506 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
+ exec numactl --physcpubind=48-63,176-191 --membind=3 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 192 --test-batch-size 64 --optimizer FusedAdam --lr 2.875e-3 --warmup-steps 200 --remain-steps 4054 --decay-interval 506 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
num_sockets = 2 num_nodes=8 cores_per_socket=64
+ exec numactl --physcpubind=64-79,192-207 --membind=4 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 192 --test-batch-size 64 --optimizer FusedAdam --lr 2.875e-3 --warmup-steps 200 --remain-steps 4054 --decay-interval 506 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
num_sockets = 2 num_nodes=8 cores_per_socket=64
num_sockets = 2 num_nodes=8 cores_per_socket=64
num_sockets = 2 num_nodes=8 cores_per_socket=64
+ exec numactl --physcpubind=96-111,224-239 --membind=6 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 192 --test-batch-size 64 --optimizer FusedAdam --lr 2.875e-3 --warmup-steps 200 --remain-steps 4054 --decay-interval 506 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
+ exec numactl --physcpubind=112-127,240-255 --membind=7 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 192 --test-batch-size 64 --optimizer FusedAdam --lr 2.875e-3 --warmup-steps 200 --remain-steps 4054 --decay-interval 506 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
+ exec numactl --physcpubind=16-31,144-159 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 192 --test-batch-size 64 --optimizer FusedAdam --lr 2.875e-3 --warmup-steps 200 --remain-steps 4054 --decay-interval 506 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
num_sockets = 2 num_nodes=8 cores_per_socket=64
+ exec numactl --physcpubind=32-47,160-175 --membind=2 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 192 --test-batch-size 64 --optimizer FusedAdam --lr 2.875e-3 --warmup-steps 200 --remain-steps 4054 --decay-interval 506 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
num_sockets = 2 num_nodes=8 cores_per_socket=64
+ exec numactl --physcpubind=16-31,144-159 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 192 --test-batch-size 64 --optimizer FusedAdam --lr 2.875e-3 --warmup-steps 200 --remain-steps 4054 --decay-interval 506 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
num_sockets = 2 num_nodes=8 cores_per_socket=64
num_sockets = 2 num_nodes=8 cores_per_socket=64
num_sockets = 2 num_nodes=8 cores_per_socket=64
num_sockets = 2 num_nodes=8 cores_per_socket=64
+ exec numactl --physcpubind=80-95,208-223 --membind=5 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 192 --test-batch-size 64 --optimizer FusedAdam --lr 2.875e-3 --warmup-steps 200 --remain-steps 4054 --decay-interval 506 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
num_sockets = 2 num_nodes=8 cores_per_socket=64
+ exec numactl --physcpubind=112-127,240-255 --membind=7 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 192 --test-batch-size 64 --optimizer FusedAdam --lr 2.875e-3 --warmup-steps 200 --remain-steps 4054 --decay-interval 506 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
+ exec numactl --physcpubind=64-79,192-207 --membind=4 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 192 --test-batch-size 64 --optimizer FusedAdam --lr 2.875e-3 --warmup-steps 200 --remain-steps 4054 --decay-interval 506 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
+ exec numactl --physcpubind=96-111,224-239 --membind=6 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 192 --test-batch-size 64 --optimizer FusedAdam --lr 2.875e-3 --warmup-steps 200 --remain-steps 4054 --decay-interval 506 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
+ exec numactl --physcpubind=0-15,128-143 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 192 --test-batch-size 64 --optimizer FusedAdam --lr 2.875e-3 --warmup-steps 200 --remain-steps 4054 --decay-interval 506 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
:::MLLOG {"namespace": "", "time_ms": 1593113694320, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593113694416, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593113694418, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593113694503, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593113694531, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593113694542, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593113694551, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593113694563, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593113694579, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593113694725, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593113694729, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593113694741, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593113694756, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593113694776, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593113694788, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593113694809, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
NCCL version 2.7.5+cuda11.0
0: Saving results to: results/gnmt_wmt16
0: Run arguments: Namespace(apex_message_size=10000000.0, apex_num_allreduce_streams=1, batching='bucketing', beam_size=5, cov_penalty_factor=0.1, cuda=True, cudnn=True, dataset_dir='/data', decay_factor=0.5, decay_interval=506, decay_steps=4, distributed_weight_update=2, dropout=0.2, dwu_e5m2_allgather=False, dwu_full_pipeline=False, dwu_grad_norm=True, dwu_group_size=0, dwu_num_ag_pg=0, dwu_num_ar_pg=2, dwu_num_blocks=3, dwu_num_chunks=2, dwu_num_rs_pg=1, dwu_overlap_reductions=True, enable_apex_allreduce_overlap=False, env=False, epochs=8, eval=True, fused_attention=True, fused_xentropy=True, grad_clip=5.0, hidden_size=1024, init_scale=1024, intra_epoch_eval=0, keep_checkpoints=0, len_norm_const=5.0, len_norm_factor=0.6, local_rank=0, log_all_ranks=False, lr=0.002875, math='fp16', max_length_test=150, max_length_train=75, max_size=None, min_length_test=0, min_length_train=0, num_buckets=5, num_layers=4, optimizer='FusedAdam', optimizer_extra='{}', prealloc_mode='once', preproc_data_dir='/preproc_data/75', print_freq=10, rank=0, remain_steps=4054, results_dir='results', resume=None, save='gnmt_wmt16', save_all=False, save_freq=5000, save_path='results/gnmt_wmt16', seed=None, shard_size=80, share_embedding=True, smoothing=0.1, start_epoch=0, target_bleu=24.0, test_batch_size=64, test_loader_workers=0, train_batch_size=192, train_global_batch_size=None, train_iter_size=1, train_loader_workers=1, upscale_interval=128, use_preproc_data=True, warmup_steps=200)
0: L2 promotion: 128B
0: Using random master seed: 1611093239
:::MLLOG {"namespace": "", "time_ms": 1593113704328, "event_type": "POINT_IN_TIME", "key": "seed", "value": 1611093239, "metadata": {"file": "/workspace/rnn_translator/seq2seq/utils.py", "lineno": 144}}
0: Worker 0 is using worker seed: 894311725
0: Building vocabulary from /data/vocab.bpe.32000
0: Size of vocabulary: 32320
0: Fused attention flag set to True
0: GNMT(
  (embedder): Embedding(32320, 1024, padding_idx=0)
  (encoder): ResidualRecurrentEncoder(
    (rnn_layers): ModuleList(
      (0): EmuBidirLSTM(
        (bidir): LSTM(1024, 1024, bidirectional=True)
        (layer1): LSTM(1024, 1024)
        (layer2): LSTM(1024, 1024)
      )
      (1): LSTM(2048, 1024)
      (2): LSTM(1024, 1024)
      (3): LSTM(1024, 1024)
    )
    (dropout): Dropout(p=0.2, inplace=False)
    (embedder): Embedding(32320, 1024, padding_idx=0)
  )
  (decoder): ResidualRecurrentDecoder(
    (att_rnn): RecurrentAttention(
      (rnn): LSTM(1024, 1024)
      (attn): BahdanauAttention(
        (linear_q): Linear(in_features=1024, out_features=1024, bias=False)
        (linear_k): Linear(in_features=1024, out_features=1024, bias=False)
      )
      (dropout): Dropout(p=0.2, inplace=False)
    )
    (rnn_layers): ModuleList(
      (0): LSTM(2048, 1024)
      (1): LSTM(2048, 1024)
      (2): LSTM(2048, 1024)
    )
    (embedder): Embedding(32320, 1024, padding_idx=0)
    (classifier): Classifier(
      (classifier): Linear(in_features=1024, out_features=32320, bias=True)
    )
    (dropout): Dropout(p=0.2, inplace=False)
  )
)
0: Building LabelSmoothingLoss (smoothing: 0.1)
0: Fused xentropy flag set to True
0: Training optimizer config: {'optimizer': 'FusedAdam', 'lr': 0.002875}
0: Number of parameters: 160671297
0: Saving state of the tokenizer
self._net_total_param_size=160671360, self._total_param_size=160677888, dwu_min_page_size=12288, self._block_size=53559296, self._chunk_size=26779648, self._shard_size=3347456
[0, 9, 34]
self._net_total_param_size=160671360, self._total_param_size=160677888, dwu_min_page_size=12288, self._block_size=53559296, self._chunk_size=26779648, self._shard_size=3347456
[0, 9, 34]
self._net_total_param_size=160671360, self._total_param_size=160677888, dwu_min_page_size=12288, self._block_size=53559296, self._chunk_size=26779648, self._shard_size=3347456
[0, 9, 34]
self._net_total_param_size=160671360, self._total_param_size=160677888, dwu_min_page_size=12288, self._block_size=53559296, self._chunk_size=26779648, self._shard_size=3347456
[0, 9, 34]
self._net_total_param_size=160671360, self._total_param_size=160677888, dwu_min_page_size=12288, self._block_size=53559296, self._chunk_size=26779648, self._shard_size=3347456
[0, 9, 34]
self._net_total_param_size=160671360, self._total_param_size=160677888, dwu_min_page_size=12288, self._block_size=53559296, self._chunk_size=26779648, self._shard_size=3347456
[0, 9, 34]
self._net_total_param_size=160671360, self._total_param_size=160677888, dwu_min_page_size=12288, self._block_size=53559296, self._chunk_size=26779648, self._shard_size=3347456
[0, 9, 34]
self._net_total_param_size=160671360, self._total_param_size=160677888, dwu_min_page_size=12288, self._block_size=53559296, self._chunk_size=26779648, self._shard_size=3347456
[0, 9, 34]
self._net_total_param_size=160671360, self._total_param_size=160677888, dwu_min_page_size=12288, self._block_size=53559296, self._chunk_size=26779648, self._shard_size=3347456
[0, 9, 34]
self._net_total_param_size=160671360, self._total_param_size=160677888, dwu_min_page_size=12288, self._block_size=53559296, self._chunk_size=26779648, self._shard_size=3347456
[0, 9, 34]
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([1314816]), new_param_packed_fragment.size()=torch.Size([1314816]), master_param_fragment.size()=torch.Size([1314816])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([2024448]), new_param_packed_fragment.size()=torch.Size([2024448]), master_param_fragment.size()=torch.Size([2024448])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([2832448]), new_param_packed_fragment.size()=torch.Size([2832448]), master_param_fragment.size()=torch.Size([2832448])
model_param_fragment.size()=torch.Size([515008]), new_param_packed_fragment.size()=torch.Size([515008]), master_param_fragment.size()=torch.Size([515008])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
NCCL version 2.7.5+cuda11.0
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([2169856]), new_param_packed_fragment.size()=torch.Size([2169856]), master_param_fragment.size()=torch.Size([2169856])
model_param_fragment.size()=torch.Size([1177600]), new_param_packed_fragment.size()=torch.Size([1177600]), master_param_fragment.size()=torch.Size([1177600])
model_param_fragment.size()=torch.Size([572416]), new_param_packed_fragment.size()=torch.Size([572416]), master_param_fragment.size()=torch.Size([572416])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([2766848]), new_param_packed_fragment.size()=torch.Size([2766848]), master_param_fragment.size()=torch.Size([2766848])
model_param_fragment.size()=torch.Size([1082432]), new_param_packed_fragment.size()=torch.Size([1082432]), master_param_fragment.size()=torch.Size([1082432])
model_param_fragment.size()=torch.Size([2265024]), new_param_packed_fragment.size()=torch.Size([2265024]), master_param_fragment.size()=torch.Size([2265024])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
NCCL version 2.7.5+cuda11.0
model_param_fragment.size()=torch.Size([467968]), new_param_packed_fragment.size()=torch.Size([467968]), master_param_fragment.size()=torch.Size([467968])
model_param_fragment.size()=torch.Size([2879488]), new_param_packed_fragment.size()=torch.Size([2879488]), master_param_fragment.size()=torch.Size([2879488])
model_param_fragment.size()=torch.Size([3073024]), new_param_packed_fragment.size()=torch.Size([3073024]), master_param_fragment.size()=torch.Size([3073024])
model_param_fragment.size()=torch.Size([274432]), new_param_packed_fragment.size()=torch.Size([274432]), master_param_fragment.size()=torch.Size([274432])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
NCCL version 2.7.5+cuda11.0
self._net_total_param_size=160671360, self._total_param_size=160677888, dwu_min_page_size=12288, self._block_size=53559296, self._chunk_size=26779648, self._shard_size=3347456
[0, 9, 34]
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([467968]), new_param_packed_fragment.size()=torch.Size([467968]), master_param_fragment.size()=torch.Size([467968])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([2879488]), new_param_packed_fragment.size()=torch.Size([2879488]), master_param_fragment.size()=torch.Size([2879488])
model_param_fragment.size()=torch.Size([3073024]), new_param_packed_fragment.size()=torch.Size([3073024]), master_param_fragment.size()=torch.Size([3073024])
model_param_fragment.size()=torch.Size([274432]), new_param_packed_fragment.size()=torch.Size([274432]), master_param_fragment.size()=torch.Size([274432])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([2274304]), new_param_packed_fragment.size()=torch.Size([2274304]), master_param_fragment.size()=torch.Size([2274304])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([2217984]), new_param_packed_fragment.size()=torch.Size([2217984]), master_param_fragment.size()=torch.Size([2217984])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([1064960]), new_param_packed_fragment.size()=torch.Size([1064960]), master_param_fragment.size()=torch.Size([1064960])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([1121280]), new_param_packed_fragment.size()=torch.Size([1121280]), master_param_fragment.size()=torch.Size([1121280])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([628736]), new_param_packed_fragment.size()=torch.Size([628736]), master_param_fragment.size()=torch.Size([628736])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([3308608]), new_param_packed_fragment.size()=torch.Size([3308608]), master_param_fragment.size()=torch.Size([3308608])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([32320]), new_param_packed_fragment.size()=torch.Size([32320]), master_param_fragment.size()=torch.Size([32320])
model_param_fragment.size()=torch.Size([1024]), new_param_packed_fragment.size()=torch.Size([1024]), master_param_fragment.size()=torch.Size([1024])
model_param_fragment.size()=torch.Size([1]), new_param_packed_fragment.size()=torch.Size([1]), master_param_fragment.size()=torch.Size([1])
model_param_fragment.size()=torch.Size([1024]), new_param_packed_fragment.size()=torch.Size([1024]), master_param_fragment.size()=torch.Size([1024])
model_param_fragment.size()=torch.Size([1048576]), new_param_packed_fragment.size()=torch.Size([1048576]), master_param_fragment.size()=torch.Size([1048576])
model_param_fragment.size()=torch.Size([1048576]), new_param_packed_fragment.size()=torch.Size([1048576]), master_param_fragment.size()=torch.Size([1048576])
model_param_fragment.size()=torch.Size([611264]), new_param_packed_fragment.size()=torch.Size([611264]), master_param_fragment.size()=torch.Size([611264])
model_param_fragment.size()=torch.Size([1130560]), new_param_packed_fragment.size()=torch.Size([1130560]), master_param_fragment.size()=torch.Size([1130560])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([2208704]), new_param_packed_fragment.size()=torch.Size([2208704]), master_param_fragment.size()=torch.Size([2208704])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
NCCL version 2.7.5+cuda11.0
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3016704]), new_param_packed_fragment.size()=torch.Size([3016704]), master_param_fragment.size()=torch.Size([3016704])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([322560]), new_param_packed_fragment.size()=torch.Size([322560]), master_param_fragment.size()=torch.Size([322560])
model_param_fragment.size()=torch.Size([1427456]), new_param_packed_fragment.size()=torch.Size([1427456]), master_param_fragment.size()=torch.Size([1427456])
model_param_fragment.size()=torch.Size([1920000]), new_param_packed_fragment.size()=torch.Size([1920000]), master_param_fragment.size()=torch.Size([1920000])
model_param_fragment.size()=torch.Size([1929280]), new_param_packed_fragment.size()=torch.Size([1929280]), master_param_fragment.size()=torch.Size([1929280])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([1409984]), new_param_packed_fragment.size()=torch.Size([1409984]), master_param_fragment.size()=torch.Size([1409984])
model_param_fragment.size()=torch.Size([331840]), new_param_packed_fragment.size()=torch.Size([331840]), master_param_fragment.size()=torch.Size([331840])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([3007424]), new_param_packed_fragment.size()=torch.Size([3007424]), master_param_fragment.size()=torch.Size([3007424])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
NCCL version 2.7.5+cuda11.0
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([1314816]), new_param_packed_fragment.size()=torch.Size([1314816]), master_param_fragment.size()=torch.Size([1314816])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([2024448]), new_param_packed_fragment.size()=torch.Size([2024448]), master_param_fragment.size()=torch.Size([2024448])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([2832448]), new_param_packed_fragment.size()=torch.Size([2832448]), master_param_fragment.size()=torch.Size([2832448])
model_param_fragment.size()=torch.Size([515008]), new_param_packed_fragment.size()=torch.Size([515008]), master_param_fragment.size()=torch.Size([515008])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3129344]), new_param_packed_fragment.size()=torch.Size([3129344]), master_param_fragment.size()=torch.Size([3129344])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([218112]), new_param_packed_fragment.size()=torch.Size([218112]), master_param_fragment.size()=torch.Size([218112])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([2217984]), new_param_packed_fragment.size()=torch.Size([2217984]), master_param_fragment.size()=torch.Size([2217984])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([1121280]), new_param_packed_fragment.size()=torch.Size([1121280]), master_param_fragment.size()=torch.Size([1121280])
model_param_fragment.size()=torch.Size([628736]), new_param_packed_fragment.size()=torch.Size([628736]), master_param_fragment.size()=torch.Size([628736])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([1024]), new_param_packed_fragment.size()=torch.Size([1024]), master_param_fragment.size()=torch.Size([1024])
model_param_fragment.size()=torch.Size([1]), new_param_packed_fragment.size()=torch.Size([1]), master_param_fragment.size()=torch.Size([1])
model_param_fragment.size()=torch.Size([1024]), new_param_packed_fragment.size()=torch.Size([1024]), master_param_fragment.size()=torch.Size([1024])
model_param_fragment.size()=torch.Size([1048576]), new_param_packed_fragment.size()=torch.Size([1048576]), master_param_fragment.size()=torch.Size([1048576])
model_param_fragment.size()=torch.Size([1048576]), new_param_packed_fragment.size()=torch.Size([1048576]), master_param_fragment.size()=torch.Size([1048576])
model_param_fragment.size()=torch.Size([611264]), new_param_packed_fragment.size()=torch.Size([611264]), master_param_fragment.size()=torch.Size([611264])
model_param_fragment.size()=torch.Size([1130560]), new_param_packed_fragment.size()=torch.Size([1130560]), master_param_fragment.size()=torch.Size([1130560])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([2208704]), new_param_packed_fragment.size()=torch.Size([2208704]), master_param_fragment.size()=torch.Size([2208704])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([2968576]), new_param_packed_fragment.size()=torch.Size([2968576]), master_param_fragment.size()=torch.Size([2968576])
model_param_fragment.size()=torch.Size([378880]), new_param_packed_fragment.size()=torch.Size([378880]), master_param_fragment.size()=torch.Size([378880])
model_param_fragment.size()=torch.Size([1371136]), new_param_packed_fragment.size()=torch.Size([1371136]), master_param_fragment.size()=torch.Size([1371136])
model_param_fragment.size()=torch.Size([1976320]), new_param_packed_fragment.size()=torch.Size([1976320]), master_param_fragment.size()=torch.Size([1976320])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([283712]), new_param_packed_fragment.size()=torch.Size([283712]), master_param_fragment.size()=torch.Size([283712])
model_param_fragment.size()=torch.Size([3063744]), new_param_packed_fragment.size()=torch.Size([3063744]), master_param_fragment.size()=torch.Size([3063744])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
0: Initializing dwu fp16 optimizer
self._net_total_param_size=160671360, self._total_param_size=160677888, dwu_min_page_size=12288, self._block_size=53559296, self._chunk_size=26779648, self._shard_size=3347456
[0, 9, 34]
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3129344]), new_param_packed_fragment.size()=torch.Size([3129344]), master_param_fragment.size()=torch.Size([3129344])
model_param_fragment.size()=torch.Size([218112]), new_param_packed_fragment.size()=torch.Size([218112]), master_param_fragment.size()=torch.Size([218112])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
self._net_total_param_size=160671360, self._total_param_size=160677888, dwu_min_page_size=12288, self._block_size=53559296, self._chunk_size=26779648, self._shard_size=3347456
[0, 9, 34]
self._net_total_param_size=160671360, self._total_param_size=160677888, dwu_min_page_size=12288, self._block_size=53559296, self._chunk_size=26779648, self._shard_size=3347456
[0, 9, 34]
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([2968576]), new_param_packed_fragment.size()=torch.Size([2968576]), master_param_fragment.size()=torch.Size([2968576])
model_param_fragment.size()=torch.Size([378880]), new_param_packed_fragment.size()=torch.Size([378880]), master_param_fragment.size()=torch.Size([378880])
model_param_fragment.size()=torch.Size([1371136]), new_param_packed_fragment.size()=torch.Size([1371136]), master_param_fragment.size()=torch.Size([1371136])
model_param_fragment.size()=torch.Size([1976320]), new_param_packed_fragment.size()=torch.Size([1976320]), master_param_fragment.size()=torch.Size([1976320])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([283712]), new_param_packed_fragment.size()=torch.Size([283712]), master_param_fragment.size()=torch.Size([283712])
model_param_fragment.size()=torch.Size([3063744]), new_param_packed_fragment.size()=torch.Size([3063744]), master_param_fragment.size()=torch.Size([3063744])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
NCCL version 2.7.5+cuda11.0
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([2169856]), new_param_packed_fragment.size()=torch.Size([2169856]), master_param_fragment.size()=torch.Size([2169856])
model_param_fragment.size()=torch.Size([1177600]), new_param_packed_fragment.size()=torch.Size([1177600]), master_param_fragment.size()=torch.Size([1177600])
model_param_fragment.size()=torch.Size([572416]), new_param_packed_fragment.size()=torch.Size([572416]), master_param_fragment.size()=torch.Size([572416])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([2766848]), new_param_packed_fragment.size()=torch.Size([2766848]), master_param_fragment.size()=torch.Size([2766848])
model_param_fragment.size()=torch.Size([1082432]), new_param_packed_fragment.size()=torch.Size([1082432]), master_param_fragment.size()=torch.Size([1082432])
model_param_fragment.size()=torch.Size([2265024]), new_param_packed_fragment.size()=torch.Size([2265024]), master_param_fragment.size()=torch.Size([2265024])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
self._net_total_param_size=160671360, self._total_param_size=160677888, dwu_min_page_size=12288, self._block_size=53559296, self._chunk_size=26779648, self._shard_size=3347456
[0, 9, 34]
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3016704]), new_param_packed_fragment.size()=torch.Size([3016704]), master_param_fragment.size()=torch.Size([3016704])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([322560]), new_param_packed_fragment.size()=torch.Size([322560]), master_param_fragment.size()=torch.Size([322560])
model_param_fragment.size()=torch.Size([1427456]), new_param_packed_fragment.size()=torch.Size([1427456]), master_param_fragment.size()=torch.Size([1427456])
model_param_fragment.size()=torch.Size([1920000]), new_param_packed_fragment.size()=torch.Size([1920000]), master_param_fragment.size()=torch.Size([1920000])
model_param_fragment.size()=torch.Size([1929280]), new_param_packed_fragment.size()=torch.Size([1929280]), master_param_fragment.size()=torch.Size([1929280])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([1409984]), new_param_packed_fragment.size()=torch.Size([1409984]), master_param_fragment.size()=torch.Size([1409984])
model_param_fragment.size()=torch.Size([331840]), new_param_packed_fragment.size()=torch.Size([331840]), master_param_fragment.size()=torch.Size([331840])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([3007424]), new_param_packed_fragment.size()=torch.Size([3007424]), master_param_fragment.size()=torch.Size([3007424])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
NCCL version 2.7.5+cuda11.0
self._net_total_param_size=160671360, self._total_param_size=160677888, dwu_min_page_size=12288, self._block_size=53559296, self._chunk_size=26779648, self._shard_size=3347456
[0, 9, 34]
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([2274304]), new_param_packed_fragment.size()=torch.Size([2274304]), master_param_fragment.size()=torch.Size([2274304])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([1064960]), new_param_packed_fragment.size()=torch.Size([1064960]), master_param_fragment.size()=torch.Size([1064960])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3308608]), new_param_packed_fragment.size()=torch.Size([3308608]), master_param_fragment.size()=torch.Size([3308608])
model_param_fragment.size()=torch.Size([32320]), new_param_packed_fragment.size()=torch.Size([32320]), master_param_fragment.size()=torch.Size([32320])
NCCL version 2.7.5+cuda11.0
0: Using optimizer: DistributedFusedAdam (
Parameter Group 0
    betas: (0.9, 0.999)
    bias_correction: True
    eps: 1e-08
    lr: 0.002875
    max_grad_norm: 5.0
    weight_decay: 0.0
)
:::MLLOG {"namespace": "", "time_ms": 1593113716247, "event_type": "POINT_IN_TIME", "key": "opt_name", "value": "adam", "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/trainer.py", "lineno": 176}}
:::MLLOG {"namespace": "", "time_ms": 1593113716247, "event_type": "POINT_IN_TIME", "key": "opt_base_learning_rate", "value": 0.002875, "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/trainer.py", "lineno": 178}}
:::MLLOG {"namespace": "", "time_ms": 1593113716247, "event_type": "POINT_IN_TIME", "key": "opt_adam_beta_1", "value": 0.9, "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/trainer.py", "lineno": 180}}
:::MLLOG {"namespace": "", "time_ms": 1593113716247, "event_type": "POINT_IN_TIME", "key": "opt_adam_beta_2", "value": 0.999, "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/trainer.py", "lineno": 182}}
:::MLLOG {"namespace": "", "time_ms": 1593113716248, "event_type": "POINT_IN_TIME", "key": "opt_adam_epsilon", "value": 1e-08, "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/trainer.py", "lineno": 184}}
0: Executing preallocation
:::MLLOG {"namespace": "", "time_ms": 1593113717706, "event_type": "INTERVAL_END", "key": "init_stop", "value": null, "metadata": {"file": "train.py", "lineno": 429}}
:::MLLOG {"namespace": "", "time_ms": 1593113717707, "event_type": "INTERVAL_START", "key": "run_start", "value": null, "metadata": {"file": "train.py", "lineno": 430}}
:::MLLOG {"namespace": "", "time_ms": 1593113717707, "event_type": "POINT_IN_TIME", "key": "max_sequence_length", "value": 75, "metadata": {"file": "train.py", "lineno": 435, "method": "discard"}}
0: Reading preprocessed data file from /preproc_data/75/training.bin
0: Preprocessed data: length: 3975116 min length: 0 max length: 75
0: Opening preprocessed data /preproc_data/75/training.bin for reading
0: Processing data from /data/newstest2014.tok.bpe.32000.en
0: Filtering data, min len: 0, max len: 150
0: Pairs before: 3003, after: 3003
:::MLLOG {"namespace": "", "time_ms": 1593113717962, "event_type": "POINT_IN_TIME", "key": "global_batch_size", "value": 3072, "metadata": {"file": "train.py", "lineno": 480}}
:::MLLOG {"namespace": "", "time_ms": 1593113717963, "event_type": "POINT_IN_TIME", "key": "train_samples", "value": 3965952, "metadata": {"file": "train.py", "lineno": 488}}
:::MLLOG {"namespace": "", "time_ms": 1593113717963, "event_type": "POINT_IN_TIME", "key": "eval_samples", "value": 3003, "metadata": {"file": "train.py", "lineno": 490}}
0: Training LR schedule config: {'warmup_steps': 200, 'remain_steps': 4054, 'decay_interval': 506, 'decay_steps': 4, 'decay_factor': 0.5}
0: Scheduler warmup steps: 200
0: Scheduler remain steps: 4054
0: Scheduler decay interval: 506
0: Scheduler decay factor: 0.5
0: Scheduler max decay steps: 4
:::MLLOG {"namespace": "", "time_ms": 1593113717963, "event_type": "POINT_IN_TIME", "key": "opt_learning_rate_alt_decay_func", "value": true, "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/lr_scheduler.py", "lineno": 78}}
:::MLLOG {"namespace": "", "time_ms": 1593113717964, "event_type": "POINT_IN_TIME", "key": "opt_learning_rate_alt_warmup_func", "value": true, "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/lr_scheduler.py", "lineno": 80}}
:::MLLOG {"namespace": "", "time_ms": 1593113717964, "event_type": "POINT_IN_TIME", "key": "opt_learning_rate_decay_interval", "value": 506, "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/lr_scheduler.py", "lineno": 83}}
:::MLLOG {"namespace": "", "time_ms": 1593113717964, "event_type": "POINT_IN_TIME", "key": "opt_learning_rate_decay_factor", "value": 0.5, "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/lr_scheduler.py", "lineno": 85}}
:::MLLOG {"namespace": "", "time_ms": 1593113717964, "event_type": "POINT_IN_TIME", "key": "opt_learning_rate_decay_steps", "value": 4, "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/lr_scheduler.py", "lineno": 87}}
:::MLLOG {"namespace": "", "time_ms": 1593113717964, "event_type": "POINT_IN_TIME", "key": "opt_learning_rate_remain_steps", "value": 4054, "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/lr_scheduler.py", "lineno": 89}}
:::MLLOG {"namespace": "", "time_ms": 1593113717964, "event_type": "POINT_IN_TIME", "key": "opt_learning_rate_warmup_steps", "value": 200, "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/lr_scheduler.py", "lineno": 91}}
:::MLLOG {"namespace": "", "time_ms": 1593113717964, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "train.py", "lineno": 538, "first_epoch_num": 1, "epoch_count": 1}}
:::MLLOG {"namespace": "", "time_ms": 1593113717965, "event_type": "INTERVAL_START", "key": "epoch_start", "value": null, "metadata": {"file": "train.py", "lineno": 541, "epoch_num": 1}}
0: Starting epoch 0
0: Sampler for epoch 0 uses seed 1394479699
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
0: TRAIN [0][0/1291]	Time 0.372 (0.372)	Data 1.91e-01 (1.91e-01)	Tok/s 19995 (19995)	Loss/tok 10.6469 (10.6469)	LR 2.942e-05
0: TRAIN [0][10/1291]	Time 0.059 (0.084)	Data 9.04e-05 (1.75e-02)	Tok/s 220915 (187738)	Loss/tok 9.6107 (9.9505)	LR 3.704e-05
0: TRAIN [0][20/1291]	Time 0.041 (0.071)	Data 8.08e-05 (9.20e-03)	Tok/s 190384 (196634)	Loss/tok 9.0590 (9.6579)	LR 4.663e-05
0: Gradient norm: inf
0: Skipped batch, new scale: 512.0
0: TRAIN [0][30/1291]	Time 0.058 (0.066)	Data 1.52e-04 (6.27e-03)	Tok/s 215096 (198996)	Loss/tok 8.9622 (9.4539)	LR 5.736e-05
0: TRAIN [0][40/1291]	Time 0.042 (0.063)	Data 8.51e-05 (4.76e-03)	Tok/s 186070 (199307)	Loss/tok 8.5857 (9.2901)	LR 7.222e-05
0: Gradient norm: inf
0: Skipped batch, new scale: 256.0
0: TRAIN [0][50/1291]	Time 0.026 (0.061)	Data 8.32e-05 (3.85e-03)	Tok/s 154879 (199346)	Loss/tok 8.0122 (9.1431)	LR 8.885e-05
0: Gradient norm: inf
0: Skipped batch, new scale: 128.0
0: TRAIN [0][60/1291]	Time 0.058 (0.059)	Data 1.01e-04 (3.23e-03)	Tok/s 218029 (198553)	Loss/tok 8.2104 (9.0196)	LR 1.093e-04
0: Gradient norm: inf
0: Skipped batch, new scale: 64.0
0: TRAIN [0][70/1291]	Time 0.025 (0.057)	Data 1.41e-04 (2.79e-03)	Tok/s 157809 (198126)	Loss/tok 7.5313 (8.9543)	LR 1.345e-04
0: TRAIN [0][80/1291]	Time 0.059 (0.056)	Data 8.25e-05 (2.46e-03)	Tok/s 215528 (197718)	Loss/tok 7.9867 (8.8499)	LR 1.693e-04
0: TRAIN [0][90/1291]	Time 0.042 (0.056)	Data 8.20e-05 (2.20e-03)	Tok/s 181565 (197932)	Loss/tok 7.7217 (8.7523)	LR 2.131e-04
0: TRAIN [0][100/1291]	Time 0.024 (0.055)	Data 8.20e-05 (1.99e-03)	Tok/s 160545 (198176)	Loss/tok 7.2149 (8.6744)	LR 2.683e-04
0: TRAIN [0][110/1291]	Time 0.058 (0.054)	Data 8.15e-05 (1.82e-03)	Tok/s 219004 (197231)	Loss/tok 8.0817 (8.6124)	LR 3.378e-04
0: TRAIN [0][120/1291]	Time 0.025 (0.054)	Data 8.20e-05 (1.68e-03)	Tok/s 166434 (197919)	Loss/tok 6.9664 (8.5466)	LR 4.252e-04
0: TRAIN [0][130/1291]	Time 0.025 (0.054)	Data 9.06e-05 (1.56e-03)	Tok/s 161733 (198101)	Loss/tok 6.9899 (8.4909)	LR 5.354e-04
0: TRAIN [0][140/1291]	Time 0.025 (0.054)	Data 8.15e-05 (1.45e-03)	Tok/s 160305 (197492)	Loss/tok 6.8464 (8.4432)	LR 6.740e-04
0: Gradient norm: inf
0: Skipped batch, new scale: 32.0
0: TRAIN [0][150/1291]	Time 0.058 (0.053)	Data 8.23e-05 (1.36e-03)	Tok/s 212766 (197849)	Loss/tok 7.6669 (8.3970)	LR 8.292e-04
0: TRAIN [0][160/1291]	Time 0.058 (0.054)	Data 8.39e-05 (1.28e-03)	Tok/s 213069 (198430)	Loss/tok 7.5038 (8.3418)	LR 1.044e-03
0: TRAIN [0][170/1291]	Time 0.096 (0.054)	Data 8.63e-05 (1.21e-03)	Tok/s 234712 (198702)	Loss/tok 7.5605 (8.2831)	LR 1.314e-03
0: TRAIN [0][180/1291]	Time 0.058 (0.054)	Data 8.44e-05 (1.15e-03)	Tok/s 214536 (199496)	Loss/tok 7.1184 (8.2166)	LR 1.654e-03
0: TRAIN [0][190/1291]	Time 0.096 (0.055)	Data 8.03e-05 (1.10e-03)	Tok/s 234027 (200178)	Loss/tok 7.1737 (8.1463)	LR 2.083e-03
0: TRAIN [0][200/1291]	Time 0.042 (0.055)	Data 1.38e-04 (1.05e-03)	Tok/s 183609 (200180)	Loss/tok 6.6706 (8.0820)	LR 2.622e-03
0: TRAIN [0][210/1291]	Time 0.042 (0.054)	Data 8.20e-05 (1.00e-03)	Tok/s 181740 (199746)	Loss/tok 6.5723 (8.0292)	LR 2.875e-03
0: TRAIN [0][220/1291]	Time 0.041 (0.054)	Data 1.39e-04 (9.60e-04)	Tok/s 189478 (199800)	Loss/tok 6.1572 (7.9661)	LR 2.875e-03
0: TRAIN [0][230/1291]	Time 0.024 (0.054)	Data 9.13e-05 (9.23e-04)	Tok/s 159764 (199601)	Loss/tok 5.2576 (7.9049)	LR 2.875e-03
0: TRAIN [0][240/1291]	Time 0.025 (0.053)	Data 8.27e-05 (8.89e-04)	Tok/s 159368 (199447)	Loss/tok 5.2142 (7.8439)	LR 2.875e-03
0: TRAIN [0][250/1291]	Time 0.058 (0.053)	Data 8.18e-05 (8.57e-04)	Tok/s 219867 (199046)	Loss/tok 6.0821 (7.7853)	LR 2.875e-03
0: TRAIN [0][260/1291]	Time 0.059 (0.054)	Data 1.06e-04 (8.29e-04)	Tok/s 216004 (199766)	Loss/tok 6.0104 (7.7019)	LR 2.875e-03
0: Upscaling, new scale: 64.0
0: TRAIN [0][270/1291]	Time 0.059 (0.053)	Data 8.06e-05 (8.02e-04)	Tok/s 213056 (199757)	Loss/tok 5.8833 (7.6353)	LR 2.875e-03
0: TRAIN [0][280/1291]	Time 0.041 (0.053)	Data 8.56e-05 (7.77e-04)	Tok/s 186842 (199720)	Loss/tok 5.5235 (7.5692)	LR 2.875e-03
0: TRAIN [0][290/1291]	Time 0.042 (0.053)	Data 8.15e-05 (7.53e-04)	Tok/s 186181 (199892)	Loss/tok 5.3461 (7.4998)	LR 2.875e-03
0: TRAIN [0][300/1291]	Time 0.058 (0.053)	Data 1.23e-04 (7.31e-04)	Tok/s 217808 (199969)	Loss/tok 5.5291 (7.4336)	LR 2.875e-03
0: TRAIN [0][310/1291]	Time 0.042 (0.053)	Data 8.13e-05 (7.10e-04)	Tok/s 181697 (199961)	Loss/tok 5.1000 (7.3691)	LR 2.875e-03
0: TRAIN [0][320/1291]	Time 0.041 (0.053)	Data 8.11e-05 (6.91e-04)	Tok/s 189036 (199676)	Loss/tok 4.9665 (7.3145)	LR 2.875e-03
0: TRAIN [0][330/1291]	Time 0.041 (0.053)	Data 7.92e-05 (6.73e-04)	Tok/s 187871 (199598)	Loss/tok 4.9089 (7.2548)	LR 2.875e-03
0: TRAIN [0][340/1291]	Time 0.058 (0.053)	Data 9.80e-05 (6.56e-04)	Tok/s 217043 (199313)	Loss/tok 5.0162 (7.2000)	LR 2.875e-03
0: TRAIN [0][350/1291]	Time 0.058 (0.053)	Data 8.11e-05 (6.40e-04)	Tok/s 215623 (199405)	Loss/tok 4.9836 (7.1374)	LR 2.875e-03
0: TRAIN [0][360/1291]	Time 0.042 (0.053)	Data 8.32e-05 (6.25e-04)	Tok/s 183844 (199527)	Loss/tok 4.6025 (7.0732)	LR 2.875e-03
0: TRAIN [0][370/1291]	Time 0.058 (0.053)	Data 8.20e-05 (6.11e-04)	Tok/s 217898 (199675)	Loss/tok 4.8383 (7.0078)	LR 2.875e-03
0: TRAIN [0][380/1291]	Time 0.076 (0.053)	Data 8.54e-05 (5.97e-04)	Tok/s 233472 (199590)	Loss/tok 4.8152 (6.9508)	LR 2.875e-03
0: TRAIN [0][390/1291]	Time 0.042 (0.053)	Data 8.68e-05 (5.84e-04)	Tok/s 184123 (199754)	Loss/tok 4.4156 (6.8878)	LR 2.875e-03
0: Upscaling, new scale: 128.0
0: TRAIN [0][400/1291]	Time 0.042 (0.053)	Data 8.20e-05 (5.72e-04)	Tok/s 186978 (199714)	Loss/tok 4.3915 (6.8322)	LR 2.875e-03
0: TRAIN [0][410/1291]	Time 0.042 (0.053)	Data 9.97e-05 (5.61e-04)	Tok/s 188492 (200046)	Loss/tok 4.1304 (6.7664)	LR 2.875e-03
0: TRAIN [0][420/1291]	Time 0.058 (0.053)	Data 9.63e-05 (5.49e-04)	Tok/s 217033 (199942)	Loss/tok 4.5123 (6.7174)	LR 2.875e-03
0: TRAIN [0][430/1291]	Time 0.058 (0.053)	Data 8.42e-05 (5.39e-04)	Tok/s 213757 (200242)	Loss/tok 4.5040 (6.6563)	LR 2.875e-03
0: TRAIN [0][440/1291]	Time 0.041 (0.053)	Data 8.06e-05 (5.29e-04)	Tok/s 189895 (200316)	Loss/tok 4.2390 (6.6028)	LR 2.875e-03
0: TRAIN [0][450/1291]	Time 0.041 (0.053)	Data 1.40e-04 (5.20e-04)	Tok/s 188244 (200284)	Loss/tok 4.1022 (6.5536)	LR 2.875e-03
0: TRAIN [0][460/1291]	Time 0.097 (0.053)	Data 8.11e-05 (5.10e-04)	Tok/s 230728 (200623)	Loss/tok 4.6795 (6.4922)	LR 2.875e-03
0: TRAIN [0][470/1291]	Time 0.076 (0.053)	Data 8.11e-05 (5.01e-04)	Tok/s 227430 (200744)	Loss/tok 4.4992 (6.4426)	LR 2.875e-03
0: TRAIN [0][480/1291]	Time 0.042 (0.053)	Data 1.39e-04 (4.93e-04)	Tok/s 189574 (200879)	Loss/tok 4.1057 (6.3946)	LR 2.875e-03
0: TRAIN [0][490/1291]	Time 0.042 (0.053)	Data 8.70e-05 (4.85e-04)	Tok/s 187031 (200882)	Loss/tok 3.8904 (6.3516)	LR 2.875e-03
0: TRAIN [0][500/1291]	Time 0.058 (0.053)	Data 1.38e-04 (4.77e-04)	Tok/s 217195 (201102)	Loss/tok 4.2257 (6.3035)	LR 2.875e-03
0: TRAIN [0][510/1291]	Time 0.042 (0.053)	Data 8.46e-05 (4.70e-04)	Tok/s 188565 (201273)	Loss/tok 3.8743 (6.2561)	LR 2.875e-03
0: TRAIN [0][520/1291]	Time 0.042 (0.053)	Data 8.37e-05 (4.63e-04)	Tok/s 184060 (201206)	Loss/tok 3.7519 (6.2166)	LR 2.875e-03
0: Upscaling, new scale: 256.0
0: TRAIN [0][530/1291]	Time 0.042 (0.053)	Data 7.89e-05 (4.56e-04)	Tok/s 184593 (201424)	Loss/tok 3.7808 (6.1690)	LR 2.875e-03
0: TRAIN [0][540/1291]	Time 0.041 (0.054)	Data 1.35e-04 (4.49e-04)	Tok/s 189057 (201521)	Loss/tok 3.8682 (6.1271)	LR 2.875e-03
0: TRAIN [0][550/1291]	Time 0.041 (0.054)	Data 8.42e-05 (4.43e-04)	Tok/s 186498 (201589)	Loss/tok 3.6658 (6.0868)	LR 2.875e-03
0: TRAIN [0][560/1291]	Time 0.077 (0.054)	Data 8.63e-05 (4.36e-04)	Tok/s 227533 (201560)	Loss/tok 4.2919 (6.0505)	LR 2.875e-03
0: TRAIN [0][570/1291]	Time 0.058 (0.054)	Data 8.13e-05 (4.30e-04)	Tok/s 215001 (201541)	Loss/tok 4.1787 (6.0172)	LR 2.875e-03
0: TRAIN [0][580/1291]	Time 0.058 (0.053)	Data 8.03e-05 (4.24e-04)	Tok/s 215930 (201409)	Loss/tok 3.9779 (5.9873)	LR 2.875e-03
0: TRAIN [0][590/1291]	Time 0.042 (0.053)	Data 1.01e-04 (4.19e-04)	Tok/s 186628 (201249)	Loss/tok 3.7265 (5.9590)	LR 2.875e-03
0: TRAIN [0][600/1291]	Time 0.042 (0.053)	Data 8.32e-05 (4.13e-04)	Tok/s 186171 (201154)	Loss/tok 3.6865 (5.9293)	LR 2.875e-03
0: TRAIN [0][610/1291]	Time 0.076 (0.053)	Data 8.11e-05 (4.08e-04)	Tok/s 229134 (201272)	Loss/tok 4.2618 (5.8955)	LR 2.875e-03
0: TRAIN [0][620/1291]	Time 0.058 (0.053)	Data 8.46e-05 (4.03e-04)	Tok/s 217700 (201322)	Loss/tok 3.9961 (5.8639)	LR 2.875e-03
0: TRAIN [0][630/1291]	Time 0.042 (0.053)	Data 9.16e-05 (3.98e-04)	Tok/s 185761 (201480)	Loss/tok 3.7965 (5.8295)	LR 2.875e-03
0: TRAIN [0][640/1291]	Time 0.041 (0.053)	Data 8.30e-05 (3.93e-04)	Tok/s 183366 (201305)	Loss/tok 3.8171 (5.8048)	LR 2.875e-03
0: TRAIN [0][650/1291]	Time 0.041 (0.053)	Data 8.39e-05 (3.89e-04)	Tok/s 186278 (201273)	Loss/tok 3.5846 (5.7772)	LR 2.875e-03
0: Upscaling, new scale: 512.0
0: TRAIN [0][660/1291]	Time 0.042 (0.053)	Data 1.38e-04 (3.84e-04)	Tok/s 183632 (201439)	Loss/tok 3.5972 (5.7456)	LR 2.875e-03
0: TRAIN [0][670/1291]	Time 0.042 (0.053)	Data 8.34e-05 (3.80e-04)	Tok/s 187877 (201474)	Loss/tok 3.7098 (5.7180)	LR 2.875e-03
0: TRAIN [0][680/1291]	Time 0.058 (0.053)	Data 1.38e-04 (3.76e-04)	Tok/s 213999 (201396)	Loss/tok 3.8594 (5.6919)	LR 2.875e-03
0: TRAIN [0][690/1291]	Time 0.058 (0.053)	Data 1.38e-04 (3.72e-04)	Tok/s 213309 (201365)	Loss/tok 3.8866 (5.6665)	LR 2.875e-03
0: TRAIN [0][700/1291]	Time 0.042 (0.054)	Data 9.94e-05 (3.68e-04)	Tok/s 187011 (201482)	Loss/tok 3.5904 (5.6365)	LR 2.875e-03
0: TRAIN [0][710/1291]	Time 0.059 (0.054)	Data 8.58e-05 (3.64e-04)	Tok/s 219146 (201435)	Loss/tok 3.8553 (5.6121)	LR 2.875e-03
0: TRAIN [0][720/1291]	Time 0.042 (0.054)	Data 8.65e-05 (3.60e-04)	Tok/s 185242 (201456)	Loss/tok 3.5522 (5.5871)	LR 2.875e-03
0: TRAIN [0][730/1291]	Time 0.041 (0.053)	Data 8.30e-05 (3.56e-04)	Tok/s 191209 (201398)	Loss/tok 3.4586 (5.5647)	LR 2.875e-03
0: TRAIN [0][740/1291]	Time 0.042 (0.053)	Data 8.34e-05 (3.53e-04)	Tok/s 185391 (201317)	Loss/tok 3.7298 (5.5438)	LR 2.875e-03
0: TRAIN [0][750/1291]	Time 0.059 (0.054)	Data 7.92e-05 (3.49e-04)	Tok/s 211876 (201410)	Loss/tok 3.8804 (5.5179)	LR 2.875e-03
0: TRAIN [0][760/1291]	Time 0.041 (0.053)	Data 8.23e-05 (3.46e-04)	Tok/s 189854 (201321)	Loss/tok 3.4829 (5.4973)	LR 2.875e-03
0: TRAIN [0][770/1291]	Time 0.058 (0.053)	Data 8.23e-05 (3.43e-04)	Tok/s 214485 (201405)	Loss/tok 3.7912 (5.4736)	LR 2.875e-03
0: TRAIN [0][780/1291]	Time 0.059 (0.054)	Data 8.61e-05 (3.39e-04)	Tok/s 214010 (201482)	Loss/tok 3.8516 (5.4501)	LR 2.875e-03
0: Upscaling, new scale: 1024.0
0: TRAIN [0][790/1291]	Time 0.076 (0.054)	Data 8.49e-05 (3.36e-04)	Tok/s 230078 (201586)	Loss/tok 4.0560 (5.4282)	LR 2.875e-03
0: TRAIN [0][800/1291]	Time 0.076 (0.054)	Data 8.85e-05 (3.33e-04)	Tok/s 226748 (201540)	Loss/tok 3.9732 (5.4086)	LR 2.875e-03
0: TRAIN [0][810/1291]	Time 0.041 (0.053)	Data 8.18e-05 (3.30e-04)	Tok/s 187361 (201479)	Loss/tok 3.6011 (5.3900)	LR 2.875e-03
0: TRAIN [0][820/1291]	Time 0.042 (0.053)	Data 8.94e-05 (3.27e-04)	Tok/s 183366 (201472)	Loss/tok 3.5369 (5.3699)	LR 2.875e-03
0: TRAIN [0][830/1291]	Time 0.042 (0.054)	Data 7.96e-05 (3.24e-04)	Tok/s 184145 (201547)	Loss/tok 3.5540 (5.3487)	LR 2.875e-03
0: TRAIN [0][840/1291]	Time 0.076 (0.054)	Data 9.11e-05 (3.22e-04)	Tok/s 230686 (201573)	Loss/tok 3.9411 (5.3298)	LR 2.875e-03
0: TRAIN [0][850/1291]	Time 0.058 (0.053)	Data 8.08e-05 (3.19e-04)	Tok/s 217729 (201422)	Loss/tok 3.7045 (5.3147)	LR 2.875e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 512.0
0: TRAIN [0][860/1291]	Time 0.076 (0.053)	Data 8.30e-05 (3.16e-04)	Tok/s 231206 (201428)	Loss/tok 3.8623 (5.2964)	LR 2.875e-03
0: TRAIN [0][870/1291]	Time 0.041 (0.053)	Data 1.36e-04 (3.14e-04)	Tok/s 185620 (201278)	Loss/tok 3.5380 (5.2811)	LR 2.875e-03
0: TRAIN [0][880/1291]	Time 0.058 (0.053)	Data 1.03e-04 (3.11e-04)	Tok/s 216342 (201398)	Loss/tok 3.6727 (5.2611)	LR 2.875e-03
0: TRAIN [0][890/1291]	Time 0.059 (0.053)	Data 1.01e-04 (3.09e-04)	Tok/s 209922 (201428)	Loss/tok 3.8529 (5.2437)	LR 2.875e-03
0: TRAIN [0][900/1291]	Time 0.077 (0.053)	Data 1.44e-04 (3.06e-04)	Tok/s 232536 (201431)	Loss/tok 3.8108 (5.2266)	LR 2.875e-03
0: TRAIN [0][910/1291]	Time 0.059 (0.053)	Data 1.28e-04 (3.04e-04)	Tok/s 214359 (201376)	Loss/tok 3.7026 (5.2109)	LR 2.875e-03
0: TRAIN [0][920/1291]	Time 0.059 (0.053)	Data 7.92e-05 (3.02e-04)	Tok/s 213671 (201372)	Loss/tok 3.7184 (5.1950)	LR 2.875e-03
0: TRAIN [0][930/1291]	Time 0.042 (0.053)	Data 8.23e-05 (3.00e-04)	Tok/s 183978 (201325)	Loss/tok 3.3800 (5.1800)	LR 2.875e-03
0: TRAIN [0][940/1291]	Time 0.042 (0.053)	Data 8.03e-05 (2.97e-04)	Tok/s 186516 (201352)	Loss/tok 3.3466 (5.1636)	LR 2.875e-03
0: TRAIN [0][950/1291]	Time 0.025 (0.053)	Data 8.08e-05 (2.95e-04)	Tok/s 159733 (201341)	Loss/tok 3.0307 (5.1483)	LR 2.875e-03
0: TRAIN [0][960/1291]	Time 0.041 (0.053)	Data 8.15e-05 (2.93e-04)	Tok/s 182455 (201339)	Loss/tok 3.4970 (5.1334)	LR 2.875e-03
0: TRAIN [0][970/1291]	Time 0.059 (0.053)	Data 8.25e-05 (2.91e-04)	Tok/s 213777 (201374)	Loss/tok 3.4468 (5.1182)	LR 2.875e-03
0: TRAIN [0][980/1291]	Time 0.058 (0.053)	Data 8.68e-05 (2.89e-04)	Tok/s 214752 (201367)	Loss/tok 3.7489 (5.1042)	LR 2.875e-03
0: Upscaling, new scale: 1024.0
0: TRAIN [0][990/1291]	Time 0.058 (0.053)	Data 8.18e-05 (2.87e-04)	Tok/s 214397 (201241)	Loss/tok 3.6894 (5.0918)	LR 2.875e-03
0: TRAIN [0][1000/1291]	Time 0.041 (0.053)	Data 8.08e-05 (2.85e-04)	Tok/s 187036 (201143)	Loss/tok 3.4510 (5.0795)	LR 2.875e-03
0: TRAIN [0][1010/1291]	Time 0.041 (0.053)	Data 9.13e-05 (2.83e-04)	Tok/s 191232 (201236)	Loss/tok 3.3931 (5.0645)	LR 2.875e-03
0: TRAIN [0][1020/1291]	Time 0.059 (0.053)	Data 8.39e-05 (2.81e-04)	Tok/s 215675 (201266)	Loss/tok 3.8094 (5.0502)	LR 2.875e-03
0: TRAIN [0][1030/1291]	Time 0.058 (0.053)	Data 1.37e-04 (2.79e-04)	Tok/s 221434 (201194)	Loss/tok 3.7023 (5.0382)	LR 2.875e-03
0: TRAIN [0][1040/1291]	Time 0.058 (0.053)	Data 8.01e-05 (2.77e-04)	Tok/s 215644 (201297)	Loss/tok 3.4847 (5.0232)	LR 2.875e-03
0: TRAIN [0][1050/1291]	Time 0.076 (0.053)	Data 8.18e-05 (2.76e-04)	Tok/s 231963 (201254)	Loss/tok 3.8770 (5.0107)	LR 2.875e-03
0: TRAIN [0][1060/1291]	Time 0.058 (0.053)	Data 8.25e-05 (2.74e-04)	Tok/s 217868 (201218)	Loss/tok 3.6358 (4.9989)	LR 2.875e-03
0: TRAIN [0][1070/1291]	Time 0.076 (0.053)	Data 1.34e-04 (2.72e-04)	Tok/s 226683 (201177)	Loss/tok 3.7028 (4.9869)	LR 2.875e-03
0: TRAIN [0][1080/1291]	Time 0.058 (0.053)	Data 8.25e-05 (2.71e-04)	Tok/s 210693 (201210)	Loss/tok 3.6637 (4.9736)	LR 2.875e-03
0: TRAIN [0][1090/1291]	Time 0.041 (0.053)	Data 8.11e-05 (2.69e-04)	Tok/s 183335 (201050)	Loss/tok 3.2927 (4.9634)	LR 2.875e-03
0: TRAIN [0][1100/1291]	Time 0.076 (0.053)	Data 8.37e-05 (2.67e-04)	Tok/s 230405 (200961)	Loss/tok 3.7827 (4.9526)	LR 2.875e-03
0: Upscaling, new scale: 2048.0
0: TRAIN [0][1110/1291]	Time 0.042 (0.053)	Data 8.70e-05 (2.66e-04)	Tok/s 186553 (200980)	Loss/tok 3.4309 (4.9402)	LR 2.875e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 1024.0
0: TRAIN [0][1120/1291]	Time 0.025 (0.053)	Data 8.01e-05 (2.64e-04)	Tok/s 158411 (201035)	Loss/tok 2.7825 (4.9275)	LR 2.875e-03
0: TRAIN [0][1130/1291]	Time 0.042 (0.053)	Data 8.03e-05 (2.63e-04)	Tok/s 186363 (201021)	Loss/tok 3.3312 (4.9157)	LR 2.875e-03
0: TRAIN [0][1140/1291]	Time 0.042 (0.053)	Data 1.39e-04 (2.61e-04)	Tok/s 188461 (201054)	Loss/tok 3.3142 (4.9038)	LR 2.875e-03
0: TRAIN [0][1150/1291]	Time 0.025 (0.053)	Data 8.42e-05 (2.60e-04)	Tok/s 162805 (200974)	Loss/tok 2.8249 (4.8937)	LR 2.875e-03
0: TRAIN [0][1160/1291]	Time 0.058 (0.053)	Data 7.41e-05 (2.58e-04)	Tok/s 215480 (200916)	Loss/tok 3.4773 (4.8834)	LR 2.875e-03
0: TRAIN [0][1170/1291]	Time 0.059 (0.053)	Data 7.68e-05 (2.57e-04)	Tok/s 217232 (200934)	Loss/tok 3.5996 (4.8720)	LR 2.875e-03
0: TRAIN [0][1180/1291]	Time 0.058 (0.053)	Data 1.17e-04 (2.55e-04)	Tok/s 216041 (200941)	Loss/tok 3.5642 (4.8611)	LR 2.875e-03
0: TRAIN [0][1190/1291]	Time 0.058 (0.053)	Data 8.94e-05 (2.54e-04)	Tok/s 216689 (200915)	Loss/tok 3.5974 (4.8515)	LR 2.875e-03
0: TRAIN [0][1200/1291]	Time 0.025 (0.053)	Data 7.99e-05 (2.53e-04)	Tok/s 154955 (200891)	Loss/tok 2.9376 (4.8412)	LR 2.875e-03
0: TRAIN [0][1210/1291]	Time 0.042 (0.053)	Data 7.49e-05 (2.51e-04)	Tok/s 182211 (200831)	Loss/tok 3.3752 (4.8316)	LR 2.875e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 512.0
0: TRAIN [0][1220/1291]	Time 0.077 (0.053)	Data 8.54e-05 (2.50e-04)	Tok/s 227346 (200867)	Loss/tok 3.8411 (4.8209)	LR 2.875e-03
0: TRAIN [0][1230/1291]	Time 0.041 (0.053)	Data 8.15e-05 (2.48e-04)	Tok/s 191559 (200945)	Loss/tok 3.3790 (4.8096)	LR 2.875e-03
0: TRAIN [0][1240/1291]	Time 0.042 (0.053)	Data 7.49e-05 (2.47e-04)	Tok/s 180783 (200917)	Loss/tok 3.2792 (4.7999)	LR 2.875e-03
0: TRAIN [0][1250/1291]	Time 0.042 (0.053)	Data 7.18e-05 (2.46e-04)	Tok/s 185475 (200891)	Loss/tok 3.3431 (4.7902)	LR 2.875e-03
0: TRAIN [0][1260/1291]	Time 0.097 (0.053)	Data 7.70e-05 (2.44e-04)	Tok/s 232683 (200898)	Loss/tok 4.0683 (4.7807)	LR 2.875e-03
0: TRAIN [0][1270/1291]	Time 0.041 (0.053)	Data 7.70e-05 (2.43e-04)	Tok/s 194552 (200979)	Loss/tok 3.2538 (4.7701)	LR 2.875e-03
0: TRAIN [0][1280/1291]	Time 0.059 (0.053)	Data 7.63e-05 (2.42e-04)	Tok/s 214693 (200974)	Loss/tok 3.6654 (4.7610)	LR 2.875e-03
0: TRAIN [0][1290/1291]	Time 0.059 (0.053)	Data 4.27e-05 (2.42e-04)	Tok/s 215593 (201033)	Loss/tok 3.6124 (4.7505)	LR 2.875e-03
:::MLLOG {"namespace": "", "time_ms": 1593113786388, "event_type": "INTERVAL_END", "key": "epoch_stop", "value": null, "metadata": {"file": "train.py", "lineno": 551, "epoch_num": 1}}
:::MLLOG {"namespace": "", "time_ms": 1593113786389, "event_type": "INTERVAL_START", "key": "eval_start", "value": null, "metadata": {"file": "train.py", "lineno": 556, "epoch_num": 1}}
0: Running evaluation on test set
0: TEST [0][0/3]	Time 0.393 (0.393)	Decoder iters 149.0 (149.0)	Tok/s 22627 (22627)
0: Running moses detokenizer
0: BLEU(score=19.734428948068956, counts=[34596, 15796, 8350, 4588], totals=[65548, 62545, 59543, 56546], precisions=[52.779642399462986, 25.255416100407707, 14.023478830425072, 8.113748098892938], bp=1.0, sys_len=65548, ref_len=64676)
0: Finished evaluation on test set
:::MLLOG {"namespace": "", "time_ms": 1593113787671, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.1973, "metadata": {"file": "train.py", "lineno": 562, "epoch_num": 1}}
:::MLLOG {"namespace": "", "time_ms": 1593113787671, "event_type": "INTERVAL_END", "key": "eval_stop", "value": null, "metadata": {"file": "train.py", "lineno": 565, "epoch_num": 1}}
0: Summary: Epoch: 0	Training Loss: 4.7493	Test BLEU: 19.73
0: Performance: Epoch: 0	Training: 3215183 Tok/s
0: Finished epoch 0
:::MLLOG {"namespace": "", "time_ms": 1593113787671, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "train.py", "lineno": 584, "first_epoch_num": 1}}
:::MLLOG {"namespace": "", "time_ms": 1593113787672, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "train.py", "lineno": 538, "first_epoch_num": 2, "epoch_count": 1}}
:::MLLOG {"namespace": "", "time_ms": 1593113787672, "event_type": "INTERVAL_START", "key": "epoch_start", "value": null, "metadata": {"file": "train.py", "lineno": 541, "epoch_num": 2}}
0: Starting epoch 1
0: Sampler for epoch 1 uses seed 229627625
0: TRAIN [1][0/1291]	Time 0.288 (0.288)	Data 1.65e-01 (1.65e-01)	Tok/s 26775 (26775)	Loss/tok 3.1168 (3.1168)	LR 2.875e-03
0: TRAIN [1][10/1291]	Time 0.076 (0.074)	Data 8.34e-05 (1.50e-02)	Tok/s 229920 (181367)	Loss/tok 3.8266 (3.4954)	LR 2.875e-03
0: TRAIN [1][20/1291]	Time 0.097 (0.067)	Data 9.04e-05 (7.92e-03)	Tok/s 230619 (195203)	Loss/tok 3.8874 (3.5411)	LR 2.875e-03
0: TRAIN [1][30/1291]	Time 0.041 (0.063)	Data 8.44e-05 (5.40e-03)	Tok/s 186046 (197100)	Loss/tok 3.1784 (3.5370)	LR 2.875e-03
0: TRAIN [1][40/1291]	Time 0.058 (0.061)	Data 8.65e-05 (4.10e-03)	Tok/s 219714 (198041)	Loss/tok 3.4681 (3.5475)	LR 2.875e-03
0: TRAIN [1][50/1291]	Time 0.058 (0.060)	Data 8.96e-05 (3.32e-03)	Tok/s 215814 (199200)	Loss/tok 3.6466 (3.5452)	LR 2.875e-03
0: Upscaling, new scale: 1024.0
0: TRAIN [1][60/1291]	Time 0.058 (0.057)	Data 8.37e-05 (2.79e-03)	Tok/s 216578 (197060)	Loss/tok 3.5275 (3.5151)	LR 2.875e-03
0: TRAIN [1][70/1291]	Time 0.025 (0.056)	Data 9.11e-05 (2.41e-03)	Tok/s 155211 (197259)	Loss/tok 2.8254 (3.5063)	LR 2.875e-03
0: TRAIN [1][80/1291]	Time 0.097 (0.056)	Data 9.18e-05 (2.13e-03)	Tok/s 229499 (198130)	Loss/tok 3.9115 (3.5151)	LR 2.875e-03
0: TRAIN [1][90/1291]	Time 0.042 (0.055)	Data 8.25e-05 (1.90e-03)	Tok/s 184619 (198189)	Loss/tok 3.2218 (3.5067)	LR 2.875e-03
0: TRAIN [1][100/1291]	Time 0.058 (0.055)	Data 8.75e-05 (1.72e-03)	Tok/s 215935 (198443)	Loss/tok 3.6108 (3.5005)	LR 2.875e-03
0: TRAIN [1][110/1291]	Time 0.059 (0.055)	Data 1.05e-04 (1.58e-03)	Tok/s 212987 (198891)	Loss/tok 3.3966 (3.5027)	LR 2.875e-03
0: TRAIN [1][120/1291]	Time 0.041 (0.055)	Data 1.41e-04 (1.45e-03)	Tok/s 190957 (199374)	Loss/tok 3.2579 (3.5036)	LR 2.875e-03
0: TRAIN [1][130/1291]	Time 0.042 (0.054)	Data 8.18e-05 (1.35e-03)	Tok/s 186103 (198681)	Loss/tok 3.3159 (3.4952)	LR 2.875e-03
0: TRAIN [1][140/1291]	Time 0.042 (0.054)	Data 8.82e-05 (1.26e-03)	Tok/s 186143 (198360)	Loss/tok 3.2174 (3.4880)	LR 2.875e-03
0: TRAIN [1][150/1291]	Time 0.041 (0.053)	Data 8.49e-05 (1.18e-03)	Tok/s 190070 (197747)	Loss/tok 3.2251 (3.4751)	LR 2.875e-03
0: TRAIN [1][160/1291]	Time 0.041 (0.053)	Data 1.46e-04 (1.12e-03)	Tok/s 189077 (197579)	Loss/tok 3.2805 (3.4748)	LR 2.875e-03
0: TRAIN [1][170/1291]	Time 0.025 (0.053)	Data 8.49e-05 (1.06e-03)	Tok/s 161738 (198065)	Loss/tok 2.7965 (3.4763)	LR 2.875e-03
0: TRAIN [1][180/1291]	Time 0.042 (0.053)	Data 1.39e-04 (1.00e-03)	Tok/s 185058 (198672)	Loss/tok 3.2754 (3.4778)	LR 2.875e-03
0: Upscaling, new scale: 2048.0
0: TRAIN [1][190/1291]	Time 0.059 (0.053)	Data 8.37e-05 (9.55e-04)	Tok/s 217970 (198730)	Loss/tok 3.4471 (3.4754)	LR 2.875e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 1024.0
0: TRAIN [1][200/1291]	Time 0.042 (0.053)	Data 8.68e-05 (9.13e-04)	Tok/s 185780 (199031)	Loss/tok 3.2516 (3.4779)	LR 2.875e-03
0: TRAIN [1][210/1291]	Time 0.059 (0.053)	Data 8.11e-05 (8.74e-04)	Tok/s 216271 (199391)	Loss/tok 3.5209 (3.4763)	LR 2.875e-03
0: TRAIN [1][220/1291]	Time 0.042 (0.053)	Data 8.32e-05 (8.38e-04)	Tok/s 185099 (198951)	Loss/tok 3.2977 (3.4725)	LR 2.875e-03
0: TRAIN [1][230/1291]	Time 0.058 (0.053)	Data 9.06e-05 (8.06e-04)	Tok/s 215045 (198990)	Loss/tok 3.3337 (3.4716)	LR 2.875e-03
0: TRAIN [1][240/1291]	Time 0.042 (0.053)	Data 8.65e-05 (7.76e-04)	Tok/s 184153 (198702)	Loss/tok 3.2492 (3.4662)	LR 2.875e-03
0: TRAIN [1][250/1291]	Time 0.058 (0.053)	Data 9.08e-05 (7.49e-04)	Tok/s 215103 (199493)	Loss/tok 3.4559 (3.4768)	LR 2.875e-03
0: TRAIN [1][260/1291]	Time 0.042 (0.053)	Data 8.85e-05 (7.23e-04)	Tok/s 185613 (199074)	Loss/tok 3.2635 (3.4721)	LR 2.875e-03
0: TRAIN [1][270/1291]	Time 0.076 (0.053)	Data 8.61e-05 (7.00e-04)	Tok/s 227155 (199472)	Loss/tok 3.5926 (3.4772)	LR 2.875e-03
0: TRAIN [1][280/1291]	Time 0.059 (0.053)	Data 1.43e-04 (6.79e-04)	Tok/s 214474 (199598)	Loss/tok 3.4714 (3.4812)	LR 2.875e-03
0: TRAIN [1][290/1291]	Time 0.041 (0.053)	Data 8.75e-05 (6.59e-04)	Tok/s 187946 (199425)	Loss/tok 3.2226 (3.4797)	LR 2.875e-03
0: TRAIN [1][300/1291]	Time 0.058 (0.053)	Data 9.01e-05 (6.40e-04)	Tok/s 215075 (199696)	Loss/tok 3.4538 (3.4817)	LR 2.875e-03
0: TRAIN [1][310/1291]	Time 0.041 (0.053)	Data 8.77e-05 (6.22e-04)	Tok/s 191283 (199629)	Loss/tok 3.1719 (3.4825)	LR 2.875e-03
0: TRAIN [1][320/1291]	Time 0.041 (0.053)	Data 9.47e-05 (6.06e-04)	Tok/s 189810 (199409)	Loss/tok 3.2043 (3.4781)	LR 2.875e-03
0: Upscaling, new scale: 2048.0
0: TRAIN [1][330/1291]	Time 0.059 (0.053)	Data 8.27e-05 (5.91e-04)	Tok/s 213266 (199729)	Loss/tok 3.5725 (3.4789)	LR 2.875e-03
0: TRAIN [1][340/1291]	Time 0.041 (0.053)	Data 8.42e-05 (5.76e-04)	Tok/s 190306 (199340)	Loss/tok 3.2575 (3.4744)	LR 2.875e-03
0: TRAIN [1][350/1291]	Time 0.041 (0.053)	Data 8.96e-05 (5.62e-04)	Tok/s 185116 (199171)	Loss/tok 3.0961 (3.4712)	LR 2.875e-03
0: TRAIN [1][360/1291]	Time 0.041 (0.053)	Data 8.58e-05 (5.49e-04)	Tok/s 186474 (199264)	Loss/tok 3.2245 (3.4731)	LR 2.875e-03
0: TRAIN [1][370/1291]	Time 0.042 (0.053)	Data 8.51e-05 (5.37e-04)	Tok/s 186776 (199478)	Loss/tok 3.1999 (3.4724)	LR 2.875e-03
0: TRAIN [1][380/1291]	Time 0.042 (0.053)	Data 1.08e-04 (5.25e-04)	Tok/s 185049 (199609)	Loss/tok 3.2747 (3.4723)	LR 2.875e-03
0: TRAIN [1][390/1291]	Time 0.042 (0.053)	Data 8.34e-05 (5.14e-04)	Tok/s 188206 (199352)	Loss/tok 3.2804 (3.4684)	LR 2.875e-03
0: TRAIN [1][400/1291]	Time 0.059 (0.053)	Data 9.06e-05 (5.03e-04)	Tok/s 217084 (199480)	Loss/tok 3.4769 (3.4715)	LR 2.875e-03
0: TRAIN [1][410/1291]	Time 0.042 (0.053)	Data 8.73e-05 (4.93e-04)	Tok/s 185817 (199418)	Loss/tok 3.1919 (3.4694)	LR 2.875e-03
0: TRAIN [1][420/1291]	Time 0.077 (0.053)	Data 8.13e-05 (4.84e-04)	Tok/s 225325 (199648)	Loss/tok 3.6559 (3.4685)	LR 2.875e-03
0: TRAIN [1][430/1291]	Time 0.077 (0.053)	Data 8.37e-05 (4.75e-04)	Tok/s 224910 (199789)	Loss/tok 3.6780 (3.4682)	LR 2.875e-03
0: TRAIN [1][440/1291]	Time 0.041 (0.053)	Data 8.63e-05 (4.66e-04)	Tok/s 187131 (199714)	Loss/tok 3.1267 (3.4654)	LR 2.875e-03
0: TRAIN [1][450/1291]	Time 0.041 (0.053)	Data 1.43e-04 (4.58e-04)	Tok/s 191055 (199432)	Loss/tok 3.1246 (3.4613)	LR 2.875e-03
0: Upscaling, new scale: 4096.0
0: TRAIN [1][460/1291]	Time 0.076 (0.053)	Data 1.38e-04 (4.50e-04)	Tok/s 227676 (199345)	Loss/tok 3.6895 (3.4586)	LR 2.875e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 2048.0
0: TRAIN [1][470/1291]	Time 0.097 (0.053)	Data 8.87e-05 (4.43e-04)	Tok/s 228711 (199531)	Loss/tok 3.8115 (3.4619)	LR 2.875e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 1024.0
0: TRAIN [1][480/1291]	Time 0.059 (0.053)	Data 9.32e-05 (4.35e-04)	Tok/s 217668 (199839)	Loss/tok 3.3529 (3.4664)	LR 2.875e-03
0: TRAIN [1][490/1291]	Time 0.076 (0.053)	Data 8.89e-05 (4.28e-04)	Tok/s 228276 (199596)	Loss/tok 3.7730 (3.4657)	LR 2.875e-03
0: TRAIN [1][500/1291]	Time 0.042 (0.053)	Data 8.34e-05 (4.22e-04)	Tok/s 183931 (199478)	Loss/tok 3.1779 (3.4634)	LR 2.875e-03
0: TRAIN [1][510/1291]	Time 0.042 (0.053)	Data 1.00e-04 (4.15e-04)	Tok/s 186288 (199606)	Loss/tok 3.3401 (3.4626)	LR 2.875e-03
0: TRAIN [1][520/1291]	Time 0.042 (0.053)	Data 8.06e-05 (4.09e-04)	Tok/s 183203 (199561)	Loss/tok 3.1190 (3.4625)	LR 2.875e-03
0: TRAIN [1][530/1291]	Time 0.077 (0.053)	Data 8.34e-05 (4.03e-04)	Tok/s 226847 (199484)	Loss/tok 3.5546 (3.4599)	LR 2.875e-03
0: TRAIN [1][540/1291]	Time 0.041 (0.053)	Data 8.39e-05 (3.97e-04)	Tok/s 191342 (199331)	Loss/tok 3.3678 (3.4577)	LR 2.875e-03
0: TRAIN [1][550/1291]	Time 0.042 (0.053)	Data 8.25e-05 (3.92e-04)	Tok/s 183894 (199395)	Loss/tok 3.2931 (3.4556)	LR 2.875e-03
0: TRAIN [1][560/1291]	Time 0.042 (0.053)	Data 8.39e-05 (3.86e-04)	Tok/s 179002 (199546)	Loss/tok 3.2385 (3.4579)	LR 2.875e-03
0: TRAIN [1][570/1291]	Time 0.059 (0.053)	Data 8.77e-05 (3.81e-04)	Tok/s 213578 (199663)	Loss/tok 3.4720 (3.4606)	LR 2.875e-03
0: TRAIN [1][580/1291]	Time 0.042 (0.053)	Data 9.06e-05 (3.76e-04)	Tok/s 185563 (199764)	Loss/tok 3.2906 (3.4610)	LR 2.875e-03
0: TRAIN [1][590/1291]	Time 0.077 (0.053)	Data 1.09e-04 (3.71e-04)	Tok/s 225105 (199954)	Loss/tok 3.7914 (3.4616)	LR 2.875e-03
0: TRAIN [1][600/1291]	Time 0.097 (0.053)	Data 8.46e-05 (3.67e-04)	Tok/s 232514 (199992)	Loss/tok 3.7284 (3.4605)	LR 2.875e-03
0: Upscaling, new scale: 2048.0
0: TRAIN [1][610/1291]	Time 0.041 (0.053)	Data 8.46e-05 (3.62e-04)	Tok/s 184977 (200024)	Loss/tok 3.3202 (3.4595)	LR 2.875e-03
0: TRAIN [1][620/1291]	Time 0.059 (0.053)	Data 8.70e-05 (3.58e-04)	Tok/s 209262 (200061)	Loss/tok 3.5414 (3.4600)	LR 2.875e-03
0: TRAIN [1][630/1291]	Time 0.058 (0.053)	Data 8.51e-05 (3.54e-04)	Tok/s 217049 (200045)	Loss/tok 3.4684 (3.4597)	LR 2.875e-03
0: TRAIN [1][640/1291]	Time 0.097 (0.053)	Data 1.14e-04 (3.50e-04)	Tok/s 232449 (200172)	Loss/tok 3.7623 (3.4613)	LR 2.875e-03
0: TRAIN [1][650/1291]	Time 0.042 (0.053)	Data 9.61e-05 (3.46e-04)	Tok/s 181085 (200081)	Loss/tok 3.2809 (3.4593)	LR 2.875e-03
0: TRAIN [1][660/1291]	Time 0.097 (0.053)	Data 8.63e-05 (3.42e-04)	Tok/s 230900 (200036)	Loss/tok 3.7854 (3.4585)	LR 2.875e-03
0: TRAIN [1][670/1291]	Time 0.058 (0.053)	Data 8.13e-05 (3.38e-04)	Tok/s 213133 (199942)	Loss/tok 3.5179 (3.4574)	LR 2.875e-03
0: TRAIN [1][680/1291]	Time 0.058 (0.053)	Data 8.42e-05 (3.35e-04)	Tok/s 217163 (200041)	Loss/tok 3.4648 (3.4577)	LR 2.875e-03
0: TRAIN [1][690/1291]	Time 0.042 (0.053)	Data 8.87e-05 (3.31e-04)	Tok/s 189314 (200010)	Loss/tok 3.0371 (3.4552)	LR 2.875e-03
0: TRAIN [1][700/1291]	Time 0.025 (0.053)	Data 1.12e-04 (3.28e-04)	Tok/s 161720 (200075)	Loss/tok 2.6767 (3.4546)	LR 2.875e-03
0: TRAIN [1][710/1291]	Time 0.059 (0.053)	Data 1.15e-04 (3.24e-04)	Tok/s 212529 (200208)	Loss/tok 3.4027 (3.4557)	LR 2.875e-03
0: TRAIN [1][720/1291]	Time 0.059 (0.053)	Data 8.25e-05 (3.21e-04)	Tok/s 213255 (200324)	Loss/tok 3.5319 (3.4543)	LR 2.875e-03
0: TRAIN [1][730/1291]	Time 0.042 (0.053)	Data 8.54e-05 (3.18e-04)	Tok/s 183276 (200371)	Loss/tok 3.1216 (3.4540)	LR 2.875e-03
0: Upscaling, new scale: 4096.0
0: Gradient norm: inf
0: Skipped batch, new scale: 2048.0
0: TRAIN [1][740/1291]	Time 0.076 (0.053)	Data 8.63e-05 (3.15e-04)	Tok/s 227831 (200411)	Loss/tok 3.5646 (3.4532)	LR 2.875e-03
0: TRAIN [1][750/1291]	Time 0.041 (0.053)	Data 9.54e-05 (3.12e-04)	Tok/s 192403 (200412)	Loss/tok 3.2238 (3.4524)	LR 2.875e-03
0: TRAIN [1][760/1291]	Time 0.058 (0.053)	Data 8.49e-05 (3.09e-04)	Tok/s 214928 (200580)	Loss/tok 3.4841 (3.4534)	LR 2.875e-03
0: TRAIN [1][770/1291]	Time 0.059 (0.053)	Data 8.49e-05 (3.06e-04)	Tok/s 217041 (200648)	Loss/tok 3.4974 (3.4527)	LR 2.875e-03
0: TRAIN [1][780/1291]	Time 0.077 (0.053)	Data 8.37e-05 (3.04e-04)	Tok/s 229291 (200597)	Loss/tok 3.4683 (3.4511)	LR 2.875e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 1024.0
0: TRAIN [1][790/1291]	Time 0.076 (0.053)	Data 8.42e-05 (3.01e-04)	Tok/s 229378 (200682)	Loss/tok 3.6696 (3.4511)	LR 2.875e-03
0: TRAIN [1][800/1291]	Time 0.041 (0.053)	Data 8.39e-05 (2.98e-04)	Tok/s 185885 (200701)	Loss/tok 3.2375 (3.4501)	LR 2.875e-03
0: TRAIN [1][810/1291]	Time 0.076 (0.053)	Data 8.80e-05 (2.96e-04)	Tok/s 231954 (200771)	Loss/tok 3.6854 (3.4497)	LR 2.875e-03
0: TRAIN [1][820/1291]	Time 0.076 (0.053)	Data 8.70e-05 (2.93e-04)	Tok/s 227234 (200828)	Loss/tok 3.7498 (3.4496)	LR 2.875e-03
0: TRAIN [1][830/1291]	Time 0.042 (0.053)	Data 8.54e-05 (2.91e-04)	Tok/s 185940 (200788)	Loss/tok 3.2206 (3.4484)	LR 2.875e-03
0: TRAIN [1][840/1291]	Time 0.076 (0.053)	Data 8.01e-05 (2.88e-04)	Tok/s 231354 (200650)	Loss/tok 3.6099 (3.4471)	LR 2.875e-03
0: TRAIN [1][850/1291]	Time 0.042 (0.053)	Data 8.80e-05 (2.86e-04)	Tok/s 189359 (200618)	Loss/tok 3.1641 (3.4468)	LR 2.875e-03
0: TRAIN [1][860/1291]	Time 0.025 (0.053)	Data 8.89e-05 (2.84e-04)	Tok/s 160184 (200688)	Loss/tok 2.6877 (3.4461)	LR 2.875e-03
0: TRAIN [1][870/1291]	Time 0.097 (0.053)	Data 8.58e-05 (2.82e-04)	Tok/s 229911 (200710)	Loss/tok 3.9056 (3.4458)	LR 2.875e-03
0: TRAIN [1][880/1291]	Time 0.042 (0.053)	Data 8.92e-05 (2.79e-04)	Tok/s 181216 (200721)	Loss/tok 3.2103 (3.4447)	LR 2.875e-03
0: TRAIN [1][890/1291]	Time 0.097 (0.053)	Data 8.51e-05 (2.77e-04)	Tok/s 232550 (200804)	Loss/tok 3.6335 (3.4442)	LR 2.875e-03
0: TRAIN [1][900/1291]	Time 0.042 (0.053)	Data 8.39e-05 (2.75e-04)	Tok/s 189036 (200797)	Loss/tok 3.0578 (3.4425)	LR 2.875e-03
0: TRAIN [1][910/1291]	Time 0.077 (0.053)	Data 8.37e-05 (2.73e-04)	Tok/s 230489 (200831)	Loss/tok 3.5539 (3.4425)	LR 2.875e-03
0: Upscaling, new scale: 2048.0
0: TRAIN [1][920/1291]	Time 0.042 (0.053)	Data 8.27e-05 (2.71e-04)	Tok/s 187909 (200772)	Loss/tok 3.1832 (3.4413)	LR 2.875e-03
0: TRAIN [1][930/1291]	Time 0.042 (0.053)	Data 8.49e-05 (2.69e-04)	Tok/s 188971 (200852)	Loss/tok 3.1851 (3.4407)	LR 2.875e-03
0: TRAIN [1][940/1291]	Time 0.042 (0.053)	Data 8.32e-05 (2.67e-04)	Tok/s 182255 (200871)	Loss/tok 3.1244 (3.4406)	LR 2.875e-03
0: TRAIN [1][950/1291]	Time 0.041 (0.053)	Data 9.13e-05 (2.66e-04)	Tok/s 185975 (200920)	Loss/tok 3.2234 (3.4405)	LR 2.875e-03
0: TRAIN [1][960/1291]	Time 0.042 (0.053)	Data 8.23e-05 (2.64e-04)	Tok/s 185127 (200872)	Loss/tok 3.2992 (3.4399)	LR 2.875e-03
0: TRAIN [1][970/1291]	Time 0.059 (0.053)	Data 8.49e-05 (2.62e-04)	Tok/s 210242 (200822)	Loss/tok 3.3505 (3.4388)	LR 2.875e-03
0: TRAIN [1][980/1291]	Time 0.077 (0.053)	Data 9.47e-05 (2.60e-04)	Tok/s 227999 (200748)	Loss/tok 3.5037 (3.4375)	LR 2.875e-03
0: TRAIN [1][990/1291]	Time 0.042 (0.053)	Data 8.25e-05 (2.59e-04)	Tok/s 180624 (200645)	Loss/tok 3.1956 (3.4371)	LR 2.875e-03
0: TRAIN [1][1000/1291]	Time 0.042 (0.053)	Data 8.37e-05 (2.57e-04)	Tok/s 191918 (200686)	Loss/tok 3.1155 (3.4362)	LR 2.875e-03
0: TRAIN [1][1010/1291]	Time 0.076 (0.053)	Data 8.13e-05 (2.55e-04)	Tok/s 231035 (200609)	Loss/tok 3.4972 (3.4349)	LR 2.875e-03
0: TRAIN [1][1020/1291]	Time 0.059 (0.053)	Data 8.56e-05 (2.54e-04)	Tok/s 216786 (200644)	Loss/tok 3.3762 (3.4342)	LR 2.875e-03
0: TRAIN [1][1030/1291]	Time 0.042 (0.053)	Data 8.49e-05 (2.52e-04)	Tok/s 184556 (200663)	Loss/tok 3.2799 (3.4349)	LR 2.875e-03
0: TRAIN [1][1040/1291]	Time 0.058 (0.053)	Data 8.27e-05 (2.51e-04)	Tok/s 215749 (200622)	Loss/tok 3.3639 (3.4340)	LR 2.875e-03
0: Upscaling, new scale: 4096.0
0: TRAIN [1][1050/1291]	Time 0.077 (0.053)	Data 8.82e-05 (2.49e-04)	Tok/s 227387 (200566)	Loss/tok 3.5052 (3.4329)	LR 2.875e-03
0: TRAIN [1][1060/1291]	Time 0.041 (0.053)	Data 9.11e-05 (2.48e-04)	Tok/s 183680 (200530)	Loss/tok 3.1098 (3.4323)	LR 2.875e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 2048.0
0: TRAIN [1][1070/1291]	Time 0.041 (0.053)	Data 9.92e-05 (2.46e-04)	Tok/s 186730 (200545)	Loss/tok 3.1180 (3.4322)	LR 2.875e-03
0: TRAIN [1][1080/1291]	Time 0.041 (0.053)	Data 8.44e-05 (2.45e-04)	Tok/s 187601 (200444)	Loss/tok 3.0324 (3.4304)	LR 2.875e-03
0: TRAIN [1][1090/1291]	Time 0.041 (0.053)	Data 1.38e-04 (2.44e-04)	Tok/s 185419 (200382)	Loss/tok 3.1412 (3.4294)	LR 2.875e-03
0: TRAIN [1][1100/1291]	Time 0.058 (0.053)	Data 8.08e-05 (2.42e-04)	Tok/s 213346 (200390)	Loss/tok 3.3010 (3.4285)	LR 2.875e-03
0: TRAIN [1][1110/1291]	Time 0.059 (0.053)	Data 8.18e-05 (2.41e-04)	Tok/s 214208 (200383)	Loss/tok 3.4712 (3.4277)	LR 2.875e-03
0: TRAIN [1][1120/1291]	Time 0.041 (0.053)	Data 1.40e-04 (2.40e-04)	Tok/s 189898 (200314)	Loss/tok 3.1300 (3.4267)	LR 2.875e-03
0: TRAIN [1][1130/1291]	Time 0.042 (0.053)	Data 8.03e-05 (2.38e-04)	Tok/s 184154 (200347)	Loss/tok 3.1015 (3.4263)	LR 2.875e-03
0: TRAIN [1][1140/1291]	Time 0.097 (0.053)	Data 1.39e-04 (2.37e-04)	Tok/s 227584 (200412)	Loss/tok 3.7790 (3.4272)	LR 2.875e-03
0: TRAIN [1][1150/1291]	Time 0.024 (0.053)	Data 8.03e-05 (2.36e-04)	Tok/s 162602 (200306)	Loss/tok 2.7375 (3.4267)	LR 2.875e-03
0: TRAIN [1][1160/1291]	Time 0.077 (0.053)	Data 1.37e-04 (2.34e-04)	Tok/s 226579 (200304)	Loss/tok 3.5336 (3.4258)	LR 2.875e-03
0: TRAIN [1][1170/1291]	Time 0.025 (0.053)	Data 1.40e-04 (2.33e-04)	Tok/s 158654 (200254)	Loss/tok 2.7469 (3.4252)	LR 2.875e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 1024.0
0: TRAIN [1][1180/1291]	Time 0.041 (0.053)	Data 8.63e-05 (2.32e-04)	Tok/s 189182 (200259)	Loss/tok 3.2099 (3.4254)	LR 2.875e-03
0: TRAIN [1][1190/1291]	Time 0.042 (0.053)	Data 8.03e-05 (2.31e-04)	Tok/s 185998 (200227)	Loss/tok 3.0169 (3.4243)	LR 2.875e-03
0: TRAIN [1][1200/1291]	Time 0.042 (0.053)	Data 8.51e-05 (2.30e-04)	Tok/s 183713 (200199)	Loss/tok 3.2234 (3.4241)	LR 2.875e-03
0: TRAIN [1][1210/1291]	Time 0.042 (0.053)	Data 8.94e-05 (2.29e-04)	Tok/s 187457 (200263)	Loss/tok 3.0898 (3.4238)	LR 2.875e-03
0: TRAIN [1][1220/1291]	Time 0.097 (0.053)	Data 8.01e-05 (2.27e-04)	Tok/s 229663 (200305)	Loss/tok 3.8342 (3.4244)	LR 2.875e-03
0: TRAIN [1][1230/1291]	Time 0.042 (0.053)	Data 7.92e-05 (2.26e-04)	Tok/s 187253 (200306)	Loss/tok 3.1124 (3.4238)	LR 2.875e-03
0: TRAIN [1][1240/1291]	Time 0.058 (0.053)	Data 8.30e-05 (2.25e-04)	Tok/s 216388 (200352)	Loss/tok 3.3630 (3.4234)	LR 2.875e-03
0: TRAIN [1][1250/1291]	Time 0.058 (0.053)	Data 1.42e-04 (2.24e-04)	Tok/s 216740 (200394)	Loss/tok 3.3580 (3.4229)	LR 2.875e-03
0: TRAIN [1][1260/1291]	Time 0.042 (0.053)	Data 8.85e-05 (2.23e-04)	Tok/s 187793 (200349)	Loss/tok 2.9339 (3.4218)	LR 2.875e-03
0: TRAIN [1][1270/1291]	Time 0.058 (0.053)	Data 8.42e-05 (2.22e-04)	Tok/s 216774 (200350)	Loss/tok 3.3320 (3.4211)	LR 2.875e-03
0: TRAIN [1][1280/1291]	Time 0.058 (0.053)	Data 1.39e-04 (2.21e-04)	Tok/s 215546 (200418)	Loss/tok 3.2914 (3.4204)	LR 2.875e-03
0: TRAIN [1][1290/1291]	Time 0.041 (0.053)	Data 4.34e-05 (2.21e-04)	Tok/s 189264 (200455)	Loss/tok 3.1163 (3.4204)	LR 2.875e-03
:::MLLOG {"namespace": "", "time_ms": 1593113856189, "event_type": "INTERVAL_END", "key": "epoch_stop", "value": null, "metadata": {"file": "train.py", "lineno": 551, "epoch_num": 2}}
:::MLLOG {"namespace": "", "time_ms": 1593113856190, "event_type": "INTERVAL_START", "key": "eval_start", "value": null, "metadata": {"file": "train.py", "lineno": 556, "epoch_num": 2}}
0: Running evaluation on test set
0: TEST [1][0/3]	Time 0.385 (0.385)	Decoder iters 149.0 (149.0)	Tok/s 23159 (23159)
0: Running moses detokenizer
0: BLEU(score=22.030173247486747, counts=[35497, 17011, 9385, 5409], totals=[64438, 61435, 58432, 55434], precisions=[55.08706043018095, 27.68942785057378, 16.061404709748082, 9.757549518346142], bp=0.996313339685272, sys_len=64438, ref_len=64676)
0: Finished evaluation on test set
:::MLLOG {"namespace": "", "time_ms": 1593113857460, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.22030000000000002, "metadata": {"file": "train.py", "lineno": 562, "epoch_num": 2}}
:::MLLOG {"namespace": "", "time_ms": 1593113857460, "event_type": "INTERVAL_END", "key": "eval_stop", "value": null, "metadata": {"file": "train.py", "lineno": 565, "epoch_num": 2}}
0: Summary: Epoch: 1	Training Loss: 3.4218	Test BLEU: 22.03
0: Performance: Epoch: 1	Training: 3207521 Tok/s
0: Finished epoch 1
:::MLLOG {"namespace": "", "time_ms": 1593113857460, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "train.py", "lineno": 584, "first_epoch_num": 2}}
:::MLLOG {"namespace": "", "time_ms": 1593113857460, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "train.py", "lineno": 538, "first_epoch_num": 3, "epoch_count": 1}}
:::MLLOG {"namespace": "", "time_ms": 1593113857461, "event_type": "INTERVAL_START", "key": "epoch_start", "value": null, "metadata": {"file": "train.py", "lineno": 541, "epoch_num": 3}}
0: Starting epoch 2
0: Sampler for epoch 2 uses seed 366747360
0: TRAIN [2][0/1291]	Time 0.307 (0.307)	Data 1.78e-01 (1.78e-01)	Tok/s 24983 (24983)	Loss/tok 3.0534 (3.0534)	LR 2.875e-03
0: TRAIN [2][10/1291]	Time 0.025 (0.074)	Data 8.03e-05 (1.63e-02)	Tok/s 157718 (182486)	Loss/tok 2.5970 (3.2348)	LR 2.875e-03
0: Upscaling, new scale: 2048.0
0: TRAIN [2][20/1291]	Time 0.041 (0.064)	Data 8.15e-05 (8.57e-03)	Tok/s 189537 (189986)	Loss/tok 3.1534 (3.2573)	LR 2.875e-03
0: TRAIN [2][30/1291]	Time 0.042 (0.060)	Data 8.30e-05 (5.83e-03)	Tok/s 188944 (193943)	Loss/tok 3.0441 (3.2466)	LR 2.875e-03
0: TRAIN [2][40/1291]	Time 0.041 (0.059)	Data 8.06e-05 (4.43e-03)	Tok/s 190505 (195982)	Loss/tok 3.1624 (3.2704)	LR 2.875e-03
0: TRAIN [2][50/1291]	Time 0.042 (0.056)	Data 7.92e-05 (3.58e-03)	Tok/s 186634 (195688)	Loss/tok 3.0509 (3.2566)	LR 2.875e-03
0: TRAIN [2][60/1291]	Time 0.042 (0.055)	Data 8.34e-05 (3.00e-03)	Tok/s 188868 (195566)	Loss/tok 3.1715 (3.2420)	LR 2.875e-03
0: TRAIN [2][70/1291]	Time 0.097 (0.055)	Data 9.06e-05 (2.59e-03)	Tok/s 229423 (197026)	Loss/tok 3.8192 (3.2687)	LR 2.875e-03
0: TRAIN [2][80/1291]	Time 0.097 (0.056)	Data 9.04e-05 (2.28e-03)	Tok/s 233476 (198175)	Loss/tok 3.6540 (3.2833)	LR 2.875e-03
0: TRAIN [2][90/1291]	Time 0.042 (0.056)	Data 9.66e-05 (2.04e-03)	Tok/s 187328 (198113)	Loss/tok 3.0092 (3.2840)	LR 2.875e-03
0: TRAIN [2][100/1291]	Time 0.025 (0.055)	Data 8.54e-05 (1.85e-03)	Tok/s 157152 (198045)	Loss/tok 2.7199 (3.2787)	LR 2.875e-03
0: TRAIN [2][110/1291]	Time 0.041 (0.054)	Data 8.27e-05 (1.69e-03)	Tok/s 187254 (197835)	Loss/tok 3.1220 (3.2754)	LR 2.875e-03
0: TRAIN [2][120/1291]	Time 0.059 (0.054)	Data 8.61e-05 (1.56e-03)	Tok/s 210014 (197301)	Loss/tok 3.3585 (3.2707)	LR 2.875e-03
0: TRAIN [2][130/1291]	Time 0.042 (0.054)	Data 8.77e-05 (1.45e-03)	Tok/s 186829 (197786)	Loss/tok 3.2403 (3.2798)	LR 2.875e-03
0: TRAIN [2][140/1291]	Time 0.042 (0.054)	Data 8.68e-05 (1.35e-03)	Tok/s 183630 (198222)	Loss/tok 2.9930 (3.2799)	LR 2.875e-03
0: Upscaling, new scale: 4096.0
0: Gradient norm: inf
0: Skipped batch, new scale: 2048.0
0: TRAIN [2][150/1291]	Time 0.077 (0.055)	Data 8.58e-05 (1.27e-03)	Tok/s 225010 (198749)	Loss/tok 3.3792 (3.2850)	LR 2.875e-03
0: TRAIN [2][160/1291]	Time 0.042 (0.054)	Data 8.30e-05 (1.19e-03)	Tok/s 188388 (198479)	Loss/tok 3.2147 (3.2798)	LR 2.875e-03
0: TRAIN [2][170/1291]	Time 0.058 (0.054)	Data 8.23e-05 (1.13e-03)	Tok/s 216546 (198151)	Loss/tok 3.2465 (3.2792)	LR 2.875e-03
0: TRAIN [2][180/1291]	Time 0.041 (0.053)	Data 8.03e-05 (1.07e-03)	Tok/s 188573 (197499)	Loss/tok 3.1532 (3.2723)	LR 2.875e-03
0: TRAIN [2][190/1291]	Time 0.025 (0.053)	Data 8.42e-05 (1.02e-03)	Tok/s 157792 (197595)	Loss/tok 2.5957 (3.2708)	LR 2.875e-03
0: TRAIN [2][200/1291]	Time 0.076 (0.053)	Data 8.46e-05 (9.72e-04)	Tok/s 227427 (198205)	Loss/tok 3.6536 (3.2771)	LR 2.875e-03
0: TRAIN [2][210/1291]	Time 0.042 (0.053)	Data 8.18e-05 (9.30e-04)	Tok/s 187730 (198361)	Loss/tok 3.0989 (3.2790)	LR 2.875e-03
0: TRAIN [2][220/1291]	Time 0.041 (0.053)	Data 7.92e-05 (8.92e-04)	Tok/s 188002 (197965)	Loss/tok 3.0797 (3.2742)	LR 2.875e-03
0: TRAIN [2][230/1291]	Time 0.076 (0.053)	Data 8.03e-05 (8.57e-04)	Tok/s 229499 (198543)	Loss/tok 3.3855 (3.2761)	LR 2.875e-03
0: TRAIN [2][240/1291]	Time 0.059 (0.053)	Data 8.01e-05 (8.24e-04)	Tok/s 211915 (198846)	Loss/tok 3.3624 (3.2764)	LR 2.875e-03
0: TRAIN [2][250/1291]	Time 0.041 (0.054)	Data 7.92e-05 (7.95e-04)	Tok/s 180723 (199083)	Loss/tok 3.0464 (3.2768)	LR 2.875e-03
0: TRAIN [2][260/1291]	Time 0.076 (0.054)	Data 9.18e-05 (7.67e-04)	Tok/s 233744 (199411)	Loss/tok 3.2819 (3.2806)	LR 2.875e-03
0: TRAIN [2][270/1291]	Time 0.059 (0.054)	Data 8.18e-05 (7.42e-04)	Tok/s 215143 (199272)	Loss/tok 3.2988 (3.2788)	LR 2.875e-03
0: Upscaling, new scale: 4096.0
0: Gradient norm: inf
0: Skipped batch, new scale: 2048.0
0: TRAIN [2][280/1291]	Time 0.042 (0.053)	Data 8.25e-05 (7.19e-04)	Tok/s 185338 (199292)	Loss/tok 3.1012 (3.2784)	LR 2.875e-03
0: TRAIN [2][290/1291]	Time 0.058 (0.054)	Data 8.34e-05 (6.97e-04)	Tok/s 213960 (199746)	Loss/tok 3.4543 (3.2810)	LR 2.875e-03
0: TRAIN [2][300/1291]	Time 0.042 (0.054)	Data 7.94e-05 (6.77e-04)	Tok/s 183694 (199670)	Loss/tok 2.9769 (3.2797)	LR 2.875e-03
0: TRAIN [2][310/1291]	Time 0.025 (0.053)	Data 8.20e-05 (6.57e-04)	Tok/s 152889 (199650)	Loss/tok 2.7012 (3.2794)	LR 2.875e-03
0: TRAIN [2][320/1291]	Time 0.058 (0.054)	Data 8.01e-05 (6.40e-04)	Tok/s 215789 (199943)	Loss/tok 3.2801 (3.2812)	LR 2.875e-03
0: TRAIN [2][330/1291]	Time 0.076 (0.054)	Data 8.39e-05 (6.23e-04)	Tok/s 225589 (199995)	Loss/tok 3.5284 (3.2812)	LR 2.875e-03
0: TRAIN [2][340/1291]	Time 0.058 (0.054)	Data 8.15e-05 (6.07e-04)	Tok/s 213889 (200133)	Loss/tok 3.3635 (3.2815)	LR 2.875e-03
0: TRAIN [2][350/1291]	Time 0.025 (0.053)	Data 8.11e-05 (5.92e-04)	Tok/s 163420 (199845)	Loss/tok 2.6756 (3.2790)	LR 2.875e-03
0: TRAIN [2][360/1291]	Time 0.025 (0.053)	Data 8.27e-05 (5.78e-04)	Tok/s 163662 (199621)	Loss/tok 2.5914 (3.2795)	LR 2.875e-03
0: TRAIN [2][370/1291]	Time 0.059 (0.053)	Data 8.42e-05 (5.64e-04)	Tok/s 217646 (199877)	Loss/tok 3.3112 (3.2831)	LR 2.875e-03
0: TRAIN [2][380/1291]	Time 0.059 (0.053)	Data 7.99e-05 (5.52e-04)	Tok/s 214378 (199895)	Loss/tok 3.3379 (3.2827)	LR 2.875e-03
0: TRAIN [2][390/1291]	Time 0.058 (0.053)	Data 7.82e-05 (5.40e-04)	Tok/s 218651 (199891)	Loss/tok 3.2675 (3.2819)	LR 2.875e-03
0: TRAIN [2][400/1291]	Time 0.042 (0.053)	Data 9.61e-05 (5.28e-04)	Tok/s 190593 (200043)	Loss/tok 3.1953 (3.2849)	LR 2.875e-03
0: Upscaling, new scale: 4096.0
0: TRAIN [2][410/1291]	Time 0.058 (0.053)	Data 8.30e-05 (5.17e-04)	Tok/s 212792 (200242)	Loss/tok 3.2689 (3.2844)	LR 2.875e-03
0: TRAIN [2][420/1291]	Time 0.041 (0.053)	Data 8.08e-05 (5.07e-04)	Tok/s 190079 (200266)	Loss/tok 3.0164 (3.2829)	LR 2.875e-03
0: TRAIN [2][430/1291]	Time 0.059 (0.053)	Data 7.68e-05 (4.97e-04)	Tok/s 215689 (200310)	Loss/tok 3.3008 (3.2836)	LR 2.875e-03
0: TRAIN [2][440/1291]	Time 0.041 (0.053)	Data 7.96e-05 (4.88e-04)	Tok/s 184812 (199977)	Loss/tok 3.0139 (3.2798)	LR 2.875e-03
0: TRAIN [2][450/1291]	Time 0.076 (0.053)	Data 8.27e-05 (4.79e-04)	Tok/s 229731 (200019)	Loss/tok 3.4368 (3.2798)	LR 2.875e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 2048.0
0: TRAIN [2][460/1291]	Time 0.059 (0.053)	Data 7.82e-05 (4.70e-04)	Tok/s 213887 (200374)	Loss/tok 3.3361 (3.2825)	LR 2.875e-03
0: TRAIN [2][470/1291]	Time 0.043 (0.053)	Data 8.03e-05 (4.62e-04)	Tok/s 182815 (200281)	Loss/tok 3.1112 (3.2809)	LR 2.875e-03
0: TRAIN [2][480/1291]	Time 0.058 (0.053)	Data 8.13e-05 (4.54e-04)	Tok/s 210337 (200312)	Loss/tok 3.3473 (3.2810)	LR 2.875e-03
0: TRAIN [2][490/1291]	Time 0.058 (0.053)	Data 8.01e-05 (4.46e-04)	Tok/s 217218 (200102)	Loss/tok 3.2116 (3.2796)	LR 2.875e-03
0: TRAIN [2][500/1291]	Time 0.058 (0.053)	Data 8.85e-05 (4.39e-04)	Tok/s 220520 (200052)	Loss/tok 3.3054 (3.2790)	LR 2.875e-03
0: TRAIN [2][510/1291]	Time 0.076 (0.053)	Data 8.11e-05 (4.32e-04)	Tok/s 228754 (200019)	Loss/tok 3.5131 (3.2798)	LR 2.875e-03
0: TRAIN [2][520/1291]	Time 0.025 (0.053)	Data 8.06e-05 (4.25e-04)	Tok/s 159626 (199887)	Loss/tok 2.7356 (3.2791)	LR 2.875e-03
0: TRAIN [2][530/1291]	Time 0.098 (0.053)	Data 8.18e-05 (4.19e-04)	Tok/s 228073 (199938)	Loss/tok 3.7075 (3.2812)	LR 2.875e-03
0: TRAIN [2][540/1291]	Time 0.058 (0.053)	Data 8.01e-05 (4.13e-04)	Tok/s 216259 (199891)	Loss/tok 3.5031 (3.2812)	LR 2.875e-03
0: TRAIN [2][550/1291]	Time 0.058 (0.053)	Data 8.23e-05 (4.07e-04)	Tok/s 217856 (199947)	Loss/tok 3.2730 (3.2810)	LR 2.875e-03
0: TRAIN [2][560/1291]	Time 0.041 (0.053)	Data 7.96e-05 (4.01e-04)	Tok/s 188518 (199860)	Loss/tok 2.9583 (3.2818)	LR 2.875e-03
0: TRAIN [2][570/1291]	Time 0.041 (0.053)	Data 8.30e-05 (3.95e-04)	Tok/s 187360 (199795)	Loss/tok 3.0844 (3.2811)	LR 2.875e-03
0: Upscaling, new scale: 4096.0
0: TRAIN [2][580/1291]	Time 0.042 (0.053)	Data 7.99e-05 (3.90e-04)	Tok/s 187630 (199950)	Loss/tok 2.9309 (3.2816)	LR 2.875e-03
0: TRAIN [2][590/1291]	Time 0.058 (0.053)	Data 8.13e-05 (3.85e-04)	Tok/s 212379 (200022)	Loss/tok 3.2280 (3.2805)	LR 2.875e-03
0: TRAIN [2][600/1291]	Time 0.042 (0.053)	Data 8.06e-05 (3.80e-04)	Tok/s 187417 (200163)	Loss/tok 3.0864 (3.2816)	LR 2.875e-03
0: TRAIN [2][610/1291]	Time 0.076 (0.053)	Data 8.08e-05 (3.75e-04)	Tok/s 229714 (200339)	Loss/tok 3.4028 (3.2828)	LR 2.875e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 2048.0
0: TRAIN [2][620/1291]	Time 0.042 (0.053)	Data 8.77e-05 (3.70e-04)	Tok/s 188805 (200487)	Loss/tok 3.0402 (3.2838)	LR 2.875e-03
0: TRAIN [2][630/1291]	Time 0.041 (0.053)	Data 7.92e-05 (3.65e-04)	Tok/s 192775 (200517)	Loss/tok 3.0065 (3.2833)	LR 2.875e-03
0: TRAIN [2][640/1291]	Time 0.042 (0.053)	Data 7.94e-05 (3.61e-04)	Tok/s 180362 (200639)	Loss/tok 3.0135 (3.2855)	LR 2.875e-03
0: TRAIN [2][650/1291]	Time 0.043 (0.053)	Data 8.13e-05 (3.57e-04)	Tok/s 186652 (200574)	Loss/tok 3.0257 (3.2840)	LR 2.875e-03
0: TRAIN [2][660/1291]	Time 0.059 (0.053)	Data 8.27e-05 (3.52e-04)	Tok/s 215361 (200515)	Loss/tok 3.1673 (3.2831)	LR 2.875e-03
0: TRAIN [2][670/1291]	Time 0.076 (0.053)	Data 8.18e-05 (3.48e-04)	Tok/s 227366 (200574)	Loss/tok 3.5128 (3.2854)	LR 2.875e-03
0: TRAIN [2][680/1291]	Time 0.041 (0.053)	Data 8.03e-05 (3.45e-04)	Tok/s 185533 (200408)	Loss/tok 3.2193 (3.2836)	LR 2.875e-03
0: TRAIN [2][690/1291]	Time 0.077 (0.053)	Data 7.89e-05 (3.41e-04)	Tok/s 224126 (200451)	Loss/tok 3.4569 (3.2839)	LR 2.875e-03
0: TRAIN [2][700/1291]	Time 0.077 (0.053)	Data 8.37e-05 (3.37e-04)	Tok/s 229592 (200451)	Loss/tok 3.5054 (3.2844)	LR 2.875e-03
0: TRAIN [2][710/1291]	Time 0.058 (0.053)	Data 8.18e-05 (3.33e-04)	Tok/s 212987 (200477)	Loss/tok 3.3719 (3.2845)	LR 2.875e-03
0: TRAIN [2][720/1291]	Time 0.097 (0.053)	Data 8.01e-05 (3.30e-04)	Tok/s 228088 (200574)	Loss/tok 3.6662 (3.2857)	LR 2.875e-03
0: TRAIN [2][730/1291]	Time 0.041 (0.053)	Data 7.94e-05 (3.27e-04)	Tok/s 189258 (200458)	Loss/tok 3.1078 (3.2846)	LR 2.875e-03
0: TRAIN [2][740/1291]	Time 0.078 (0.053)	Data 8.18e-05 (3.23e-04)	Tok/s 225406 (200570)	Loss/tok 3.4340 (3.2862)	LR 2.875e-03
0: Upscaling, new scale: 4096.0
0: TRAIN [2][750/1291]	Time 0.058 (0.053)	Data 8.01e-05 (3.20e-04)	Tok/s 217179 (200671)	Loss/tok 3.2507 (3.2880)	LR 2.875e-03
0: TRAIN [2][760/1291]	Time 0.059 (0.053)	Data 8.03e-05 (3.17e-04)	Tok/s 210907 (200669)	Loss/tok 3.5132 (3.2889)	LR 2.875e-03
0: TRAIN [2][770/1291]	Time 0.042 (0.053)	Data 1.10e-04 (3.14e-04)	Tok/s 186361 (200655)	Loss/tok 3.0760 (3.2890)	LR 2.875e-03
0: TRAIN [2][780/1291]	Time 0.041 (0.053)	Data 7.77e-05 (3.11e-04)	Tok/s 188785 (200523)	Loss/tok 3.0012 (3.2873)	LR 2.875e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 2048.0
0: TRAIN [2][790/1291]	Time 0.058 (0.053)	Data 7.96e-05 (3.08e-04)	Tok/s 213021 (200463)	Loss/tok 3.3842 (3.2875)	LR 2.875e-03
0: TRAIN [2][800/1291]	Time 0.042 (0.053)	Data 9.73e-05 (3.05e-04)	Tok/s 185030 (200495)	Loss/tok 3.0362 (3.2867)	LR 2.875e-03
0: TRAIN [2][810/1291]	Time 0.076 (0.053)	Data 9.58e-05 (3.03e-04)	Tok/s 228543 (200425)	Loss/tok 3.5482 (3.2867)	LR 2.875e-03
0: TRAIN [2][820/1291]	Time 0.096 (0.053)	Data 1.37e-04 (3.00e-04)	Tok/s 232602 (200367)	Loss/tok 3.6674 (3.2865)	LR 2.875e-03
0: TRAIN [2][830/1291]	Time 0.042 (0.053)	Data 7.94e-05 (2.98e-04)	Tok/s 189095 (200337)	Loss/tok 3.0291 (3.2865)	LR 2.875e-03
0: TRAIN [2][840/1291]	Time 0.058 (0.053)	Data 8.01e-05 (2.95e-04)	Tok/s 214547 (200299)	Loss/tok 3.4079 (3.2856)	LR 2.875e-03
0: TRAIN [2][850/1291]	Time 0.041 (0.053)	Data 7.84e-05 (2.93e-04)	Tok/s 181622 (200276)	Loss/tok 3.0834 (3.2856)	LR 2.875e-03
0: TRAIN [2][860/1291]	Time 0.058 (0.053)	Data 1.38e-04 (2.90e-04)	Tok/s 213499 (200438)	Loss/tok 3.1411 (3.2855)	LR 2.875e-03
0: TRAIN [2][870/1291]	Time 0.042 (0.053)	Data 8.01e-05 (2.88e-04)	Tok/s 181995 (200497)	Loss/tok 3.0573 (3.2853)	LR 2.875e-03
0: TRAIN [2][880/1291]	Time 0.059 (0.053)	Data 7.99e-05 (2.86e-04)	Tok/s 214107 (200508)	Loss/tok 3.3373 (3.2851)	LR 2.875e-03
0: TRAIN [2][890/1291]	Time 0.025 (0.053)	Data 9.08e-05 (2.84e-04)	Tok/s 159331 (200439)	Loss/tok 2.6957 (3.2846)	LR 2.875e-03
0: TRAIN [2][900/1291]	Time 0.097 (0.053)	Data 1.04e-04 (2.82e-04)	Tok/s 229573 (200514)	Loss/tok 3.7531 (3.2857)	LR 2.875e-03
0: Upscaling, new scale: 4096.0
0: TRAIN [2][910/1291]	Time 0.042 (0.053)	Data 7.96e-05 (2.80e-04)	Tok/s 180220 (200427)	Loss/tok 3.0162 (3.2849)	LR 2.875e-03
0: TRAIN [2][920/1291]	Time 0.041 (0.053)	Data 1.33e-04 (2.78e-04)	Tok/s 190011 (200365)	Loss/tok 3.1114 (3.2837)	LR 2.875e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 2048.0
0: Gradient norm: inf
0: Skipped batch, new scale: 1024.0
0: TRAIN [2][930/1291]	Time 0.059 (0.053)	Data 8.01e-05 (2.76e-04)	Tok/s 220072 (200243)	Loss/tok 3.2687 (3.2838)	LR 2.875e-03
0: TRAIN [2][940/1291]	Time 0.076 (0.053)	Data 7.92e-05 (2.74e-04)	Tok/s 233936 (200253)	Loss/tok 3.4457 (3.2832)	LR 2.875e-03
0: TRAIN [2][950/1291]	Time 0.058 (0.053)	Data 8.20e-05 (2.72e-04)	Tok/s 210816 (200366)	Loss/tok 3.2930 (3.2837)	LR 2.875e-03
0: TRAIN [2][960/1291]	Time 0.042 (0.053)	Data 8.18e-05 (2.70e-04)	Tok/s 183975 (200415)	Loss/tok 3.0660 (3.2845)	LR 2.875e-03
0: TRAIN [2][970/1291]	Time 0.024 (0.053)	Data 1.35e-04 (2.68e-04)	Tok/s 162811 (200266)	Loss/tok 2.6072 (3.2837)	LR 2.875e-03
0: TRAIN [2][980/1291]	Time 0.025 (0.053)	Data 7.82e-05 (2.66e-04)	Tok/s 161048 (200291)	Loss/tok 2.7024 (3.2848)	LR 2.875e-03
0: TRAIN [2][990/1291]	Time 0.097 (0.053)	Data 1.39e-04 (2.64e-04)	Tok/s 232289 (200451)	Loss/tok 3.7147 (3.2862)	LR 2.875e-03
0: TRAIN [2][1000/1291]	Time 0.024 (0.053)	Data 7.68e-05 (2.63e-04)	Tok/s 166692 (200431)	Loss/tok 2.5917 (3.2857)	LR 2.875e-03
0: TRAIN [2][1010/1291]	Time 0.042 (0.053)	Data 9.20e-05 (2.61e-04)	Tok/s 185923 (200483)	Loss/tok 3.0770 (3.2860)	LR 2.875e-03
0: TRAIN [2][1020/1291]	Time 0.059 (0.053)	Data 7.75e-05 (2.59e-04)	Tok/s 215265 (200476)	Loss/tok 3.3058 (3.2862)	LR 2.875e-03
0: TRAIN [2][1030/1291]	Time 0.041 (0.053)	Data 7.89e-05 (2.58e-04)	Tok/s 186663 (200390)	Loss/tok 3.1455 (3.2855)	LR 2.875e-03
0: TRAIN [2][1040/1291]	Time 0.058 (0.053)	Data 8.20e-05 (2.56e-04)	Tok/s 216449 (200410)	Loss/tok 3.2610 (3.2855)	LR 2.875e-03
0: TRAIN [2][1050/1291]	Time 0.059 (0.053)	Data 8.06e-05 (2.54e-04)	Tok/s 212605 (200489)	Loss/tok 3.2780 (3.2862)	LR 2.875e-03
0: Upscaling, new scale: 2048.0
0: TRAIN [2][1060/1291]	Time 0.059 (0.053)	Data 7.99e-05 (2.53e-04)	Tok/s 215127 (200599)	Loss/tok 3.2419 (3.2867)	LR 2.875e-03
0: TRAIN [2][1070/1291]	Time 0.041 (0.053)	Data 1.38e-04 (2.51e-04)	Tok/s 190042 (200606)	Loss/tok 3.0192 (3.2862)	LR 2.875e-03
0: TRAIN [2][1080/1291]	Time 0.041 (0.053)	Data 7.84e-05 (2.50e-04)	Tok/s 187923 (200507)	Loss/tok 3.1564 (3.2853)	LR 2.875e-03
0: TRAIN [2][1090/1291]	Time 0.042 (0.053)	Data 9.47e-05 (2.48e-04)	Tok/s 189458 (200543)	Loss/tok 3.0780 (3.2851)	LR 2.875e-03
0: TRAIN [2][1100/1291]	Time 0.042 (0.053)	Data 8.03e-05 (2.47e-04)	Tok/s 185889 (200524)	Loss/tok 2.9962 (3.2845)	LR 2.875e-03
0: TRAIN [2][1110/1291]	Time 0.058 (0.053)	Data 1.37e-04 (2.46e-04)	Tok/s 215229 (200562)	Loss/tok 3.2956 (3.2855)	LR 2.875e-03
0: TRAIN [2][1120/1291]	Time 0.041 (0.053)	Data 8.27e-05 (2.44e-04)	Tok/s 194948 (200536)	Loss/tok 3.1382 (3.2844)	LR 2.875e-03
0: TRAIN [2][1130/1291]	Time 0.058 (0.053)	Data 7.96e-05 (2.43e-04)	Tok/s 217719 (200552)	Loss/tok 3.3227 (3.2841)	LR 2.875e-03
0: TRAIN [2][1140/1291]	Time 0.041 (0.053)	Data 1.99e-04 (2.42e-04)	Tok/s 184853 (200545)	Loss/tok 3.1278 (3.2835)	LR 2.875e-03
0: TRAIN [2][1150/1291]	Time 0.041 (0.053)	Data 7.94e-05 (2.40e-04)	Tok/s 192433 (200447)	Loss/tok 2.9335 (3.2826)	LR 2.875e-03
0: TRAIN [2][1160/1291]	Time 0.042 (0.053)	Data 8.75e-05 (2.39e-04)	Tok/s 179880 (200455)	Loss/tok 3.1665 (3.2816)	LR 2.875e-03
0: TRAIN [2][1170/1291]	Time 0.059 (0.053)	Data 8.94e-05 (2.38e-04)	Tok/s 211707 (200591)	Loss/tok 3.3698 (3.2827)	LR 2.875e-03
0: TRAIN [2][1180/1291]	Time 0.042 (0.053)	Data 8.06e-05 (2.36e-04)	Tok/s 189354 (200559)	Loss/tok 3.0828 (3.2823)	LR 2.875e-03
0: Upscaling, new scale: 4096.0
0: TRAIN [2][1190/1291]	Time 0.041 (0.053)	Data 7.72e-05 (2.35e-04)	Tok/s 182493 (200489)	Loss/tok 3.0101 (3.2821)	LR 2.875e-03
0: TRAIN [2][1200/1291]	Time 0.058 (0.053)	Data 8.20e-05 (2.34e-04)	Tok/s 217378 (200515)	Loss/tok 3.2476 (3.2814)	LR 2.875e-03
0: TRAIN [2][1210/1291]	Time 0.041 (0.053)	Data 8.18e-05 (2.33e-04)	Tok/s 187032 (200467)	Loss/tok 3.0464 (3.2804)	LR 2.875e-03
0: TRAIN [2][1220/1291]	Time 0.059 (0.053)	Data 8.18e-05 (2.32e-04)	Tok/s 214637 (200493)	Loss/tok 3.1807 (3.2803)	LR 2.875e-03
0: TRAIN [2][1230/1291]	Time 0.058 (0.053)	Data 7.87e-05 (2.30e-04)	Tok/s 217240 (200554)	Loss/tok 3.2541 (3.2804)	LR 2.875e-03
0: TRAIN [2][1240/1291]	Time 0.042 (0.053)	Data 7.77e-05 (2.29e-04)	Tok/s 180781 (200604)	Loss/tok 3.0790 (3.2805)	LR 2.875e-03
0: TRAIN [2][1250/1291]	Time 0.059 (0.053)	Data 1.36e-04 (2.28e-04)	Tok/s 213007 (200626)	Loss/tok 3.2916 (3.2804)	LR 2.875e-03
0: TRAIN [2][1260/1291]	Time 0.058 (0.053)	Data 7.94e-05 (2.27e-04)	Tok/s 220423 (200597)	Loss/tok 3.2925 (3.2801)	LR 2.875e-03
0: TRAIN [2][1270/1291]	Time 0.042 (0.053)	Data 1.28e-04 (2.26e-04)	Tok/s 185796 (200603)	Loss/tok 3.0927 (3.2800)	LR 2.875e-03
0: TRAIN [2][1280/1291]	Time 0.041 (0.053)	Data 1.33e-04 (2.25e-04)	Tok/s 184739 (200581)	Loss/tok 3.1154 (3.2794)	LR 2.875e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 2048.0
0: TRAIN [2][1290/1291]	Time 0.059 (0.053)	Data 4.39e-05 (2.25e-04)	Tok/s 213955 (200535)	Loss/tok 3.3061 (3.2791)	LR 2.875e-03
:::MLLOG {"namespace": "", "time_ms": 1593113925972, "event_type": "INTERVAL_END", "key": "epoch_stop", "value": null, "metadata": {"file": "train.py", "lineno": 551, "epoch_num": 3}}
:::MLLOG {"namespace": "", "time_ms": 1593113925972, "event_type": "INTERVAL_START", "key": "eval_start", "value": null, "metadata": {"file": "train.py", "lineno": 556, "epoch_num": 3}}
0: Running evaluation on test set
0: TEST [2][0/3]	Time 0.403 (0.403)	Decoder iters 149.0 (149.0)	Tok/s 22048 (22048)
0: Running moses detokenizer
0: BLEU(score=22.819085847547946, counts=[36372, 17748, 9882, 5740], totals=[65217, 62214, 59211, 56212], precisions=[55.770734624407744, 28.52734111293278, 16.689466484268127, 10.211342773784956], bp=1.0, sys_len=65217, ref_len=64676)
0: Finished evaluation on test set
:::MLLOG {"namespace": "", "time_ms": 1593113927263, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.22820000000000001, "metadata": {"file": "train.py", "lineno": 562, "epoch_num": 3}}
:::MLLOG {"namespace": "", "time_ms": 1593113927263, "event_type": "INTERVAL_END", "key": "eval_stop", "value": null, "metadata": {"file": "train.py", "lineno": 565, "epoch_num": 3}}
0: Summary: Epoch: 2	Training Loss: 3.2783	Test BLEU: 22.82
0: Performance: Epoch: 2	Training: 3208845 Tok/s
0: Finished epoch 2
:::MLLOG {"namespace": "", "time_ms": 1593113927263, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "train.py", "lineno": 584, "first_epoch_num": 3}}
:::MLLOG {"namespace": "", "time_ms": 1593113927263, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "train.py", "lineno": 538, "first_epoch_num": 4, "epoch_count": 1}}
:::MLLOG {"namespace": "", "time_ms": 1593113927263, "event_type": "INTERVAL_START", "key": "epoch_start", "value": null, "metadata": {"file": "train.py", "lineno": 541, "epoch_num": 4}}
0: Starting epoch 3
0: Sampler for epoch 3 uses seed 627301622
0: TRAIN [3][0/1291]	Time 0.307 (0.307)	Data 1.70e-01 (1.70e-01)	Tok/s 40748 (40748)	Loss/tok 3.1264 (3.1264)	LR 2.875e-03
0: TRAIN [3][10/1291]	Time 0.025 (0.072)	Data 8.06e-05 (1.56e-02)	Tok/s 158110 (178892)	Loss/tok 2.5747 (3.1578)	LR 2.875e-03
0: TRAIN [3][20/1291]	Time 0.041 (0.058)	Data 8.54e-05 (8.20e-03)	Tok/s 187812 (182573)	Loss/tok 2.8651 (3.0920)	LR 2.875e-03
0: TRAIN [3][30/1291]	Time 0.026 (0.056)	Data 8.77e-05 (5.58e-03)	Tok/s 150669 (187933)	Loss/tok 2.5514 (3.1172)	LR 2.875e-03
0: TRAIN [3][40/1291]	Time 0.025 (0.054)	Data 8.20e-05 (4.25e-03)	Tok/s 156764 (189676)	Loss/tok 2.5816 (3.1335)	LR 2.875e-03
0: TRAIN [3][50/1291]	Time 0.042 (0.053)	Data 9.16e-05 (3.43e-03)	Tok/s 185255 (191202)	Loss/tok 2.8613 (3.1170)	LR 2.875e-03
0: TRAIN [3][60/1291]	Time 0.042 (0.052)	Data 9.61e-05 (2.89e-03)	Tok/s 184056 (192391)	Loss/tok 3.0098 (3.1194)	LR 2.875e-03
0: TRAIN [3][70/1291]	Time 0.025 (0.051)	Data 1.41e-04 (2.49e-03)	Tok/s 161475 (192774)	Loss/tok 2.5961 (3.1132)	LR 2.875e-03
0: TRAIN [3][80/1291]	Time 0.042 (0.052)	Data 8.51e-05 (2.19e-03)	Tok/s 186795 (194652)	Loss/tok 2.9546 (3.1297)	LR 2.875e-03
0: TRAIN [3][90/1291]	Time 0.059 (0.052)	Data 8.15e-05 (1.96e-03)	Tok/s 216299 (195250)	Loss/tok 3.1854 (3.1294)	LR 2.875e-03
0: TRAIN [3][100/1291]	Time 0.077 (0.053)	Data 1.08e-04 (1.78e-03)	Tok/s 228700 (197096)	Loss/tok 3.3264 (3.1475)	LR 2.875e-03
0: TRAIN [3][110/1291]	Time 0.058 (0.054)	Data 8.03e-05 (1.63e-03)	Tok/s 217911 (198267)	Loss/tok 3.2883 (3.1614)	LR 2.875e-03
0: TRAIN [3][120/1291]	Time 0.076 (0.054)	Data 8.56e-05 (1.50e-03)	Tok/s 226663 (198601)	Loss/tok 3.4842 (3.1715)	LR 2.875e-03
0: Upscaling, new scale: 4096.0
0: TRAIN [3][130/1291]	Time 0.025 (0.054)	Data 8.27e-05 (1.39e-03)	Tok/s 159304 (198969)	Loss/tok 2.6302 (3.1827)	LR 2.875e-03
0: TRAIN [3][140/1291]	Time 0.058 (0.054)	Data 8.20e-05 (1.30e-03)	Tok/s 215747 (198565)	Loss/tok 3.2690 (3.1795)	LR 2.875e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 2048.0
0: TRAIN [3][150/1291]	Time 0.097 (0.054)	Data 8.11e-05 (1.22e-03)	Tok/s 226900 (198859)	Loss/tok 3.5842 (3.1818)	LR 2.875e-03
0: TRAIN [3][160/1291]	Time 0.059 (0.054)	Data 8.15e-05 (1.15e-03)	Tok/s 214344 (198741)	Loss/tok 3.2291 (3.1783)	LR 2.875e-03
0: TRAIN [3][170/1291]	Time 0.058 (0.054)	Data 1.56e-04 (1.09e-03)	Tok/s 220096 (199241)	Loss/tok 3.1572 (3.1834)	LR 2.875e-03
0: TRAIN [3][180/1291]	Time 0.025 (0.053)	Data 8.34e-05 (1.03e-03)	Tok/s 156998 (198692)	Loss/tok 2.5902 (3.1770)	LR 2.875e-03
0: TRAIN [3][190/1291]	Time 0.042 (0.054)	Data 9.13e-05 (9.84e-04)	Tok/s 187331 (199423)	Loss/tok 3.0248 (3.1828)	LR 2.875e-03
0: TRAIN [3][200/1291]	Time 0.041 (0.054)	Data 1.76e-04 (9.40e-04)	Tok/s 190981 (199683)	Loss/tok 2.9530 (3.1872)	LR 2.875e-03
0: TRAIN [3][210/1291]	Time 0.058 (0.054)	Data 8.49e-05 (9.00e-04)	Tok/s 215812 (199807)	Loss/tok 3.1198 (3.1864)	LR 1.437e-03
0: TRAIN [3][220/1291]	Time 0.041 (0.053)	Data 8.25e-05 (8.64e-04)	Tok/s 192507 (199256)	Loss/tok 2.9367 (3.1811)	LR 1.437e-03
0: TRAIN [3][230/1291]	Time 0.097 (0.054)	Data 8.46e-05 (8.31e-04)	Tok/s 230726 (199643)	Loss/tok 3.5668 (3.1867)	LR 1.437e-03
0: TRAIN [3][240/1291]	Time 0.041 (0.054)	Data 8.39e-05 (8.00e-04)	Tok/s 187439 (199774)	Loss/tok 2.8396 (3.1859)	LR 1.437e-03
0: TRAIN [3][250/1291]	Time 0.059 (0.053)	Data 1.19e-04 (7.71e-04)	Tok/s 216503 (199370)	Loss/tok 3.2557 (3.1817)	LR 1.437e-03
0: TRAIN [3][260/1291]	Time 0.041 (0.053)	Data 1.40e-04 (7.46e-04)	Tok/s 187717 (199490)	Loss/tok 2.9700 (3.1801)	LR 1.437e-03
0: TRAIN [3][270/1291]	Time 0.076 (0.053)	Data 8.34e-05 (7.21e-04)	Tok/s 228689 (199134)	Loss/tok 3.4319 (3.1768)	LR 1.437e-03
0: Upscaling, new scale: 4096.0
0: TRAIN [3][280/1291]	Time 0.058 (0.053)	Data 8.87e-05 (6.99e-04)	Tok/s 213894 (199071)	Loss/tok 3.1929 (3.1751)	LR 1.437e-03
0: TRAIN [3][290/1291]	Time 0.058 (0.053)	Data 8.27e-05 (6.78e-04)	Tok/s 216224 (199332)	Loss/tok 3.1700 (3.1748)	LR 1.437e-03
0: TRAIN [3][300/1291]	Time 0.041 (0.053)	Data 8.32e-05 (6.58e-04)	Tok/s 188985 (199204)	Loss/tok 2.9814 (3.1720)	LR 1.437e-03
0: TRAIN [3][310/1291]	Time 0.097 (0.053)	Data 8.34e-05 (6.40e-04)	Tok/s 228418 (199418)	Loss/tok 3.6347 (3.1744)	LR 1.437e-03
0: TRAIN [3][320/1291]	Time 0.026 (0.053)	Data 8.08e-05 (6.23e-04)	Tok/s 151716 (199243)	Loss/tok 2.4341 (3.1735)	LR 1.437e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 2048.0
0: TRAIN [3][330/1291]	Time 0.076 (0.053)	Data 8.13e-05 (6.07e-04)	Tok/s 225833 (199387)	Loss/tok 3.4468 (3.1740)	LR 1.437e-03
0: TRAIN [3][340/1291]	Time 0.041 (0.053)	Data 8.25e-05 (5.91e-04)	Tok/s 183525 (199376)	Loss/tok 2.9482 (3.1719)	LR 1.437e-03
0: TRAIN [3][350/1291]	Time 0.097 (0.053)	Data 8.42e-05 (5.77e-04)	Tok/s 230192 (199628)	Loss/tok 3.5308 (3.1747)	LR 1.437e-03
0: TRAIN [3][360/1291]	Time 0.042 (0.053)	Data 8.82e-05 (5.63e-04)	Tok/s 188169 (199542)	Loss/tok 2.9797 (3.1762)	LR 1.437e-03
0: TRAIN [3][370/1291]	Time 0.041 (0.053)	Data 8.25e-05 (5.51e-04)	Tok/s 187212 (199419)	Loss/tok 2.9591 (3.1739)	LR 1.437e-03
0: TRAIN [3][380/1291]	Time 0.059 (0.053)	Data 9.94e-05 (5.38e-04)	Tok/s 216206 (199561)	Loss/tok 3.2668 (3.1761)	LR 1.437e-03
0: TRAIN [3][390/1291]	Time 0.041 (0.053)	Data 8.25e-05 (5.27e-04)	Tok/s 188493 (199917)	Loss/tok 2.9752 (3.1761)	LR 1.437e-03
0: TRAIN [3][400/1291]	Time 0.058 (0.053)	Data 8.27e-05 (5.16e-04)	Tok/s 212518 (200107)	Loss/tok 3.1731 (3.1749)	LR 1.437e-03
0: TRAIN [3][410/1291]	Time 0.059 (0.053)	Data 8.25e-05 (5.06e-04)	Tok/s 215879 (200382)	Loss/tok 3.0500 (3.1757)	LR 1.437e-03
0: TRAIN [3][420/1291]	Time 0.076 (0.054)	Data 8.18e-05 (4.96e-04)	Tok/s 227167 (200475)	Loss/tok 3.4228 (3.1760)	LR 1.437e-03
0: TRAIN [3][430/1291]	Time 0.042 (0.053)	Data 8.39e-05 (4.87e-04)	Tok/s 184454 (200255)	Loss/tok 2.9810 (3.1730)	LR 1.437e-03
0: TRAIN [3][440/1291]	Time 0.076 (0.053)	Data 8.82e-05 (4.78e-04)	Tok/s 231340 (200288)	Loss/tok 3.2236 (3.1726)	LR 1.437e-03
0: TRAIN [3][450/1291]	Time 0.059 (0.053)	Data 8.73e-05 (4.69e-04)	Tok/s 208987 (200301)	Loss/tok 3.1459 (3.1730)	LR 1.437e-03
0: Upscaling, new scale: 4096.0
0: TRAIN [3][460/1291]	Time 0.076 (0.053)	Data 8.32e-05 (4.61e-04)	Tok/s 229532 (200476)	Loss/tok 3.2217 (3.1728)	LR 1.437e-03
0: TRAIN [3][470/1291]	Time 0.077 (0.053)	Data 8.39e-05 (4.53e-04)	Tok/s 227299 (200461)	Loss/tok 3.2998 (3.1725)	LR 1.437e-03
0: TRAIN [3][480/1291]	Time 0.042 (0.054)	Data 8.82e-05 (4.46e-04)	Tok/s 185773 (200739)	Loss/tok 2.8639 (3.1748)	LR 1.437e-03
0: TRAIN [3][490/1291]	Time 0.042 (0.054)	Data 8.58e-05 (4.38e-04)	Tok/s 179438 (200750)	Loss/tok 2.9614 (3.1733)	LR 1.437e-03
0: TRAIN [3][500/1291]	Time 0.025 (0.053)	Data 8.25e-05 (4.32e-04)	Tok/s 162221 (200638)	Loss/tok 2.5200 (3.1721)	LR 1.437e-03
0: TRAIN [3][510/1291]	Time 0.041 (0.053)	Data 8.08e-05 (4.25e-04)	Tok/s 188011 (200583)	Loss/tok 2.9097 (3.1711)	LR 1.437e-03
0: TRAIN [3][520/1291]	Time 0.042 (0.053)	Data 8.20e-05 (4.18e-04)	Tok/s 185925 (200485)	Loss/tok 2.9105 (3.1701)	LR 1.437e-03
0: TRAIN [3][530/1291]	Time 0.042 (0.053)	Data 8.32e-05 (4.12e-04)	Tok/s 177906 (200496)	Loss/tok 2.9618 (3.1694)	LR 1.437e-03
0: TRAIN [3][540/1291]	Time 0.058 (0.054)	Data 8.25e-05 (4.06e-04)	Tok/s 212588 (200800)	Loss/tok 3.2825 (3.1706)	LR 1.437e-03
0: TRAIN [3][550/1291]	Time 0.076 (0.053)	Data 8.20e-05 (4.01e-04)	Tok/s 228181 (200715)	Loss/tok 3.2008 (3.1686)	LR 1.437e-03
0: TRAIN [3][560/1291]	Time 0.042 (0.053)	Data 8.49e-05 (3.95e-04)	Tok/s 186031 (200746)	Loss/tok 2.9328 (3.1678)	LR 1.437e-03
0: TRAIN [3][570/1291]	Time 0.076 (0.053)	Data 8.49e-05 (3.90e-04)	Tok/s 228043 (200830)	Loss/tok 3.3587 (3.1674)	LR 1.437e-03
0: TRAIN [3][580/1291]	Time 0.059 (0.053)	Data 8.15e-05 (3.84e-04)	Tok/s 217384 (200856)	Loss/tok 3.0627 (3.1669)	LR 1.437e-03
0: Upscaling, new scale: 8192.0
0: TRAIN [3][590/1291]	Time 0.042 (0.053)	Data 8.08e-05 (3.79e-04)	Tok/s 180405 (200726)	Loss/tok 2.7864 (3.1649)	LR 1.437e-03
0: TRAIN [3][600/1291]	Time 0.059 (0.053)	Data 8.34e-05 (3.75e-04)	Tok/s 215571 (200841)	Loss/tok 3.1333 (3.1663)	LR 1.437e-03
0: TRAIN [3][610/1291]	Time 0.059 (0.053)	Data 8.30e-05 (3.70e-04)	Tok/s 213250 (200990)	Loss/tok 3.1683 (3.1654)	LR 1.437e-03
0: TRAIN [3][620/1291]	Time 0.059 (0.053)	Data 8.30e-05 (3.66e-04)	Tok/s 213339 (201108)	Loss/tok 3.1614 (3.1655)	LR 1.437e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 4096.0
0: TRAIN [3][630/1291]	Time 0.058 (0.053)	Data 7.89e-05 (3.62e-04)	Tok/s 219914 (201088)	Loss/tok 3.1834 (3.1657)	LR 1.437e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 2048.0
0: TRAIN [3][640/1291]	Time 0.042 (0.053)	Data 9.13e-05 (3.58e-04)	Tok/s 183261 (200993)	Loss/tok 2.8370 (3.1655)	LR 1.437e-03
0: TRAIN [3][650/1291]	Time 0.042 (0.053)	Data 8.32e-05 (3.53e-04)	Tok/s 185287 (200826)	Loss/tok 2.8733 (3.1634)	LR 1.437e-03
0: TRAIN [3][660/1291]	Time 0.059 (0.053)	Data 1.40e-04 (3.50e-04)	Tok/s 214922 (200838)	Loss/tok 3.1140 (3.1633)	LR 1.437e-03
0: TRAIN [3][670/1291]	Time 0.042 (0.053)	Data 7.89e-05 (3.46e-04)	Tok/s 185655 (200946)	Loss/tok 2.9298 (3.1631)	LR 1.437e-03
0: TRAIN [3][680/1291]	Time 0.042 (0.053)	Data 1.13e-04 (3.42e-04)	Tok/s 183494 (200944)	Loss/tok 2.9312 (3.1656)	LR 1.437e-03
0: TRAIN [3][690/1291]	Time 0.059 (0.054)	Data 8.80e-05 (3.38e-04)	Tok/s 213820 (201053)	Loss/tok 3.2181 (3.1673)	LR 1.437e-03
0: TRAIN [3][700/1291]	Time 0.076 (0.054)	Data 9.85e-05 (3.35e-04)	Tok/s 229263 (201219)	Loss/tok 3.3682 (3.1688)	LR 1.437e-03
0: TRAIN [3][710/1291]	Time 0.097 (0.054)	Data 9.30e-05 (3.32e-04)	Tok/s 232517 (201176)	Loss/tok 3.4612 (3.1686)	LR 1.437e-03
0: TRAIN [3][720/1291]	Time 0.059 (0.054)	Data 8.32e-05 (3.28e-04)	Tok/s 212959 (201115)	Loss/tok 3.1564 (3.1672)	LR 7.187e-04
0: TRAIN [3][730/1291]	Time 0.058 (0.054)	Data 9.54e-05 (3.25e-04)	Tok/s 220590 (201148)	Loss/tok 3.2673 (3.1669)	LR 7.187e-04
0: TRAIN [3][740/1291]	Time 0.041 (0.054)	Data 1.40e-04 (3.22e-04)	Tok/s 186894 (201296)	Loss/tok 2.8862 (3.1672)	LR 7.187e-04
0: TRAIN [3][750/1291]	Time 0.042 (0.054)	Data 8.27e-05 (3.19e-04)	Tok/s 188776 (201233)	Loss/tok 3.1059 (3.1665)	LR 7.187e-04
0: Upscaling, new scale: 4096.0
0: TRAIN [3][760/1291]	Time 0.059 (0.054)	Data 8.23e-05 (3.16e-04)	Tok/s 213499 (201197)	Loss/tok 3.0600 (3.1654)	LR 7.187e-04
0: TRAIN [3][770/1291]	Time 0.077 (0.054)	Data 9.54e-05 (3.13e-04)	Tok/s 227510 (201268)	Loss/tok 3.3890 (3.1666)	LR 7.187e-04
0: TRAIN [3][780/1291]	Time 0.058 (0.054)	Data 1.47e-04 (3.10e-04)	Tok/s 214403 (201416)	Loss/tok 3.1382 (3.1672)	LR 7.187e-04
0: TRAIN [3][790/1291]	Time 0.059 (0.054)	Data 8.27e-05 (3.07e-04)	Tok/s 217428 (201483)	Loss/tok 3.1608 (3.1674)	LR 7.187e-04
0: TRAIN [3][800/1291]	Time 0.059 (0.054)	Data 8.25e-05 (3.05e-04)	Tok/s 212044 (201476)	Loss/tok 2.9940 (3.1662)	LR 7.187e-04
0: TRAIN [3][810/1291]	Time 0.041 (0.054)	Data 1.50e-04 (3.02e-04)	Tok/s 190519 (201325)	Loss/tok 2.8695 (3.1645)	LR 7.187e-04
0: TRAIN [3][820/1291]	Time 0.025 (0.054)	Data 8.15e-05 (2.99e-04)	Tok/s 160075 (201367)	Loss/tok 2.5445 (3.1647)	LR 7.187e-04
0: TRAIN [3][830/1291]	Time 0.042 (0.054)	Data 1.04e-04 (2.97e-04)	Tok/s 185875 (201366)	Loss/tok 2.9845 (3.1644)	LR 7.187e-04
0: TRAIN [3][840/1291]	Time 0.097 (0.054)	Data 8.92e-05 (2.95e-04)	Tok/s 232806 (201445)	Loss/tok 3.4674 (3.1645)	LR 7.187e-04
0: TRAIN [3][850/1291]	Time 0.042 (0.054)	Data 8.08e-05 (2.92e-04)	Tok/s 184989 (201579)	Loss/tok 2.8826 (3.1648)	LR 7.187e-04
0: TRAIN [3][860/1291]	Time 0.058 (0.054)	Data 8.96e-05 (2.90e-04)	Tok/s 212696 (201493)	Loss/tok 3.1511 (3.1641)	LR 7.187e-04
0: TRAIN [3][870/1291]	Time 0.076 (0.054)	Data 8.20e-05 (2.88e-04)	Tok/s 232890 (201515)	Loss/tok 3.2445 (3.1639)	LR 7.187e-04
0: TRAIN [3][880/1291]	Time 0.041 (0.054)	Data 1.39e-04 (2.86e-04)	Tok/s 184217 (201386)	Loss/tok 2.9410 (3.1631)	LR 7.187e-04
0: Upscaling, new scale: 8192.0
0: TRAIN [3][890/1291]	Time 0.059 (0.054)	Data 1.07e-04 (2.83e-04)	Tok/s 215180 (201332)	Loss/tok 3.0627 (3.1620)	LR 7.187e-04
0: TRAIN [3][900/1291]	Time 0.077 (0.053)	Data 8.25e-05 (2.81e-04)	Tok/s 225887 (201288)	Loss/tok 3.2893 (3.1612)	LR 7.187e-04
0: TRAIN [3][910/1291]	Time 0.024 (0.053)	Data 9.54e-05 (2.79e-04)	Tok/s 163892 (201240)	Loss/tok 2.6281 (3.1601)	LR 7.187e-04
0: TRAIN [3][920/1291]	Time 0.025 (0.053)	Data 8.20e-05 (2.77e-04)	Tok/s 155213 (201021)	Loss/tok 2.4712 (3.1584)	LR 7.187e-04
0: TRAIN [3][930/1291]	Time 0.041 (0.053)	Data 8.18e-05 (2.75e-04)	Tok/s 183271 (201027)	Loss/tok 2.9552 (3.1575)	LR 7.187e-04
0: TRAIN [3][940/1291]	Time 0.076 (0.053)	Data 8.01e-05 (2.73e-04)	Tok/s 231137 (201037)	Loss/tok 3.1363 (3.1569)	LR 7.187e-04
0: TRAIN [3][950/1291]	Time 0.041 (0.053)	Data 8.06e-05 (2.71e-04)	Tok/s 190533 (200936)	Loss/tok 3.0104 (3.1565)	LR 7.187e-04
0: TRAIN [3][960/1291]	Time 0.077 (0.053)	Data 8.27e-05 (2.69e-04)	Tok/s 228274 (201045)	Loss/tok 3.3697 (3.1572)	LR 7.187e-04
0: TRAIN [3][970/1291]	Time 0.059 (0.053)	Data 8.39e-05 (2.67e-04)	Tok/s 214820 (201176)	Loss/tok 3.1512 (3.1575)	LR 7.187e-04
0: TRAIN [3][980/1291]	Time 0.042 (0.053)	Data 7.96e-05 (2.65e-04)	Tok/s 179103 (201016)	Loss/tok 2.9907 (3.1558)	LR 7.187e-04
0: TRAIN [3][990/1291]	Time 0.059 (0.053)	Data 8.32e-05 (2.64e-04)	Tok/s 212658 (201066)	Loss/tok 3.0712 (3.1556)	LR 7.187e-04
0: TRAIN [3][1000/1291]	Time 0.042 (0.053)	Data 1.03e-04 (2.62e-04)	Tok/s 186090 (201085)	Loss/tok 2.9811 (3.1552)	LR 7.187e-04
0: TRAIN [3][1010/1291]	Time 0.042 (0.053)	Data 8.03e-05 (2.60e-04)	Tok/s 182864 (201063)	Loss/tok 2.9899 (3.1556)	LR 7.187e-04
0: Upscaling, new scale: 8192.0
0: TRAIN [3][1020/1291]	Time 0.041 (0.053)	Data 1.39e-04 (2.59e-04)	Tok/s 191918 (201088)	Loss/tok 2.9395 (3.1552)	LR 7.187e-04
0: TRAIN [3][1030/1291]	Time 0.041 (0.053)	Data 8.56e-05 (2.57e-04)	Tok/s 188986 (201087)	Loss/tok 2.9519 (3.1547)	LR 7.187e-04
0: TRAIN [3][1040/1291]	Time 0.077 (0.053)	Data 8.99e-05 (2.55e-04)	Tok/s 230379 (201080)	Loss/tok 3.2443 (3.1544)	LR 7.187e-04
0: TRAIN [3][1050/1291]	Time 0.097 (0.053)	Data 8.08e-05 (2.54e-04)	Tok/s 232188 (201118)	Loss/tok 3.4076 (3.1545)	LR 7.187e-04
0: TRAIN [3][1060/1291]	Time 0.041 (0.053)	Data 7.96e-05 (2.52e-04)	Tok/s 189365 (201061)	Loss/tok 2.9756 (3.1539)	LR 7.187e-04
0: TRAIN [3][1070/1291]	Time 0.025 (0.053)	Data 8.34e-05 (2.51e-04)	Tok/s 159463 (200955)	Loss/tok 2.5880 (3.1528)	LR 7.187e-04
0: TRAIN [3][1080/1291]	Time 0.041 (0.053)	Data 8.70e-05 (2.49e-04)	Tok/s 183500 (200950)	Loss/tok 2.9873 (3.1520)	LR 7.187e-04
0: TRAIN [3][1090/1291]	Time 0.076 (0.053)	Data 8.23e-05 (2.48e-04)	Tok/s 229152 (201007)	Loss/tok 3.3971 (3.1516)	LR 7.187e-04
0: TRAIN [3][1100/1291]	Time 0.025 (0.053)	Data 8.03e-05 (2.46e-04)	Tok/s 164494 (200884)	Loss/tok 2.6318 (3.1503)	LR 7.187e-04
0: TRAIN [3][1110/1291]	Time 0.059 (0.053)	Data 1.41e-04 (2.45e-04)	Tok/s 214992 (200850)	Loss/tok 3.0520 (3.1501)	LR 7.187e-04
0: Gradient norm: inf
0: Skipped batch, new scale: 4096.0
0: TRAIN [3][1120/1291]	Time 0.059 (0.053)	Data 8.61e-05 (2.44e-04)	Tok/s 213515 (200923)	Loss/tok 3.1680 (3.1497)	LR 7.187e-04
0: TRAIN [3][1130/1291]	Time 0.025 (0.053)	Data 8.08e-05 (2.42e-04)	Tok/s 163186 (200905)	Loss/tok 2.5101 (3.1489)	LR 7.187e-04
0: TRAIN [3][1140/1291]	Time 0.058 (0.053)	Data 8.63e-05 (2.41e-04)	Tok/s 216949 (200929)	Loss/tok 3.0709 (3.1482)	LR 7.187e-04
0: TRAIN [3][1150/1291]	Time 0.025 (0.053)	Data 8.94e-05 (2.40e-04)	Tok/s 159908 (200884)	Loss/tok 2.5461 (3.1478)	LR 7.187e-04
0: TRAIN [3][1160/1291]	Time 0.075 (0.053)	Data 1.44e-04 (2.39e-04)	Tok/s 167019 (200887)	Loss/tok 3.1099 (3.1474)	LR 7.187e-04
0: TRAIN [3][1170/1291]	Time 0.058 (0.053)	Data 8.46e-05 (2.37e-04)	Tok/s 217908 (200857)	Loss/tok 3.0404 (3.1467)	LR 7.187e-04
0: TRAIN [3][1180/1291]	Time 0.024 (0.053)	Data 1.37e-04 (2.36e-04)	Tok/s 161360 (200916)	Loss/tok 2.5868 (3.1468)	LR 7.187e-04
0: TRAIN [3][1190/1291]	Time 0.058 (0.053)	Data 8.30e-05 (2.35e-04)	Tok/s 216666 (200960)	Loss/tok 3.1159 (3.1465)	LR 7.187e-04
0: TRAIN [3][1200/1291]	Time 0.077 (0.053)	Data 8.27e-05 (2.34e-04)	Tok/s 230471 (200937)	Loss/tok 3.1501 (3.1458)	LR 7.187e-04
0: TRAIN [3][1210/1291]	Time 0.058 (0.053)	Data 8.42e-05 (2.33e-04)	Tok/s 213851 (200866)	Loss/tok 3.0992 (3.1454)	LR 7.187e-04
0: TRAIN [3][1220/1291]	Time 0.041 (0.053)	Data 7.94e-05 (2.31e-04)	Tok/s 189979 (200873)	Loss/tok 2.8345 (3.1452)	LR 3.594e-04
0: TRAIN [3][1230/1291]	Time 0.025 (0.053)	Data 8.13e-05 (2.30e-04)	Tok/s 162822 (200783)	Loss/tok 2.6220 (3.1444)	LR 3.594e-04
0: Upscaling, new scale: 8192.0
0: TRAIN [3][1240/1291]	Time 0.058 (0.053)	Data 8.06e-05 (2.29e-04)	Tok/s 214787 (200829)	Loss/tok 3.0624 (3.1450)	LR 3.594e-04
0: TRAIN [3][1250/1291]	Time 0.076 (0.053)	Data 8.63e-05 (2.28e-04)	Tok/s 229267 (200753)	Loss/tok 3.3527 (3.1448)	LR 3.594e-04
0: TRAIN [3][1260/1291]	Time 0.025 (0.053)	Data 8.30e-05 (2.27e-04)	Tok/s 158320 (200689)	Loss/tok 2.4307 (3.1450)	LR 3.594e-04
0: TRAIN [3][1270/1291]	Time 0.041 (0.053)	Data 8.06e-05 (2.26e-04)	Tok/s 185508 (200605)	Loss/tok 2.9102 (3.1440)	LR 3.594e-04
0: TRAIN [3][1280/1291]	Time 0.042 (0.053)	Data 8.32e-05 (2.25e-04)	Tok/s 185537 (200570)	Loss/tok 2.8391 (3.1438)	LR 3.594e-04
0: Gradient norm: inf
0: Skipped batch, new scale: 4096.0
0: TRAIN [3][1290/1291]	Time 0.076 (0.053)	Data 4.34e-05 (2.25e-04)	Tok/s 229347 (200593)	Loss/tok 3.2633 (3.1442)	LR 3.594e-04
:::MLLOG {"namespace": "", "time_ms": 1593113995770, "event_type": "INTERVAL_END", "key": "epoch_stop", "value": null, "metadata": {"file": "train.py", "lineno": 551, "epoch_num": 4}}
:::MLLOG {"namespace": "", "time_ms": 1593113995770, "event_type": "INTERVAL_START", "key": "eval_start", "value": null, "metadata": {"file": "train.py", "lineno": 556, "epoch_num": 4}}
0: Running evaluation on test set
0: TEST [3][0/3]	Time 0.309 (0.309)	Decoder iters 107.0 (107.0)	Tok/s 28920 (28920)
0: Running moses detokenizer
0: BLEU(score=24.17356061109929, counts=[37132, 18642, 10624, 6304], totals=[65297, 62294, 59291, 56293], precisions=[56.866318513867405, 29.925835553985937, 17.918402455684674, 11.198550441440322], bp=1.0, sys_len=65297, ref_len=64676)
0: Target accuracy reached
0: Finished evaluation on test set
:::MLLOG {"namespace": "", "time_ms": 1593113996967, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.24170000000000003, "metadata": {"file": "train.py", "lineno": 562, "epoch_num": 4}}
:::MLLOG {"namespace": "", "time_ms": 1593113996967, "event_type": "INTERVAL_END", "key": "eval_stop", "value": null, "metadata": {"file": "train.py", "lineno": 565, "epoch_num": 4}}
0: Summary: Epoch: 3	Training Loss: 3.1467	Test BLEU: 24.17
0: Performance: Epoch: 3	Training: 3208738 Tok/s
0: Finished epoch 3
:::MLLOG {"namespace": "", "time_ms": 1593113996967, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "train.py", "lineno": 584, "first_epoch_num": 4}}
0: Closing preprocessed data file
:::MLLOG {"namespace": "", "time_ms": 1593113996968, "event_type": "INTERVAL_END", "key": "run_stop", "value": null, "metadata": {"file": "train.py", "lineno": 595, "status": "success"}}
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
ENDING TIMING RUN AT 2020-06-25 12:40:03 PM
RESULT,RNN_TRANSLATOR,,311,nvidia,2020-06-25 12:34:52 PM
ENDING TIMING RUN AT 2020-06-25 12:40:03 PM
RESULT,RNN_TRANSLATOR,,311,nvidia,2020-06-25 12:34:52 PM
ENDING TIMING RUN AT 2020-06-25 12:40:04 PM
RESULT,RNN_TRANSLATOR,,312,nvidia,2020-06-25 12:34:52 PM
ENDING TIMING RUN AT 2020-06-25 12:40:04 PM
ENDING TIMING RUN AT 2020-06-25 12:40:04 PM
RESULT,RNN_TRANSLATOR,,312,nvidia,2020-06-25 12:34:52 PM
RESULT,RNN_TRANSLATOR,,312,nvidia,2020-06-25 12:34:52 PM
ENDING TIMING RUN AT 2020-06-25 12:40:04 PM
RESULT,RNN_TRANSLATOR,,312,nvidia,2020-06-25 12:34:52 PM
ENDING TIMING RUN AT 2020-06-25 12:40:04 PM
ENDING TIMING RUN AT 2020-06-25 12:40:04 PM
RESULT,RNN_TRANSLATOR,,312,nvidia,2020-06-25 12:34:52 PM
RESULT,RNN_TRANSLATOR,,312,nvidia,2020-06-25 12:34:52 PM
ENDING TIMING RUN AT 2020-06-25 12:40:04 PM
ENDING TIMING RUN AT 2020-06-25 12:40:04 PM
RESULT,RNN_TRANSLATOR,,312,nvidia,2020-06-25 12:34:52 PM
RESULT,RNN_TRANSLATOR,,312,nvidia,2020-06-25 12:34:52 PM
ENDING TIMING RUN AT 2020-06-25 12:40:04 PM
ENDING TIMING RUN AT 2020-06-25 12:40:04 PM
RESULT,RNN_TRANSLATOR,,312,nvidia,2020-06-25 12:34:52 PM
RESULT,RNN_TRANSLATOR,,312,nvidia,2020-06-25 12:34:52 PM
ENDING TIMING RUN AT 2020-06-25 12:40:04 PM
RESULT,RNN_TRANSLATOR,,312,nvidia,2020-06-25 12:34:52 PM
ENDING TIMING RUN AT 2020-06-25 12:40:04 PM
RESULT,RNN_TRANSLATOR,,312,nvidia,2020-06-25 12:34:52 PM
ENDING TIMING RUN AT 2020-06-25 12:40:04 PM
RESULT,RNN_TRANSLATOR,,312,nvidia,2020-06-25 12:34:52 PM
ENDING TIMING RUN AT 2020-06-25 12:40:04 PM
RESULT,RNN_TRANSLATOR,,312,nvidia,2020-06-25 12:34:52 PM
