+ echo 'Beginning trial 1 of 5'
Beginning trial 1 of 5
+ srun --ntasks=2 --container-name=rnn_translator python -c '
import mlperf_log_utils
from mlperf_logging.mllog import constants
mlperf_log_utils.mlperf_submission_log(constants.GNMT)'
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
:::MLLOG {"namespace": "", "time_ms": 1593112967847, "event_type": "POINT_IN_TIME", "key": "submission_benchmark", "value": "gnmt", "metadata": {"file": "/workspace/rnn_translator/mlperf_log_utils.py", "lineno": 19}}
:::MLLOG {"namespace": "", "time_ms": 1593112967885, "event_type": "POINT_IN_TIME", "key": "submission_org", "value": "NVIDIA", "metadata": {"file": "/workspace/rnn_translator/mlperf_log_utils.py", "lineno": 24}}
:::MLLOG {"namespace": "", "time_ms": 1593112967885, "event_type": "POINT_IN_TIME", "key": "submission_division", "value": "closed", "metadata": {"file": "/workspace/rnn_translator/mlperf_log_utils.py", "lineno": 28}}
:::MLLOG {"namespace": "", "time_ms": 1593112967885, "event_type": "POINT_IN_TIME", "key": "submission_status", "value": "onprem", "metadata": {"file": "/workspace/rnn_translator/mlperf_log_utils.py", "lineno": 32}}
:::MLLOG {"namespace": "", "time_ms": 1593112967885, "event_type": "POINT_IN_TIME", "key": "submission_platform", "value": "2xNVIDIA DGX A100", "metadata": {"file": "/workspace/rnn_translator/mlperf_log_utils.py", "lineno": 36}}
+ '[' 1 -eq 1 ']'
+ srun --ntasks=2 bash -c 'echo -n '\''Clearing cache on '\'' && hostname && sync && sudo /sbin/sysctl vm.drop_caches=3'
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
Clearing cache on luna-0102
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
Clearing cache on luna-0101
vm.drop_caches = 3
vm.drop_caches = 3
+ srun --ntasks=2 --container-name=rnn_translator python -c '
from mlperf_logging.mllog import constants
from seq2seq.utils import log_event
log_event(key=constants.CACHE_CLEAR, value=True)'
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
:::MLLOG {"namespace": "", "time_ms": 1593112973382, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
:::MLLOG {"namespace": "", "time_ms": 1593112973437, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
+ srun --mpi=none --ntasks=16 --ntasks-per-node=8 --container-name=rnn_translator --container-mounts=/lustre/fsr/datasets/wmt16_de_en:/data,/raid/scratch/svcnvdlfw/gnmt:/preproc_data,/lustre/fsw/mlperf-ci/14251653/results:/results ./run_and_time.sh
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
STARTING TIMING RUN AT 2020-06-25 12:22:55 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=2.875e-3
+ TRAIN_BATCH_SIZE=192
+ TEST_BATCH_SIZE=64
+ WARMUP_STEPS=200
+ REMAIN_STEPS=4054
+ DECAY_INTERVAL=506
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=8
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 3    --dwu-num-chunks 2    --dwu-num-rs-pg 1    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-overlap-reductions    --dwu-grad-norm    '
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 6 ']'
+ '[' 16 -gt 2 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGXA100_multi_2x8x192_dist == *\D\G\X\A\1\0\0* ]]
Using TCMalloc
+ echo 'Using TCMalloc'
+ export LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
running benchmark
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 192 --test-batch-size 64 --optimizer FusedAdam --lr 2.875e-3 --warmup-steps 200 --remain-steps 4054 --decay-interval 506 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
STARTING TIMING RUN AT 2020-06-25 12:22:55 PM
Using TCMalloc
running benchmark
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=2.875e-3
+ TRAIN_BATCH_SIZE=192
+ TEST_BATCH_SIZE=64
+ WARMUP_STEPS=200
+ REMAIN_STEPS=4054
+ DECAY_INTERVAL=506
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=8
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 3    --dwu-num-chunks 2    --dwu-num-rs-pg 1    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-overlap-reductions    --dwu-grad-norm    '
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 5 ']'
+ '[' 16 -gt 2 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGXA100_multi_2x8x192_dist == *\D\G\X\A\1\0\0* ]]
+ echo 'Using TCMalloc'
+ export LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 192 --test-batch-size 64 --optimizer FusedAdam --lr 2.875e-3 --warmup-steps 200 --remain-steps 4054 --decay-interval 506 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
STARTING TIMING RUN AT 2020-06-25 12:22:55 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=2.875e-3
+ TRAIN_BATCH_SIZE=192
+ TEST_BATCH_SIZE=64
+ WARMUP_STEPS=200
+ REMAIN_STEPS=4054
+ DECAY_INTERVAL=506
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=8
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 3    --dwu-num-chunks 2    --dwu-num-rs-pg 1    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-overlap-reductions    --dwu-grad-norm    '
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 7 ']'
+ '[' 16 -gt 2 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGXA100_multi_2x8x192_dist == *\D\G\X\A\1\0\0* ]]
+ echo 'Using TCMalloc'
Using TCMalloc
+ export LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
running benchmark
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 192 --test-batch-size 64 --optimizer FusedAdam --lr 2.875e-3 --warmup-steps 200 --remain-steps 4054 --decay-interval 506 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
STARTING TIMING RUN AT 2020-06-25 12:22:55 PM
STARTING TIMING RUN AT 2020-06-25 12:22:55 PM
+ DATASET_DIR=/data
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=2.875e-3
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=2.875e-3
+ TRAIN_BATCH_SIZE=192
+ TRAIN_BATCH_SIZE=192
+ TEST_BATCH_SIZE=64
+ TEST_BATCH_SIZE=64
+ WARMUP_STEPS=200
+ WARMUP_STEPS=200
+ REMAIN_STEPS=4054
+ REMAIN_STEPS=4054
+ DECAY_INTERVAL=506
+ DECAY_INTERVAL=506
+ TARGET=24.0
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=8
+ NUMEPOCHS=8
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 3    --dwu-num-chunks 2    --dwu-num-rs-pg 1    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-overlap-reductions    --dwu-grad-norm    '
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 3    --dwu-num-chunks 2    --dwu-num-rs-pg 1    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-overlap-reductions    --dwu-grad-norm    '
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    '
+ MATH=fp16
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    '
+ MATH=fp16
+ declare -a CMD
+ declare -a CMD
+ '[' -n 0 ']'
+ '[' -n 2 ']'
+ '[' 16 -gt 2 ']'
+ '[' 16 -gt 2 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGXA100_multi_2x8x192_dist == *\D\G\X\A\1\0\0* ]]
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGXA100_multi_2x8x192_dist == *\D\G\X\A\1\0\0* ]]
+ echo 'Using TCMalloc'
Using TCMalloc
+ export LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ echo 'running benchmark'
Using TCMalloc
+ echo 'Using TCMalloc'
+ export LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ echo 'running benchmark'
running benchmark
running benchmark
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 192 --test-batch-size 64 --optimizer FusedAdam --lr 2.875e-3 --warmup-steps 200 --remain-steps 4054 --decay-interval 506 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 192 --test-batch-size 64 --optimizer FusedAdam --lr 2.875e-3 --warmup-steps 200 --remain-steps 4054 --decay-interval 506 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
STARTING TIMING RUN AT 2020-06-25 12:22:55 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=2.875e-3
+ TRAIN_BATCH_SIZE=192
+ TEST_BATCH_SIZE=64
+ WARMUP_STEPS=200
+ REMAIN_STEPS=4054
+ DECAY_INTERVAL=506
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=8
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 3    --dwu-num-chunks 2    --dwu-num-rs-pg 1    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-overlap-reductions    --dwu-grad-norm    '
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 1 ']'
+ '[' 16 -gt 2 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGXA100_multi_2x8x192_dist == *\D\G\X\A\1\0\0* ]]
Using TCMalloc
+ echo 'Using TCMalloc'
+ export LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
running benchmark
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 192 --test-batch-size 64 --optimizer FusedAdam --lr 2.875e-3 --warmup-steps 200 --remain-steps 4054 --decay-interval 506 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
STARTING TIMING RUN AT 2020-06-25 12:22:55 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=2.875e-3
+ TRAIN_BATCH_SIZE=192
+ TEST_BATCH_SIZE=64
+ WARMUP_STEPS=200
+ REMAIN_STEPS=4054
+ DECAY_INTERVAL=506
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=8
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 3    --dwu-num-chunks 2    --dwu-num-rs-pg 1    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-overlap-reductions    --dwu-grad-norm    '
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 4 ']'
+ '[' 16 -gt 2 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGXA100_multi_2x8x192_dist == *\D\G\X\A\1\0\0* ]]
+ echo 'Using TCMalloc'
Using TCMalloc
+ export LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
running benchmark
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 192 --test-batch-size 64 --optimizer FusedAdam --lr 2.875e-3 --warmup-steps 200 --remain-steps 4054 --decay-interval 506 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
STARTING TIMING RUN AT 2020-06-25 12:22:55 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=2.875e-3
+ TRAIN_BATCH_SIZE=192
+ TEST_BATCH_SIZE=64
+ WARMUP_STEPS=200
+ REMAIN_STEPS=4054
+ DECAY_INTERVAL=506
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=8
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 3    --dwu-num-chunks 2    --dwu-num-rs-pg 1    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-overlap-reductions    --dwu-grad-norm    '
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 3 ']'
+ '[' 16 -gt 2 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGXA100_multi_2x8x192_dist == *\D\G\X\A\1\0\0* ]]
Using TCMalloc
+ echo 'Using TCMalloc'
+ export LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
running benchmark
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 192 --test-batch-size 64 --optimizer FusedAdam --lr 2.875e-3 --warmup-steps 200 --remain-steps 4054 --decay-interval 506 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
STARTING TIMING RUN AT 2020-06-25 12:22:55 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=2.875e-3
+ TRAIN_BATCH_SIZE=192
+ TEST_BATCH_SIZE=64
+ WARMUP_STEPS=200
+ REMAIN_STEPS=4054
+ DECAY_INTERVAL=506
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=8
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 3    --dwu-num-chunks 2    --dwu-num-rs-pg 1    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-overlap-reductions    --dwu-grad-norm    '
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 1 ']'
+ '[' 16 -gt 2 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
Using TCMalloc
running benchmark
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGXA100_multi_2x8x192_dist == *\D\G\X\A\1\0\0* ]]
+ echo 'Using TCMalloc'
+ export LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 192 --test-batch-size 64 --optimizer FusedAdam --lr 2.875e-3 --warmup-steps 200 --remain-steps 4054 --decay-interval 506 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
STARTING TIMING RUN AT 2020-06-25 12:22:55 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=2.875e-3
+ TRAIN_BATCH_SIZE=192
+ TEST_BATCH_SIZE=64
+ WARMUP_STEPS=200
+ REMAIN_STEPS=4054
+ DECAY_INTERVAL=506
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=8
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 3    --dwu-num-chunks 2    --dwu-num-rs-pg 1    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-overlap-reductions    --dwu-grad-norm    '
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 6 ']'
+ '[' 16 -gt 2 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGXA100_multi_2x8x192_dist == *\D\G\X\A\1\0\0* ]]
Using TCMalloc
+ echo 'Using TCMalloc'
+ export LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ echo 'running benchmark'
running benchmark
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 192 --test-batch-size 64 --optimizer FusedAdam --lr 2.875e-3 --warmup-steps 200 --remain-steps 4054 --decay-interval 506 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
STARTING TIMING RUN AT 2020-06-25 12:22:55 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=2.875e-3
+ TRAIN_BATCH_SIZE=192
+ TEST_BATCH_SIZE=64
+ WARMUP_STEPS=200
+ REMAIN_STEPS=4054
+ DECAY_INTERVAL=506
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=8
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 3    --dwu-num-chunks 2    --dwu-num-rs-pg 1    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-overlap-reductions    --dwu-grad-norm    '
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 0 ']'
+ '[' 16 -gt 2 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGXA100_multi_2x8x192_dist == *\D\G\X\A\1\0\0* ]]
Using TCMalloc
+ echo 'Using TCMalloc'
+ export LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
running benchmark
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 192 --test-batch-size 64 --optimizer FusedAdam --lr 2.875e-3 --warmup-steps 200 --remain-steps 4054 --decay-interval 506 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
STARTING TIMING RUN AT 2020-06-25 12:22:55 PM
STARTING TIMING RUN AT 2020-06-25 12:22:55 PM
+ DATASET_DIR=/data
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ RESULTS_DIR=gnmt_wmt16
+ LR=2.875e-3
+ LR=2.875e-3
+ TRAIN_BATCH_SIZE=192
+ TRAIN_BATCH_SIZE=192
+ TEST_BATCH_SIZE=64
+ TEST_BATCH_SIZE=64
+ WARMUP_STEPS=200
+ WARMUP_STEPS=200
+ REMAIN_STEPS=4054
+ DECAY_INTERVAL=506
+ REMAIN_STEPS=4054
+ TARGET=24.0
+ DECAY_INTERVAL=506
+ MAX_SEQ_LEN=75
+ TARGET=24.0
STARTING TIMING RUN AT 2020-06-25 12:22:55 PM
+ NUMEPOCHS=8
+ MAX_SEQ_LEN=75
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 3    --dwu-num-chunks 2    --dwu-num-rs-pg 1    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-overlap-reductions    --dwu-grad-norm    '
+ NUMEPOCHS=8
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    '
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 3    --dwu-num-chunks 2    --dwu-num-rs-pg 1    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-overlap-reductions    --dwu-grad-norm    '
+ MATH=fp16
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ declare -a CMD
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    '
+ MATH=fp16
+ RESULTS_DIR=gnmt_wmt16
+ LR=2.875e-3
+ declare -a CMD
+ TRAIN_BATCH_SIZE=192
+ TEST_BATCH_SIZE=64
+ WARMUP_STEPS=200
+ REMAIN_STEPS=4054
+ DECAY_INTERVAL=506
+ TARGET=24.0
+ '[' -n 4 ']'
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=8
STARTING TIMING RUN AT 2020-06-25 12:22:55 PM
+ '[' 16 -gt 2 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 3    --dwu-num-chunks 2    --dwu-num-rs-pg 1    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-overlap-reductions    --dwu-grad-norm    '
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    '
+ '[' -n 5 ']'
+ '[' 16 -gt 2 ']'
+ MATH=fp16
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ declare -a CMD
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ RESULTS_DIR=gnmt_wmt16
+ LR=2.875e-3
+ [[ DGXA100_multi_2x8x192_dist == *\D\G\X\A\1\0\0* ]]
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ echo 'Using TCMalloc'
+ TRAIN_BATCH_SIZE=192
Using TCMalloc
+ export LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ [[ DGXA100_multi_2x8x192_dist == *\D\G\X\A\1\0\0* ]]
+ echo 'Using TCMalloc'
+ export LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ TEST_BATCH_SIZE=64
+ WARMUP_STEPS=200
running benchmark
+ echo 'running benchmark'
Using TCMalloc
running benchmark
+ LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ echo 'running benchmark'
+ '[' -n 7 ']'
+ '[' 16 -gt 2 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ REMAIN_STEPS=4054
+ DECAY_INTERVAL=506
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=8
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGXA100_multi_2x8x192_dist == *\D\G\X\A\1\0\0* ]]
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 3    --dwu-num-chunks 2    --dwu-num-rs-pg 1    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-overlap-reductions    --dwu-grad-norm    '
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    '
Using TCMalloc
+ echo 'Using TCMalloc'
+ export LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ MATH=fp16
+ declare -a CMD
running benchmark
+ echo 'running benchmark'
+ '[' -n 2 ']'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 192 --test-batch-size 64 --optimizer FusedAdam --lr 2.875e-3 --warmup-steps 200 --remain-steps 4054 --decay-interval 506 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
+ '[' 16 -gt 2 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 192 --test-batch-size 64 --optimizer FusedAdam --lr 2.875e-3 --warmup-steps 200 --remain-steps 4054 --decay-interval 506 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGXA100_multi_2x8x192_dist == *\D\G\X\A\1\0\0* ]]
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 192 --test-batch-size 64 --optimizer FusedAdam --lr 2.875e-3 --warmup-steps 200 --remain-steps 4054 --decay-interval 506 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
Using TCMalloc
+ echo 'Using TCMalloc'
+ export LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
running benchmark
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 192 --test-batch-size 64 --optimizer FusedAdam --lr 2.875e-3 --warmup-steps 200 --remain-steps 4054 --decay-interval 506 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
STARTING TIMING RUN AT 2020-06-25 12:22:55 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=2.875e-3
+ TRAIN_BATCH_SIZE=192
+ TEST_BATCH_SIZE=64
+ WARMUP_STEPS=200
+ REMAIN_STEPS=4054
+ DECAY_INTERVAL=506
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=8
+ DIST_OPTS='   --distributed-weight-update 2    --dwu-num-blocks 3    --dwu-num-chunks 2    --dwu-num-rs-pg 1    --dwu-num-ar-pg 2    --dwu-num-ag-pg 0    --dwu-overlap-reductions    --dwu-grad-norm    '
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 3 ']'
+ '[' 16 -gt 2 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGXA100_multi_2x8x192_dist == *\D\G\X\A\1\0\0* ]]
Using TCMalloc
+ echo 'Using TCMalloc'
+ export LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
+ LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4
running benchmark
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 192 --test-batch-size 64 --optimizer FusedAdam --lr 2.875e-3 --warmup-steps 200 --remain-steps 4054 --decay-interval 506 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
num_sockets = 2 num_nodes=8 cores_per_socket=64
+ exec numactl --physcpubind=96-111,224-239 --membind=6 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 192 --test-batch-size 64 --optimizer FusedAdam --lr 2.875e-3 --warmup-steps 200 --remain-steps 4054 --decay-interval 506 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
num_sockets = 2 num_nodes=8 cores_per_socket=64
num_sockets = 2 num_nodes=8 cores_per_socket=64
+ exec numactl --physcpubind=64-79,192-207 --membind=4 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 192 --test-batch-size 64 --optimizer FusedAdam --lr 2.875e-3 --warmup-steps 200 --remain-steps 4054 --decay-interval 506 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
+ exec numactl --physcpubind=80-95,208-223 --membind=5 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 192 --test-batch-size 64 --optimizer FusedAdam --lr 2.875e-3 --warmup-steps 200 --remain-steps 4054 --decay-interval 506 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
num_sockets = 2 num_nodes=8 cores_per_socket=64
num_sockets = 2 num_nodes=8 cores_per_socket=64
+ exec numactl --physcpubind=96-111,224-239 --membind=6 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 192 --test-batch-size 64 --optimizer FusedAdam --lr 2.875e-3 --warmup-steps 200 --remain-steps 4054 --decay-interval 506 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
num_sockets = 2 num_nodes=8 cores_per_socket=64
+ exec numactl --physcpubind=16-31,144-159 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 192 --test-batch-size 64 --optimizer FusedAdam --lr 2.875e-3 --warmup-steps 200 --remain-steps 4054 --decay-interval 506 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
+ exec numactl --physcpubind=112-127,240-255 --membind=7 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 192 --test-batch-size 64 --optimizer FusedAdam --lr 2.875e-3 --warmup-steps 200 --remain-steps 4054 --decay-interval 506 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
num_sockets = 2 num_nodes=8 cores_per_socket=64
+ exec numactl --physcpubind=112-127,240-255 --membind=7 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 192 --test-batch-size 64 --optimizer FusedAdam --lr 2.875e-3 --warmup-steps 200 --remain-steps 4054 --decay-interval 506 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
num_sockets = 2 num_nodes=8 cores_per_socket=64
+ exec numactl --physcpubind=32-47,160-175 --membind=2 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 192 --test-batch-size 64 --optimizer FusedAdam --lr 2.875e-3 --warmup-steps 200 --remain-steps 4054 --decay-interval 506 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
num_sockets = 2 num_nodes=8 cores_per_socket=64
num_sockets = 2 num_nodes=8 cores_per_socket=64
num_sockets = 2 num_nodes=8 cores_per_socket=64
+ exec numactl --physcpubind=64-79,192-207 --membind=4 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 192 --test-batch-size 64 --optimizer FusedAdam --lr 2.875e-3 --warmup-steps 200 --remain-steps 4054 --decay-interval 506 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
+ exec numactl --physcpubind=48-63,176-191 --membind=3 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 192 --test-batch-size 64 --optimizer FusedAdam --lr 2.875e-3 --warmup-steps 200 --remain-steps 4054 --decay-interval 506 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
+ exec numactl --physcpubind=0-15,128-143 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 192 --test-batch-size 64 --optimizer FusedAdam --lr 2.875e-3 --warmup-steps 200 --remain-steps 4054 --decay-interval 506 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
num_sockets = 2 num_nodes=8 cores_per_socket=64
num_sockets = 2 num_nodes=8 cores_per_socket=64
num_sockets = 2 num_nodes=8 cores_per_socket=64
num_sockets = 2 num_nodes=8 cores_per_socket=64
num_sockets = 2 num_nodes=8 cores_per_socket=64
+ exec numactl --physcpubind=80-95,208-223 --membind=5 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 192 --test-batch-size 64 --optimizer FusedAdam --lr 2.875e-3 --warmup-steps 200 --remain-steps 4054 --decay-interval 506 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
+ exec numactl --physcpubind=0-15,128-143 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 192 --test-batch-size 64 --optimizer FusedAdam --lr 2.875e-3 --warmup-steps 200 --remain-steps 4054 --decay-interval 506 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
+ exec numactl --physcpubind=48-63,176-191 --membind=3 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 192 --test-batch-size 64 --optimizer FusedAdam --lr 2.875e-3 --warmup-steps 200 --remain-steps 4054 --decay-interval 506 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
+ exec numactl --physcpubind=32-47,160-175 --membind=2 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 192 --test-batch-size 64 --optimizer FusedAdam --lr 2.875e-3 --warmup-steps 200 --remain-steps 4054 --decay-interval 506 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
+ exec numactl --physcpubind=16-31,144-159 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 8 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 192 --test-batch-size 64 --optimizer FusedAdam --lr 2.875e-3 --warmup-steps 200 --remain-steps 4054 --decay-interval 506 --distributed-weight-update 2 --dwu-num-blocks 3 --dwu-num-chunks 2 --dwu-num-rs-pg 1 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-grad-norm --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks
:::MLLOG {"namespace": "", "time_ms": 1593112977463, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593112977594, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593112977600, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593112977605, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593112977631, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593112977644, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593112977671, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593112977680, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593112977684, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593112977701, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593112977704, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593112977711, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593112977714, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593112977719, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593112977717, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1593112977736, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
NCCL version 2.7.5+cuda11.0
0: Saving results to: results/gnmt_wmt16
0: Run arguments: Namespace(apex_message_size=10000000.0, apex_num_allreduce_streams=1, batching='bucketing', beam_size=5, cov_penalty_factor=0.1, cuda=True, cudnn=True, dataset_dir='/data', decay_factor=0.5, decay_interval=506, decay_steps=4, distributed_weight_update=2, dropout=0.2, dwu_e5m2_allgather=False, dwu_full_pipeline=False, dwu_grad_norm=True, dwu_group_size=0, dwu_num_ag_pg=0, dwu_num_ar_pg=2, dwu_num_blocks=3, dwu_num_chunks=2, dwu_num_rs_pg=1, dwu_overlap_reductions=True, enable_apex_allreduce_overlap=False, env=False, epochs=8, eval=True, fused_attention=True, fused_xentropy=True, grad_clip=5.0, hidden_size=1024, init_scale=1024, intra_epoch_eval=0, keep_checkpoints=0, len_norm_const=5.0, len_norm_factor=0.6, local_rank=0, log_all_ranks=False, lr=0.002875, math='fp16', max_length_test=150, max_length_train=75, max_size=None, min_length_test=0, min_length_train=0, num_buckets=5, num_layers=4, optimizer='FusedAdam', optimizer_extra='{}', prealloc_mode='once', preproc_data_dir='/preproc_data/75', print_freq=10, rank=0, remain_steps=4054, results_dir='results', resume=None, save='gnmt_wmt16', save_all=False, save_freq=5000, save_path='results/gnmt_wmt16', seed=None, shard_size=80, share_embedding=True, smoothing=0.1, start_epoch=0, target_bleu=24.0, test_batch_size=64, test_loader_workers=0, train_batch_size=192, train_global_batch_size=None, train_iter_size=1, train_loader_workers=1, upscale_interval=128, use_preproc_data=True, warmup_steps=200)
0: L2 promotion: 128B
0: Using random master seed: 266021007
:::MLLOG {"namespace": "", "time_ms": 1593112987747, "event_type": "POINT_IN_TIME", "key": "seed", "value": 266021007, "metadata": {"file": "/workspace/rnn_translator/seq2seq/utils.py", "lineno": 144}}
0: Worker 0 is using worker seed: 753651973
0: Building vocabulary from /data/vocab.bpe.32000
0: Size of vocabulary: 32320
0: Fused attention flag set to True
0: GNMT(
  (embedder): Embedding(32320, 1024, padding_idx=0)
  (encoder): ResidualRecurrentEncoder(
    (rnn_layers): ModuleList(
      (0): EmuBidirLSTM(
        (bidir): LSTM(1024, 1024, bidirectional=True)
        (layer1): LSTM(1024, 1024)
        (layer2): LSTM(1024, 1024)
      )
      (1): LSTM(2048, 1024)
      (2): LSTM(1024, 1024)
      (3): LSTM(1024, 1024)
    )
    (dropout): Dropout(p=0.2, inplace=False)
    (embedder): Embedding(32320, 1024, padding_idx=0)
  )
  (decoder): ResidualRecurrentDecoder(
    (att_rnn): RecurrentAttention(
      (rnn): LSTM(1024, 1024)
      (attn): BahdanauAttention(
        (linear_q): Linear(in_features=1024, out_features=1024, bias=False)
        (linear_k): Linear(in_features=1024, out_features=1024, bias=False)
      )
      (dropout): Dropout(p=0.2, inplace=False)
    )
    (rnn_layers): ModuleList(
      (0): LSTM(2048, 1024)
      (1): LSTM(2048, 1024)
      (2): LSTM(2048, 1024)
    )
    (embedder): Embedding(32320, 1024, padding_idx=0)
    (classifier): Classifier(
      (classifier): Linear(in_features=1024, out_features=32320, bias=True)
    )
    (dropout): Dropout(p=0.2, inplace=False)
  )
)
0: Building LabelSmoothingLoss (smoothing: 0.1)
0: Fused xentropy flag set to True
0: Training optimizer config: {'optimizer': 'FusedAdam', 'lr': 0.002875}
0: Number of parameters: 160671297
0: Saving state of the tokenizer
self._net_total_param_size=160671360, self._total_param_size=160677888, dwu_min_page_size=12288, self._block_size=53559296, self._chunk_size=26779648, self._shard_size=3347456
[0, 9, 34]
self._net_total_param_size=160671360, self._total_param_size=160677888, dwu_min_page_size=12288, self._block_size=53559296, self._chunk_size=26779648, self._shard_size=3347456
[0, 9, 34]
self._net_total_param_size=160671360, self._total_param_size=160677888, dwu_min_page_size=12288, self._block_size=53559296, self._chunk_size=26779648, self._shard_size=3347456
[0, 9, 34]
self._net_total_param_size=160671360, self._total_param_size=160677888, dwu_min_page_size=12288, self._block_size=53559296, self._chunk_size=26779648, self._shard_size=3347456
[0, 9, 34]
self._net_total_param_size=160671360, self._total_param_size=160677888, dwu_min_page_size=12288, self._block_size=53559296, self._chunk_size=26779648, self._shard_size=3347456
[0, 9, 34]
self._net_total_param_size=160671360, self._total_param_size=160677888, dwu_min_page_size=12288, self._block_size=53559296, self._chunk_size=26779648, self._shard_size=3347456
[0, 9, 34]
self._net_total_param_size=160671360, self._total_param_size=160677888, dwu_min_page_size=12288, self._block_size=53559296, self._chunk_size=26779648, self._shard_size=3347456
[0, 9, 34]
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
self._net_total_param_size=160671360, self._total_param_size=160677888, dwu_min_page_size=12288, self._block_size=53559296, self._chunk_size=26779648, self._shard_size=3347456
[0, 9, 34]
model_param_fragment.size()=torch.Size([1314816]), new_param_packed_fragment.size()=torch.Size([1314816]), master_param_fragment.size()=torch.Size([1314816])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([2024448]), new_param_packed_fragment.size()=torch.Size([2024448]), master_param_fragment.size()=torch.Size([2024448])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
self._net_total_param_size=160671360, self._total_param_size=160677888, dwu_min_page_size=12288, self._block_size=53559296, self._chunk_size=26779648, self._shard_size=3347456
[0, 9, 34]
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([2832448]), new_param_packed_fragment.size()=torch.Size([2832448]), master_param_fragment.size()=torch.Size([2832448])
model_param_fragment.size()=torch.Size([515008]), new_param_packed_fragment.size()=torch.Size([515008]), master_param_fragment.size()=torch.Size([515008])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([2169856]), new_param_packed_fragment.size()=torch.Size([2169856]), master_param_fragment.size()=torch.Size([2169856])
model_param_fragment.size()=torch.Size([1177600]), new_param_packed_fragment.size()=torch.Size([1177600]), master_param_fragment.size()=torch.Size([1177600])
model_param_fragment.size()=torch.Size([572416]), new_param_packed_fragment.size()=torch.Size([572416]), master_param_fragment.size()=torch.Size([572416])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([2766848]), new_param_packed_fragment.size()=torch.Size([2766848]), master_param_fragment.size()=torch.Size([2766848])
model_param_fragment.size()=torch.Size([1082432]), new_param_packed_fragment.size()=torch.Size([1082432]), master_param_fragment.size()=torch.Size([1082432])
model_param_fragment.size()=torch.Size([2265024]), new_param_packed_fragment.size()=torch.Size([2265024]), master_param_fragment.size()=torch.Size([2265024])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
NCCL version 2.7.5+cuda11.0
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3016704]), new_param_packed_fragment.size()=torch.Size([3016704]), master_param_fragment.size()=torch.Size([3016704])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([322560]), new_param_packed_fragment.size()=torch.Size([322560]), master_param_fragment.size()=torch.Size([322560])
model_param_fragment.size()=torch.Size([1427456]), new_param_packed_fragment.size()=torch.Size([1427456]), master_param_fragment.size()=torch.Size([1427456])
model_param_fragment.size()=torch.Size([1920000]), new_param_packed_fragment.size()=torch.Size([1920000]), master_param_fragment.size()=torch.Size([1920000])
model_param_fragment.size()=torch.Size([1929280]), new_param_packed_fragment.size()=torch.Size([1929280]), master_param_fragment.size()=torch.Size([1929280])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([1409984]), new_param_packed_fragment.size()=torch.Size([1409984]), master_param_fragment.size()=torch.Size([1409984])
model_param_fragment.size()=torch.Size([331840]), new_param_packed_fragment.size()=torch.Size([331840]), master_param_fragment.size()=torch.Size([331840])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([3007424]), new_param_packed_fragment.size()=torch.Size([3007424]), master_param_fragment.size()=torch.Size([3007424])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
self._net_total_param_size=160671360, self._total_param_size=160677888, dwu_min_page_size=12288, self._block_size=53559296, self._chunk_size=26779648, self._shard_size=3347456
[0, 9, 34]
self._net_total_param_size=160671360, self._total_param_size=160677888, dwu_min_page_size=12288, self._block_size=53559296, self._chunk_size=26779648, self._shard_size=3347456
[0, 9, 34]
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([2169856]), new_param_packed_fragment.size()=torch.Size([2169856]), master_param_fragment.size()=torch.Size([2169856])
model_param_fragment.size()=torch.Size([1177600]), new_param_packed_fragment.size()=torch.Size([1177600]), master_param_fragment.size()=torch.Size([1177600])
model_param_fragment.size()=torch.Size([572416]), new_param_packed_fragment.size()=torch.Size([572416]), master_param_fragment.size()=torch.Size([572416])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([2766848]), new_param_packed_fragment.size()=torch.Size([2766848]), master_param_fragment.size()=torch.Size([2766848])
model_param_fragment.size()=torch.Size([1082432]), new_param_packed_fragment.size()=torch.Size([1082432]), master_param_fragment.size()=torch.Size([1082432])
model_param_fragment.size()=torch.Size([2265024]), new_param_packed_fragment.size()=torch.Size([2265024]), master_param_fragment.size()=torch.Size([2265024])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([467968]), new_param_packed_fragment.size()=torch.Size([467968]), master_param_fragment.size()=torch.Size([467968])
model_param_fragment.size()=torch.Size([2879488]), new_param_packed_fragment.size()=torch.Size([2879488]), master_param_fragment.size()=torch.Size([2879488])
model_param_fragment.size()=torch.Size([3073024]), new_param_packed_fragment.size()=torch.Size([3073024]), master_param_fragment.size()=torch.Size([3073024])
model_param_fragment.size()=torch.Size([274432]), new_param_packed_fragment.size()=torch.Size([274432]), master_param_fragment.size()=torch.Size([274432])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([1314816]), new_param_packed_fragment.size()=torch.Size([1314816]), master_param_fragment.size()=torch.Size([1314816])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([2024448]), new_param_packed_fragment.size()=torch.Size([2024448]), master_param_fragment.size()=torch.Size([2024448])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([2832448]), new_param_packed_fragment.size()=torch.Size([2832448]), master_param_fragment.size()=torch.Size([2832448])
model_param_fragment.size()=torch.Size([515008]), new_param_packed_fragment.size()=torch.Size([515008]), master_param_fragment.size()=torch.Size([515008])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
NCCL version 2.7.5+cuda11.0
0: Initializing dwu fp16 optimizer
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([2968576]), new_param_packed_fragment.size()=torch.Size([2968576]), master_param_fragment.size()=torch.Size([2968576])
model_param_fragment.size()=torch.Size([378880]), new_param_packed_fragment.size()=torch.Size([378880]), master_param_fragment.size()=torch.Size([378880])
model_param_fragment.size()=torch.Size([1371136]), new_param_packed_fragment.size()=torch.Size([1371136]), master_param_fragment.size()=torch.Size([1371136])
model_param_fragment.size()=torch.Size([1976320]), new_param_packed_fragment.size()=torch.Size([1976320]), master_param_fragment.size()=torch.Size([1976320])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([283712]), new_param_packed_fragment.size()=torch.Size([283712]), master_param_fragment.size()=torch.Size([283712])
model_param_fragment.size()=torch.Size([3063744]), new_param_packed_fragment.size()=torch.Size([3063744]), master_param_fragment.size()=torch.Size([3063744])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([2217984]), new_param_packed_fragment.size()=torch.Size([2217984]), master_param_fragment.size()=torch.Size([2217984])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
self._net_total_param_size=160671360, self._total_param_size=160677888, dwu_min_page_size=12288, self._block_size=53559296, self._chunk_size=26779648, self._shard_size=3347456
model_param_fragment.size()=torch.Size([1121280]), new_param_packed_fragment.size()=torch.Size([1121280]), master_param_fragment.size()=torch.Size([1121280])
[0, 9, 34]
model_param_fragment.size()=torch.Size([628736]), new_param_packed_fragment.size()=torch.Size([628736]), master_param_fragment.size()=torch.Size([628736])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([1024]), new_param_packed_fragment.size()=torch.Size([1024]), master_param_fragment.size()=torch.Size([1024])
model_param_fragment.size()=torch.Size([1]), new_param_packed_fragment.size()=torch.Size([1]), master_param_fragment.size()=torch.Size([1])
model_param_fragment.size()=torch.Size([1024]), new_param_packed_fragment.size()=torch.Size([1024]), master_param_fragment.size()=torch.Size([1024])
model_param_fragment.size()=torch.Size([1048576]), new_param_packed_fragment.size()=torch.Size([1048576]), master_param_fragment.size()=torch.Size([1048576])
model_param_fragment.size()=torch.Size([1048576]), new_param_packed_fragment.size()=torch.Size([1048576]), master_param_fragment.size()=torch.Size([1048576])
model_param_fragment.size()=torch.Size([611264]), new_param_packed_fragment.size()=torch.Size([611264]), master_param_fragment.size()=torch.Size([611264])
model_param_fragment.size()=torch.Size([1130560]), new_param_packed_fragment.size()=torch.Size([1130560]), master_param_fragment.size()=torch.Size([1130560])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([2208704]), new_param_packed_fragment.size()=torch.Size([2208704]), master_param_fragment.size()=torch.Size([2208704])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([2274304]), new_param_packed_fragment.size()=torch.Size([2274304]), master_param_fragment.size()=torch.Size([2274304])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([1064960]), new_param_packed_fragment.size()=torch.Size([1064960]), master_param_fragment.size()=torch.Size([1064960])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3308608]), new_param_packed_fragment.size()=torch.Size([3308608]), master_param_fragment.size()=torch.Size([3308608])
model_param_fragment.size()=torch.Size([32320]), new_param_packed_fragment.size()=torch.Size([32320]), master_param_fragment.size()=torch.Size([32320])
NCCL version 2.7.5+cuda11.0
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([467968]), new_param_packed_fragment.size()=torch.Size([467968]), master_param_fragment.size()=torch.Size([467968])
model_param_fragment.size()=torch.Size([2879488]), new_param_packed_fragment.size()=torch.Size([2879488]), master_param_fragment.size()=torch.Size([2879488])
model_param_fragment.size()=torch.Size([3073024]), new_param_packed_fragment.size()=torch.Size([3073024]), master_param_fragment.size()=torch.Size([3073024])
model_param_fragment.size()=torch.Size([274432]), new_param_packed_fragment.size()=torch.Size([274432]), master_param_fragment.size()=torch.Size([274432])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([2217984]), new_param_packed_fragment.size()=torch.Size([2217984]), master_param_fragment.size()=torch.Size([2217984])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([1121280]), new_param_packed_fragment.size()=torch.Size([1121280]), master_param_fragment.size()=torch.Size([1121280])
model_param_fragment.size()=torch.Size([628736]), new_param_packed_fragment.size()=torch.Size([628736]), master_param_fragment.size()=torch.Size([628736])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([1024]), new_param_packed_fragment.size()=torch.Size([1024]), master_param_fragment.size()=torch.Size([1024])
model_param_fragment.size()=torch.Size([1]), new_param_packed_fragment.size()=torch.Size([1]), master_param_fragment.size()=torch.Size([1])
model_param_fragment.size()=torch.Size([1024]), new_param_packed_fragment.size()=torch.Size([1024]), master_param_fragment.size()=torch.Size([1024])
model_param_fragment.size()=torch.Size([1048576]), new_param_packed_fragment.size()=torch.Size([1048576]), master_param_fragment.size()=torch.Size([1048576])
model_param_fragment.size()=torch.Size([1048576]), new_param_packed_fragment.size()=torch.Size([1048576]), master_param_fragment.size()=torch.Size([1048576])
model_param_fragment.size()=torch.Size([611264]), new_param_packed_fragment.size()=torch.Size([611264]), master_param_fragment.size()=torch.Size([611264])
model_param_fragment.size()=torch.Size([1130560]), new_param_packed_fragment.size()=torch.Size([1130560]), master_param_fragment.size()=torch.Size([1130560])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([2208704]), new_param_packed_fragment.size()=torch.Size([2208704]), master_param_fragment.size()=torch.Size([2208704])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
NCCL version 2.7.5+cuda11.0
NCCL version 2.7.5+cuda11.0
self._net_total_param_size=160671360, self._total_param_size=160677888, dwu_min_page_size=12288, self._block_size=53559296, self._chunk_size=26779648, self._shard_size=3347456
[0, 9, 34]
self._net_total_param_size=160671360, self._total_param_size=160677888, dwu_min_page_size=12288, self._block_size=53559296, self._chunk_size=26779648, self._shard_size=3347456
[0, 9, 34]
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3129344]), new_param_packed_fragment.size()=torch.Size([3129344]), master_param_fragment.size()=torch.Size([3129344])
model_param_fragment.size()=torch.Size([218112]), new_param_packed_fragment.size()=torch.Size([218112]), master_param_fragment.size()=torch.Size([218112])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3129344]), new_param_packed_fragment.size()=torch.Size([3129344]), master_param_fragment.size()=torch.Size([3129344])
model_param_fragment.size()=torch.Size([218112]), new_param_packed_fragment.size()=torch.Size([218112]), master_param_fragment.size()=torch.Size([218112])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([2968576]), new_param_packed_fragment.size()=torch.Size([2968576]), master_param_fragment.size()=torch.Size([2968576])
model_param_fragment.size()=torch.Size([378880]), new_param_packed_fragment.size()=torch.Size([378880]), master_param_fragment.size()=torch.Size([378880])
model_param_fragment.size()=torch.Size([1371136]), new_param_packed_fragment.size()=torch.Size([1371136]), master_param_fragment.size()=torch.Size([1371136])
model_param_fragment.size()=torch.Size([1976320]), new_param_packed_fragment.size()=torch.Size([1976320]), master_param_fragment.size()=torch.Size([1976320])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([283712]), new_param_packed_fragment.size()=torch.Size([283712]), master_param_fragment.size()=torch.Size([283712])
model_param_fragment.size()=torch.Size([3063744]), new_param_packed_fragment.size()=torch.Size([3063744]), master_param_fragment.size()=torch.Size([3063744])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
NCCL version 2.7.5+cuda11.0
self._net_total_param_size=160671360, self._total_param_size=160677888, dwu_min_page_size=12288, self._block_size=53559296, self._chunk_size=26779648, self._shard_size=3347456
[0, 9, 34]
self._net_total_param_size=160671360, self._total_param_size=160677888, dwu_min_page_size=12288, self._block_size=53559296, self._chunk_size=26779648, self._shard_size=3347456
[0, 9, 34]
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3016704]), new_param_packed_fragment.size()=torch.Size([3016704]), master_param_fragment.size()=torch.Size([3016704])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([322560]), new_param_packed_fragment.size()=torch.Size([322560]), master_param_fragment.size()=torch.Size([322560])
model_param_fragment.size()=torch.Size([1427456]), new_param_packed_fragment.size()=torch.Size([1427456]), master_param_fragment.size()=torch.Size([1427456])
model_param_fragment.size()=torch.Size([1920000]), new_param_packed_fragment.size()=torch.Size([1920000]), master_param_fragment.size()=torch.Size([1920000])
model_param_fragment.size()=torch.Size([1929280]), new_param_packed_fragment.size()=torch.Size([1929280]), master_param_fragment.size()=torch.Size([1929280])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([1409984]), new_param_packed_fragment.size()=torch.Size([1409984]), master_param_fragment.size()=torch.Size([1409984])
model_param_fragment.size()=torch.Size([331840]), new_param_packed_fragment.size()=torch.Size([331840]), master_param_fragment.size()=torch.Size([331840])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([3007424]), new_param_packed_fragment.size()=torch.Size([3007424]), master_param_fragment.size()=torch.Size([3007424])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
NCCL version 2.7.5+cuda11.0
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([2274304]), new_param_packed_fragment.size()=torch.Size([2274304]), master_param_fragment.size()=torch.Size([2274304])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([1064960]), new_param_packed_fragment.size()=torch.Size([1064960]), master_param_fragment.size()=torch.Size([1064960])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3347456]), new_param_packed_fragment.size()=torch.Size([3347456]), master_param_fragment.size()=torch.Size([3347456])
model_param_fragment.size()=torch.Size([3308608]), new_param_packed_fragment.size()=torch.Size([3308608]), master_param_fragment.size()=torch.Size([3308608])
model_param_fragment.size()=torch.Size([32320]), new_param_packed_fragment.size()=torch.Size([32320]), master_param_fragment.size()=torch.Size([32320])
NCCL version 2.7.5+cuda11.0
0: Using optimizer: DistributedFusedAdam (
Parameter Group 0
    betas: (0.9, 0.999)
    bias_correction: True
    eps: 1e-08
    lr: 0.002875
    max_grad_norm: 5.0
    weight_decay: 0.0
)
:::MLLOG {"namespace": "", "time_ms": 1593112999590, "event_type": "POINT_IN_TIME", "key": "opt_name", "value": "adam", "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/trainer.py", "lineno": 176}}
:::MLLOG {"namespace": "", "time_ms": 1593112999590, "event_type": "POINT_IN_TIME", "key": "opt_base_learning_rate", "value": 0.002875, "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/trainer.py", "lineno": 178}}
:::MLLOG {"namespace": "", "time_ms": 1593112999590, "event_type": "POINT_IN_TIME", "key": "opt_adam_beta_1", "value": 0.9, "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/trainer.py", "lineno": 180}}
:::MLLOG {"namespace": "", "time_ms": 1593112999590, "event_type": "POINT_IN_TIME", "key": "opt_adam_beta_2", "value": 0.999, "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/trainer.py", "lineno": 182}}
:::MLLOG {"namespace": "", "time_ms": 1593112999590, "event_type": "POINT_IN_TIME", "key": "opt_adam_epsilon", "value": 1e-08, "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/trainer.py", "lineno": 184}}
0: Executing preallocation
:::MLLOG {"namespace": "", "time_ms": 1593113001036, "event_type": "INTERVAL_END", "key": "init_stop", "value": null, "metadata": {"file": "train.py", "lineno": 429}}
:::MLLOG {"namespace": "", "time_ms": 1593113001037, "event_type": "INTERVAL_START", "key": "run_start", "value": null, "metadata": {"file": "train.py", "lineno": 430}}
:::MLLOG {"namespace": "", "time_ms": 1593113001037, "event_type": "POINT_IN_TIME", "key": "max_sequence_length", "value": 75, "metadata": {"file": "train.py", "lineno": 435, "method": "discard"}}
0: Reading preprocessed data file from /preproc_data/75/training.bin
0: Preprocessed data: length: 3975116 min length: 0 max length: 75
0: Opening preprocessed data /preproc_data/75/training.bin for reading
0: Processing data from /data/newstest2014.tok.bpe.32000.en
0: Filtering data, min len: 0, max len: 150
0: Pairs before: 3003, after: 3003
:::MLLOG {"namespace": "", "time_ms": 1593113001290, "event_type": "POINT_IN_TIME", "key": "global_batch_size", "value": 3072, "metadata": {"file": "train.py", "lineno": 480}}
:::MLLOG {"namespace": "", "time_ms": 1593113001291, "event_type": "POINT_IN_TIME", "key": "train_samples", "value": 3965952, "metadata": {"file": "train.py", "lineno": 488}}
:::MLLOG {"namespace": "", "time_ms": 1593113001291, "event_type": "POINT_IN_TIME", "key": "eval_samples", "value": 3003, "metadata": {"file": "train.py", "lineno": 490}}
0: Training LR schedule config: {'warmup_steps': 200, 'remain_steps': 4054, 'decay_interval': 506, 'decay_steps': 4, 'decay_factor': 0.5}
0: Scheduler warmup steps: 200
0: Scheduler remain steps: 4054
0: Scheduler decay interval: 506
0: Scheduler decay factor: 0.5
0: Scheduler max decay steps: 4
:::MLLOG {"namespace": "", "time_ms": 1593113001291, "event_type": "POINT_IN_TIME", "key": "opt_learning_rate_alt_decay_func", "value": true, "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/lr_scheduler.py", "lineno": 78}}
:::MLLOG {"namespace": "", "time_ms": 1593113001291, "event_type": "POINT_IN_TIME", "key": "opt_learning_rate_alt_warmup_func", "value": true, "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/lr_scheduler.py", "lineno": 80}}
:::MLLOG {"namespace": "", "time_ms": 1593113001291, "event_type": "POINT_IN_TIME", "key": "opt_learning_rate_decay_interval", "value": 506, "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/lr_scheduler.py", "lineno": 83}}
:::MLLOG {"namespace": "", "time_ms": 1593113001292, "event_type": "POINT_IN_TIME", "key": "opt_learning_rate_decay_factor", "value": 0.5, "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/lr_scheduler.py", "lineno": 85}}
:::MLLOG {"namespace": "", "time_ms": 1593113001292, "event_type": "POINT_IN_TIME", "key": "opt_learning_rate_decay_steps", "value": 4, "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/lr_scheduler.py", "lineno": 87}}
:::MLLOG {"namespace": "", "time_ms": 1593113001292, "event_type": "POINT_IN_TIME", "key": "opt_learning_rate_remain_steps", "value": 4054, "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/lr_scheduler.py", "lineno": 89}}
:::MLLOG {"namespace": "", "time_ms": 1593113001292, "event_type": "POINT_IN_TIME", "key": "opt_learning_rate_warmup_steps", "value": 200, "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/lr_scheduler.py", "lineno": 91}}
:::MLLOG {"namespace": "", "time_ms": 1593113001292, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "train.py", "lineno": 538, "first_epoch_num": 1, "epoch_count": 1}}
:::MLLOG {"namespace": "", "time_ms": 1593113001292, "event_type": "INTERVAL_START", "key": "epoch_start", "value": null, "metadata": {"file": "train.py", "lineno": 541, "epoch_num": 1}}
0: Starting epoch 0
0: Sampler for epoch 0 uses seed 2287066089
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
0: TRAIN [0][0/1291]	Time 0.302 (0.302)	Data 1.98e-01 (1.98e-01)	Tok/s 26581 (26581)	Loss/tok 10.6382 (10.6382)	LR 2.942e-05
0: TRAIN [0][10/1291]	Time 0.045 (0.076)	Data 1.44e-04 (1.81e-02)	Tok/s 175093 (186249)	Loss/tok 9.4595 (9.9657)	LR 3.704e-05
0: TRAIN [0][20/1291]	Time 0.058 (0.065)	Data 1.06e-04 (9.54e-03)	Tok/s 219407 (191951)	Loss/tok 9.2240 (9.6785)	LR 4.663e-05
0: TRAIN [0][30/1291]	Time 0.059 (0.060)	Data 8.46e-05 (6.49e-03)	Tok/s 209788 (193999)	Loss/tok 8.8898 (9.4658)	LR 5.870e-05
0: Gradient norm: inf
0: Skipped batch, new scale: 512.0
0: Gradient norm: inf
0: Skipped batch, new scale: 256.0
0: TRAIN [0][40/1291]	Time 0.041 (0.060)	Data 8.49e-05 (4.93e-03)	Tok/s 195092 (198740)	Loss/tok 8.6659 (9.2923)	LR 7.057e-05
0: TRAIN [0][50/1291]	Time 0.096 (0.058)	Data 1.02e-04 (3.98e-03)	Tok/s 229958 (199583)	Loss/tok 8.5942 (9.1453)	LR 8.885e-05
0: TRAIN [0][60/1291]	Time 0.041 (0.057)	Data 8.58e-05 (3.35e-03)	Tok/s 185592 (198737)	Loss/tok 8.1634 (9.0112)	LR 1.119e-04
0: TRAIN [0][70/1291]	Time 0.058 (0.056)	Data 8.32e-05 (2.89e-03)	Tok/s 217846 (198821)	Loss/tok 8.1174 (8.8962)	LR 1.408e-04
0: TRAIN [0][80/1291]	Time 0.058 (0.056)	Data 8.96e-05 (2.54e-03)	Tok/s 215191 (199325)	Loss/tok 8.0212 (8.7868)	LR 1.773e-04
0: Gradient norm: inf
0: Skipped batch, new scale: 128.0
0: TRAIN [0][90/1291]	Time 0.042 (0.055)	Data 9.97e-05 (2.27e-03)	Tok/s 185745 (199533)	Loss/tok 8.2402 (8.7012)	LR 2.181e-04
0: Gradient norm: inf
0: Skipped batch, new scale: 64.0
0: TRAIN [0][100/1291]	Time 0.058 (0.056)	Data 1.40e-04 (2.06e-03)	Tok/s 219919 (200846)	Loss/tok 7.9883 (8.6700)	LR 2.683e-04
0: TRAIN [0][110/1291]	Time 0.075 (0.056)	Data 8.51e-05 (1.88e-03)	Tok/s 229240 (201806)	Loss/tok 7.9496 (8.5941)	LR 3.378e-04
0: TRAIN [0][120/1291]	Time 0.058 (0.056)	Data 8.13e-05 (1.74e-03)	Tok/s 216957 (201777)	Loss/tok 7.8623 (8.5323)	LR 4.252e-04
0: TRAIN [0][130/1291]	Time 0.041 (0.056)	Data 8.13e-05 (1.61e-03)	Tok/s 187708 (201908)	Loss/tok 7.6777 (8.4789)	LR 5.354e-04
0: TRAIN [0][140/1291]	Time 0.024 (0.055)	Data 8.46e-05 (1.50e-03)	Tok/s 165545 (201564)	Loss/tok 6.8393 (8.4335)	LR 6.740e-04
0: TRAIN [0][150/1291]	Time 0.075 (0.055)	Data 8.77e-05 (1.41e-03)	Tok/s 233028 (201547)	Loss/tok 7.7583 (8.3832)	LR 8.485e-04
0: TRAIN [0][160/1291]	Time 0.042 (0.055)	Data 7.92e-05 (1.33e-03)	Tok/s 186989 (201725)	Loss/tok 7.3145 (8.3299)	LR 1.068e-03
0: TRAIN [0][170/1291]	Time 0.058 (0.055)	Data 1.32e-04 (1.25e-03)	Tok/s 214468 (201952)	Loss/tok 7.3389 (8.2687)	LR 1.345e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 32.0
0: TRAIN [0][180/1291]	Time 0.041 (0.054)	Data 7.99e-05 (1.19e-03)	Tok/s 187454 (201502)	Loss/tok 7.0652 (8.2214)	LR 1.654e-03
0: TRAIN [0][190/1291]	Time 0.042 (0.054)	Data 8.25e-05 (1.13e-03)	Tok/s 183280 (201166)	Loss/tok 6.7725 (8.1696)	LR 2.083e-03
0: TRAIN [0][200/1291]	Time 0.058 (0.054)	Data 8.56e-05 (1.08e-03)	Tok/s 217616 (200824)	Loss/tok 6.8914 (8.1163)	LR 2.622e-03
0: TRAIN [0][210/1291]	Time 0.076 (0.054)	Data 7.94e-05 (1.04e-03)	Tok/s 231309 (200898)	Loss/tok 7.0416 (8.0530)	LR 2.875e-03
0: TRAIN [0][220/1291]	Time 0.060 (0.054)	Data 7.89e-05 (9.93e-04)	Tok/s 209156 (200956)	Loss/tok 6.6365 (7.9899)	LR 2.875e-03
0: TRAIN [0][230/1291]	Time 0.041 (0.053)	Data 8.42e-05 (9.54e-04)	Tok/s 186053 (200996)	Loss/tok 6.2614 (7.9262)	LR 2.875e-03
0: TRAIN [0][240/1291]	Time 0.076 (0.054)	Data 8.25e-05 (9.18e-04)	Tok/s 230275 (201608)	Loss/tok 6.4284 (7.8428)	LR 2.875e-03
0: TRAIN [0][250/1291]	Time 0.042 (0.054)	Data 8.46e-05 (8.84e-04)	Tok/s 179536 (201509)	Loss/tok 5.8759 (7.7774)	LR 2.875e-03
0: TRAIN [0][260/1291]	Time 0.058 (0.054)	Data 1.59e-04 (8.55e-04)	Tok/s 222534 (201541)	Loss/tok 6.0520 (7.7115)	LR 2.875e-03
0: TRAIN [0][270/1291]	Time 0.041 (0.054)	Data 8.15e-05 (8.26e-04)	Tok/s 185730 (201334)	Loss/tok 5.5230 (7.6497)	LR 2.875e-03
0: TRAIN [0][280/1291]	Time 0.041 (0.054)	Data 8.39e-05 (8.00e-04)	Tok/s 187948 (201224)	Loss/tok 5.3520 (7.5877)	LR 2.875e-03
0: TRAIN [0][290/1291]	Time 0.058 (0.053)	Data 8.11e-05 (7.76e-04)	Tok/s 217227 (200980)	Loss/tok 5.7279 (7.5287)	LR 2.875e-03
0: TRAIN [0][300/1291]	Time 0.058 (0.053)	Data 8.32e-05 (7.53e-04)	Tok/s 214491 (201083)	Loss/tok 5.5591 (7.4609)	LR 2.875e-03
0: Upscaling, new scale: 64.0
0: TRAIN [0][310/1291]	Time 0.058 (0.054)	Data 8.46e-05 (7.32e-04)	Tok/s 219688 (201552)	Loss/tok 5.3396 (7.3842)	LR 2.875e-03
0: TRAIN [0][320/1291]	Time 0.075 (0.054)	Data 1.39e-04 (7.12e-04)	Tok/s 231463 (201457)	Loss/tok 5.5844 (7.3214)	LR 2.875e-03
0: TRAIN [0][330/1291]	Time 0.042 (0.054)	Data 8.32e-05 (6.93e-04)	Tok/s 186575 (201444)	Loss/tok 4.7890 (7.2577)	LR 2.875e-03
0: TRAIN [0][340/1291]	Time 0.042 (0.054)	Data 1.39e-04 (6.76e-04)	Tok/s 184559 (201702)	Loss/tok 4.8883 (7.1865)	LR 2.875e-03
0: TRAIN [0][350/1291]	Time 0.058 (0.054)	Data 8.20e-05 (6.59e-04)	Tok/s 220581 (201702)	Loss/tok 4.9344 (7.1251)	LR 2.875e-03
0: TRAIN [0][360/1291]	Time 0.042 (0.054)	Data 1.02e-04 (6.43e-04)	Tok/s 191364 (201773)	Loss/tok 4.4970 (7.0635)	LR 2.875e-03
0: TRAIN [0][370/1291]	Time 0.076 (0.054)	Data 8.13e-05 (6.28e-04)	Tok/s 230438 (201631)	Loss/tok 5.0669 (7.0074)	LR 2.875e-03
0: TRAIN [0][380/1291]	Time 0.058 (0.054)	Data 8.06e-05 (6.14e-04)	Tok/s 222655 (201441)	Loss/tok 4.6817 (6.9521)	LR 2.875e-03
0: TRAIN [0][390/1291]	Time 0.058 (0.054)	Data 7.96e-05 (6.01e-04)	Tok/s 217802 (201694)	Loss/tok 4.7270 (6.8856)	LR 2.875e-03
0: TRAIN [0][400/1291]	Time 0.059 (0.054)	Data 8.27e-05 (5.88e-04)	Tok/s 217961 (201916)	Loss/tok 4.4311 (6.8223)	LR 2.875e-03
0: TRAIN [0][410/1291]	Time 0.025 (0.054)	Data 9.30e-05 (5.76e-04)	Tok/s 159203 (201677)	Loss/tok 3.5939 (6.7772)	LR 2.875e-03
0: TRAIN [0][420/1291]	Time 0.041 (0.054)	Data 1.01e-04 (5.64e-04)	Tok/s 185058 (201646)	Loss/tok 4.1135 (6.7252)	LR 2.875e-03
0: Upscaling, new scale: 128.0
0: TRAIN [0][430/1291]	Time 0.058 (0.054)	Data 8.39e-05 (5.53e-04)	Tok/s 216638 (201790)	Loss/tok 4.4917 (6.6697)	LR 2.875e-03
0: TRAIN [0][440/1291]	Time 0.058 (0.053)	Data 8.61e-05 (5.43e-04)	Tok/s 221998 (201557)	Loss/tok 4.4535 (6.6253)	LR 2.875e-03
0: TRAIN [0][450/1291]	Time 0.058 (0.053)	Data 1.14e-04 (5.33e-04)	Tok/s 220029 (201417)	Loss/tok 4.2760 (6.5803)	LR 2.875e-03
0: TRAIN [0][460/1291]	Time 0.041 (0.053)	Data 8.63e-05 (5.23e-04)	Tok/s 189093 (201712)	Loss/tok 3.9367 (6.5228)	LR 2.875e-03
0: TRAIN [0][470/1291]	Time 0.097 (0.054)	Data 8.94e-05 (5.14e-04)	Tok/s 233338 (201890)	Loss/tok 4.6308 (6.4675)	LR 2.875e-03
0: TRAIN [0][480/1291]	Time 0.058 (0.054)	Data 8.25e-05 (5.05e-04)	Tok/s 215544 (201813)	Loss/tok 4.3859 (6.4246)	LR 2.875e-03
0: TRAIN [0][490/1291]	Time 0.041 (0.053)	Data 8.20e-05 (4.97e-04)	Tok/s 185400 (201575)	Loss/tok 3.9497 (6.3880)	LR 2.875e-03
0: TRAIN [0][500/1291]	Time 0.075 (0.053)	Data 8.58e-05 (4.89e-04)	Tok/s 229843 (201701)	Loss/tok 4.4735 (6.3409)	LR 2.875e-03
0: TRAIN [0][510/1291]	Time 0.058 (0.053)	Data 7.99e-05 (4.81e-04)	Tok/s 219235 (201849)	Loss/tok 4.2146 (6.2964)	LR 2.875e-03
0: TRAIN [0][520/1291]	Time 0.075 (0.054)	Data 8.13e-05 (4.73e-04)	Tok/s 233065 (201956)	Loss/tok 4.2272 (6.2504)	LR 2.875e-03
0: TRAIN [0][530/1291]	Time 0.042 (0.053)	Data 8.08e-05 (4.66e-04)	Tok/s 179316 (201806)	Loss/tok 3.6907 (6.2143)	LR 2.875e-03
0: TRAIN [0][540/1291]	Time 0.042 (0.053)	Data 8.27e-05 (4.59e-04)	Tok/s 182902 (201633)	Loss/tok 3.8222 (6.1798)	LR 2.875e-03
0: TRAIN [0][550/1291]	Time 0.058 (0.053)	Data 7.94e-05 (4.52e-04)	Tok/s 217834 (201561)	Loss/tok 4.2028 (6.1435)	LR 2.875e-03
0: Upscaling, new scale: 256.0
0: TRAIN [0][560/1291]	Time 0.042 (0.053)	Data 8.08e-05 (4.46e-04)	Tok/s 182864 (201522)	Loss/tok 3.8025 (6.1071)	LR 2.875e-03
0: TRAIN [0][570/1291]	Time 0.042 (0.053)	Data 1.16e-04 (4.40e-04)	Tok/s 182634 (201353)	Loss/tok 3.6160 (6.0765)	LR 2.875e-03
0: TRAIN [0][580/1291]	Time 0.058 (0.053)	Data 8.49e-05 (4.33e-04)	Tok/s 215116 (201463)	Loss/tok 3.8610 (6.0382)	LR 2.875e-03
0: TRAIN [0][590/1291]	Time 0.058 (0.053)	Data 8.20e-05 (4.28e-04)	Tok/s 214912 (201498)	Loss/tok 3.9746 (6.0038)	LR 2.875e-03
0: TRAIN [0][600/1291]	Time 0.076 (0.053)	Data 1.36e-04 (4.22e-04)	Tok/s 231561 (201550)	Loss/tok 4.3029 (5.9688)	LR 2.875e-03
0: TRAIN [0][610/1291]	Time 0.059 (0.053)	Data 9.06e-05 (4.17e-04)	Tok/s 214441 (201711)	Loss/tok 4.1554 (5.9343)	LR 2.875e-03
0: TRAIN [0][620/1291]	Time 0.076 (0.053)	Data 8.13e-05 (4.12e-04)	Tok/s 234367 (201720)	Loss/tok 3.9746 (5.9028)	LR 2.875e-03
0: TRAIN [0][630/1291]	Time 0.059 (0.053)	Data 1.02e-04 (4.06e-04)	Tok/s 217057 (201942)	Loss/tok 3.8504 (5.8655)	LR 2.875e-03
0: TRAIN [0][640/1291]	Time 0.041 (0.053)	Data 8.01e-05 (4.01e-04)	Tok/s 191357 (201924)	Loss/tok 3.5721 (5.8366)	LR 2.875e-03
0: TRAIN [0][650/1291]	Time 0.042 (0.053)	Data 9.25e-05 (3.97e-04)	Tok/s 184829 (201853)	Loss/tok 3.5116 (5.8087)	LR 2.875e-03
0: TRAIN [0][660/1291]	Time 0.058 (0.053)	Data 8.15e-05 (3.92e-04)	Tok/s 214173 (201785)	Loss/tok 3.9048 (5.7825)	LR 2.875e-03
0: TRAIN [0][670/1291]	Time 0.042 (0.053)	Data 9.97e-05 (3.88e-04)	Tok/s 185517 (201808)	Loss/tok 3.6936 (5.7546)	LR 2.875e-03
0: TRAIN [0][680/1291]	Time 0.025 (0.053)	Data 8.06e-05 (3.83e-04)	Tok/s 160085 (201691)	Loss/tok 3.0366 (5.7308)	LR 2.875e-03
0: Upscaling, new scale: 512.0
0: TRAIN [0][690/1291]	Time 0.042 (0.053)	Data 8.20e-05 (3.79e-04)	Tok/s 184144 (201792)	Loss/tok 3.5927 (5.7021)	LR 2.875e-03
0: TRAIN [0][700/1291]	Time 0.042 (0.053)	Data 8.27e-05 (3.75e-04)	Tok/s 189160 (201873)	Loss/tok 3.5111 (5.6740)	LR 2.875e-03
0: TRAIN [0][710/1291]	Time 0.076 (0.053)	Data 8.49e-05 (3.71e-04)	Tok/s 230509 (201769)	Loss/tok 4.0766 (5.6519)	LR 2.875e-03
0: TRAIN [0][720/1291]	Time 0.024 (0.053)	Data 1.44e-04 (3.67e-04)	Tok/s 166313 (201630)	Loss/tok 3.0661 (5.6305)	LR 2.875e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 256.0
0: TRAIN [0][730/1291]	Time 0.058 (0.053)	Data 8.15e-05 (3.63e-04)	Tok/s 216329 (201564)	Loss/tok 3.7504 (5.6073)	LR 2.875e-03
0: TRAIN [0][740/1291]	Time 0.042 (0.053)	Data 8.27e-05 (3.60e-04)	Tok/s 183707 (201685)	Loss/tok 3.5365 (5.5797)	LR 2.875e-03
0: TRAIN [0][750/1291]	Time 0.059 (0.053)	Data 8.30e-05 (3.56e-04)	Tok/s 216431 (201662)	Loss/tok 3.9629 (5.5578)	LR 2.875e-03
0: TRAIN [0][760/1291]	Time 0.041 (0.053)	Data 7.94e-05 (3.53e-04)	Tok/s 186706 (201710)	Loss/tok 3.5392 (5.5346)	LR 2.875e-03
0: TRAIN [0][770/1291]	Time 0.041 (0.053)	Data 8.06e-05 (3.49e-04)	Tok/s 190784 (201531)	Loss/tok 3.5567 (5.5164)	LR 2.875e-03
0: TRAIN [0][780/1291]	Time 0.024 (0.053)	Data 8.08e-05 (3.46e-04)	Tok/s 157933 (201435)	Loss/tok 3.1100 (5.4970)	LR 2.875e-03
0: TRAIN [0][790/1291]	Time 0.058 (0.053)	Data 1.36e-04 (3.43e-04)	Tok/s 217443 (201615)	Loss/tok 3.7816 (5.4721)	LR 2.875e-03
0: TRAIN [0][800/1291]	Time 0.041 (0.053)	Data 8.20e-05 (3.40e-04)	Tok/s 184873 (201612)	Loss/tok 3.5284 (5.4514)	LR 2.875e-03
0: TRAIN [0][810/1291]	Time 0.076 (0.053)	Data 8.20e-05 (3.37e-04)	Tok/s 231514 (201475)	Loss/tok 3.8148 (5.4333)	LR 2.875e-03
0: TRAIN [0][820/1291]	Time 0.041 (0.053)	Data 8.15e-05 (3.34e-04)	Tok/s 189673 (201413)	Loss/tok 3.5903 (5.4150)	LR 2.875e-03
0: TRAIN [0][830/1291]	Time 0.058 (0.053)	Data 8.30e-05 (3.31e-04)	Tok/s 213839 (201490)	Loss/tok 3.8343 (5.3944)	LR 2.875e-03
0: TRAIN [0][840/1291]	Time 0.041 (0.053)	Data 7.92e-05 (3.28e-04)	Tok/s 186816 (201598)	Loss/tok 3.4965 (5.3722)	LR 2.875e-03
0: TRAIN [0][850/1291]	Time 0.077 (0.053)	Data 8.03e-05 (3.26e-04)	Tok/s 227163 (201673)	Loss/tok 4.0440 (5.3524)	LR 2.875e-03
0: Upscaling, new scale: 512.0
0: TRAIN [0][860/1291]	Time 0.041 (0.053)	Data 9.49e-05 (3.23e-04)	Tok/s 191333 (201608)	Loss/tok 3.5042 (5.3347)	LR 2.875e-03
0: TRAIN [0][870/1291]	Time 0.041 (0.053)	Data 7.99e-05 (3.20e-04)	Tok/s 189130 (201522)	Loss/tok 3.4447 (5.3184)	LR 2.875e-03
0: TRAIN [0][880/1291]	Time 0.059 (0.053)	Data 8.18e-05 (3.18e-04)	Tok/s 213507 (201621)	Loss/tok 3.7570 (5.2985)	LR 2.875e-03
0: TRAIN [0][890/1291]	Time 0.076 (0.053)	Data 8.15e-05 (3.15e-04)	Tok/s 235033 (201636)	Loss/tok 3.9186 (5.2810)	LR 2.875e-03
0: TRAIN [0][900/1291]	Time 0.025 (0.053)	Data 8.20e-05 (3.13e-04)	Tok/s 158011 (201713)	Loss/tok 2.9190 (5.2617)	LR 2.875e-03
0: TRAIN [0][910/1291]	Time 0.058 (0.053)	Data 1.40e-04 (3.11e-04)	Tok/s 219236 (201596)	Loss/tok 3.7618 (5.2466)	LR 2.875e-03
0: TRAIN [0][920/1291]	Time 0.097 (0.053)	Data 7.82e-05 (3.08e-04)	Tok/s 231380 (201653)	Loss/tok 4.2407 (5.2286)	LR 2.875e-03
0: TRAIN [0][930/1291]	Time 0.058 (0.053)	Data 8.01e-05 (3.06e-04)	Tok/s 218300 (201595)	Loss/tok 3.7083 (5.2136)	LR 2.875e-03
0: TRAIN [0][940/1291]	Time 0.042 (0.053)	Data 8.18e-05 (3.04e-04)	Tok/s 188781 (201681)	Loss/tok 3.4154 (5.1962)	LR 2.875e-03
0: TRAIN [0][950/1291]	Time 0.058 (0.053)	Data 7.99e-05 (3.01e-04)	Tok/s 215972 (201650)	Loss/tok 3.7549 (5.1814)	LR 2.875e-03
0: TRAIN [0][960/1291]	Time 0.042 (0.053)	Data 7.99e-05 (2.99e-04)	Tok/s 184624 (201654)	Loss/tok 3.3976 (5.1658)	LR 2.875e-03
0: TRAIN [0][970/1291]	Time 0.059 (0.053)	Data 8.58e-05 (2.97e-04)	Tok/s 215652 (201756)	Loss/tok 3.6451 (5.1491)	LR 2.875e-03
0: TRAIN [0][980/1291]	Time 0.043 (0.053)	Data 8.34e-05 (2.95e-04)	Tok/s 182592 (201805)	Loss/tok 3.2399 (5.1328)	LR 2.875e-03
0: Upscaling, new scale: 1024.0
0: TRAIN [0][990/1291]	Time 0.058 (0.053)	Data 7.92e-05 (2.93e-04)	Tok/s 218116 (201827)	Loss/tok 3.6557 (5.1176)	LR 2.875e-03
0: TRAIN [0][1000/1291]	Time 0.042 (0.053)	Data 8.03e-05 (2.91e-04)	Tok/s 186750 (201681)	Loss/tok 3.5219 (5.1059)	LR 2.875e-03
0: TRAIN [0][1010/1291]	Time 0.025 (0.053)	Data 8.58e-05 (2.89e-04)	Tok/s 159430 (201588)	Loss/tok 2.8952 (5.0928)	LR 2.875e-03
0: TRAIN [0][1020/1291]	Time 0.041 (0.053)	Data 8.73e-05 (2.87e-04)	Tok/s 188874 (201599)	Loss/tok 3.4770 (5.0789)	LR 2.875e-03
0: TRAIN [0][1030/1291]	Time 0.025 (0.053)	Data 8.30e-05 (2.85e-04)	Tok/s 161236 (201626)	Loss/tok 3.0270 (5.0646)	LR 2.875e-03
0: TRAIN [0][1040/1291]	Time 0.042 (0.053)	Data 7.82e-05 (2.83e-04)	Tok/s 186408 (201545)	Loss/tok 3.3694 (5.0525)	LR 2.875e-03
0: TRAIN [0][1050/1291]	Time 0.076 (0.053)	Data 8.18e-05 (2.81e-04)	Tok/s 233598 (201571)	Loss/tok 3.6796 (5.0391)	LR 2.875e-03
0: TRAIN [0][1060/1291]	Time 0.042 (0.053)	Data 7.84e-05 (2.79e-04)	Tok/s 186257 (201516)	Loss/tok 3.4057 (5.0273)	LR 2.875e-03
0: TRAIN [0][1070/1291]	Time 0.042 (0.053)	Data 7.94e-05 (2.78e-04)	Tok/s 186997 (201568)	Loss/tok 3.3549 (5.0131)	LR 2.875e-03
0: TRAIN [0][1080/1291]	Time 0.041 (0.053)	Data 8.18e-05 (2.76e-04)	Tok/s 189461 (201575)	Loss/tok 3.3585 (5.0006)	LR 2.875e-03
0: TRAIN [0][1090/1291]	Time 0.058 (0.053)	Data 7.82e-05 (2.74e-04)	Tok/s 220232 (201611)	Loss/tok 3.6293 (4.9875)	LR 2.875e-03
0: TRAIN [0][1100/1291]	Time 0.041 (0.053)	Data 7.92e-05 (2.72e-04)	Tok/s 189091 (201616)	Loss/tok 3.4729 (4.9753)	LR 2.875e-03
0: TRAIN [0][1110/1291]	Time 0.041 (0.053)	Data 8.42e-05 (2.71e-04)	Tok/s 189820 (201587)	Loss/tok 3.4277 (4.9640)	LR 2.875e-03
0: Upscaling, new scale: 2048.0
0: TRAIN [0][1120/1291]	Time 0.042 (0.053)	Data 7.82e-05 (2.69e-04)	Tok/s 189501 (201547)	Loss/tok 3.3241 (4.9527)	LR 2.875e-03
0: TRAIN [0][1130/1291]	Time 0.042 (0.053)	Data 7.96e-05 (2.67e-04)	Tok/s 180933 (201576)	Loss/tok 3.3223 (4.9404)	LR 2.875e-03
0: TRAIN [0][1140/1291]	Time 0.059 (0.053)	Data 8.01e-05 (2.66e-04)	Tok/s 207788 (201541)	Loss/tok 3.6428 (4.9293)	LR 2.875e-03
0: TRAIN [0][1150/1291]	Time 0.041 (0.053)	Data 7.92e-05 (2.64e-04)	Tok/s 181743 (201493)	Loss/tok 3.3308 (4.9183)	LR 2.875e-03
0: TRAIN [0][1160/1291]	Time 0.042 (0.053)	Data 8.08e-05 (2.62e-04)	Tok/s 182256 (201466)	Loss/tok 3.4205 (4.9076)	LR 2.875e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 1024.0
0: TRAIN [0][1170/1291]	Time 0.041 (0.053)	Data 8.06e-05 (2.61e-04)	Tok/s 191964 (201443)	Loss/tok 3.3428 (4.8967)	LR 2.875e-03
0: TRAIN [0][1180/1291]	Time 0.042 (0.053)	Data 8.25e-05 (2.59e-04)	Tok/s 189954 (201462)	Loss/tok 3.3660 (4.8852)	LR 2.875e-03
0: TRAIN [0][1190/1291]	Time 0.041 (0.053)	Data 7.92e-05 (2.58e-04)	Tok/s 185632 (201483)	Loss/tok 3.3008 (4.8742)	LR 2.875e-03
0: TRAIN [0][1200/1291]	Time 0.041 (0.053)	Data 8.30e-05 (2.56e-04)	Tok/s 190134 (201474)	Loss/tok 3.4542 (4.8639)	LR 2.875e-03
0: TRAIN [0][1210/1291]	Time 0.076 (0.053)	Data 8.37e-05 (2.55e-04)	Tok/s 226744 (201543)	Loss/tok 3.7713 (4.8523)	LR 2.875e-03
0: TRAIN [0][1220/1291]	Time 0.058 (0.053)	Data 8.15e-05 (2.54e-04)	Tok/s 220181 (201513)	Loss/tok 3.5900 (4.8421)	LR 2.875e-03
0: TRAIN [0][1230/1291]	Time 0.042 (0.053)	Data 8.25e-05 (2.52e-04)	Tok/s 187281 (201503)	Loss/tok 3.2917 (4.8318)	LR 2.875e-03
0: TRAIN [0][1240/1291]	Time 0.042 (0.053)	Data 8.06e-05 (2.51e-04)	Tok/s 182926 (201494)	Loss/tok 3.2094 (4.8219)	LR 2.875e-03
0: TRAIN [0][1250/1291]	Time 0.041 (0.053)	Data 7.96e-05 (2.50e-04)	Tok/s 184397 (201435)	Loss/tok 3.3393 (4.8128)	LR 2.875e-03
0: TRAIN [0][1260/1291]	Time 0.041 (0.053)	Data 8.01e-05 (2.48e-04)	Tok/s 187968 (201464)	Loss/tok 3.3040 (4.8025)	LR 2.875e-03
0: TRAIN [0][1270/1291]	Time 0.059 (0.053)	Data 7.87e-05 (2.47e-04)	Tok/s 216069 (201513)	Loss/tok 3.5957 (4.7921)	LR 2.875e-03
0: TRAIN [0][1280/1291]	Time 0.058 (0.053)	Data 7.92e-05 (2.46e-04)	Tok/s 213276 (201430)	Loss/tok 3.7332 (4.7839)	LR 2.875e-03
0: TRAIN [0][1290/1291]	Time 0.041 (0.053)	Data 3.86e-05 (2.46e-04)	Tok/s 187277 (201351)	Loss/tok 3.3517 (4.7756)	LR 2.875e-03
:::MLLOG {"namespace": "", "time_ms": 1593113069525, "event_type": "INTERVAL_END", "key": "epoch_stop", "value": null, "metadata": {"file": "train.py", "lineno": 551, "epoch_num": 1}}
:::MLLOG {"namespace": "", "time_ms": 1593113069525, "event_type": "INTERVAL_START", "key": "eval_start", "value": null, "metadata": {"file": "train.py", "lineno": 556, "epoch_num": 1}}
0: Running evaluation on test set
0: TEST [0][0/3]	Time 0.406 (0.406)	Decoder iters 149.0 (149.0)	Tok/s 25521 (25521)
0: Running moses detokenizer
0: BLEU(score=18.149944961183163, counts=[34279, 15355, 8005, 4361], totals=[68739, 65736, 62733, 59734], precisions=[49.86834257117503, 23.358585858585858, 12.760429120239746, 7.300699768975793], bp=1.0, sys_len=68739, ref_len=64676)
0: Finished evaluation on test set
:::MLLOG {"namespace": "", "time_ms": 1593113070866, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.1815, "metadata": {"file": "train.py", "lineno": 562, "epoch_num": 1}}
:::MLLOG {"namespace": "", "time_ms": 1593113070866, "event_type": "INTERVAL_END", "key": "eval_stop", "value": null, "metadata": {"file": "train.py", "lineno": 565, "epoch_num": 1}}
0: Summary: Epoch: 0	Training Loss: 4.7729	Test BLEU: 18.15
0: Performance: Epoch: 0	Training: 3220692 Tok/s
0: Finished epoch 0
:::MLLOG {"namespace": "", "time_ms": 1593113070867, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "train.py", "lineno": 584, "first_epoch_num": 1}}
:::MLLOG {"namespace": "", "time_ms": 1593113070867, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "train.py", "lineno": 538, "first_epoch_num": 2, "epoch_count": 1}}
:::MLLOG {"namespace": "", "time_ms": 1593113070867, "event_type": "INTERVAL_START", "key": "epoch_start", "value": null, "metadata": {"file": "train.py", "lineno": 541, "epoch_num": 2}}
0: Starting epoch 1
0: Sampler for epoch 1 uses seed 764706012
0: TRAIN [1][0/1291]	Time 0.270 (0.270)	Data 1.79e-01 (1.79e-01)	Tok/s 28558 (28558)	Loss/tok 3.2363 (3.2363)	LR 2.875e-03
0: Upscaling, new scale: 2048.0
0: TRAIN [1][10/1291]	Time 0.058 (0.080)	Data 8.82e-05 (1.64e-02)	Tok/s 215390 (191780)	Loss/tok 3.4612 (3.5588)	LR 2.875e-03
0: TRAIN [1][20/1291]	Time 0.042 (0.067)	Data 8.85e-05 (8.63e-03)	Tok/s 184690 (195692)	Loss/tok 3.2086 (3.5333)	LR 2.875e-03
0: TRAIN [1][30/1291]	Time 0.058 (0.062)	Data 8.99e-05 (5.88e-03)	Tok/s 210863 (196474)	Loss/tok 3.6905 (3.5408)	LR 2.875e-03
0: TRAIN [1][40/1291]	Time 0.042 (0.060)	Data 8.58e-05 (4.47e-03)	Tok/s 186389 (197563)	Loss/tok 3.3461 (3.5295)	LR 2.875e-03
0: TRAIN [1][50/1291]	Time 0.042 (0.058)	Data 8.46e-05 (3.62e-03)	Tok/s 184446 (197326)	Loss/tok 3.2826 (3.5202)	LR 2.875e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 1024.0
0: TRAIN [1][60/1291]	Time 0.058 (0.057)	Data 1.04e-04 (3.04e-03)	Tok/s 221725 (198529)	Loss/tok 3.5199 (3.5151)	LR 2.875e-03
0: TRAIN [1][70/1291]	Time 0.042 (0.056)	Data 8.25e-05 (2.62e-03)	Tok/s 187404 (197889)	Loss/tok 3.3068 (3.5018)	LR 2.875e-03
0: TRAIN [1][80/1291]	Time 0.041 (0.055)	Data 8.30e-05 (2.31e-03)	Tok/s 191645 (197856)	Loss/tok 3.2707 (3.4940)	LR 2.875e-03
0: TRAIN [1][90/1291]	Time 0.041 (0.055)	Data 7.89e-05 (2.07e-03)	Tok/s 189472 (197887)	Loss/tok 3.3076 (3.4956)	LR 2.875e-03
0: TRAIN [1][100/1291]	Time 0.058 (0.055)	Data 1.36e-04 (1.87e-03)	Tok/s 218369 (198236)	Loss/tok 3.5366 (3.4924)	LR 2.875e-03
0: TRAIN [1][110/1291]	Time 0.097 (0.055)	Data 7.84e-05 (1.71e-03)	Tok/s 232144 (199396)	Loss/tok 3.9226 (3.5022)	LR 2.875e-03
0: TRAIN [1][120/1291]	Time 0.076 (0.055)	Data 8.56e-05 (1.58e-03)	Tok/s 230393 (199410)	Loss/tok 3.6913 (3.4961)	LR 2.875e-03
0: TRAIN [1][130/1291]	Time 0.058 (0.055)	Data 9.42e-05 (1.46e-03)	Tok/s 212168 (199687)	Loss/tok 3.5341 (3.4938)	LR 2.875e-03
0: TRAIN [1][140/1291]	Time 0.025 (0.055)	Data 9.08e-05 (1.37e-03)	Tok/s 164477 (200261)	Loss/tok 2.7538 (3.4967)	LR 2.875e-03
0: TRAIN [1][150/1291]	Time 0.025 (0.055)	Data 1.41e-04 (1.28e-03)	Tok/s 158234 (201065)	Loss/tok 2.8495 (3.5059)	LR 2.875e-03
0: TRAIN [1][160/1291]	Time 0.076 (0.055)	Data 1.02e-04 (1.21e-03)	Tok/s 232190 (200836)	Loss/tok 3.6937 (3.5052)	LR 2.875e-03
0: TRAIN [1][170/1291]	Time 0.041 (0.056)	Data 8.65e-05 (1.14e-03)	Tok/s 187169 (201557)	Loss/tok 3.2547 (3.5162)	LR 2.875e-03
0: Upscaling, new scale: 2048.0
0: TRAIN [1][180/1291]	Time 0.042 (0.056)	Data 8.34e-05 (1.08e-03)	Tok/s 189901 (201593)	Loss/tok 3.2560 (3.5137)	LR 2.875e-03
0: TRAIN [1][190/1291]	Time 0.058 (0.055)	Data 8.03e-05 (1.03e-03)	Tok/s 215726 (201551)	Loss/tok 3.4583 (3.5057)	LR 2.875e-03
0: TRAIN [1][200/1291]	Time 0.076 (0.055)	Data 1.40e-04 (9.86e-04)	Tok/s 232742 (201361)	Loss/tok 3.5724 (3.5004)	LR 2.875e-03
0: TRAIN [1][210/1291]	Time 0.042 (0.055)	Data 8.49e-05 (9.43e-04)	Tok/s 189349 (201303)	Loss/tok 3.2980 (3.4986)	LR 2.875e-03
0: TRAIN [1][220/1291]	Time 0.076 (0.055)	Data 7.92e-05 (9.05e-04)	Tok/s 231724 (201566)	Loss/tok 3.6346 (3.4985)	LR 2.875e-03
0: TRAIN [1][230/1291]	Time 0.076 (0.055)	Data 1.39e-04 (8.70e-04)	Tok/s 231901 (201794)	Loss/tok 3.7695 (3.4991)	LR 2.875e-03
0: TRAIN [1][240/1291]	Time 0.041 (0.055)	Data 8.03e-05 (8.38e-04)	Tok/s 187923 (201848)	Loss/tok 3.2322 (3.4991)	LR 2.875e-03
0: TRAIN [1][250/1291]	Time 0.076 (0.055)	Data 1.38e-04 (8.09e-04)	Tok/s 233179 (201754)	Loss/tok 3.6031 (3.4949)	LR 2.875e-03
0: TRAIN [1][260/1291]	Time 0.041 (0.054)	Data 1.01e-04 (7.82e-04)	Tok/s 186856 (201430)	Loss/tok 3.2330 (3.4893)	LR 2.875e-03
0: TRAIN [1][270/1291]	Time 0.042 (0.054)	Data 7.89e-05 (7.56e-04)	Tok/s 183594 (200954)	Loss/tok 3.3235 (3.4874)	LR 2.875e-03
0: TRAIN [1][280/1291]	Time 0.042 (0.054)	Data 7.84e-05 (7.33e-04)	Tok/s 184019 (201023)	Loss/tok 3.1965 (3.4901)	LR 2.875e-03
0: TRAIN [1][290/1291]	Time 0.075 (0.054)	Data 1.37e-04 (7.12e-04)	Tok/s 232601 (201404)	Loss/tok 3.7537 (3.4918)	LR 2.875e-03
0: TRAIN [1][300/1291]	Time 0.059 (0.054)	Data 8.77e-05 (6.91e-04)	Tok/s 216691 (201117)	Loss/tok 3.4203 (3.4895)	LR 2.875e-03
0: Upscaling, new scale: 4096.0
0: Gradient norm: inf
0: Skipped batch, new scale: 2048.0
0: TRAIN [1][310/1291]	Time 0.058 (0.054)	Data 1.35e-04 (6.73e-04)	Tok/s 217554 (201080)	Loss/tok 3.3685 (3.4864)	LR 2.875e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 1024.0
0: TRAIN [1][320/1291]	Time 0.097 (0.054)	Data 8.30e-05 (6.54e-04)	Tok/s 232533 (201163)	Loss/tok 3.7496 (3.4875)	LR 2.875e-03
0: TRAIN [1][330/1291]	Time 0.042 (0.054)	Data 8.39e-05 (6.37e-04)	Tok/s 183348 (201095)	Loss/tok 3.0965 (3.4851)	LR 2.875e-03
0: TRAIN [1][340/1291]	Time 0.058 (0.053)	Data 9.94e-05 (6.21e-04)	Tok/s 215441 (200923)	Loss/tok 3.5573 (3.4823)	LR 2.875e-03
0: TRAIN [1][350/1291]	Time 0.042 (0.053)	Data 7.92e-05 (6.06e-04)	Tok/s 188717 (200885)	Loss/tok 3.1592 (3.4820)	LR 2.875e-03
0: TRAIN [1][360/1291]	Time 0.059 (0.053)	Data 7.72e-05 (5.92e-04)	Tok/s 215935 (200990)	Loss/tok 3.4009 (3.4799)	LR 2.875e-03
0: TRAIN [1][370/1291]	Time 0.097 (0.054)	Data 7.84e-05 (5.79e-04)	Tok/s 230059 (201138)	Loss/tok 3.8724 (3.4804)	LR 2.875e-03
0: TRAIN [1][380/1291]	Time 0.041 (0.053)	Data 7.92e-05 (5.66e-04)	Tok/s 182546 (201001)	Loss/tok 3.1791 (3.4806)	LR 2.875e-03
0: TRAIN [1][390/1291]	Time 0.041 (0.053)	Data 8.44e-05 (5.54e-04)	Tok/s 187927 (201015)	Loss/tok 3.1922 (3.4791)	LR 2.875e-03
0: TRAIN [1][400/1291]	Time 0.058 (0.053)	Data 8.65e-05 (5.43e-04)	Tok/s 217567 (201146)	Loss/tok 3.5742 (3.4808)	LR 2.875e-03
0: TRAIN [1][410/1291]	Time 0.076 (0.053)	Data 8.85e-05 (5.32e-04)	Tok/s 232035 (201012)	Loss/tok 3.6563 (3.4783)	LR 2.875e-03
0: TRAIN [1][420/1291]	Time 0.058 (0.053)	Data 8.03e-05 (5.22e-04)	Tok/s 218532 (201344)	Loss/tok 3.3159 (3.4793)	LR 2.875e-03
0: TRAIN [1][430/1291]	Time 0.042 (0.053)	Data 1.02e-04 (5.12e-04)	Tok/s 185802 (201297)	Loss/tok 3.1355 (3.4769)	LR 2.875e-03
0: TRAIN [1][440/1291]	Time 0.042 (0.053)	Data 1.38e-04 (5.03e-04)	Tok/s 185223 (201075)	Loss/tok 3.2692 (3.4746)	LR 2.875e-03
0: Upscaling, new scale: 2048.0
0: TRAIN [1][450/1291]	Time 0.058 (0.053)	Data 8.15e-05 (4.93e-04)	Tok/s 214073 (201037)	Loss/tok 3.5724 (3.4761)	LR 2.875e-03
0: TRAIN [1][460/1291]	Time 0.058 (0.053)	Data 1.39e-04 (4.85e-04)	Tok/s 215945 (201017)	Loss/tok 3.3465 (3.4754)	LR 2.875e-03
0: TRAIN [1][470/1291]	Time 0.041 (0.053)	Data 7.72e-05 (4.76e-04)	Tok/s 185988 (200992)	Loss/tok 3.2604 (3.4740)	LR 2.875e-03
0: TRAIN [1][480/1291]	Time 0.042 (0.053)	Data 8.20e-05 (4.69e-04)	Tok/s 185874 (201034)	Loss/tok 3.3733 (3.4727)	LR 2.875e-03
0: TRAIN [1][490/1291]	Time 0.041 (0.053)	Data 7.94e-05 (4.61e-04)	Tok/s 188345 (200817)	Loss/tok 3.2345 (3.4702)	LR 2.875e-03
0: TRAIN [1][500/1291]	Time 0.058 (0.053)	Data 1.23e-04 (4.54e-04)	Tok/s 220094 (200800)	Loss/tok 3.3683 (3.4684)	LR 2.875e-03
0: TRAIN [1][510/1291]	Time 0.042 (0.053)	Data 7.75e-05 (4.47e-04)	Tok/s 185614 (200761)	Loss/tok 3.1825 (3.4677)	LR 2.875e-03
0: TRAIN [1][520/1291]	Time 0.058 (0.053)	Data 8.39e-05 (4.40e-04)	Tok/s 218896 (200765)	Loss/tok 3.4871 (3.4661)	LR 2.875e-03
0: TRAIN [1][530/1291]	Time 0.042 (0.053)	Data 8.39e-05 (4.33e-04)	Tok/s 186063 (200757)	Loss/tok 3.1419 (3.4638)	LR 2.875e-03
0: TRAIN [1][540/1291]	Time 0.076 (0.053)	Data 8.32e-05 (4.27e-04)	Tok/s 232613 (200729)	Loss/tok 3.5666 (3.4619)	LR 2.875e-03
0: TRAIN [1][550/1291]	Time 0.042 (0.053)	Data 8.34e-05 (4.21e-04)	Tok/s 182587 (200898)	Loss/tok 3.3220 (3.4630)	LR 2.875e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 1024.0
0: TRAIN [1][560/1291]	Time 0.097 (0.053)	Data 8.15e-05 (4.15e-04)	Tok/s 229203 (200933)	Loss/tok 3.9691 (3.4684)	LR 2.875e-03
0: TRAIN [1][570/1291]	Time 0.076 (0.053)	Data 8.15e-05 (4.09e-04)	Tok/s 229865 (200938)	Loss/tok 3.6048 (3.4674)	LR 2.875e-03
0: TRAIN [1][580/1291]	Time 0.042 (0.053)	Data 8.73e-05 (4.03e-04)	Tok/s 182778 (200924)	Loss/tok 3.0385 (3.4667)	LR 2.875e-03
0: TRAIN [1][590/1291]	Time 0.096 (0.053)	Data 8.58e-05 (3.98e-04)	Tok/s 235417 (201004)	Loss/tok 3.7833 (3.4667)	LR 2.875e-03
0: TRAIN [1][600/1291]	Time 0.042 (0.053)	Data 8.27e-05 (3.93e-04)	Tok/s 186552 (200979)	Loss/tok 3.1973 (3.4656)	LR 2.875e-03
0: TRAIN [1][610/1291]	Time 0.042 (0.053)	Data 8.01e-05 (3.88e-04)	Tok/s 184019 (200876)	Loss/tok 3.3470 (3.4634)	LR 2.875e-03
0: TRAIN [1][620/1291]	Time 0.025 (0.053)	Data 8.20e-05 (3.83e-04)	Tok/s 159194 (200881)	Loss/tok 2.6742 (3.4624)	LR 2.875e-03
0: TRAIN [1][630/1291]	Time 0.076 (0.053)	Data 8.11e-05 (3.78e-04)	Tok/s 228046 (200755)	Loss/tok 3.5781 (3.4605)	LR 2.875e-03
0: TRAIN [1][640/1291]	Time 0.042 (0.053)	Data 1.01e-04 (3.74e-04)	Tok/s 187345 (200976)	Loss/tok 3.2684 (3.4636)	LR 2.875e-03
0: TRAIN [1][650/1291]	Time 0.096 (0.053)	Data 1.02e-04 (3.69e-04)	Tok/s 231116 (201028)	Loss/tok 3.7626 (3.4631)	LR 2.875e-03
0: TRAIN [1][660/1291]	Time 0.042 (0.053)	Data 8.15e-05 (3.65e-04)	Tok/s 185321 (201083)	Loss/tok 3.2331 (3.4640)	LR 2.875e-03
0: TRAIN [1][670/1291]	Time 0.058 (0.053)	Data 8.06e-05 (3.61e-04)	Tok/s 213275 (201111)	Loss/tok 3.4400 (3.4644)	LR 2.875e-03
0: TRAIN [1][680/1291]	Time 0.077 (0.053)	Data 8.27e-05 (3.57e-04)	Tok/s 225776 (201088)	Loss/tok 3.6515 (3.4647)	LR 2.875e-03
0: Upscaling, new scale: 2048.0
0: TRAIN [1][690/1291]	Time 0.058 (0.053)	Data 8.13e-05 (3.53e-04)	Tok/s 217869 (201091)	Loss/tok 3.3717 (3.4642)	LR 2.875e-03
0: TRAIN [1][700/1291]	Time 0.058 (0.053)	Data 8.25e-05 (3.49e-04)	Tok/s 214000 (201195)	Loss/tok 3.3812 (3.4654)	LR 2.875e-03
0: TRAIN [1][710/1291]	Time 0.076 (0.053)	Data 8.25e-05 (3.45e-04)	Tok/s 227101 (201302)	Loss/tok 3.7034 (3.4660)	LR 2.875e-03
0: TRAIN [1][720/1291]	Time 0.041 (0.053)	Data 8.25e-05 (3.42e-04)	Tok/s 186367 (201137)	Loss/tok 3.2121 (3.4638)	LR 2.875e-03
0: TRAIN [1][730/1291]	Time 0.076 (0.053)	Data 8.03e-05 (3.38e-04)	Tok/s 228102 (201265)	Loss/tok 3.5336 (3.4636)	LR 2.875e-03
0: TRAIN [1][740/1291]	Time 0.058 (0.053)	Data 8.13e-05 (3.35e-04)	Tok/s 215783 (201170)	Loss/tok 3.2606 (3.4621)	LR 2.875e-03
0: TRAIN [1][750/1291]	Time 0.059 (0.053)	Data 8.63e-05 (3.32e-04)	Tok/s 214007 (201203)	Loss/tok 3.3802 (3.4619)	LR 2.875e-03
0: TRAIN [1][760/1291]	Time 0.058 (0.053)	Data 8.01e-05 (3.28e-04)	Tok/s 215941 (201222)	Loss/tok 3.3185 (3.4603)	LR 2.875e-03
0: TRAIN [1][770/1291]	Time 0.041 (0.053)	Data 8.15e-05 (3.25e-04)	Tok/s 184991 (201039)	Loss/tok 3.0819 (3.4577)	LR 2.875e-03
0: TRAIN [1][780/1291]	Time 0.041 (0.053)	Data 8.06e-05 (3.22e-04)	Tok/s 186495 (200988)	Loss/tok 3.2028 (3.4557)	LR 2.875e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 1024.0
0: TRAIN [1][790/1291]	Time 0.041 (0.053)	Data 8.20e-05 (3.19e-04)	Tok/s 186074 (201029)	Loss/tok 3.1475 (3.4563)	LR 2.875e-03
0: TRAIN [1][800/1291]	Time 0.041 (0.053)	Data 1.55e-04 (3.16e-04)	Tok/s 181154 (200994)	Loss/tok 3.1237 (3.4566)	LR 2.875e-03
0: TRAIN [1][810/1291]	Time 0.076 (0.053)	Data 8.15e-05 (3.14e-04)	Tok/s 231200 (201095)	Loss/tok 3.5890 (3.4571)	LR 2.875e-03
0: TRAIN [1][820/1291]	Time 0.059 (0.053)	Data 8.11e-05 (3.11e-04)	Tok/s 217611 (201074)	Loss/tok 3.3612 (3.4563)	LR 2.875e-03
0: TRAIN [1][830/1291]	Time 0.042 (0.053)	Data 8.01e-05 (3.08e-04)	Tok/s 183802 (201102)	Loss/tok 3.2004 (3.4561)	LR 2.875e-03
0: TRAIN [1][840/1291]	Time 0.058 (0.053)	Data 8.18e-05 (3.05e-04)	Tok/s 216927 (201018)	Loss/tok 3.4472 (3.4546)	LR 2.875e-03
0: TRAIN [1][850/1291]	Time 0.041 (0.053)	Data 8.13e-05 (3.03e-04)	Tok/s 189119 (201176)	Loss/tok 3.2870 (3.4547)	LR 2.875e-03
0: TRAIN [1][860/1291]	Time 0.058 (0.053)	Data 8.23e-05 (3.00e-04)	Tok/s 219274 (201223)	Loss/tok 3.3772 (3.4534)	LR 2.875e-03
0: TRAIN [1][870/1291]	Time 0.077 (0.053)	Data 8.46e-05 (2.98e-04)	Tok/s 228170 (201336)	Loss/tok 3.6477 (3.4541)	LR 2.875e-03
0: TRAIN [1][880/1291]	Time 0.058 (0.053)	Data 8.08e-05 (2.95e-04)	Tok/s 220491 (201472)	Loss/tok 3.4012 (3.4551)	LR 2.875e-03
0: TRAIN [1][890/1291]	Time 0.025 (0.053)	Data 7.94e-05 (2.93e-04)	Tok/s 163139 (201381)	Loss/tok 2.7577 (3.4543)	LR 2.875e-03
0: TRAIN [1][900/1291]	Time 0.025 (0.053)	Data 7.99e-05 (2.91e-04)	Tok/s 159521 (201209)	Loss/tok 2.7667 (3.4521)	LR 2.875e-03
0: TRAIN [1][910/1291]	Time 0.042 (0.053)	Data 8.20e-05 (2.88e-04)	Tok/s 190146 (201270)	Loss/tok 3.1704 (3.4523)	LR 2.875e-03
0: Upscaling, new scale: 2048.0
0: TRAIN [1][920/1291]	Time 0.059 (0.053)	Data 8.15e-05 (2.86e-04)	Tok/s 216293 (201236)	Loss/tok 3.3439 (3.4510)	LR 2.875e-03
0: TRAIN [1][930/1291]	Time 0.058 (0.053)	Data 8.06e-05 (2.84e-04)	Tok/s 216228 (201262)	Loss/tok 3.3708 (3.4501)	LR 2.875e-03
0: TRAIN [1][940/1291]	Time 0.058 (0.053)	Data 8.13e-05 (2.82e-04)	Tok/s 218087 (201201)	Loss/tok 3.3904 (3.4485)	LR 2.875e-03
0: TRAIN [1][950/1291]	Time 0.041 (0.053)	Data 7.89e-05 (2.80e-04)	Tok/s 187211 (201234)	Loss/tok 3.1660 (3.4477)	LR 2.875e-03
0: TRAIN [1][960/1291]	Time 0.024 (0.053)	Data 8.11e-05 (2.78e-04)	Tok/s 163762 (201198)	Loss/tok 2.6832 (3.4465)	LR 2.875e-03
0: TRAIN [1][970/1291]	Time 0.076 (0.053)	Data 8.13e-05 (2.76e-04)	Tok/s 229386 (201189)	Loss/tok 3.5606 (3.4452)	LR 2.875e-03
0: TRAIN [1][980/1291]	Time 0.058 (0.053)	Data 1.40e-04 (2.74e-04)	Tok/s 214867 (201207)	Loss/tok 3.4088 (3.4442)	LR 2.875e-03
0: TRAIN [1][990/1291]	Time 0.058 (0.053)	Data 1.04e-04 (2.72e-04)	Tok/s 217159 (201148)	Loss/tok 3.3676 (3.4434)	LR 2.875e-03
0: TRAIN [1][1000/1291]	Time 0.041 (0.053)	Data 8.37e-05 (2.70e-04)	Tok/s 190293 (201169)	Loss/tok 3.1918 (3.4439)	LR 2.875e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 1024.0
0: TRAIN [1][1010/1291]	Time 0.042 (0.053)	Data 8.23e-05 (2.69e-04)	Tok/s 183265 (201164)	Loss/tok 3.2972 (3.4440)	LR 2.875e-03
0: TRAIN [1][1020/1291]	Time 0.042 (0.053)	Data 8.08e-05 (2.67e-04)	Tok/s 183770 (201205)	Loss/tok 3.2362 (3.4441)	LR 2.875e-03
0: TRAIN [1][1030/1291]	Time 0.024 (0.053)	Data 8.30e-05 (2.65e-04)	Tok/s 162587 (201099)	Loss/tok 2.6719 (3.4424)	LR 2.875e-03
0: TRAIN [1][1040/1291]	Time 0.076 (0.053)	Data 8.01e-05 (2.63e-04)	Tok/s 230090 (201170)	Loss/tok 3.7163 (3.4428)	LR 2.875e-03
0: TRAIN [1][1050/1291]	Time 0.058 (0.053)	Data 8.18e-05 (2.62e-04)	Tok/s 217017 (201162)	Loss/tok 3.3338 (3.4416)	LR 2.875e-03
0: TRAIN [1][1060/1291]	Time 0.076 (0.053)	Data 1.40e-04 (2.60e-04)	Tok/s 232712 (201142)	Loss/tok 3.4666 (3.4407)	LR 2.875e-03
0: TRAIN [1][1070/1291]	Time 0.041 (0.053)	Data 8.06e-05 (2.58e-04)	Tok/s 187376 (201066)	Loss/tok 3.2395 (3.4396)	LR 2.875e-03
0: TRAIN [1][1080/1291]	Time 0.025 (0.053)	Data 8.01e-05 (2.57e-04)	Tok/s 157540 (201098)	Loss/tok 2.8122 (3.4402)	LR 2.875e-03
0: TRAIN [1][1090/1291]	Time 0.042 (0.053)	Data 8.06e-05 (2.55e-04)	Tok/s 188781 (201118)	Loss/tok 3.2406 (3.4402)	LR 2.875e-03
0: TRAIN [1][1100/1291]	Time 0.041 (0.053)	Data 1.34e-04 (2.54e-04)	Tok/s 190961 (201230)	Loss/tok 3.2122 (3.4404)	LR 2.875e-03
0: TRAIN [1][1110/1291]	Time 0.058 (0.053)	Data 8.08e-05 (2.52e-04)	Tok/s 218610 (201192)	Loss/tok 3.2749 (3.4392)	LR 2.875e-03
0: TRAIN [1][1120/1291]	Time 0.041 (0.053)	Data 8.18e-05 (2.51e-04)	Tok/s 186964 (201126)	Loss/tok 3.2190 (3.4379)	LR 2.875e-03
0: TRAIN [1][1130/1291]	Time 0.059 (0.053)	Data 8.08e-05 (2.49e-04)	Tok/s 211195 (201148)	Loss/tok 3.4248 (3.4372)	LR 2.875e-03
0: Upscaling, new scale: 2048.0
0: TRAIN [1][1140/1291]	Time 0.058 (0.053)	Data 9.58e-05 (2.48e-04)	Tok/s 217570 (201119)	Loss/tok 3.1542 (3.4358)	LR 2.875e-03
0: TRAIN [1][1150/1291]	Time 0.058 (0.053)	Data 8.11e-05 (2.47e-04)	Tok/s 217402 (201027)	Loss/tok 3.3958 (3.4351)	LR 2.875e-03
0: TRAIN [1][1160/1291]	Time 0.041 (0.053)	Data 8.23e-05 (2.45e-04)	Tok/s 194139 (200914)	Loss/tok 3.1655 (3.4337)	LR 2.875e-03
0: TRAIN [1][1170/1291]	Time 0.025 (0.053)	Data 1.31e-04 (2.44e-04)	Tok/s 161753 (200828)	Loss/tok 2.6681 (3.4320)	LR 2.875e-03
0: TRAIN [1][1180/1291]	Time 0.058 (0.053)	Data 8.08e-05 (2.43e-04)	Tok/s 217238 (200916)	Loss/tok 3.3399 (3.4323)	LR 2.875e-03
0: TRAIN [1][1190/1291]	Time 0.058 (0.053)	Data 7.99e-05 (2.41e-04)	Tok/s 218375 (200922)	Loss/tok 3.2421 (3.4310)	LR 2.875e-03
0: TRAIN [1][1200/1291]	Time 0.041 (0.053)	Data 8.06e-05 (2.40e-04)	Tok/s 186853 (200969)	Loss/tok 3.1827 (3.4312)	LR 2.875e-03
0: TRAIN [1][1210/1291]	Time 0.042 (0.053)	Data 8.20e-05 (2.39e-04)	Tok/s 183017 (200882)	Loss/tok 3.1407 (3.4297)	LR 2.875e-03
0: TRAIN [1][1220/1291]	Time 0.058 (0.053)	Data 1.43e-04 (2.37e-04)	Tok/s 217021 (200893)	Loss/tok 3.4491 (3.4291)	LR 2.875e-03
0: TRAIN [1][1230/1291]	Time 0.041 (0.053)	Data 7.99e-05 (2.36e-04)	Tok/s 187192 (200988)	Loss/tok 3.1425 (3.4286)	LR 2.875e-03
0: TRAIN [1][1240/1291]	Time 0.041 (0.053)	Data 8.20e-05 (2.35e-04)	Tok/s 186483 (200975)	Loss/tok 3.1215 (3.4284)	LR 2.875e-03
0: TRAIN [1][1250/1291]	Time 0.058 (0.053)	Data 1.04e-04 (2.34e-04)	Tok/s 215752 (200936)	Loss/tok 3.2789 (3.4278)	LR 2.875e-03
0: Upscaling, new scale: 4096.0
0: TRAIN [1][1260/1291]	Time 0.059 (0.053)	Data 8.32e-05 (2.33e-04)	Tok/s 217394 (200993)	Loss/tok 3.3145 (3.4275)	LR 2.875e-03
0: TRAIN [1][1270/1291]	Time 0.042 (0.053)	Data 8.51e-05 (2.31e-04)	Tok/s 193081 (201057)	Loss/tok 3.1186 (3.4275)	LR 2.875e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 2048.0
0: Gradient norm: inf
0: Skipped batch, new scale: 1024.0
0: TRAIN [1][1280/1291]	Time 0.041 (0.053)	Data 8.11e-05 (2.30e-04)	Tok/s 191152 (201111)	Loss/tok 3.2390 (3.4278)	LR 2.875e-03
0: TRAIN [1][1290/1291]	Time 0.076 (0.053)	Data 4.60e-05 (2.31e-04)	Tok/s 228078 (201191)	Loss/tok 3.6791 (3.4280)	LR 2.875e-03
:::MLLOG {"namespace": "", "time_ms": 1593113139142, "event_type": "INTERVAL_END", "key": "epoch_stop", "value": null, "metadata": {"file": "train.py", "lineno": 551, "epoch_num": 2}}
:::MLLOG {"namespace": "", "time_ms": 1593113139142, "event_type": "INTERVAL_START", "key": "eval_start", "value": null, "metadata": {"file": "train.py", "lineno": 556, "epoch_num": 2}}
0: Running evaluation on test set
0: TEST [1][0/3]	Time 0.396 (0.396)	Decoder iters 149.0 (149.0)	Tok/s 22417 (22417)
0: Running moses detokenizer
0: BLEU(score=22.003951782704476, counts=[35400, 17071, 9379, 5375], totals=[64125, 61122, 58120, 55121], precisions=[55.2046783625731, 27.929387127384576, 16.13730213351686, 9.751274468895701], bp=0.9914442182222414, sys_len=64125, ref_len=64676)
0: Finished evaluation on test set
:::MLLOG {"namespace": "", "time_ms": 1593113140584, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.22, "metadata": {"file": "train.py", "lineno": 562, "epoch_num": 2}}
:::MLLOG {"namespace": "", "time_ms": 1593113140584, "event_type": "INTERVAL_END", "key": "eval_stop", "value": null, "metadata": {"file": "train.py", "lineno": 565, "epoch_num": 2}}
0: Summary: Epoch: 1	Training Loss: 3.4276	Test BLEU: 22.00
0: Performance: Epoch: 1	Training: 3218804 Tok/s
0: Finished epoch 1
:::MLLOG {"namespace": "", "time_ms": 1593113140584, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "train.py", "lineno": 584, "first_epoch_num": 2}}
:::MLLOG {"namespace": "", "time_ms": 1593113140585, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "train.py", "lineno": 538, "first_epoch_num": 3, "epoch_count": 1}}
:::MLLOG {"namespace": "", "time_ms": 1593113140585, "event_type": "INTERVAL_START", "key": "epoch_start", "value": null, "metadata": {"file": "train.py", "lineno": 541, "epoch_num": 3}}
0: Starting epoch 2
0: Sampler for epoch 2 uses seed 377903286
0: TRAIN [2][0/1291]	Time 0.283 (0.283)	Data 1.76e-01 (1.76e-01)	Tok/s 27123 (27123)	Loss/tok 3.0221 (3.0221)	LR 2.875e-03
0: TRAIN [2][10/1291]	Time 0.058 (0.082)	Data 9.54e-05 (1.61e-02)	Tok/s 217697 (197757)	Loss/tok 3.3683 (3.3806)	LR 2.875e-03
0: TRAIN [2][20/1291]	Time 0.058 (0.069)	Data 1.11e-04 (8.49e-03)	Tok/s 220537 (203128)	Loss/tok 3.2005 (3.3176)	LR 2.875e-03
0: TRAIN [2][30/1291]	Time 0.076 (0.068)	Data 8.85e-05 (5.78e-03)	Tok/s 229886 (206649)	Loss/tok 3.4408 (3.3406)	LR 2.875e-03
0: TRAIN [2][40/1291]	Time 0.042 (0.063)	Data 8.94e-05 (4.39e-03)	Tok/s 182949 (203186)	Loss/tok 3.0239 (3.3183)	LR 2.875e-03
0: TRAIN [2][50/1291]	Time 0.059 (0.059)	Data 9.25e-05 (3.55e-03)	Tok/s 212654 (200231)	Loss/tok 3.3043 (3.2923)	LR 2.875e-03
0: TRAIN [2][60/1291]	Time 0.025 (0.058)	Data 8.20e-05 (2.98e-03)	Tok/s 159076 (200559)	Loss/tok 2.5935 (3.2969)	LR 2.875e-03
0: TRAIN [2][70/1291]	Time 0.076 (0.058)	Data 9.30e-05 (2.58e-03)	Tok/s 226853 (201558)	Loss/tok 3.3978 (3.2917)	LR 2.875e-03
0: TRAIN [2][80/1291]	Time 0.041 (0.057)	Data 9.20e-05 (2.27e-03)	Tok/s 180799 (201105)	Loss/tok 3.0901 (3.2876)	LR 2.875e-03
0: TRAIN [2][90/1291]	Time 0.041 (0.056)	Data 8.54e-05 (2.03e-03)	Tok/s 189837 (201238)	Loss/tok 3.0487 (3.2848)	LR 2.875e-03
0: TRAIN [2][100/1291]	Time 0.076 (0.056)	Data 9.92e-05 (1.84e-03)	Tok/s 230073 (201938)	Loss/tok 3.3617 (3.2886)	LR 2.875e-03
0: TRAIN [2][110/1291]	Time 0.042 (0.056)	Data 8.03e-05 (1.68e-03)	Tok/s 186852 (201769)	Loss/tok 3.0119 (3.2886)	LR 2.875e-03
0: Upscaling, new scale: 2048.0
0: TRAIN [2][120/1291]	Time 0.042 (0.055)	Data 8.27e-05 (1.55e-03)	Tok/s 183002 (200728)	Loss/tok 3.0235 (3.2768)	LR 2.875e-03
0: TRAIN [2][130/1291]	Time 0.058 (0.054)	Data 8.61e-05 (1.44e-03)	Tok/s 219248 (200124)	Loss/tok 3.2525 (3.2715)	LR 2.875e-03
0: TRAIN [2][140/1291]	Time 0.097 (0.054)	Data 8.54e-05 (1.35e-03)	Tok/s 233187 (200390)	Loss/tok 3.5689 (3.2752)	LR 2.875e-03
0: TRAIN [2][150/1291]	Time 0.042 (0.054)	Data 1.19e-04 (1.26e-03)	Tok/s 187665 (200499)	Loss/tok 3.0681 (3.2746)	LR 2.875e-03
0: TRAIN [2][160/1291]	Time 0.097 (0.055)	Data 1.39e-04 (1.19e-03)	Tok/s 233348 (201106)	Loss/tok 3.5700 (3.2816)	LR 2.875e-03
0: TRAIN [2][170/1291]	Time 0.041 (0.054)	Data 8.34e-05 (1.13e-03)	Tok/s 192061 (200970)	Loss/tok 3.0882 (3.2801)	LR 2.875e-03
0: TRAIN [2][180/1291]	Time 0.058 (0.055)	Data 8.46e-05 (1.07e-03)	Tok/s 216890 (201226)	Loss/tok 3.3507 (3.2885)	LR 2.875e-03
0: TRAIN [2][190/1291]	Time 0.076 (0.055)	Data 8.11e-05 (1.02e-03)	Tok/s 229481 (201957)	Loss/tok 3.5384 (3.2903)	LR 2.875e-03
0: TRAIN [2][200/1291]	Time 0.024 (0.054)	Data 1.39e-04 (9.73e-04)	Tok/s 160680 (201432)	Loss/tok 2.6990 (3.2866)	LR 2.875e-03
0: TRAIN [2][210/1291]	Time 0.042 (0.054)	Data 8.46e-05 (9.31e-04)	Tok/s 186627 (201069)	Loss/tok 3.0678 (3.2842)	LR 2.875e-03
0: TRAIN [2][220/1291]	Time 0.059 (0.054)	Data 8.34e-05 (8.93e-04)	Tok/s 216952 (201019)	Loss/tok 3.3139 (3.2866)	LR 2.875e-03
0: TRAIN [2][230/1291]	Time 0.059 (0.054)	Data 8.80e-05 (8.58e-04)	Tok/s 213657 (201741)	Loss/tok 3.2828 (3.2937)	LR 2.875e-03
0: TRAIN [2][240/1291]	Time 0.076 (0.055)	Data 9.11e-05 (8.27e-04)	Tok/s 228962 (202074)	Loss/tok 3.5380 (3.2953)	LR 2.875e-03
0: Upscaling, new scale: 4096.0
0: Gradient norm: inf
0: Skipped batch, new scale: 2048.0
0: TRAIN [2][250/1291]	Time 0.042 (0.055)	Data 1.03e-04 (7.98e-04)	Tok/s 185615 (202238)	Loss/tok 3.1109 (3.2993)	LR 2.875e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 1024.0
0: TRAIN [2][260/1291]	Time 0.041 (0.055)	Data 8.56e-05 (7.71e-04)	Tok/s 186759 (202156)	Loss/tok 3.0849 (3.3036)	LR 2.875e-03
0: TRAIN [2][270/1291]	Time 0.041 (0.055)	Data 8.20e-05 (7.45e-04)	Tok/s 188451 (201973)	Loss/tok 2.9687 (3.2997)	LR 2.875e-03
0: TRAIN [2][280/1291]	Time 0.042 (0.054)	Data 9.80e-05 (7.22e-04)	Tok/s 186449 (201688)	Loss/tok 3.1264 (3.2978)	LR 2.875e-03
0: TRAIN [2][290/1291]	Time 0.025 (0.054)	Data 9.37e-05 (7.01e-04)	Tok/s 163816 (201639)	Loss/tok 2.7321 (3.2972)	LR 2.875e-03
0: TRAIN [2][300/1291]	Time 0.097 (0.055)	Data 1.31e-04 (6.81e-04)	Tok/s 230532 (201796)	Loss/tok 3.6116 (3.3032)	LR 2.875e-03
0: TRAIN [2][310/1291]	Time 0.076 (0.055)	Data 9.11e-05 (6.61e-04)	Tok/s 228787 (202050)	Loss/tok 3.5159 (3.3055)	LR 2.875e-03
0: TRAIN [2][320/1291]	Time 0.058 (0.054)	Data 8.18e-05 (6.44e-04)	Tok/s 214406 (201953)	Loss/tok 3.1822 (3.3028)	LR 2.875e-03
0: TRAIN [2][330/1291]	Time 0.058 (0.054)	Data 9.13e-05 (6.27e-04)	Tok/s 215938 (201995)	Loss/tok 3.2507 (3.3030)	LR 2.875e-03
0: TRAIN [2][340/1291]	Time 0.042 (0.054)	Data 9.68e-05 (6.11e-04)	Tok/s 187759 (201867)	Loss/tok 3.1488 (3.3047)	LR 2.875e-03
0: TRAIN [2][350/1291]	Time 0.058 (0.055)	Data 1.38e-04 (5.97e-04)	Tok/s 219185 (201898)	Loss/tok 3.2458 (3.3056)	LR 2.875e-03
0: TRAIN [2][360/1291]	Time 0.097 (0.054)	Data 8.15e-05 (5.83e-04)	Tok/s 229630 (201887)	Loss/tok 3.6506 (3.3052)	LR 2.875e-03
0: TRAIN [2][370/1291]	Time 0.058 (0.054)	Data 1.08e-04 (5.70e-04)	Tok/s 217745 (201810)	Loss/tok 3.3615 (3.3050)	LR 2.875e-03
0: Upscaling, new scale: 2048.0
0: TRAIN [2][380/1291]	Time 0.041 (0.054)	Data 8.34e-05 (5.57e-04)	Tok/s 183432 (201254)	Loss/tok 3.2178 (3.3014)	LR 2.875e-03
0: TRAIN [2][390/1291]	Time 0.041 (0.054)	Data 1.17e-04 (5.45e-04)	Tok/s 188978 (201396)	Loss/tok 2.9945 (3.3024)	LR 2.875e-03
0: TRAIN [2][400/1291]	Time 0.025 (0.054)	Data 8.49e-05 (5.34e-04)	Tok/s 156697 (201380)	Loss/tok 2.5817 (3.3029)	LR 2.875e-03
0: TRAIN [2][410/1291]	Time 0.076 (0.054)	Data 8.15e-05 (5.23e-04)	Tok/s 232138 (201242)	Loss/tok 3.4861 (3.3007)	LR 2.875e-03
0: TRAIN [2][420/1291]	Time 0.041 (0.054)	Data 1.44e-04 (5.13e-04)	Tok/s 187733 (201159)	Loss/tok 2.9858 (3.2983)	LR 2.875e-03
0: TRAIN [2][430/1291]	Time 0.042 (0.054)	Data 7.89e-05 (5.03e-04)	Tok/s 186223 (201184)	Loss/tok 3.0681 (3.2977)	LR 2.875e-03
0: TRAIN [2][440/1291]	Time 0.058 (0.054)	Data 7.99e-05 (4.94e-04)	Tok/s 214161 (201153)	Loss/tok 3.3534 (3.2964)	LR 2.875e-03
0: TRAIN [2][450/1291]	Time 0.042 (0.054)	Data 9.82e-05 (4.85e-04)	Tok/s 185999 (201123)	Loss/tok 3.0729 (3.2955)	LR 2.875e-03
0: TRAIN [2][460/1291]	Time 0.076 (0.054)	Data 8.32e-05 (4.76e-04)	Tok/s 224554 (201056)	Loss/tok 3.5435 (3.2949)	LR 2.875e-03
0: TRAIN [2][470/1291]	Time 0.041 (0.054)	Data 8.75e-05 (4.68e-04)	Tok/s 191687 (201095)	Loss/tok 3.0643 (3.2957)	LR 2.875e-03
0: TRAIN [2][480/1291]	Time 0.076 (0.054)	Data 8.18e-05 (4.61e-04)	Tok/s 230505 (201110)	Loss/tok 3.4880 (3.2950)	LR 2.875e-03
0: TRAIN [2][490/1291]	Time 0.097 (0.054)	Data 7.94e-05 (4.53e-04)	Tok/s 227659 (201055)	Loss/tok 3.6349 (3.2956)	LR 2.875e-03
0: TRAIN [2][500/1291]	Time 0.042 (0.054)	Data 1.04e-04 (4.46e-04)	Tok/s 184946 (201132)	Loss/tok 2.9957 (3.2970)	LR 2.875e-03
0: Upscaling, new scale: 4096.0
0: TRAIN [2][510/1291]	Time 0.058 (0.054)	Data 8.03e-05 (4.39e-04)	Tok/s 221149 (201141)	Loss/tok 3.3247 (3.2953)	LR 2.875e-03
0: TRAIN [2][520/1291]	Time 0.076 (0.053)	Data 8.15e-05 (4.32e-04)	Tok/s 229606 (201169)	Loss/tok 3.4951 (3.2946)	LR 2.875e-03
0: TRAIN [2][530/1291]	Time 0.058 (0.054)	Data 8.27e-05 (4.26e-04)	Tok/s 213842 (201423)	Loss/tok 3.3813 (3.2950)	LR 2.875e-03
0: TRAIN [2][540/1291]	Time 0.058 (0.054)	Data 7.87e-05 (4.20e-04)	Tok/s 219079 (201435)	Loss/tok 3.2773 (3.2956)	LR 2.875e-03
0: TRAIN [2][550/1291]	Time 0.041 (0.054)	Data 8.51e-05 (4.14e-04)	Tok/s 192239 (201384)	Loss/tok 2.9854 (3.2956)	LR 2.875e-03
0: TRAIN [2][560/1291]	Time 0.041 (0.054)	Data 7.82e-05 (4.08e-04)	Tok/s 190814 (201351)	Loss/tok 3.1891 (3.2943)	LR 2.875e-03
0: TRAIN [2][570/1291]	Time 0.042 (0.054)	Data 1.39e-04 (4.03e-04)	Tok/s 184059 (201486)	Loss/tok 3.1290 (3.2956)	LR 2.875e-03
0: TRAIN [2][580/1291]	Time 0.025 (0.054)	Data 9.11e-05 (3.97e-04)	Tok/s 158783 (201342)	Loss/tok 2.7077 (3.2946)	LR 2.875e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 2048.0
0: TRAIN [2][590/1291]	Time 0.058 (0.054)	Data 7.92e-05 (3.92e-04)	Tok/s 219017 (201446)	Loss/tok 3.2986 (3.2984)	LR 2.875e-03
0: TRAIN [2][600/1291]	Time 0.041 (0.054)	Data 7.82e-05 (3.87e-04)	Tok/s 188190 (201418)	Loss/tok 3.0713 (3.2994)	LR 2.875e-03
0: TRAIN [2][610/1291]	Time 0.076 (0.053)	Data 1.37e-04 (3.82e-04)	Tok/s 229961 (201224)	Loss/tok 3.4460 (3.2973)	LR 2.875e-03
0: TRAIN [2][620/1291]	Time 0.059 (0.054)	Data 8.46e-05 (3.78e-04)	Tok/s 212376 (201225)	Loss/tok 3.2127 (3.2976)	LR 2.875e-03
0: TRAIN [2][630/1291]	Time 0.042 (0.054)	Data 1.02e-04 (3.73e-04)	Tok/s 183196 (201345)	Loss/tok 3.0423 (3.2985)	LR 2.875e-03
0: TRAIN [2][640/1291]	Time 0.041 (0.054)	Data 1.37e-04 (3.69e-04)	Tok/s 187018 (201347)	Loss/tok 3.2051 (3.2985)	LR 2.875e-03
0: TRAIN [2][650/1291]	Time 0.042 (0.054)	Data 1.37e-04 (3.65e-04)	Tok/s 190901 (201469)	Loss/tok 2.9747 (3.2989)	LR 2.875e-03
0: TRAIN [2][660/1291]	Time 0.041 (0.054)	Data 1.87e-04 (3.61e-04)	Tok/s 187813 (201466)	Loss/tok 3.1146 (3.2983)	LR 2.875e-03
0: TRAIN [2][670/1291]	Time 0.058 (0.054)	Data 1.40e-04 (3.57e-04)	Tok/s 213685 (201400)	Loss/tok 3.3409 (3.2980)	LR 2.875e-03
0: TRAIN [2][680/1291]	Time 0.041 (0.053)	Data 8.06e-05 (3.53e-04)	Tok/s 189871 (201326)	Loss/tok 3.1598 (3.2970)	LR 2.875e-03
0: TRAIN [2][690/1291]	Time 0.041 (0.053)	Data 7.96e-05 (3.49e-04)	Tok/s 187465 (201392)	Loss/tok 3.0603 (3.2962)	LR 2.875e-03
0: TRAIN [2][700/1291]	Time 0.059 (0.053)	Data 7.92e-05 (3.46e-04)	Tok/s 212325 (201407)	Loss/tok 3.4696 (3.2959)	LR 2.875e-03
0: TRAIN [2][710/1291]	Time 0.042 (0.054)	Data 7.92e-05 (3.42e-04)	Tok/s 188100 (201496)	Loss/tok 3.0333 (3.2960)	LR 2.875e-03
0: Upscaling, new scale: 4096.0
0: Gradient norm: inf
0: Skipped batch, new scale: 2048.0
0: TRAIN [2][720/1291]	Time 0.041 (0.054)	Data 7.94e-05 (3.38e-04)	Tok/s 196769 (201572)	Loss/tok 3.0405 (3.2961)	LR 2.875e-03
0: TRAIN [2][730/1291]	Time 0.097 (0.054)	Data 7.75e-05 (3.35e-04)	Tok/s 228504 (201678)	Loss/tok 3.6507 (3.2984)	LR 2.875e-03
0: TRAIN [2][740/1291]	Time 0.041 (0.054)	Data 7.72e-05 (3.31e-04)	Tok/s 179523 (201661)	Loss/tok 3.0919 (3.2971)	LR 2.875e-03
0: TRAIN [2][750/1291]	Time 0.096 (0.054)	Data 7.87e-05 (3.28e-04)	Tok/s 232147 (201611)	Loss/tok 3.5908 (3.2968)	LR 2.875e-03
0: TRAIN [2][760/1291]	Time 0.058 (0.054)	Data 7.72e-05 (3.25e-04)	Tok/s 217891 (201598)	Loss/tok 3.2589 (3.2958)	LR 2.875e-03
0: TRAIN [2][770/1291]	Time 0.076 (0.054)	Data 7.61e-05 (3.21e-04)	Tok/s 231984 (201631)	Loss/tok 3.6702 (3.2964)	LR 2.875e-03
0: TRAIN [2][780/1291]	Time 0.024 (0.054)	Data 8.46e-05 (3.18e-04)	Tok/s 162117 (201662)	Loss/tok 2.6698 (3.2959)	LR 2.875e-03
0: TRAIN [2][790/1291]	Time 0.042 (0.054)	Data 8.15e-05 (3.16e-04)	Tok/s 186893 (201731)	Loss/tok 3.1483 (3.2958)	LR 2.875e-03
0: TRAIN [2][800/1291]	Time 0.041 (0.054)	Data 8.39e-05 (3.13e-04)	Tok/s 187110 (201627)	Loss/tok 3.1520 (3.2943)	LR 2.875e-03
0: TRAIN [2][810/1291]	Time 0.025 (0.054)	Data 7.92e-05 (3.10e-04)	Tok/s 157816 (201644)	Loss/tok 2.6936 (3.2949)	LR 2.875e-03
0: TRAIN [2][820/1291]	Time 0.042 (0.053)	Data 8.27e-05 (3.07e-04)	Tok/s 184451 (201546)	Loss/tok 3.0369 (3.2939)	LR 2.875e-03
0: TRAIN [2][830/1291]	Time 0.042 (0.053)	Data 7.84e-05 (3.04e-04)	Tok/s 192188 (201512)	Loss/tok 3.0822 (3.2935)	LR 2.875e-03
0: TRAIN [2][840/1291]	Time 0.059 (0.053)	Data 8.46e-05 (3.02e-04)	Tok/s 214794 (201460)	Loss/tok 3.3646 (3.2930)	LR 2.875e-03
0: Upscaling, new scale: 4096.0
0: TRAIN [2][850/1291]	Time 0.024 (0.053)	Data 8.06e-05 (2.99e-04)	Tok/s 166242 (201340)	Loss/tok 2.8821 (3.2920)	LR 2.875e-03
0: TRAIN [2][860/1291]	Time 0.042 (0.053)	Data 7.58e-05 (2.97e-04)	Tok/s 184092 (201283)	Loss/tok 3.0089 (3.2907)	LR 2.875e-03
0: TRAIN [2][870/1291]	Time 0.041 (0.053)	Data 7.72e-05 (2.94e-04)	Tok/s 184780 (201262)	Loss/tok 3.0341 (3.2896)	LR 2.875e-03
0: TRAIN [2][880/1291]	Time 0.042 (0.053)	Data 7.80e-05 (2.92e-04)	Tok/s 181421 (201282)	Loss/tok 3.1126 (3.2901)	LR 2.875e-03
0: TRAIN [2][890/1291]	Time 0.041 (0.053)	Data 7.68e-05 (2.90e-04)	Tok/s 188504 (201233)	Loss/tok 3.1010 (3.2887)	LR 2.875e-03
0: TRAIN [2][900/1291]	Time 0.058 (0.053)	Data 8.03e-05 (2.87e-04)	Tok/s 218596 (201260)	Loss/tok 3.4398 (3.2888)	LR 2.875e-03
0: TRAIN [2][910/1291]	Time 0.076 (0.053)	Data 7.84e-05 (2.85e-04)	Tok/s 232460 (201221)	Loss/tok 3.4489 (3.2881)	LR 2.875e-03
0: TRAIN [2][920/1291]	Time 0.077 (0.053)	Data 8.37e-05 (2.83e-04)	Tok/s 228167 (201386)	Loss/tok 3.4588 (3.2890)	LR 2.875e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 2048.0
0: TRAIN [2][930/1291]	Time 0.058 (0.053)	Data 8.70e-05 (2.81e-04)	Tok/s 217312 (201466)	Loss/tok 3.2790 (3.2893)	LR 2.875e-03
0: TRAIN [2][940/1291]	Time 0.041 (0.053)	Data 7.72e-05 (2.78e-04)	Tok/s 183756 (201451)	Loss/tok 3.1749 (3.2886)	LR 2.875e-03
0: TRAIN [2][950/1291]	Time 0.058 (0.053)	Data 7.77e-05 (2.76e-04)	Tok/s 217240 (201371)	Loss/tok 3.4372 (3.2879)	LR 2.875e-03
0: TRAIN [2][960/1291]	Time 0.041 (0.053)	Data 8.06e-05 (2.74e-04)	Tok/s 186648 (201450)	Loss/tok 3.1348 (3.2872)	LR 2.875e-03
0: TRAIN [2][970/1291]	Time 0.058 (0.053)	Data 9.37e-05 (2.72e-04)	Tok/s 214861 (201473)	Loss/tok 3.1353 (3.2865)	LR 2.875e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 1024.0
0: TRAIN [2][980/1291]	Time 0.058 (0.053)	Data 7.46e-05 (2.70e-04)	Tok/s 218403 (201583)	Loss/tok 3.2263 (3.2877)	LR 2.875e-03
0: TRAIN [2][990/1291]	Time 0.041 (0.053)	Data 7.58e-05 (2.68e-04)	Tok/s 180492 (201531)	Loss/tok 3.0724 (3.2874)	LR 2.875e-03
0: TRAIN [2][1000/1291]	Time 0.041 (0.053)	Data 8.03e-05 (2.67e-04)	Tok/s 189789 (201484)	Loss/tok 3.0904 (3.2864)	LR 2.875e-03
0: TRAIN [2][1010/1291]	Time 0.042 (0.053)	Data 7.61e-05 (2.65e-04)	Tok/s 186531 (201390)	Loss/tok 2.9990 (3.2850)	LR 2.875e-03
0: TRAIN [2][1020/1291]	Time 0.025 (0.053)	Data 7.58e-05 (2.63e-04)	Tok/s 158456 (201326)	Loss/tok 2.7183 (3.2843)	LR 2.875e-03
0: TRAIN [2][1030/1291]	Time 0.041 (0.053)	Data 7.72e-05 (2.61e-04)	Tok/s 188426 (201241)	Loss/tok 3.0989 (3.2834)	LR 2.875e-03
0: TRAIN [2][1040/1291]	Time 0.041 (0.053)	Data 8.06e-05 (2.59e-04)	Tok/s 187209 (201223)	Loss/tok 3.0089 (3.2821)	LR 2.875e-03
0: TRAIN [2][1050/1291]	Time 0.058 (0.053)	Data 7.70e-05 (2.58e-04)	Tok/s 215932 (201177)	Loss/tok 3.2586 (3.2824)	LR 2.875e-03
0: TRAIN [2][1060/1291]	Time 0.041 (0.053)	Data 7.80e-05 (2.56e-04)	Tok/s 186391 (201199)	Loss/tok 3.1071 (3.2827)	LR 2.875e-03
0: TRAIN [2][1070/1291]	Time 0.041 (0.053)	Data 1.28e-04 (2.54e-04)	Tok/s 186426 (201162)	Loss/tok 3.1452 (3.2827)	LR 2.875e-03
0: TRAIN [2][1080/1291]	Time 0.042 (0.053)	Data 7.65e-05 (2.53e-04)	Tok/s 182648 (201086)	Loss/tok 2.9659 (3.2817)	LR 2.875e-03
0: TRAIN [2][1090/1291]	Time 0.058 (0.053)	Data 7.84e-05 (2.51e-04)	Tok/s 215869 (201029)	Loss/tok 3.1897 (3.2808)	LR 2.875e-03
0: TRAIN [2][1100/1291]	Time 0.097 (0.053)	Data 7.77e-05 (2.50e-04)	Tok/s 231622 (201046)	Loss/tok 3.5156 (3.2811)	LR 2.875e-03
0: Upscaling, new scale: 2048.0
0: TRAIN [2][1110/1291]	Time 0.042 (0.053)	Data 7.77e-05 (2.48e-04)	Tok/s 184508 (201150)	Loss/tok 3.0450 (3.2817)	LR 2.875e-03
0: TRAIN [2][1120/1291]	Time 0.076 (0.053)	Data 8.54e-05 (2.47e-04)	Tok/s 232112 (201167)	Loss/tok 3.4352 (3.2820)	LR 2.875e-03
0: TRAIN [2][1130/1291]	Time 0.058 (0.053)	Data 7.75e-05 (2.45e-04)	Tok/s 216486 (201174)	Loss/tok 3.1024 (3.2810)	LR 2.875e-03
0: TRAIN [2][1140/1291]	Time 0.058 (0.053)	Data 7.63e-05 (2.44e-04)	Tok/s 218659 (201258)	Loss/tok 3.3087 (3.2809)	LR 2.875e-03
0: TRAIN [2][1150/1291]	Time 0.041 (0.053)	Data 7.53e-05 (2.42e-04)	Tok/s 186907 (201283)	Loss/tok 2.9619 (3.2813)	LR 2.875e-03
0: TRAIN [2][1160/1291]	Time 0.025 (0.053)	Data 7.65e-05 (2.41e-04)	Tok/s 165040 (201275)	Loss/tok 2.6158 (3.2812)	LR 2.875e-03
0: TRAIN [2][1170/1291]	Time 0.041 (0.053)	Data 7.61e-05 (2.39e-04)	Tok/s 182378 (201171)	Loss/tok 3.1329 (3.2805)	LR 2.875e-03
0: TRAIN [2][1180/1291]	Time 0.058 (0.053)	Data 8.25e-05 (2.38e-04)	Tok/s 214249 (201141)	Loss/tok 3.2282 (3.2797)	LR 2.875e-03
0: TRAIN [2][1190/1291]	Time 0.076 (0.053)	Data 7.92e-05 (2.37e-04)	Tok/s 226565 (201106)	Loss/tok 3.5593 (3.2790)	LR 2.875e-03
0: TRAIN [2][1200/1291]	Time 0.059 (0.053)	Data 7.58e-05 (2.35e-04)	Tok/s 213377 (201117)	Loss/tok 3.2166 (3.2793)	LR 2.875e-03
0: TRAIN [2][1210/1291]	Time 0.076 (0.053)	Data 7.75e-05 (2.34e-04)	Tok/s 226840 (201130)	Loss/tok 3.5076 (3.2793)	LR 2.875e-03
0: TRAIN [2][1220/1291]	Time 0.042 (0.053)	Data 7.58e-05 (2.33e-04)	Tok/s 187037 (201041)	Loss/tok 2.9840 (3.2782)	LR 2.875e-03
0: TRAIN [2][1230/1291]	Time 0.058 (0.053)	Data 7.46e-05 (2.32e-04)	Tok/s 214221 (201088)	Loss/tok 3.4383 (3.2789)	LR 2.875e-03
0: Upscaling, new scale: 4096.0
0: TRAIN [2][1240/1291]	Time 0.077 (0.053)	Data 7.70e-05 (2.30e-04)	Tok/s 230418 (201108)	Loss/tok 3.3032 (3.2787)	LR 2.875e-03
0: TRAIN [2][1250/1291]	Time 0.076 (0.053)	Data 7.89e-05 (2.29e-04)	Tok/s 230185 (201031)	Loss/tok 3.4641 (3.2783)	LR 2.875e-03
0: TRAIN [2][1260/1291]	Time 0.058 (0.053)	Data 7.63e-05 (2.28e-04)	Tok/s 219140 (201085)	Loss/tok 3.2953 (3.2786)	LR 2.875e-03
0: TRAIN [2][1270/1291]	Time 0.076 (0.053)	Data 8.01e-05 (2.27e-04)	Tok/s 226650 (201193)	Loss/tok 3.4411 (3.2796)	LR 2.875e-03
0: TRAIN [2][1280/1291]	Time 0.042 (0.053)	Data 7.65e-05 (2.26e-04)	Tok/s 182725 (201174)	Loss/tok 3.0311 (3.2792)	LR 2.875e-03
0: TRAIN [2][1290/1291]	Time 0.042 (0.053)	Data 4.22e-05 (2.26e-04)	Tok/s 183936 (201157)	Loss/tok 3.0290 (3.2790)	LR 2.875e-03
:::MLLOG {"namespace": "", "time_ms": 1593113208876, "event_type": "INTERVAL_END", "key": "epoch_stop", "value": null, "metadata": {"file": "train.py", "lineno": 551, "epoch_num": 3}}
:::MLLOG {"namespace": "", "time_ms": 1593113208876, "event_type": "INTERVAL_START", "key": "eval_start", "value": null, "metadata": {"file": "train.py", "lineno": 556, "epoch_num": 3}}
0: Running evaluation on test set
0: TEST [2][0/3]	Time 0.394 (0.394)	Decoder iters 149.0 (149.0)	Tok/s 22855 (22855)
0: Running moses detokenizer
0: BLEU(score=23.117421947508177, counts=[36077, 17709, 9968, 5841], totals=[64618, 61615, 58612, 55613], precisions=[55.83119254696834, 28.741377911222916, 17.006756295639118, 10.502939960081276], bp=0.9991028199899803, sys_len=64618, ref_len=64676)
0: Finished evaluation on test set
:::MLLOG {"namespace": "", "time_ms": 1593113210147, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.23120000000000002, "metadata": {"file": "train.py", "lineno": 562, "epoch_num": 3}}
:::MLLOG {"namespace": "", "time_ms": 1593113210147, "event_type": "INTERVAL_END", "key": "eval_stop", "value": null, "metadata": {"file": "train.py", "lineno": 565, "epoch_num": 3}}
0: Summary: Epoch: 2	Training Loss: 3.2829	Test BLEU: 23.12
0: Performance: Epoch: 2	Training: 3218432 Tok/s
0: Finished epoch 2
:::MLLOG {"namespace": "", "time_ms": 1593113210147, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "train.py", "lineno": 584, "first_epoch_num": 3}}
:::MLLOG {"namespace": "", "time_ms": 1593113210148, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "train.py", "lineno": 538, "first_epoch_num": 4, "epoch_count": 1}}
:::MLLOG {"namespace": "", "time_ms": 1593113210148, "event_type": "INTERVAL_START", "key": "epoch_start", "value": null, "metadata": {"file": "train.py", "lineno": 541, "epoch_num": 4}}
0: Starting epoch 3
0: Sampler for epoch 3 uses seed 2268031777
0: TRAIN [3][0/1291]	Time 0.278 (0.278)	Data 1.87e-01 (1.87e-01)	Tok/s 27626 (27626)	Loss/tok 2.9312 (2.9312)	LR 2.875e-03
0: TRAIN [3][10/1291]	Time 0.076 (0.078)	Data 1.08e-04 (1.71e-02)	Tok/s 228706 (187407)	Loss/tok 3.3336 (3.2159)	LR 2.875e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 2048.0
0: TRAIN [3][20/1291]	Time 0.059 (0.067)	Data 1.36e-04 (9.01e-03)	Tok/s 211420 (195330)	Loss/tok 3.1733 (3.1883)	LR 2.875e-03
0: TRAIN [3][30/1291]	Time 0.058 (0.059)	Data 8.56e-05 (6.14e-03)	Tok/s 218776 (193409)	Loss/tok 3.0056 (3.1438)	LR 2.875e-03
0: TRAIN [3][40/1291]	Time 0.041 (0.055)	Data 9.70e-05 (4.66e-03)	Tok/s 187278 (192293)	Loss/tok 2.9710 (3.1123)	LR 2.875e-03
0: TRAIN [3][50/1291]	Time 0.058 (0.055)	Data 9.13e-05 (3.77e-03)	Tok/s 219471 (194872)	Loss/tok 3.1550 (3.1255)	LR 2.875e-03
0: TRAIN [3][60/1291]	Time 0.041 (0.053)	Data 8.94e-05 (3.17e-03)	Tok/s 188049 (194428)	Loss/tok 2.9268 (3.1182)	LR 2.875e-03
0: TRAIN [3][70/1291]	Time 0.058 (0.052)	Data 9.54e-05 (2.74e-03)	Tok/s 216433 (194795)	Loss/tok 3.2548 (3.1198)	LR 2.875e-03
0: TRAIN [3][80/1291]	Time 0.042 (0.052)	Data 9.51e-05 (2.41e-03)	Tok/s 183140 (195635)	Loss/tok 2.8379 (3.1221)	LR 2.875e-03
0: TRAIN [3][90/1291]	Time 0.058 (0.052)	Data 9.20e-05 (2.16e-03)	Tok/s 223159 (195970)	Loss/tok 3.0082 (3.1227)	LR 2.875e-03
0: TRAIN [3][100/1291]	Time 0.076 (0.052)	Data 8.73e-05 (1.95e-03)	Tok/s 229784 (196898)	Loss/tok 3.4770 (3.1333)	LR 2.875e-03
0: TRAIN [3][110/1291]	Time 0.041 (0.052)	Data 8.75e-05 (1.79e-03)	Tok/s 188911 (197011)	Loss/tok 2.9502 (3.1316)	LR 2.875e-03
0: TRAIN [3][120/1291]	Time 0.058 (0.053)	Data 8.75e-05 (1.65e-03)	Tok/s 216839 (197929)	Loss/tok 3.2746 (3.1497)	LR 2.875e-03
0: TRAIN [3][130/1291]	Time 0.041 (0.053)	Data 1.45e-04 (1.53e-03)	Tok/s 188223 (197616)	Loss/tok 2.9609 (3.1479)	LR 2.875e-03
0: TRAIN [3][140/1291]	Time 0.025 (0.052)	Data 8.65e-05 (1.43e-03)	Tok/s 163413 (197136)	Loss/tok 2.5447 (3.1462)	LR 2.875e-03
0: Upscaling, new scale: 4096.0
0: TRAIN [3][150/1291]	Time 0.076 (0.052)	Data 1.52e-04 (1.34e-03)	Tok/s 231830 (197324)	Loss/tok 3.3833 (3.1482)	LR 2.875e-03
0: TRAIN [3][160/1291]	Time 0.076 (0.052)	Data 8.58e-05 (1.26e-03)	Tok/s 226041 (197456)	Loss/tok 3.3935 (3.1473)	LR 2.875e-03
0: TRAIN [3][170/1291]	Time 0.025 (0.052)	Data 8.42e-05 (1.19e-03)	Tok/s 160509 (197055)	Loss/tok 2.4966 (3.1454)	LR 2.875e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 2048.0
0: Gradient norm: inf
0: Skipped batch, new scale: 1024.0
0: TRAIN [3][180/1291]	Time 0.025 (0.052)	Data 1.45e-04 (1.13e-03)	Tok/s 164521 (197160)	Loss/tok 2.6171 (3.1530)	LR 2.875e-03
0: TRAIN [3][190/1291]	Time 0.059 (0.052)	Data 1.43e-04 (1.08e-03)	Tok/s 215825 (197753)	Loss/tok 3.1061 (3.1608)	LR 2.875e-03
0: TRAIN [3][200/1291]	Time 0.058 (0.052)	Data 1.06e-04 (1.03e-03)	Tok/s 215645 (198335)	Loss/tok 3.1722 (3.1586)	LR 2.875e-03
0: TRAIN [3][210/1291]	Time 0.058 (0.052)	Data 1.43e-04 (9.88e-04)	Tok/s 217992 (198368)	Loss/tok 3.2589 (3.1611)	LR 1.437e-03
0: TRAIN [3][220/1291]	Time 0.076 (0.052)	Data 8.73e-05 (9.47e-04)	Tok/s 230478 (198375)	Loss/tok 3.3397 (3.1601)	LR 1.437e-03
0: TRAIN [3][230/1291]	Time 0.058 (0.052)	Data 1.03e-04 (9.10e-04)	Tok/s 214891 (198963)	Loss/tok 3.1595 (3.1659)	LR 1.437e-03
0: TRAIN [3][240/1291]	Time 0.058 (0.052)	Data 9.01e-05 (8.76e-04)	Tok/s 220663 (198553)	Loss/tok 3.2432 (3.1631)	LR 1.437e-03
0: TRAIN [3][250/1291]	Time 0.024 (0.052)	Data 8.49e-05 (8.45e-04)	Tok/s 162052 (198196)	Loss/tok 2.4416 (3.1587)	LR 1.437e-03
0: TRAIN [3][260/1291]	Time 0.024 (0.052)	Data 1.44e-04 (8.17e-04)	Tok/s 163273 (198004)	Loss/tok 2.5332 (3.1554)	LR 1.437e-03
0: TRAIN [3][270/1291]	Time 0.042 (0.052)	Data 8.61e-05 (7.90e-04)	Tok/s 186915 (198050)	Loss/tok 2.9302 (3.1556)	LR 1.437e-03
0: TRAIN [3][280/1291]	Time 0.097 (0.052)	Data 8.63e-05 (7.65e-04)	Tok/s 231398 (198245)	Loss/tok 3.3565 (3.1568)	LR 1.437e-03
0: TRAIN [3][290/1291]	Time 0.097 (0.052)	Data 1.45e-04 (7.42e-04)	Tok/s 231554 (198526)	Loss/tok 3.5012 (3.1586)	LR 1.437e-03
0: TRAIN [3][300/1291]	Time 0.042 (0.052)	Data 9.70e-05 (7.20e-04)	Tok/s 189338 (198352)	Loss/tok 2.9392 (3.1550)	LR 1.437e-03
0: Upscaling, new scale: 2048.0
0: TRAIN [3][310/1291]	Time 0.041 (0.052)	Data 8.77e-05 (7.01e-04)	Tok/s 188718 (198275)	Loss/tok 2.9988 (3.1529)	LR 1.437e-03
0: TRAIN [3][320/1291]	Time 0.076 (0.051)	Data 1.37e-04 (6.82e-04)	Tok/s 232427 (198218)	Loss/tok 3.3593 (3.1517)	LR 1.437e-03
0: TRAIN [3][330/1291]	Time 0.076 (0.052)	Data 8.80e-05 (6.64e-04)	Tok/s 230571 (198581)	Loss/tok 3.4325 (3.1520)	LR 1.437e-03
0: TRAIN [3][340/1291]	Time 0.042 (0.052)	Data 9.08e-05 (6.47e-04)	Tok/s 182444 (198873)	Loss/tok 2.9673 (3.1562)	LR 1.437e-03
0: TRAIN [3][350/1291]	Time 0.058 (0.052)	Data 8.54e-05 (6.31e-04)	Tok/s 217381 (199081)	Loss/tok 3.1673 (3.1578)	LR 1.437e-03
0: TRAIN [3][360/1291]	Time 0.025 (0.052)	Data 8.34e-05 (6.17e-04)	Tok/s 160814 (199401)	Loss/tok 2.5793 (3.1588)	LR 1.437e-03
0: TRAIN [3][370/1291]	Time 0.042 (0.052)	Data 9.16e-05 (6.03e-04)	Tok/s 180357 (199054)	Loss/tok 2.9920 (3.1553)	LR 1.437e-03
0: TRAIN [3][380/1291]	Time 0.041 (0.052)	Data 1.03e-04 (5.90e-04)	Tok/s 191516 (199012)	Loss/tok 2.9686 (3.1548)	LR 1.437e-03
0: TRAIN [3][390/1291]	Time 0.096 (0.052)	Data 1.43e-04 (5.77e-04)	Tok/s 232067 (199258)	Loss/tok 3.5054 (3.1584)	LR 1.437e-03
0: TRAIN [3][400/1291]	Time 0.042 (0.052)	Data 8.63e-05 (5.65e-04)	Tok/s 185679 (199069)	Loss/tok 3.0617 (3.1564)	LR 1.437e-03
0: TRAIN [3][410/1291]	Time 0.025 (0.052)	Data 9.18e-05 (5.54e-04)	Tok/s 162021 (199246)	Loss/tok 2.5249 (3.1596)	LR 1.437e-03
0: TRAIN [3][420/1291]	Time 0.041 (0.052)	Data 1.44e-04 (5.43e-04)	Tok/s 184321 (199189)	Loss/tok 2.9264 (3.1582)	LR 1.437e-03
0: TRAIN [3][430/1291]	Time 0.059 (0.052)	Data 8.82e-05 (5.33e-04)	Tok/s 216860 (199620)	Loss/tok 3.1412 (3.1610)	LR 1.437e-03
0: Upscaling, new scale: 4096.0
0: TRAIN [3][440/1291]	Time 0.058 (0.052)	Data 8.68e-05 (5.23e-04)	Tok/s 214161 (199545)	Loss/tok 3.0667 (3.1591)	LR 1.437e-03
0: TRAIN [3][450/1291]	Time 0.042 (0.052)	Data 8.96e-05 (5.14e-04)	Tok/s 185633 (199524)	Loss/tok 2.9142 (3.1602)	LR 1.437e-03
0: TRAIN [3][460/1291]	Time 0.059 (0.052)	Data 8.75e-05 (5.05e-04)	Tok/s 212674 (199618)	Loss/tok 3.2104 (3.1613)	LR 1.437e-03
0: TRAIN [3][470/1291]	Time 0.041 (0.052)	Data 8.87e-05 (4.96e-04)	Tok/s 183972 (199649)	Loss/tok 2.9920 (3.1606)	LR 1.437e-03
0: TRAIN [3][480/1291]	Time 0.058 (0.052)	Data 1.42e-04 (4.88e-04)	Tok/s 218960 (199705)	Loss/tok 3.1234 (3.1614)	LR 1.437e-03
0: TRAIN [3][490/1291]	Time 0.058 (0.052)	Data 8.75e-05 (4.80e-04)	Tok/s 215942 (199762)	Loss/tok 3.1467 (3.1605)	LR 1.437e-03
0: TRAIN [3][500/1291]	Time 0.042 (0.052)	Data 8.51e-05 (4.72e-04)	Tok/s 186275 (199636)	Loss/tok 3.0691 (3.1606)	LR 1.437e-03
0: TRAIN [3][510/1291]	Time 0.058 (0.052)	Data 8.94e-05 (4.65e-04)	Tok/s 217583 (199563)	Loss/tok 3.2716 (3.1592)	LR 1.437e-03
0: TRAIN [3][520/1291]	Time 0.076 (0.052)	Data 8.70e-05 (4.58e-04)	Tok/s 231566 (199770)	Loss/tok 3.3373 (3.1596)	LR 1.437e-03
0: TRAIN [3][530/1291]	Time 0.058 (0.052)	Data 1.40e-04 (4.51e-04)	Tok/s 222347 (200006)	Loss/tok 3.0662 (3.1586)	LR 1.437e-03
0: TRAIN [3][540/1291]	Time 0.076 (0.052)	Data 1.44e-04 (4.44e-04)	Tok/s 229507 (199998)	Loss/tok 3.2985 (3.1577)	LR 1.437e-03
0: TRAIN [3][550/1291]	Time 0.058 (0.052)	Data 8.49e-05 (4.38e-04)	Tok/s 218920 (200087)	Loss/tok 3.1469 (3.1566)	LR 1.437e-03
0: Upscaling, new scale: 8192.0
0: TRAIN [3][560/1291]	Time 0.077 (0.052)	Data 8.54e-05 (4.32e-04)	Tok/s 227290 (200058)	Loss/tok 3.4379 (3.1561)	LR 1.437e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 4096.0
0: TRAIN [3][570/1291]	Time 0.041 (0.052)	Data 1.43e-04 (4.26e-04)	Tok/s 188615 (200164)	Loss/tok 3.0631 (3.1574)	LR 1.437e-03
0: TRAIN [3][580/1291]	Time 0.077 (0.052)	Data 1.42e-04 (4.21e-04)	Tok/s 228277 (200218)	Loss/tok 3.3664 (3.1571)	LR 1.437e-03
0: TRAIN [3][590/1291]	Time 0.077 (0.053)	Data 9.23e-05 (4.15e-04)	Tok/s 229786 (200277)	Loss/tok 3.3376 (3.1573)	LR 1.437e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 2048.0
0: TRAIN [3][600/1291]	Time 0.097 (0.053)	Data 8.85e-05 (4.10e-04)	Tok/s 230402 (200423)	Loss/tok 3.4891 (3.1597)	LR 1.437e-03
0: TRAIN [3][610/1291]	Time 0.041 (0.053)	Data 9.80e-05 (4.05e-04)	Tok/s 186827 (200380)	Loss/tok 2.9727 (3.1589)	LR 1.437e-03
0: TRAIN [3][620/1291]	Time 0.041 (0.052)	Data 8.85e-05 (4.00e-04)	Tok/s 186170 (200238)	Loss/tok 3.0428 (3.1575)	LR 1.437e-03
0: TRAIN [3][630/1291]	Time 0.058 (0.052)	Data 1.45e-04 (3.96e-04)	Tok/s 219121 (200225)	Loss/tok 3.0107 (3.1559)	LR 1.437e-03
0: TRAIN [3][640/1291]	Time 0.042 (0.052)	Data 8.70e-05 (3.91e-04)	Tok/s 187264 (200247)	Loss/tok 3.1257 (3.1562)	LR 1.437e-03
0: TRAIN [3][650/1291]	Time 0.024 (0.052)	Data 8.25e-05 (3.87e-04)	Tok/s 161659 (200158)	Loss/tok 2.6098 (3.1554)	LR 1.437e-03
0: TRAIN [3][660/1291]	Time 0.041 (0.052)	Data 1.04e-04 (3.83e-04)	Tok/s 188716 (200026)	Loss/tok 2.9698 (3.1553)	LR 1.437e-03
0: TRAIN [3][670/1291]	Time 0.076 (0.052)	Data 8.68e-05 (3.78e-04)	Tok/s 230425 (200174)	Loss/tok 3.2994 (3.1561)	LR 1.437e-03
0: TRAIN [3][680/1291]	Time 0.042 (0.052)	Data 9.68e-05 (3.74e-04)	Tok/s 187985 (200177)	Loss/tok 3.0370 (3.1569)	LR 1.437e-03
0: TRAIN [3][690/1291]	Time 0.058 (0.052)	Data 8.77e-05 (3.70e-04)	Tok/s 213782 (200253)	Loss/tok 3.1304 (3.1566)	LR 1.437e-03
0: TRAIN [3][700/1291]	Time 0.058 (0.052)	Data 8.92e-05 (3.67e-04)	Tok/s 217450 (200306)	Loss/tok 3.0431 (3.1567)	LR 1.437e-03
0: TRAIN [3][710/1291]	Time 0.076 (0.053)	Data 8.80e-05 (3.63e-04)	Tok/s 228661 (200337)	Loss/tok 3.1291 (3.1574)	LR 1.437e-03
0: TRAIN [3][720/1291]	Time 0.077 (0.053)	Data 8.65e-05 (3.59e-04)	Tok/s 227769 (200345)	Loss/tok 3.3683 (3.1567)	LR 7.187e-04
0: Upscaling, new scale: 4096.0
0: TRAIN [3][730/1291]	Time 0.058 (0.053)	Data 8.56e-05 (3.55e-04)	Tok/s 216251 (200326)	Loss/tok 3.1068 (3.1570)	LR 7.187e-04
0: TRAIN [3][740/1291]	Time 0.076 (0.053)	Data 9.18e-05 (3.52e-04)	Tok/s 233601 (200434)	Loss/tok 3.2568 (3.1573)	LR 7.187e-04
0: TRAIN [3][750/1291]	Time 0.059 (0.053)	Data 9.08e-05 (3.49e-04)	Tok/s 216610 (200649)	Loss/tok 3.0679 (3.1578)	LR 7.187e-04
0: TRAIN [3][760/1291]	Time 0.058 (0.053)	Data 8.75e-05 (3.45e-04)	Tok/s 214523 (200780)	Loss/tok 3.0790 (3.1586)	LR 7.187e-04
0: TRAIN [3][770/1291]	Time 0.097 (0.053)	Data 8.70e-05 (3.42e-04)	Tok/s 230872 (200792)	Loss/tok 3.4034 (3.1591)	LR 7.187e-04
0: TRAIN [3][780/1291]	Time 0.098 (0.053)	Data 8.42e-05 (3.39e-04)	Tok/s 230464 (200749)	Loss/tok 3.5715 (3.1584)	LR 7.187e-04
0: TRAIN [3][790/1291]	Time 0.024 (0.053)	Data 9.01e-05 (3.36e-04)	Tok/s 158983 (200730)	Loss/tok 2.5121 (3.1572)	LR 7.187e-04
0: TRAIN [3][800/1291]	Time 0.097 (0.053)	Data 9.30e-05 (3.33e-04)	Tok/s 226428 (200858)	Loss/tok 3.4351 (3.1575)	LR 7.187e-04
0: TRAIN [3][810/1291]	Time 0.042 (0.053)	Data 8.61e-05 (3.30e-04)	Tok/s 181614 (200948)	Loss/tok 2.9157 (3.1598)	LR 7.187e-04
0: TRAIN [3][820/1291]	Time 0.041 (0.053)	Data 8.56e-05 (3.27e-04)	Tok/s 191095 (200725)	Loss/tok 2.9770 (3.1577)	LR 7.187e-04
0: TRAIN [3][830/1291]	Time 0.076 (0.053)	Data 8.87e-05 (3.24e-04)	Tok/s 229754 (200727)	Loss/tok 3.3502 (3.1572)	LR 7.187e-04
0: TRAIN [3][840/1291]	Time 0.058 (0.053)	Data 8.73e-05 (3.22e-04)	Tok/s 215397 (200737)	Loss/tok 3.0948 (3.1566)	LR 7.187e-04
0: Upscaling, new scale: 8192.0
0: TRAIN [3][850/1291]	Time 0.041 (0.053)	Data 1.39e-04 (3.19e-04)	Tok/s 185889 (200780)	Loss/tok 2.8057 (3.1560)	LR 7.187e-04
0: TRAIN [3][860/1291]	Time 0.025 (0.053)	Data 1.01e-04 (3.17e-04)	Tok/s 158787 (200664)	Loss/tok 2.5715 (3.1545)	LR 7.187e-04
0: Gradient norm: inf
0: Skipped batch, new scale: 4096.0
0: TRAIN [3][870/1291]	Time 0.059 (0.053)	Data 8.63e-05 (3.14e-04)	Tok/s 214244 (200698)	Loss/tok 3.1717 (3.1553)	LR 7.187e-04
0: TRAIN [3][880/1291]	Time 0.042 (0.053)	Data 8.56e-05 (3.12e-04)	Tok/s 187661 (200706)	Loss/tok 2.9247 (3.1542)	LR 7.187e-04
0: TRAIN [3][890/1291]	Time 0.059 (0.053)	Data 8.25e-05 (3.09e-04)	Tok/s 215249 (200802)	Loss/tok 3.1316 (3.1533)	LR 7.187e-04
0: TRAIN [3][900/1291]	Time 0.059 (0.053)	Data 8.70e-05 (3.07e-04)	Tok/s 214740 (200817)	Loss/tok 3.1197 (3.1527)	LR 7.187e-04
0: TRAIN [3][910/1291]	Time 0.041 (0.053)	Data 8.56e-05 (3.04e-04)	Tok/s 185069 (200678)	Loss/tok 2.9513 (3.1512)	LR 7.187e-04
0: TRAIN [3][920/1291]	Time 0.042 (0.053)	Data 8.51e-05 (3.02e-04)	Tok/s 184330 (200731)	Loss/tok 2.8432 (3.1507)	LR 7.187e-04
0: TRAIN [3][930/1291]	Time 0.042 (0.053)	Data 9.44e-05 (3.00e-04)	Tok/s 187315 (200755)	Loss/tok 2.9803 (3.1502)	LR 7.187e-04
0: TRAIN [3][940/1291]	Time 0.041 (0.053)	Data 9.78e-05 (2.98e-04)	Tok/s 185246 (200880)	Loss/tok 2.8942 (3.1509)	LR 7.187e-04
0: TRAIN [3][950/1291]	Time 0.059 (0.053)	Data 8.25e-05 (2.96e-04)	Tok/s 215377 (201019)	Loss/tok 3.0750 (3.1501)	LR 7.187e-04
0: TRAIN [3][960/1291]	Time 0.076 (0.053)	Data 8.85e-05 (2.94e-04)	Tok/s 228844 (201029)	Loss/tok 3.3820 (3.1501)	LR 7.187e-04
0: TRAIN [3][970/1291]	Time 0.042 (0.053)	Data 8.87e-05 (2.92e-04)	Tok/s 185683 (201056)	Loss/tok 2.9005 (3.1502)	LR 7.187e-04
0: TRAIN [3][980/1291]	Time 0.042 (0.053)	Data 8.73e-05 (2.90e-04)	Tok/s 181192 (200991)	Loss/tok 2.9805 (3.1496)	LR 7.187e-04
0: TRAIN [3][990/1291]	Time 0.098 (0.053)	Data 1.09e-04 (2.88e-04)	Tok/s 232940 (201055)	Loss/tok 3.3411 (3.1501)	LR 7.187e-04
0: Upscaling, new scale: 8192.0
0: TRAIN [3][1000/1291]	Time 0.058 (0.053)	Data 8.92e-05 (2.86e-04)	Tok/s 216862 (201037)	Loss/tok 3.0883 (3.1504)	LR 7.187e-04
0: TRAIN [3][1010/1291]	Time 0.024 (0.053)	Data 8.94e-05 (2.84e-04)	Tok/s 161991 (201093)	Loss/tok 2.4731 (3.1497)	LR 7.187e-04
0: TRAIN [3][1020/1291]	Time 0.042 (0.053)	Data 8.96e-05 (2.82e-04)	Tok/s 184856 (201134)	Loss/tok 2.8844 (3.1497)	LR 7.187e-04
0: TRAIN [3][1030/1291]	Time 0.058 (0.053)	Data 8.61e-05 (2.80e-04)	Tok/s 217535 (201033)	Loss/tok 3.2660 (3.1487)	LR 7.187e-04
0: TRAIN [3][1040/1291]	Time 0.077 (0.053)	Data 1.44e-04 (2.79e-04)	Tok/s 227067 (201003)	Loss/tok 3.2618 (3.1488)	LR 7.187e-04
0: TRAIN [3][1050/1291]	Time 0.058 (0.053)	Data 8.73e-05 (2.77e-04)	Tok/s 217746 (200950)	Loss/tok 3.1409 (3.1478)	LR 7.187e-04
0: TRAIN [3][1060/1291]	Time 0.024 (0.053)	Data 9.04e-05 (2.75e-04)	Tok/s 163926 (200836)	Loss/tok 2.5276 (3.1467)	LR 7.187e-04
0: Gradient norm: inf
0: Skipped batch, new scale: 4096.0
0: TRAIN [3][1070/1291]	Time 0.097 (0.053)	Data 8.68e-05 (2.73e-04)	Tok/s 230151 (200885)	Loss/tok 3.3599 (3.1465)	LR 7.187e-04
0: TRAIN [3][1080/1291]	Time 0.058 (0.053)	Data 1.42e-04 (2.72e-04)	Tok/s 216733 (200896)	Loss/tok 3.1709 (3.1464)	LR 7.187e-04
0: TRAIN [3][1090/1291]	Time 0.041 (0.053)	Data 1.42e-04 (2.70e-04)	Tok/s 185385 (200859)	Loss/tok 2.8380 (3.1463)	LR 7.187e-04
0: TRAIN [3][1100/1291]	Time 0.058 (0.053)	Data 8.63e-05 (2.69e-04)	Tok/s 215409 (200839)	Loss/tok 3.1520 (3.1455)	LR 7.187e-04
0: TRAIN [3][1110/1291]	Time 0.097 (0.053)	Data 8.77e-05 (2.67e-04)	Tok/s 228254 (200931)	Loss/tok 3.4297 (3.1464)	LR 7.187e-04
0: TRAIN [3][1120/1291]	Time 0.025 (0.053)	Data 8.77e-05 (2.66e-04)	Tok/s 156249 (200898)	Loss/tok 2.5541 (3.1457)	LR 7.187e-04
0: TRAIN [3][1130/1291]	Time 0.076 (0.053)	Data 9.06e-05 (2.64e-04)	Tok/s 228499 (200911)	Loss/tok 3.2264 (3.1451)	LR 7.187e-04
0: TRAIN [3][1140/1291]	Time 0.058 (0.053)	Data 8.68e-05 (2.63e-04)	Tok/s 216250 (200887)	Loss/tok 3.0971 (3.1444)	LR 7.187e-04
0: TRAIN [3][1150/1291]	Time 0.042 (0.053)	Data 8.63e-05 (2.61e-04)	Tok/s 182677 (200816)	Loss/tok 2.8779 (3.1433)	LR 7.187e-04
0: TRAIN [3][1160/1291]	Time 0.059 (0.053)	Data 8.15e-05 (2.60e-04)	Tok/s 216439 (200895)	Loss/tok 3.0793 (3.1433)	LR 7.187e-04
0: TRAIN [3][1170/1291]	Time 0.076 (0.053)	Data 8.89e-05 (2.58e-04)	Tok/s 230993 (200859)	Loss/tok 3.2387 (3.1428)	LR 7.187e-04
0: TRAIN [3][1180/1291]	Time 0.077 (0.053)	Data 8.73e-05 (2.57e-04)	Tok/s 225301 (200968)	Loss/tok 3.2012 (3.1445)	LR 7.187e-04
0: TRAIN [3][1190/1291]	Time 0.025 (0.053)	Data 8.30e-05 (2.56e-04)	Tok/s 161764 (200943)	Loss/tok 2.5289 (3.1454)	LR 7.187e-04
0: Upscaling, new scale: 8192.0
0: TRAIN [3][1200/1291]	Time 0.042 (0.053)	Data 1.43e-04 (2.54e-04)	Tok/s 184835 (201009)	Loss/tok 2.9934 (3.1454)	LR 7.187e-04
0: TRAIN [3][1210/1291]	Time 0.058 (0.053)	Data 1.00e-04 (2.53e-04)	Tok/s 215792 (200997)	Loss/tok 3.0435 (3.1451)	LR 7.187e-04
0: TRAIN [3][1220/1291]	Time 0.025 (0.053)	Data 8.99e-05 (2.52e-04)	Tok/s 158706 (200933)	Loss/tok 2.5568 (3.1443)	LR 3.594e-04
0: TRAIN [3][1230/1291]	Time 0.058 (0.053)	Data 8.63e-05 (2.51e-04)	Tok/s 216540 (200855)	Loss/tok 3.0899 (3.1432)	LR 3.594e-04
0: TRAIN [3][1240/1291]	Time 0.077 (0.053)	Data 9.35e-05 (2.49e-04)	Tok/s 226167 (200999)	Loss/tok 3.3137 (3.1442)	LR 3.594e-04
0: TRAIN [3][1250/1291]	Time 0.058 (0.053)	Data 8.75e-05 (2.48e-04)	Tok/s 214670 (201033)	Loss/tok 3.1807 (3.1446)	LR 3.594e-04
0: TRAIN [3][1260/1291]	Time 0.025 (0.053)	Data 8.94e-05 (2.47e-04)	Tok/s 157237 (200989)	Loss/tok 2.5737 (3.1438)	LR 3.594e-04
0: TRAIN [3][1270/1291]	Time 0.041 (0.053)	Data 8.63e-05 (2.46e-04)	Tok/s 187704 (201006)	Loss/tok 2.8644 (3.1435)	LR 3.594e-04
0: TRAIN [3][1280/1291]	Time 0.097 (0.053)	Data 1.65e-04 (2.45e-04)	Tok/s 232262 (201078)	Loss/tok 3.3843 (3.1437)	LR 3.594e-04
0: TRAIN [3][1290/1291]	Time 0.076 (0.053)	Data 6.15e-05 (2.45e-04)	Tok/s 228685 (201064)	Loss/tok 3.3134 (3.1433)	LR 3.594e-04
:::MLLOG {"namespace": "", "time_ms": 1593113278484, "event_type": "INTERVAL_END", "key": "epoch_stop", "value": null, "metadata": {"file": "train.py", "lineno": 551, "epoch_num": 4}}
:::MLLOG {"namespace": "", "time_ms": 1593113278484, "event_type": "INTERVAL_START", "key": "eval_start", "value": null, "metadata": {"file": "train.py", "lineno": 556, "epoch_num": 4}}
0: Running evaluation on test set
0: TEST [3][0/3]	Time 0.301 (0.301)	Decoder iters 111.0 (111.0)	Tok/s 29940 (29940)
0: Running moses detokenizer
0: BLEU(score=24.250438353407525, counts=[37157, 18670, 10728, 6414], totals=[65548, 62545, 59542, 56543], precisions=[56.686702874229574, 29.850507634503156, 18.01753384165799, 11.343579222892313], bp=1.0, sys_len=65548, ref_len=64676)
0: Target accuracy reached
0: Finished evaluation on test set
:::MLLOG {"namespace": "", "time_ms": 1593113279708, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.2425, "metadata": {"file": "train.py", "lineno": 562, "epoch_num": 4}}
:::MLLOG {"namespace": "", "time_ms": 1593113279708, "event_type": "INTERVAL_END", "key": "eval_stop", "value": null, "metadata": {"file": "train.py", "lineno": 565, "epoch_num": 4}}
0: Summary: Epoch: 3	Training Loss: 3.1491	Test BLEU: 24.25
0: Performance: Epoch: 3	Training: 3216078 Tok/s
0: Finished epoch 3
:::MLLOG {"namespace": "", "time_ms": 1593113279709, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "train.py", "lineno": 584, "first_epoch_num": 4}}
0: Closing preprocessed data file
:::MLLOG {"namespace": "", "time_ms": 1593113279709, "event_type": "INTERVAL_END", "key": "run_stop", "value": null, "metadata": {"file": "train.py", "lineno": 595, "status": "success"}}
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
ENDING TIMING RUN AT 2020-06-25 12:28:06 PM
RESULT,RNN_TRANSLATOR,,311,nvidia,2020-06-25 12:22:55 PM
ENDING TIMING RUN AT 2020-06-25 12:28:06 PM
RESULT,RNN_TRANSLATOR,,311,nvidia,2020-06-25 12:22:55 PM
ENDING TIMING RUN AT 2020-06-25 12:28:07 PM
RESULT,RNN_TRANSLATOR,,312,nvidia,2020-06-25 12:22:55 PM
ENDING TIMING RUN AT 2020-06-25 12:28:07 PM
ENDING TIMING RUN AT 2020-06-25 12:28:07 PM
RESULT,RNN_TRANSLATOR,,312,nvidia,2020-06-25 12:22:55 PM
RESULT,RNN_TRANSLATOR,,312,nvidia,2020-06-25 12:22:55 PM
ENDING TIMING RUN AT 2020-06-25 12:28:07 PM
ENDING TIMING RUN AT 2020-06-25 12:28:07 PM
RESULT,RNN_TRANSLATOR,,312,nvidia,2020-06-25 12:22:55 PM
RESULT,RNN_TRANSLATOR,,312,nvidia,2020-06-25 12:22:55 PM
ENDING TIMING RUN AT 2020-06-25 12:28:07 PM
RESULT,RNN_TRANSLATOR,,312,nvidia,2020-06-25 12:22:55 PM
ENDING TIMING RUN AT 2020-06-25 12:28:07 PM
RESULT,RNN_TRANSLATOR,,312,nvidia,2020-06-25 12:22:55 PM
ENDING TIMING RUN AT 2020-06-25 12:28:07 PM
RESULT,RNN_TRANSLATOR,,312,nvidia,2020-06-25 12:22:55 PM
ENDING TIMING RUN AT 2020-06-25 12:28:07 PM
ENDING TIMING RUN AT 2020-06-25 12:28:07 PM
ENDING TIMING RUN AT 2020-06-25 12:28:07 PM
RESULT,RNN_TRANSLATOR,,312,nvidia,2020-06-25 12:22:55 PM
RESULT,RNN_TRANSLATOR,,312,nvidia,2020-06-25 12:22:55 PM
RESULT,RNN_TRANSLATOR,,312,nvidia,2020-06-25 12:22:55 PM
ENDING TIMING RUN AT 2020-06-25 12:28:07 PM
RESULT,RNN_TRANSLATOR,,312,nvidia,2020-06-25 12:22:55 PM
ENDING TIMING RUN AT 2020-06-25 12:28:07 PM
RESULT,RNN_TRANSLATOR,,312,nvidia,2020-06-25 12:22:55 PM
ENDING TIMING RUN AT 2020-06-25 12:28:07 PM
RESULT,RNN_TRANSLATOR,,312,nvidia,2020-06-25 12:22:55 PM
