+ echo 'Beginning trial 1 of 4'
Beginning trial 1 of 4
+ '[' 1 -eq 1 ']'
+ srun --ntasks=32 bash -c 'echo -n '\''Clearing cache on '\'' && hostname && sync && sudo /sbin/sysctl vm.drop_caches=3'
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
Clearing cache on circe-n091
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
Clearing cache on circe-n067
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
Clearing cache on circe-n065
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
Clearing cache on circe-n090
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
Clearing cache on circe-n077
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
Clearing cache on circe-n072
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
Clearing cache on circe-n093
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
Clearing cache on circe-n087
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
Clearing cache on circe-n086
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
Clearing cache on circe-n083
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
Clearing cache on circe-n088
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
Clearing cache on circe-n074
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
Clearing cache on circe-n076
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
Clearing cache on circe-n092
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
Clearing cache on circe-n096
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
Clearing cache on circe-n098
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
Clearing cache on circe-n078
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
Clearing cache on circe-n085
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
Clearing cache on circe-n097
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
Clearing cache on circe-n089
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
Clearing cache on circe-n071
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
Clearing cache on circe-n066
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
Clearing cache on circe-n068
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
Clearing cache on circe-n084
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
Clearing cache on circe-n094
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
Clearing cache on circe-n075
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
Clearing cache on circe-n073
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
Clearing cache on circe-n079
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
Clearing cache on circe-n095
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
Clearing cache on circe-n081
Clearing cache on circe-n082
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
Clearing cache on circe-n080
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
+ srun --ntasks=32 --container-name=language_model python -c '
import mlperf_logger
mlperf_logger.log_event(key=mlperf_logger.constants.CACHE_CLEAR, value=True)'
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: starting container ...
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
:::MLLOG {"namespace": "", "time_ms": 1593107937485, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 3}}
:::MLLOG {"namespace": "", "time_ms": 1593107937500, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 3}}
:::MLLOG {"namespace": "", "time_ms": 1593107937505, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 3}}
:::MLLOG {"namespace": "", "time_ms": 1593107937513, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 3}}
:::MLLOG {"namespace": "", "time_ms": 1593107937521, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 3}}
:::MLLOG {"namespace": "", "time_ms": 1593107937523, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 3}}
:::MLLOG {"namespace": "", "time_ms": 1593107937532, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 3}}
:::MLLOG {"namespace": "", "time_ms": 1593107937531, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 3}}
:::MLLOG {"namespace": "", "time_ms": 1593107937535, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 3}}
:::MLLOG {"namespace": "", "time_ms": 1593107937539, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 3}}
:::MLLOG {"namespace": "", "time_ms": 1593107937541, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 3}}
:::MLLOG {"namespace": "", "time_ms": 1593107937553, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 3}}
:::MLLOG {"namespace": "", "time_ms": 1593107937558, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 3}}
:::MLLOG {"namespace": "", "time_ms": 1593107937564, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 3}}
:::MLLOG {"namespace": "", "time_ms": 1593107937566, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 3}}
:::MLLOG {"namespace": "", "time_ms": 1593107937574, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 3}}
:::MLLOG {"namespace": "", "time_ms": 1593107937577, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 3}}
:::MLLOG {"namespace": "", "time_ms": 1593107937587, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 3}}
:::MLLOG {"namespace": "", "time_ms": 1593107937588, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 3}}
:::MLLOG {"namespace": "", "time_ms": 1593107937592, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 3}}
:::MLLOG {"namespace": "", "time_ms": 1593107937593, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 3}}
:::MLLOG {"namespace": "", "time_ms": 1593107937602, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 3}}
:::MLLOG {"namespace": "", "time_ms": 1593107937607, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 3}}
:::MLLOG {"namespace": "", "time_ms": 1593107937612, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 3}}
:::MLLOG {"namespace": "", "time_ms": 1593107937626, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 3}}
:::MLLOG {"namespace": "", "time_ms": 1593107937633, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 3}}
:::MLLOG {"namespace": "", "time_ms": 1593107937642, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 3}}
:::MLLOG {"namespace": "", "time_ms": 1593107937645, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 3}}
:::MLLOG {"namespace": "", "time_ms": 1593107937657, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 3}}
:::MLLOG {"namespace": "", "time_ms": 1593107937668, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 3}}
:::MLLOG {"namespace": "", "time_ms": 1593107937686, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 3}}
:::MLLOG {"namespace": "", "time_ms": 1593107937720, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 3}}
+ srun -l --mpi=none --ntasks=512 --ntasks-per-node=16 --container-name=language_model --container-mounts=/raid/datasets/bert/hdf5/new_2048_shards:/workspace/data,/raid/datasets/bert/hdf5/new_2048_shards:/workspace/data_phase2,/gpfs/fs1/svcnvdlfw//ci/14246670/ci_checkpoints:/results,/gpfs/fs1/scratch/cforster/mlperf/phase1_checkpoint:/workspace/phase1,/raid/datasets/bert/hdf5/500_shards:/workspace/evaldata sh -c '/workspace/bert/run_and_time.sh "    ./bind.sh --cpu=exclusive --ib=single --cluster=circe --     python -u /workspace/bert/run_pretraining.py         --train_batch_size=18     --learning_rate=2.0e-3     --opt_lamb_beta_1=0.86     --opt_lamb_beta_2=0.975     --warmup_proportion=0.0     --max_steps=750     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data_phase2     --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.712     --max_samples_termination=7500000     --eval_iter_start_samples=3000000 --eval_iter_samples=500000     --eval_batch_size=16 --eval_dir=/workspace/evaldata     --cache_eval_data     --output_dir=/results     --fp16 --fused_gelu_bias --dense_seq_output --fused_mha      --allreduce_post_accumulation --allreduce_post_accumulation_fp16     --gradient_accumulation_steps=1     --log_freq=1     --local_rank=${SLURM_LOCALID}     --bert_config_path=/workspace/phase1/bert_config.json" 18263 '
368: slurmstepd: pyxis: reusing existing container filesystem
128: slurmstepd: pyxis: reusing existing container filesystem
 80: slurmstepd: pyxis: reusing existing container filesystem
 32: slurmstepd: pyxis: reusing existing container filesystem
  0: slurmstepd: pyxis: reusing existing container filesystem
  0: slurmstepd: pyxis: starting container ...
432: slurmstepd: pyxis: reusing existing container filesystem
400: slurmstepd: pyxis: reusing existing container filesystem
480: slurmstepd: pyxis: reusing existing container filesystem
480: slurmstepd: pyxis: starting container ...
112: slurmstepd: pyxis: reusing existing container filesystem
192: slurmstepd: pyxis: reusing existing container filesystem
384: slurmstepd: pyxis: reusing existing container filesystem
368: slurmstepd: pyxis: starting container ...
128: slurmstepd: pyxis: starting container ...
 80: slurmstepd: pyxis: starting container ...
 32: slurmstepd: pyxis: starting container ...
432: slurmstepd: pyxis: starting container ...
400: slurmstepd: pyxis: starting container ...
112: slurmstepd: pyxis: starting container ...
192: slurmstepd: pyxis: starting container ...
384: slurmstepd: pyxis: starting container ...
256: slurmstepd: pyxis: reusing existing container filesystem
256: slurmstepd: pyxis: starting container ...
288: slurmstepd: pyxis: reusing existing container filesystem
176: slurmstepd: pyxis: reusing existing container filesystem
176: slurmstepd: pyxis: starting container ...
 64: slurmstepd: pyxis: reusing existing container filesystem
224: slurmstepd: pyxis: reusing existing container filesystem
336: slurmstepd: pyxis: reusing existing container filesystem
336: slurmstepd: pyxis: starting container ...
 16: slurmstepd: pyxis: reusing existing container filesystem
160: slurmstepd: pyxis: reusing existing container filesystem
160: slurmstepd: pyxis: starting container ...
288: slurmstepd: pyxis: starting container ...
 64: slurmstepd: pyxis: starting container ...
224: slurmstepd: pyxis: starting container ...
 16: slurmstepd: pyxis: starting container ...
352: slurmstepd: pyxis: reusing existing container filesystem
352: slurmstepd: pyxis: starting container ...
 96: slurmstepd: pyxis: reusing existing container filesystem
 96: slurmstepd: pyxis: starting container ...
 48: slurmstepd: pyxis: reusing existing container filesystem
 48: slurmstepd: pyxis: starting container ...
448: slurmstepd: pyxis: reusing existing container filesystem
448: slurmstepd: pyxis: starting container ...
240: slurmstepd: pyxis: reusing existing container filesystem
272: slurmstepd: pyxis: reusing existing container filesystem
272: slurmstepd: pyxis: starting container ...
496: slurmstepd: pyxis: reusing existing container filesystem
416: slurmstepd: pyxis: reusing existing container filesystem
144: slurmstepd: pyxis: reusing existing container filesystem
320: slurmstepd: pyxis: reusing existing container filesystem
320: slurmstepd: pyxis: starting container ...
208: slurmstepd: pyxis: reusing existing container filesystem
240: slurmstepd: pyxis: starting container ...
496: slurmstepd: pyxis: starting container ...
416: slurmstepd: pyxis: starting container ...
144: slurmstepd: pyxis: starting container ...
208: slurmstepd: pyxis: starting container ...
304: slurmstepd: pyxis: reusing existing container filesystem
304: slurmstepd: pyxis: starting container ...
464: slurmstepd: pyxis: reusing existing container filesystem
464: slurmstepd: pyxis: starting container ...
114: slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
434: slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
 64: slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
 88: slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
 85: slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
 86: slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
 82: slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
 83: slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
 81: slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
 84: slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
 92: slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
 87: slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
 90: slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
 89: slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
 93: slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
 95: slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
 94: slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
 80: slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
 91: slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
164: slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
 49: slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
119: slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
118: slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
117: slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
113: slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
115: slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
112: slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
124: slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
116: slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
120: slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
121: slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
122: slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
126: slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
127: slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
123: slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
125: slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
115: Run vars: id 394913 gpus 16 mparams
121: Run vars: id 394913 gpus 16 mparams
113: Run vars: id 394913 gpus 16 mparams
115: STARTING TIMING RUN AT 2020-06-25 10:59:00 AM
115: + eval '    ./bind.sh --cpu=exclusive --ib=single --cluster=circe --     python -u /workspace/bert/run_pretraining.py         --train_batch_size=18     --learning_rate=2.0e-3     --opt_lamb_beta_1=0.86     --opt_lamb_beta_2=0.975     --warmup_proportion=0.0     --max_steps=750     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data_phase2     --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.712     --max_samples_termination=7500000     --eval_iter_start_samples=3000000 --eval_iter_samples=500000     --eval_batch_size=16 --eval_dir=/workspace/evaldata     --cache_eval_data     --output_dir=/results     --fp16 --fused_gelu_bias --dense_seq_output --fused_mha      --allreduce_post_accumulation --allreduce_post_accumulation_fp16     --gradient_accumulation_steps=1     --log_freq=1     --local_rank=3     --bert_config_path=/workspace/phase1/bert_config.j
115: son --seed=18263'
115: ++ ./bind.sh --cpu=exclusive --ib=single --cluster=circe -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=3 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
121: STARTING TIMING RUN AT 2020-06-25 10:59:00 AM
121: + eval '    ./bind.sh --cpu=exclusive --ib=single --cluster=circe --     python -u /workspace/bert/run_pretraining.py         --train_batch_size=18     --learning_rate=2.0e-3     --opt_lamb_beta_1=0.86     --opt_lamb_beta_2=0.975     --warmup_proportion=0.0     --max_steps=750     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data_phase2     --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.712     --max_samples_termination=7500000     --eval_iter_start_samples=3000000 --eval_iter_samples=500000     --eval_batch_size=16 --eval_dir=/workspace/evaldata     --cache_eval_data     --output_dir=/results     --fp16 --fused_gelu_bias --dense_seq_output --fused_mha      --allreduce_post_accumulation --allreduce_post_accumulation_fp16     --gradient_accumulation_steps=1     --log_freq=1     --local_rank=9     --bert_config_path=/workspace/phase1/bert_config.j
121: son --seed=18263'
121: ++ ./bind.sh --cpu=exclusive --ib=single --cluster=circe -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=9 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
113: STARTING TIMING RUN AT 2020-06-25 10:59:00 AM
122: Run vars: id 394913 gpus 16 mparams
113: + eval '    ./bind.sh --cpu=exclusive --ib=single --cluster=circe --     python -u /workspace/bert/run_pretraining.py         --train_batch_size=18     --learning_rate=2.0e-3     --opt_lamb_beta_1=0.86     --opt_lamb_beta_2=0.975     --warmup_proportion=0.0     --max_steps=750     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data_phase2     --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.712     --max_samples_termination=7500000     --eval_iter_start_samples=3000000 --eval_iter_samples=500000     --eval_batch_size=16 --eval_dir=/workspace/evaldata     --cache_eval_data     --output_dir=/results     --fp16 --fused_gelu_bias --dense_seq_output --fused_mha      --allreduce_post_accumulation --allreduce_post_accumulation_fp16     --gradient_accumulation_steps=1     --log_freq=1     --local_rank=1     --bert_config_path=/workspace/phase1/bert_config.j
113: son --seed=18263'
448: slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
113: ++ ./bind.sh --cpu=exclusive --ib=single --cluster=circe -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=1 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
118: Run vars: id 394913 gpus 16 mparams
112: Run vars: id 394913 gpus 16 mparams
125: Run vars: id 394913 gpus 16 mparams
120: Run vars: id 394913 gpus 16 mparams
123: Run vars: id 394913 gpus 16 mparams
126: Run vars: id 394913 gpus 16 mparams
119: Run vars: id 394913 gpus 16 mparams
122: STARTING TIMING RUN AT 2020-06-25 10:59:00 AM
122: + eval '    ./bind.sh --cpu=exclusive --ib=single --cluster=circe --     python -u /workspace/bert/run_pretraining.py         --train_batch_size=18     --learning_rate=2.0e-3     --opt_lamb_beta_1=0.86     --opt_lamb_beta_2=0.975     --warmup_proportion=0.0     --max_steps=750     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data_phase2     --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.712     --max_samples_termination=7500000     --eval_iter_start_samples=3000000 --eval_iter_samples=500000     --eval_batch_size=16 --eval_dir=/workspace/evaldata     --cache_eval_data     --output_dir=/results     --fp16 --fused_gelu_bias --dense_seq_output --fused_mha      --allreduce_post_accumulation --allreduce_post_accumulation_fp16     --gradient_accumulation_steps=1     --log_freq=1     --local_rank=10     --bert_config_path=/workspace/phase1/bert_config.
122: json --seed=18263'
117: Run vars: id 394913 gpus 16 mparams
122: ++ ./bind.sh --cpu=exclusive --ib=single --cluster=circe -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=10 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
127: Run vars: id 394913 gpus 16 mparams
116: Run vars: id 394913 gpus 16 mparams
118: STARTING TIMING RUN AT 2020-06-25 10:59:00 AM
118: + eval '    ./bind.sh --cpu=exclusive --ib=single --cluster=circe --     python -u /workspace/bert/run_pretraining.py         --train_batch_size=18     --learning_rate=2.0e-3     --opt_lamb_beta_1=0.86     --opt_lamb_beta_2=0.975     --warmup_proportion=0.0     --max_steps=750     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data_phase2     --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.712     --max_samples_termination=7500000     --eval_iter_start_samples=3000000 --eval_iter_samples=500000     --eval_batch_size=16 --eval_dir=/workspace/evaldata     --cache_eval_data     --output_dir=/results     --fp16 --fused_gelu_bias --dense_seq_output --fused_mha      --allreduce_post_accumulation --allreduce_post_accumulation_fp16     --gradient_accumulation_steps=1     --log_freq=1     --local_rank=6     --bert_config_path=/workspace/phase1/bert_config.j
118: son --seed=18263'
118: ++ ./bind.sh --cpu=exclusive --ib=single --cluster=circe -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=6 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
124: Run vars: id 394913 gpus 16 mparams
112: STARTING TIMING RUN AT 2020-06-25 10:59:00 AM
120: STARTING TIMING RUN AT 2020-06-25 10:59:00 AM
112: + eval '    ./bind.sh --cpu=exclusive --ib=single --cluster=circe --     python -u /workspace/bert/run_pretraining.py         --train_batch_size=18     --learning_rate=2.0e-3     --opt_lamb_beta_1=0.86     --opt_lamb_beta_2=0.975     --warmup_proportion=0.0     --max_steps=750     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data_phase2     --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.712     --max_samples_termination=7500000     --eval_iter_start_samples=3000000 --eval_iter_samples=500000     --eval_batch_size=16 --eval_dir=/workspace/evaldata     --cache_eval_data     --output_dir=/results     --fp16 --fused_gelu_bias --dense_seq_output --fused_mha      --allreduce_post_accumulation --allreduce_post_accumulation_fp16     --gradient_accumulation_steps=1     --log_freq=1     --local_rank=0     --bert_config_path=/workspace/phase1/bert_config.j
112: son --seed=18263'
120: + eval '    ./bind.sh --cpu=exclusive --ib=single --cluster=circe --     python -u /workspace/bert/run_pretraining.py         --train_batch_size=18     --learning_rate=2.0e-3     --opt_lamb_beta_1=0.86     --opt_lamb_beta_2=0.975     --warmup_proportion=0.0     --max_steps=750     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data_phase2     --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.712     --max_samples_termination=7500000     --eval_iter_start_samples=3000000 --eval_iter_samples=500000     --eval_batch_size=16 --eval_dir=/workspace/evaldata     --cache_eval_data     --output_dir=/results     --fp16 --fused_gelu_bias --dense_seq_output --fused_mha      --allreduce_post_accumulation --allreduce_post_accumulation_fp16     --gradient_accumulation_steps=1     --log_freq=1     --local_rank=8     --bert_config_path=/workspace/phase1/bert_config.j
120: son --seed=18263'
112: ++ ./bind.sh --cpu=exclusive --ib=single --cluster=circe -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=0 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
120: ++ ./bind.sh --cpu=exclusive --ib=single --cluster=circe -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=8 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
114: Run vars: id 394913 gpus 16 mparams
125: STARTING TIMING RUN AT 2020-06-25 10:59:00 AM
125: + eval '    ./bind.sh --cpu=exclusive --ib=single --cluster=circe --     python -u /workspace/bert/run_pretraining.py         --train_batch_size=18     --learning_rate=2.0e-3     --opt_lamb_beta_1=0.86     --opt_lamb_beta_2=0.975     --warmup_proportion=0.0     --max_steps=750     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data_phase2     --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.712     --max_samples_termination=7500000     --eval_iter_start_samples=3000000 --eval_iter_samples=500000     --eval_batch_size=16 --eval_dir=/workspace/evaldata     --cache_eval_data     --output_dir=/results     --fp16 --fused_gelu_bias --dense_seq_output --fused_mha      --allreduce_post_accumulation --allreduce_post_accumulation_fp16     --gradient_accumulation_steps=1     --log_freq=1     --local_rank=13     --bert_config_path=/workspace/phase1/bert_config.
125: json --seed=18263'
125: ++ ./bind.sh --cpu=exclusive --ib=single --cluster=circe -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=13 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
123: STARTING TIMING RUN AT 2020-06-25 10:59:00 AM
123: + eval '    ./bind.sh --cpu=exclusive --ib=single --cluster=circe --     python -u /workspace/bert/run_pretraining.py         --train_batch_size=18     --learning_rate=2.0e-3     --opt_lamb_beta_1=0.86     --opt_lamb_beta_2=0.975     --warmup_proportion=0.0     --max_steps=750     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data_phase2     --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.712     --max_samples_termination=7500000     --eval_iter_start_samples=3000000 --eval_iter_samples=500000     --eval_batch_size=16 --eval_dir=/workspace/evaldata     --cache_eval_data     --output_dir=/results     --fp16 --fused_gelu_bias --dense_seq_output --fused_mha      --allreduce_post_accumulation --allreduce_post_accumulation_fp16     --gradient_accumulation_steps=1     --log_freq=1     --local_rank=11     --bert_config_path=/workspace/phase1/bert_config.
123: json --seed=18263'
123: ++ ./bind.sh --cpu=exclusive --ib=single --cluster=circe -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=11 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
 32: slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
126: STARTING TIMING RUN AT 2020-06-25 10:59:00 AM
126: + eval '    ./bind.sh --cpu=exclusive --ib=single --cluster=circe --     python -u /workspace/bert/run_pretraining.py         --train_batch_size=18     --learning_rate=2.0e-3     --opt_lamb_beta_1=0.86     --opt_lamb_beta_2=0.975     --warmup_proportion=0.0     --max_steps=750     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data_phase2     --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.712     --max_samples_termination=7500000     --eval_iter_start_samples=3000000 --eval_iter_samples=500000     --eval_batch_size=16 --eval_dir=/workspace/evaldata     --cache_eval_data     --output_dir=/results     --fp16 --fused_gelu_bias --dense_seq_output --fused_mha      --allreduce_post_accumulation --allreduce_post_accumulation_fp16     --gradient_accumulation_steps=1     --log_freq=1     --local_rank=14     --bert_config_path=/workspace/phase1/bert_config.
126: json --seed=18263'
126: ++ ./bind.sh --cpu=exclusive --ib=single --cluster=circe -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=14 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
234: slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
119: STARTING TIMING RUN AT 2020-06-25 10:59:00 AM
119: + eval '    ./bind.sh --cpu=exclusive --ib=single --cluster=circe --     python -u /workspace/bert/run_pretraining.py         --train_batch_size=18     --learning_rate=2.0e-3     --opt_lamb_beta_1=0.86     --opt_lamb_beta_2=0.975     --warmup_proportion=0.0     --max_steps=750     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data_phase2     --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.712     --max_samples_termination=7500000     --eval_iter_start_samples=3000000 --eval_iter_samples=500000     --eval_batch_size=16 --eval_dir=/workspace/evaldata     --cache_eval_data     --output_dir=/results     --fp16 --fused_gelu_bias --dense_seq_output --fused_mha      --allreduce_post_accumulation --allreduce_post_accumulation_fp16     --gradient_accumulation_steps=1     --log_freq=1     --local_rank=7     --bert_config_path=/workspace/phase1/bert_config.j
119: son --seed=18263'
119: ++ ./bind.sh --cpu=exclusive --ib=single --cluster=circe -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=7 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
127: STARTING TIMING RUN AT 2020-06-25 10:59:00 AM
127: + eval '    ./bind.sh --cpu=exclusive --ib=single --cluster=circe --     python -u /workspace/bert/run_pretraining.py         --train_batch_size=18     --learning_rate=2.0e-3     --opt_lamb_beta_1=0.86     --opt_lamb_beta_2=0.975     --warmup_proportion=0.0     --max_steps=750     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data_phase2     --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.712     --max_samples_termination=7500000     --eval_iter_start_samples=3000000 --eval_iter_samples=500000     --eval_batch_size=16 --eval_dir=/workspace/evaldata     --cache_eval_data     --output_dir=/results     --fp16 --fused_gelu_bias --dense_seq_output --fused_mha      --allreduce_post_accumulation --allreduce_post_accumulation_fp16     --gradient_accumulation_steps=1     --log_freq=1     --local_rank=15     --bert_config_path=/workspace/phase1/bert_config.
127: json --seed=18263'
127: ++ ./bind.sh --cpu=exclusive --ib=single --cluster=circe -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=15 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
116: STARTING TIMING RUN AT 2020-06-25 10:59:00 AM
116: + eval '    ./bind.sh --cpu=exclusive --ib=single --cluster=circe --     python -u /workspace/bert/run_pretraining.py         --train_batch_size=18     --learning_rate=2.0e-3     --opt_lamb_beta_1=0.86     --opt_lamb_beta_2=0.975     --warmup_proportion=0.0     --max_steps=750     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data_phase2     --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.712     --max_samples_termination=7500000     --eval_iter_start_samples=3000000 --eval_iter_samples=500000     --eval_batch_size=16 --eval_dir=/workspace/evaldata     --cache_eval_data     --output_dir=/results     --fp16 --fused_gelu_bias --dense_seq_output --fused_mha      --allreduce_post_accumulation --allreduce_post_accumulation_fp16     --gradient_accumulation_steps=1     --log_freq=1     --local_rank=4     --bert_config_path=/workspace/phase1/bert_config.j
116: son --seed=18263'
117: STARTING TIMING RUN AT 2020-06-25 10:59:00 AM
124: STARTING TIMING RUN AT 2020-06-25 10:59:00 AM
116: ++ ./bind.sh --cpu=exclusive --ib=single --cluster=circe -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=4 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
117: + eval '    ./bind.sh --cpu=exclusive --ib=single --cluster=circe --     python -u /workspace/bert/run_pretraining.py         --train_batch_size=18     --learning_rate=2.0e-3     --opt_lamb_beta_1=0.86     --opt_lamb_beta_2=0.975     --warmup_proportion=0.0     --max_steps=750     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data_phase2     --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.712     --max_samples_termination=7500000     --eval_iter_start_samples=3000000 --eval_iter_samples=500000     --eval_batch_size=16 --eval_dir=/workspace/evaldata     --cache_eval_data     --output_dir=/results     --fp16 --fused_gelu_bias --dense_seq_output --fused_mha      --allreduce_post_accumulation --allreduce_post_accumulation_fp16     --gradient_accumulation_steps=1     --log_freq=1     --local_rank=5     --bert_config_path=/workspace/phase1/bert_config.j
117: son --seed=18263'
124: + eval '    ./bind.sh --cpu=exclusive --ib=single --cluster=circe --     python -u /workspace/bert/run_pretraining.py         --train_batch_size=18     --learning_rate=2.0e-3     --opt_lamb_beta_1=0.86     --opt_lamb_beta_2=0.975     --warmup_proportion=0.0     --max_steps=750     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data_phase2     --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.712     --max_samples_termination=7500000     --eval_iter_start_samples=3000000 --eval_iter_samples=500000     --eval_batch_size=16 --eval_dir=/workspace/evaldata     --cache_eval_data     --output_dir=/results     --fp16 --fused_gelu_bias --dense_seq_output --fused_mha      --allreduce_post_accumulation --allreduce_post_accumulation_fp16     --gradient_accumulation_steps=1     --log_freq=1     --local_rank=12     --bert_config_path=/workspace/phase1/bert_config.
124: json --seed=18263'
117: ++ ./bind.sh --cpu=exclusive --ib=single --cluster=circe -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=5 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
124: ++ ./bind.sh --cpu=exclusive --ib=single --cluster=circe -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=12 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
114: STARTING TIMING RUN AT 2020-06-25 10:59:00 AM
114: + eval '    ./bind.sh --cpu=exclusive --ib=single --cluster=circe --     python -u /workspace/bert/run_pretraining.py         --train_batch_size=18     --learning_rate=2.0e-3     --opt_lamb_beta_1=0.86     --opt_lamb_beta_2=0.975     --warmup_proportion=0.0     --max_steps=750     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data_phase2     --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.712     --max_samples_termination=7500000     --eval_iter_start_samples=3000000 --eval_iter_samples=500000     --eval_batch_size=16 --eval_dir=/workspace/evaldata     --cache_eval_data     --output_dir=/results     --fp16 --fused_gelu_bias --dense_seq_output --fused_mha      --allreduce_post_accumulation --allreduce_post_accumulation_fp16     --gradient_accumulation_steps=1     --log_freq=1     --local_rank=2     --bert_config_path=/workspace/phase1/bert_config.j
320: slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
114: son --seed=18263'
114: ++ ./bind.sh --cpu=exclusive --ib=single --cluster=circe -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=2 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
496: slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
131: slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
128: slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
134: slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
130: slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
129: slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
135: slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
140: slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
137: slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
141: slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
139: slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
133: slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
142: slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
136: slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
143: slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
138: slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
132: slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
259: slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
433: slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
432: slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
436: slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
435: slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
443: slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
445: slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
437: slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
446: slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
442: slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
444: slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
447: slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
439: slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
438: slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
441: slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
440: slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
436: Run vars: id 394913 gpus 16 mparams
433: Run vars: id 394913 gpus 16 mparams
432: Run vars: id 394913 gpus 16 mparams
435: Run vars: id 394913 gpus 16 mparams
443: Run vars: id 394913 gpus 16 mparams
434: Run vars: id 394913 gpus 16 mparams
441: Run vars: id 394913 gpus 16 mparams
439: Run vars: id 394913 gpus 16 mparams
195: slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
437: Run vars: id 394913 gpus 16 mparams
447: Run vars: id 394913 gpus 16 mparams
446: Run vars: id 394913 gpus 16 mparams
440: Run vars: id 394913 gpus 16 mparams
444: Run vars: id 394913 gpus 16 mparams
438: Run vars: id 394913 gpus 16 mparams
432: STARTING TIMING RUN AT 2020-06-25 10:59:00 AM
432: + eval '    ./bind.sh --cpu=exclusive --ib=single --cluster=circe --     python -u /workspace/bert/run_pretraining.py         --train_batch_size=18     --learning_rate=2.0e-3     --opt_lamb_beta_1=0.86     --opt_lamb_beta_2=0.975     --warmup_proportion=0.0     --max_steps=750     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data_phase2     --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.712     --max_samples_termination=7500000     --eval_iter_start_samples=3000000 --eval_iter_samples=500000     --eval_batch_size=16 --eval_dir=/workspace/evaldata     --cache_eval_data     --output_dir=/results     --fp16 --fused_gelu_bias --dense_seq_output --fused_mha      --allreduce_post_accumulation --allreduce_post_accumulation_fp16     --gradient_accumulation_steps=1     --log_freq=1     --local_rank=0     --bert_config_path=/workspace/phase1/bert_config.j
432: son --seed=18263'
436: STARTING TIMING RUN AT 2020-06-25 10:59:00 AM
436: + eval '    ./bind.sh --cpu=exclusive --ib=single --cluster=circe --     python -u /workspace/bert/run_pretraining.py         --train_batch_size=18     --learning_rate=2.0e-3     --opt_lamb_beta_1=0.86     --opt_lamb_beta_2=0.975     --warmup_proportion=0.0     --max_steps=750     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data_phase2     --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.712     --max_samples_termination=7500000     --eval_iter_start_samples=3000000 --eval_iter_samples=500000     --eval_batch_size=16 --eval_dir=/workspace/evaldata     --cache_eval_data     --output_dir=/results     --fp16 --fused_gelu_bias --dense_seq_output --fused_mha      --allreduce_post_accumulation --allreduce_post_accumulation_fp16     --gradient_accumulation_steps=1     --log_freq=1     --local_rank=4     --bert_config_path=/workspace/phase1/bert_config.j
436: son --seed=18263'
432: ++ ./bind.sh --cpu=exclusive --ib=single --cluster=circe -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=0 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
442: Run vars: id 394913 gpus 16 mparams
436: ++ ./bind.sh --cpu=exclusive --ib=single --cluster=circe -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=4 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
434: STARTING TIMING RUN AT 2020-06-25 10:59:00 AM
434: + eval '    ./bind.sh --cpu=exclusive --ib=single --cluster=circe --     python -u /workspace/bert/run_pretraining.py         --train_batch_size=18     --learning_rate=2.0e-3     --opt_lamb_beta_1=0.86     --opt_lamb_beta_2=0.975     --warmup_proportion=0.0     --max_steps=750     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data_phase2     --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.712     --max_samples_termination=7500000     --eval_iter_start_samples=3000000 --eval_iter_samples=500000     --eval_batch_size=16 --eval_dir=/workspace/evaldata     --cache_eval_data     --output_dir=/results     --fp16 --fused_gelu_bias --dense_seq_output --fused_mha      --allreduce_post_accumulation --allreduce_post_accumulation_fp16     --gradient_accumulation_steps=1     --log_freq=1     --local_rank=2     --bert_config_path=/workspace/phase1/bert_config.j
434: son --seed=18263'
434: ++ ./bind.sh --cpu=exclusive --ib=single --cluster=circe -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=2 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
433: STARTING TIMING RUN AT 2020-06-25 10:59:00 AM
433: + eval '    ./bind.sh --cpu=exclusive --ib=single --cluster=circe --     python -u /workspace/bert/run_pretraining.py         --train_batch_size=18     --learning_rate=2.0e-3     --opt_lamb_beta_1=0.86     --opt_lamb_beta_2=0.975     --warmup_proportion=0.0     --max_steps=750     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data_phase2     --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.712     --max_samples_termination=7500000     --eval_iter_start_samples=3000000 --eval_iter_samples=500000     --eval_batch_size=16 --eval_dir=/workspace/evaldata     --cache_eval_data     --output_dir=/results     --fp16 --fused_gelu_bias --dense_seq_output --fused_mha      --allreduce_post_accumulation --allreduce_post_accumulation_fp16     --gradient_accumulation_steps=1     --log_freq=1     --local_rank=1     --bert_config_path=/workspace/phase1/bert_config.j
433: son --seed=18263'
435: STARTING TIMING RUN AT 2020-06-25 10:59:00 AM
445: Run vars: id 394913 gpus 16 mparams
433: ++ ./bind.sh --cpu=exclusive --ib=single --cluster=circe -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=1 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
435: + eval '    ./bind.sh --cpu=exclusive --ib=single --cluster=circe --     python -u /workspace/bert/run_pretraining.py         --train_batch_size=18     --learning_rate=2.0e-3     --opt_lamb_beta_1=0.86     --opt_lamb_beta_2=0.975     --warmup_proportion=0.0     --max_steps=750     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data_phase2     --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.712     --max_samples_termination=7500000     --eval_iter_start_samples=3000000 --eval_iter_samples=500000     --eval_batch_size=16 --eval_dir=/workspace/evaldata     --cache_eval_data     --output_dir=/results     --fp16 --fused_gelu_bias --dense_seq_output --fused_mha      --allreduce_post_accumulation --allreduce_post_accumulation_fp16     --gradient_accumulation_steps=1     --log_freq=1     --local_rank=3     --bert_config_path=/workspace/phase1/bert_config.j
435: son --seed=18263'
439: STARTING TIMING RUN AT 2020-06-25 10:59:00 AM
439: + eval '    ./bind.sh --cpu=exclusive --ib=single --cluster=circe --     python -u /workspace/bert/run_pretraining.py         --train_batch_size=18     --learning_rate=2.0e-3     --opt_lamb_beta_1=0.86     --opt_lamb_beta_2=0.975     --warmup_proportion=0.0     --max_steps=750     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data_phase2     --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.712     --max_samples_termination=7500000     --eval_iter_start_samples=3000000 --eval_iter_samples=500000     --eval_batch_size=16 --eval_dir=/workspace/evaldata     --cache_eval_data     --output_dir=/results     --fp16 --fused_gelu_bias --dense_seq_output --fused_mha      --allreduce_post_accumulation --allreduce_post_accumulation_fp16     --gradient_accumulation_steps=1     --log_freq=1     --local_rank=7     --bert_config_path=/workspace/phase1/bert_config.j
439: son --seed=18263'
443: STARTING TIMING RUN AT 2020-06-25 10:59:00 AM
443: + eval '    ./bind.sh --cpu=exclusive --ib=single --cluster=circe --     python -u /workspace/bert/run_pretraining.py         --train_batch_size=18     --learning_rate=2.0e-3     --opt_lamb_beta_1=0.86     --opt_lamb_beta_2=0.975     --warmup_proportion=0.0     --max_steps=750     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data_phase2     --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.712     --max_samples_termination=7500000     --eval_iter_start_samples=3000000 --eval_iter_samples=500000     --eval_batch_size=16 --eval_dir=/workspace/evaldata     --cache_eval_data     --output_dir=/results     --fp16 --fused_gelu_bias --dense_seq_output --fused_mha      --allreduce_post_accumulation --allreduce_post_accumulation_fp16     --gradient_accumulation_steps=1     --log_freq=1     --local_rank=11     --bert_config_path=/workspace/phase1/bert_config.
443: json --seed=18263'
435: ++ ./bind.sh --cpu=exclusive --ib=single --cluster=circe -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=3 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
439: ++ ./bind.sh --cpu=exclusive --ib=single --cluster=circe -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=7 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
443: ++ ./bind.sh --cpu=exclusive --ib=single --cluster=circe -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=11 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
441: STARTING TIMING RUN AT 2020-06-25 10:59:00 AM
437: STARTING TIMING RUN AT 2020-06-25 10:59:00 AM
441: + eval '    ./bind.sh --cpu=exclusive --ib=single --cluster=circe --     python -u /workspace/bert/run_pretraining.py         --train_batch_size=18     --learning_rate=2.0e-3     --opt_lamb_beta_1=0.86     --opt_lamb_beta_2=0.975     --warmup_proportion=0.0     --max_steps=750     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data_phase2     --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.712     --max_samples_termination=7500000     --eval_iter_start_samples=3000000 --eval_iter_samples=500000     --eval_batch_size=16 --eval_dir=/workspace/evaldata     --cache_eval_data     --output_dir=/results     --fp16 --fused_gelu_bias --dense_seq_output --fused_mha      --allreduce_post_accumulation --allreduce_post_accumulation_fp16     --gradient_accumulation_steps=1     --log_freq=1     --local_rank=9     --bert_config_path=/workspace/phase1/bert_config.j
441: son --seed=18263'
437: + eval '    ./bind.sh --cpu=exclusive --ib=single --cluster=circe --     python -u /workspace/bert/run_pretraining.py         --train_batch_size=18     --learning_rate=2.0e-3     --opt_lamb_beta_1=0.86     --opt_lamb_beta_2=0.975     --warmup_proportion=0.0     --max_steps=750     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data_phase2     --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.712     --max_samples_termination=7500000     --eval_iter_start_samples=3000000 --eval_iter_samples=500000     --eval_batch_size=16 --eval_dir=/workspace/evaldata     --cache_eval_data     --output_dir=/results     --fp16 --fused_gelu_bias --dense_seq_output --fused_mha      --allreduce_post_accumulation --allreduce_post_accumulation_fp16     --gradient_accumulation_steps=1     --log_freq=1     --local_rank=5     --bert_config_path=/workspace/phase1/bert_config.j
437: son --seed=18263'
441: ++ ./bind.sh --cpu=exclusive --ib=single --cluster=circe -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=9 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
437: ++ ./bind.sh --cpu=exclusive --ib=single --cluster=circe -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=5 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
447: STARTING TIMING RUN AT 2020-06-25 10:59:00 AM
447: + eval '    ./bind.sh --cpu=exclusive --ib=single --cluster=circe --     python -u /workspace/bert/run_pretraining.py         --train_batch_size=18     --learning_rate=2.0e-3     --opt_lamb_beta_1=0.86     --opt_lamb_beta_2=0.975     --warmup_proportion=0.0     --max_steps=750     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data_phase2     --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.712     --max_samples_termination=7500000     --eval_iter_start_samples=3000000 --eval_iter_samples=500000     --eval_batch_size=16 --eval_dir=/workspace/evaldata     --cache_eval_data     --output_dir=/results     --fp16 --fused_gelu_bias --dense_seq_output --fused_mha      --allreduce_post_accumulation --allreduce_post_accumulation_fp16     --gradient_accumulation_steps=1     --log_freq=1     --local_rank=15     --bert_config_path=/workspace/phase1/bert_config.
447: json --seed=18263'
447: ++ ./bind.sh --cpu=exclusive --ib=single --cluster=circe -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=15 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
440: STARTING TIMING RUN AT 2020-06-25 10:59:00 AM
440: + eval '    ./bind.sh --cpu=exclusive --ib=single --cluster=circe --     python -u /workspace/bert/run_pretraining.py         --train_batch_size=18     --learning_rate=2.0e-3     --opt_lamb_beta_1=0.86     --opt_lamb_beta_2=0.975     --warmup_proportion=0.0     --max_steps=750     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data_phase2     --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.712     --max_samples_termination=7500000     --eval_iter_start_samples=3000000 --eval_iter_samples=500000     --eval_batch_size=16 --eval_dir=/workspace/evaldata     --cache_eval_data     --output_dir=/results     --fp16 --fused_gelu_bias --dense_seq_output --fused_mha      --allreduce_post_accumulation --allreduce_post_accumulation_fp16     --gradient_accumulation_steps=1     --log_freq=1     --local_rank=8     --bert_config_path=/workspace/phase1/bert_config.j
440: son --seed=18263'
446: STARTING TIMING RUN AT 2020-06-25 10:59:00 AM
440: ++ ./bind.sh --cpu=exclusive --ib=single --cluster=circe -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=8 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
446: + eval '    ./bind.sh --cpu=exclusive --ib=single --cluster=circe --     python -u /workspace/bert/run_pretraining.py         --train_batch_size=18     --learning_rate=2.0e-3     --opt_lamb_beta_1=0.86     --opt_lamb_beta_2=0.975     --warmup_proportion=0.0     --max_steps=750     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data_phase2     --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.712     --max_samples_termination=7500000     --eval_iter_start_samples=3000000 --eval_iter_samples=500000     --eval_batch_size=16 --eval_dir=/workspace/evaldata     --cache_eval_data     --output_dir=/results     --fp16 --fused_gelu_bias --dense_seq_output --fused_mha      --allreduce_post_accumulation --allreduce_post_accumulation_fp16     --gradient_accumulation_steps=1     --log_freq=1     --local_rank=14     --bert_config_path=/workspace/phase1/bert_config.
446: json --seed=18263'
446: ++ ./bind.sh --cpu=exclusive --ib=single --cluster=circe -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=14 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
 69: slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
 68: slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
 71: slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
 72: slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
 67: slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
 66: slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
 70: slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
 76: slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
 79: slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
 78: slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
 77: slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
 74: slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
 73: slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
 75: slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
 65: slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
 68: Run vars: id 394913 gpus 16 mparams
 66: Run vars: id 394913 gpus 16 mparams
 77: Run vars: id 394913 gpus 16 mparams
 79: Run vars: id 394913 gpus 16 mparams
 68: STARTING TIMING RUN AT 2020-06-25 10:59:00 AM
 68: + eval '    ./bind.sh --cpu=exclusive --ib=single --cluster=circe --     python -u /workspace/bert/run_pretraining.py         --train_batch_size=18     --learning_rate=2.0e-3     --opt_lamb_beta_1=0.86     --opt_lamb_beta_2=0.975     --warmup_proportion=0.0     --max_steps=750     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data_phase2     --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.712     --max_samples_termination=7500000     --eval_iter_start_samples=3000000 --eval_iter_samples=500000     --eval_batch_size=16 --eval_dir=/workspace/evaldata     --cache_eval_data     --output_dir=/results     --fp16 --fused_gelu_bias --dense_seq_output --fused_mha      --allreduce_post_accumulation --allreduce_post_accumulation_fp16     --gradient_accumulation_steps=1     --log_freq=1     --local_rank=4     --bert_config_path=/workspace/phase1/bert_config.j
 68: son --seed=18263'
444: STARTING TIMING RUN AT 2020-06-25 10:59:00 AM
438: STARTING TIMING RUN AT 2020-06-25 10:59:00 AM
444: + eval '    ./bind.sh --cpu=exclusive --ib=single --cluster=circe --     python -u /workspace/bert/run_pretraining.py         --train_batch_size=18     --learning_rate=2.0e-3     --opt_lamb_beta_1=0.86     --opt_lamb_beta_2=0.975     --warmup_proportion=0.0     --max_steps=750     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data_phase2     --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.712     --max_samples_termination=7500000     --eval_iter_start_samples=3000000 --eval_iter_samples=500000     --eval_batch_size=16 --eval_dir=/workspace/evaldata     --cache_eval_data     --output_dir=/results     --fp16 --fused_gelu_bias --dense_seq_output --fused_mha      --allreduce_post_accumulation --allreduce_post_accumulation_fp16     --gradient_accumulation_steps=1     --log_freq=1     --local_rank=12     --bert_config_path=/workspace/phase1/bert_config.
444: json --seed=18263'
438: + eval '    ./bind.sh --cpu=exclusive --ib=single --cluster=circe --     python -u /workspace/bert/run_pretraining.py         --train_batch_size=18     --learning_rate=2.0e-3     --opt_lamb_beta_1=0.86     --opt_lamb_beta_2=0.975     --warmup_proportion=0.0     --max_steps=750     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data_phase2     --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.712     --max_samples_termination=7500000     --eval_iter_start_samples=3000000 --eval_iter_samples=500000     --eval_batch_size=16 --eval_dir=/workspace/evaldata     --cache_eval_data     --output_dir=/results     --fp16 --fused_gelu_bias --dense_seq_output --fused_mha      --allreduce_post_accumulation --allreduce_post_accumulation_fp16     --gradient_accumulation_steps=1     --log_freq=1     --local_rank=6     --bert_config_path=/workspace/phase1/bert_config.j
438: son --seed=18263'
444: ++ ./bind.sh --cpu=exclusive --ib=single --cluster=circe -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=12 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
438: ++ ./bind.sh --cpu=exclusive --ib=single --cluster=circe -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=6 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
442: STARTING TIMING RUN AT 2020-06-25 10:59:00 AM
442: + eval '    ./bind.sh --cpu=exclusive --ib=single --cluster=circe --     python -u /workspace/bert/run_pretraining.py         --train_batch_size=18     --learning_rate=2.0e-3     --opt_lamb_beta_1=0.86     --opt_lamb_beta_2=0.975     --warmup_proportion=0.0     --max_steps=750     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data_phase2     --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.712     --max_samples_termination=7500000     --eval_iter_start_samples=3000000 --eval_iter_samples=500000     --eval_batch_size=16 --eval_dir=/workspace/evaldata     --cache_eval_data     --output_dir=/results     --fp16 --fused_gelu_bias --dense_seq_output --fused_mha      --allreduce_post_accumulation --allreduce_post_accumulation_fp16     --gradient_accumulation_steps=1     --log_freq=1     --local_rank=10     --bert_config_path=/workspace/phase1/bert_config.
442: json --seed=18263'
442: ++ ./bind.sh --cpu=exclusive --ib=single --cluster=circe -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=10 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
445: STARTING TIMING RUN AT 2020-06-25 10:59:00 AM
445: + eval '    ./bind.sh --cpu=exclusive --ib=single --cluster=circe --     python -u /workspace/bert/run_pretraining.py         --train_batch_size=18     --learning_rate=2.0e-3     --opt_lamb_beta_1=0.86     --opt_lamb_beta_2=0.975     --warmup_proportion=0.0     --max_steps=750     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data_phase2     --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.712     --max_samples_termination=7500000     --eval_iter_start_samples=3000000 --eval_iter_samples=500000     --eval_batch_size=16 --eval_dir=/workspace/evaldata     --cache_eval_data     --output_dir=/results     --fp16 --fused_gelu_bias --dense_seq_output --fused_mha      --allreduce_post_accumulation --allreduce_post_accumulation_fp16     --gradient_accumulation_steps=1     --log_freq=1     --local_rank=13     --bert_config_path=/workspace/phase1/bert_config.
465: slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
445: json --seed=18263'
479: slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
477: slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
478: slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
 68: ++ ./bind.sh --cpu=exclusive --ib=single --cluster=circe -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=4 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
 71: Run vars: id 394913 gpus 16 mparams
 70: Run vars: id 394913 gpus 16 mparams
 77: STARTING TIMING RUN AT 2020-06-25 10:59:00 AM
 77: + eval '    ./bind.sh --cpu=exclusive --ib=single --cluster=circe --     python -u /workspace/bert/run_pretraining.py         --train_batch_size=18     --learning_rate=2.0e-3     --opt_lamb_beta_1=0.86     --opt_lamb_beta_2=0.975     --warmup_proportion=0.0     --max_steps=750     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data_phase2     --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.712     --max_samples_termination=7500000     --eval_iter_start_samples=3000000 --eval_iter_samples=500000     --eval_batch_size=16 --eval_dir=/workspace/evaldata     --cache_eval_data     --output_dir=/results     --fp16 --fused_gelu_bias --dense_seq_output --fused_mha      --allreduce_post_accumulation --allreduce_post_accumulation_fp16     --gradient_accumulation_steps=1     --log_freq=1     --local_rank=13     --bert_config_path=/workspace/phase1/bert_config.
 77: json --seed=18263'
 64: Run vars: id 394913 gpus 16 mparams
 66: STARTING TIMING RUN AT 2020-06-25 10:59:00 AM
 77: ++ ./bind.sh --cpu=exclusive --ib=single --cluster=circe -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=13 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
 66: + eval '    ./bind.sh --cpu=exclusive --ib=single --cluster=circe --     python -u /workspace/bert/run_pretraining.py         --train_batch_size=18     --learning_rate=2.0e-3     --opt_lamb_beta_1=0.86     --opt_lamb_beta_2=0.975     --warmup_proportion=0.0     --max_steps=750     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data_phase2     --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.712     --max_samples_termination=7500000     --eval_iter_start_samples=3000000 --eval_iter_samples=500000     --eval_batch_size=16 --eval_dir=/workspace/evaldata     --cache_eval_data     --output_dir=/results     --fp16 --fused_gelu_bias --dense_seq_output --fused_mha      --allreduce_post_accumulation --allreduce_post_accumulation_fp16     --gradient_accumulation_steps=1     --log_freq=1     --local_rank=2     --bert_config_path=/workspace/phase1/bert_config.j
 66: son --seed=18263'
471: slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
469: slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
464: slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
466: slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
474: slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
476: slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
475: slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
468: slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
467: slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
472: slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
473: slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
 66: ++ ./bind.sh --cpu=exclusive --ib=single --cluster=circe -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=2 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
 67: Run vars: id 394913 gpus 16 mparams
 74: Run vars: id 394913 gpus 16 mparams
 65: Run vars: id 394913 gpus 16 mparams
 79: STARTING TIMING RUN AT 2020-06-25 10:59:00 AM
 79: + eval '    ./bind.sh --cpu=exclusive --ib=single --cluster=circe --     python -u /workspace/bert/run_pretraining.py         --train_batch_size=18     --learning_rate=2.0e-3     --opt_lamb_beta_1=0.86     --opt_lamb_beta_2=0.975     --warmup_proportion=0.0     --max_steps=750     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data_phase2     --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.712     --max_samples_termination=7500000     --eval_iter_start_samples=3000000 --eval_iter_samples=500000     --eval_batch_size=16 --eval_dir=/workspace/evaldata     --cache_eval_data     --output_dir=/results     --fp16 --fused_gelu_bias --dense_seq_output --fused_mha      --allreduce_post_accumulation --allreduce_post_accumulation_fp16     --gradient_accumulation_steps=1     --log_freq=1     --local_rank=15     --bert_config_path=/workspace/phase1/bert_config.
 79: json --seed=18263'
470: slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
 86: Run vars: id 394913 gpus 16 mparams
 79: ++ ./bind.sh --cpu=exclusive --ib=single --cluster=circe -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=15 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
 83: Run vars: id 394913 gpus 16 mparams
 50: slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
 73: Run vars: id 394913 gpus 16 mparams
 63: slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
 85: Run vars: id 394913 gpus 16 mparams
 48: slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
 76: Run vars: id 394913 gpus 16 mparams
 56: slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
 94: Run vars: id 394913 gpus 16 mparams
 53: slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
 72: Run vars: id 394913 gpus 16 mparams
 51: slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
 69: Run vars: id 394913 gpus 16 mparams
 52: slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
 75: Run vars: id 394913 gpus 16 mparams
 71: + eval '    ./bind.sh --cpu=exclusive --ib=single --cluster=circe --     python -u /workspace/bert/run_pretraining.py         --train_batch_size=18     --learning_rate=2.0e-3     --opt_lamb_beta_1=0.86     --opt_lamb_beta_2=0.975     --warmup_proportion=0.0     --max_steps=750     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data_phase2     --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.712     --max_samples_termination=7500000     --eval_iter_start_samples=3000000 --eval_iter_samples=500000     --eval_batch_size=16 --eval_dir=/workspace/evaldata     --cache_eval_data     --output_dir=/results     --fp16 --fused_gelu_bias --dense_seq_output --fused_mha      --allreduce_post_accumulation --allreduce_post_accumulation_fp16     --gradient_accumulation_steps=1     --log_freq=1     --local_rank=7     --bert_config_path=/workspace/phase1/bert_config.j
 78: Run vars: id 394913 gpus 16 mparams
 55: slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
 71: STARTING TIMING RUN AT 2020-06-25 10:59:00 AM
 71: son --seed=18263'
 54: slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
 58: slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
 57: slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
 59: slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
 62: slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
 48: Run vars: id 394913 gpus 16 mparams
 60: slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
 88: Run vars: id 394913 gpus 16 mparams
 61: slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
 49: Run vars: id 394913 gpus 16 mparams
 50: Run vars: id 394913 gpus 16 mparams
 48: STARTING TIMING RUN AT 2020-06-25 10:59:00 AM
 48: + eval '    ./bind.sh --cpu=exclusive --ib=single --cluster=circe --     python -u /workspace/bert/run_pretraining.py         --train_batch_size=18     --learning_rate=2.0e-3     --opt_lamb_beta_1=0.86     --opt_lamb_beta_2=0.975     --warmup_proportion=0.0     --max_steps=750     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data_phase2     --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.712     --max_samples_termination=7500000     --eval_iter_start_samples=3000000 --eval_iter_samples=500000     --eval_batch_size=16 --eval_dir=/workspace/evaldata     --cache_eval_data     --output_dir=/results     --fp16 --fused_gelu_bias --dense_seq_output --fused_mha      --allreduce_post_accumulation --allreduce_post_accumulation_fp16     --gradient_accumulation_steps=1     --log_freq=1     --local_rank=0     --bert_config_path=/workspace/phase1/bert_config.j
 48: son --seed=18263'
 48: ++ ./bind.sh --cpu=exclusive --ib=single --cluster=circe -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=0 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
 49: STARTING TIMING RUN AT 2020-06-25 10:59:00 AM
 50: STARTING TIMING RUN AT 2020-06-25 10:59:00 AM
 49: + eval '    ./bind.sh --cpu=exclusive --ib=single --cluster=circe --     python -u /workspace/bert/run_pretraining.py         --train_batch_size=18     --learning_rate=2.0e-3     --opt_lamb_beta_1=0.86     --opt_lamb_beta_2=0.975     --warmup_proportion=0.0     --max_steps=750     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data_phase2     --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.712     --max_samples_termination=7500000     --eval_iter_start_samples=3000000 --eval_iter_samples=500000     --eval_batch_size=16 --eval_dir=/workspace/evaldata     --cache_eval_data     --output_dir=/results     --fp16 --fused_gelu_bias --dense_seq_output --fused_mha      --allreduce_post_accumulation --allreduce_post_accumulation_fp16     --gradient_accumulation_steps=1     --log_freq=1     --local_rank=1     --bert_config_path=/workspace/phase1/bert_config.j
 84: Run vars: id 394913 gpus 16 mparams
 49: son --seed=18263'
 50: + eval '    ./bind.sh --cpu=exclusive --ib=single --cluster=circe --     python -u /workspace/bert/run_pretraining.py         --train_batch_size=18     --learning_rate=2.0e-3     --opt_lamb_beta_1=0.86     --opt_lamb_beta_2=0.975     --warmup_proportion=0.0     --max_steps=750     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data_phase2     --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.712     --max_samples_termination=7500000     --eval_iter_start_samples=3000000 --eval_iter_samples=500000     --eval_batch_size=16 --eval_dir=/workspace/evaldata     --cache_eval_data     --output_dir=/results     --fp16 --fused_gelu_bias --dense_seq_output --fused_mha      --allreduce_post_accumulation --allreduce_post_accumulation_fp16     --gradient_accumulation_steps=1     --log_freq=1     --local_rank=2     --bert_config_path=/workspace/phase1/bert_config.j
 50: son --seed=18263'
 49: ++ ./bind.sh --cpu=exclusive --ib=single --cluster=circe -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=1 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
 50: ++ ./bind.sh --cpu=exclusive --ib=single --cluster=circe -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=2 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
 71: ++ ./bind.sh --cpu=exclusive --ib=single --cluster=circe -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=7 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
 70: STARTING TIMING RUN AT 2020-06-25 10:59:00 AM
 70: + eval '    ./bind.sh --cpu=exclusive --ib=single --cluster=circe --     python -u /workspace/bert/run_pretraining.py         --train_batch_size=18     --learning_rate=2.0e-3     --opt_lamb_beta_1=0.86     --opt_lamb_beta_2=0.975     --warmup_proportion=0.0     --max_steps=750     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data_phase2     --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.712     --max_samples_termination=7500000     --eval_iter_start_samples=3000000 --eval_iter_samples=500000     --eval_batch_size=16 --eval_dir=/workspace/evaldata     --cache_eval_data     --output_dir=/results     --fp16 --fused_gelu_bias --dense_seq_output --fused_mha      --allreduce_post_accumulation --allreduce_post_accumulation_fp16     --gradient_accumulation_steps=1     --log_freq=1     --local_rank=6     --bert_config_path=/workspace/phase1/bert_config.j
 70: son --seed=18263'
 81: Run vars: id 394913 gpus 16 mparams
450: slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
 70: ++ ./bind.sh --cpu=exclusive --ib=single --cluster=circe -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=6 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
 64: STARTING TIMING RUN AT 2020-06-25 10:59:00 AM
459: slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
458: slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
 64: + eval '    ./bind.sh --cpu=exclusive --ib=single --cluster=circe --     python -u /workspace/bert/run_pretraining.py         --train_batch_size=18     --learning_rate=2.0e-3     --opt_lamb_beta_1=0.86     --opt_lamb_beta_2=0.975     --warmup_proportion=0.0     --max_steps=750     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data_phase2     --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.712     --max_samples_termination=7500000     --eval_iter_start_samples=3000000 --eval_iter_samples=500000     --eval_batch_size=16 --eval_dir=/workspace/evaldata     --cache_eval_data     --output_dir=/results     --fp16 --fused_gelu_bias --dense_seq_output --fused_mha      --allreduce_post_accumulation --allreduce_post_accumulation_fp16     --gradient_accumulation_steps=1     --log_freq=1     --local_rank=0     --bert_config_path=/workspace/phase1/bert_config.j
449: slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
 64: son --seed=18263'
455: slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
457: slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
460: slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
456: slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
461: slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
462: slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
463: slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
453: slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
450: Run vars: id 394913 gpus 16 mparams
454: slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
449: Run vars: id 394913 gpus 16 mparams
451: slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
456: Run vars: id 394913 gpus 16 mparams
452: slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
450: STARTING TIMING RUN AT 2020-06-25 10:59:00 AM
450: + eval '    ./bind.sh --cpu=exclusive --ib=single --cluster=circe --     python -u /workspace/bert/run_pretraining.py         --train_batch_size=18     --learning_rate=2.0e-3     --opt_lamb_beta_1=0.86     --opt_lamb_beta_2=0.975     --warmup_proportion=0.0     --max_steps=750     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data_phase2     --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.712     --max_samples_termination=7500000     --eval_iter_start_samples=3000000 --eval_iter_samples=500000     --eval_batch_size=16 --eval_dir=/workspace/evaldata     --cache_eval_data     --output_dir=/results     --fp16 --fused_gelu_bias --dense_seq_output --fused_mha      --allreduce_post_accumulation --allreduce_post_accumulation_fp16     --gradient_accumulation_steps=1     --log_freq=1     --local_rank=2     --bert_config_path=/workspace/phase1/bert_config.j
 67: STARTING TIMING RUN AT 2020-06-25 10:59:00 AM
450: son --seed=18263'
161: Run vars: id 394913 gpus 16 mparams
 64: ++ ./bind.sh --cpu=exclusive --ib=single --cluster=circe -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=0 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
175: Run vars: id 394913 gpus 16 mparams
 67: + eval '    ./bind.sh --cpu=exclusive --ib=single --cluster=circe --     python -u /workspace/bert/run_pretraining.py         --train_batch_size=18     --learning_rate=2.0e-3     --opt_lamb_beta_1=0.86     --opt_lamb_beta_2=0.975     --warmup_proportion=0.0     --max_steps=750     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data_phase2     --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.712     --max_samples_termination=7500000     --eval_iter_start_samples=3000000 --eval_iter_samples=500000     --eval_batch_size=16 --eval_dir=/workspace/evaldata     --cache_eval_data     --output_dir=/results     --fp16 --fused_gelu_bias --dense_seq_output --fused_mha      --allreduce_post_accumulation --allreduce_post_accumulation_fp16     --gradient_accumulation_steps=1     --log_freq=1     --local_rank=3     --bert_config_path=/workspace/phase1/bert_config.j
161: STARTING TIMING RUN AT 2020-06-25 10:59:00 AM
 67: son --seed=18263'
164: Run vars: id 394913 gpus 16 mparams
160: slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
165: Run vars: id 394913 gpus 16 mparams
161: slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
449: STARTING TIMING RUN AT 2020-06-25 10:59:00 AM
165: slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
 73: STARTING TIMING RUN AT 2020-06-25 10:59:00 AM
166: slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
 76: STARTING TIMING RUN AT 2020-06-25 10:59:00 AM
163: slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
 74: STARTING TIMING RUN AT 2020-06-25 10:59:00 AM
162: slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
 65: STARTING TIMING RUN AT 2020-06-25 10:59:00 AM
169: slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
 72: STARTING TIMING RUN AT 2020-06-25 10:59:00 AM
167: slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
 80: Run vars: id 394913 gpus 16 mparams
170: slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
 75: STARTING TIMING RUN AT 2020-06-25 10:59:00 AM
173: slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
 69: STARTING TIMING RUN AT 2020-06-25 10:59:00 AM
174: slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
 85: STARTING TIMING RUN AT 2020-06-25 10:59:00 AM
175: slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
167: Run vars: id 394913 gpus 16 mparams
172: slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
175: STARTING TIMING RUN AT 2020-06-25 10:59:00 AM
171: slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
 86: STARTING TIMING RUN AT 2020-06-25 10:59:00 AM
168: slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
 94: STARTING TIMING RUN AT 2020-06-25 10:59:00 AM
161: + eval '    ./bind.sh --cpu=exclusive --ib=single --cluster=circe --     python -u /workspace/bert/run_pretraining.py         --train_batch_size=18     --learning_rate=2.0e-3     --opt_lamb_beta_1=0.86     --opt_lamb_beta_2=0.975     --warmup_proportion=0.0     --max_steps=750     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data_phase2     --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.712     --max_samples_termination=7500000     --eval_iter_start_samples=3000000 --eval_iter_samples=500000     --eval_batch_size=16 --eval_dir=/workspace/evaldata     --cache_eval_data     --output_dir=/results     --fp16 --fused_gelu_bias --dense_seq_output --fused_mha      --allreduce_post_accumulation --allreduce_post_accumulation_fp16     --gradient_accumulation_steps=1     --log_freq=1     --local_rank=1     --bert_config_path=/workspace/phase1/bert_config.j
 90: Run vars: id 394913 gpus 16 mparams
161: son --seed=18263'
 93: Run vars: id 394913 gpus 16 mparams
161: ++ ./bind.sh --cpu=exclusive --ib=single --cluster=circe -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=1 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
 88: STARTING TIMING RUN AT 2020-06-25 10:59:00 AM
450: ++ ./bind.sh --cpu=exclusive --ib=single --cluster=circe -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=2 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
 84: STARTING TIMING RUN AT 2020-06-25 10:59:00 AM
449: + eval '    ./bind.sh --cpu=exclusive --ib=single --cluster=circe --     python -u /workspace/bert/run_pretraining.py         --train_batch_size=18     --learning_rate=2.0e-3     --opt_lamb_beta_1=0.86     --opt_lamb_beta_2=0.975     --warmup_proportion=0.0     --max_steps=750     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data_phase2     --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.712     --max_samples_termination=7500000     --eval_iter_start_samples=3000000 --eval_iter_samples=500000     --eval_batch_size=16 --eval_dir=/workspace/evaldata     --cache_eval_data     --output_dir=/results     --fp16 --fused_gelu_bias --dense_seq_output --fused_mha      --allreduce_post_accumulation --allreduce_post_accumulation_fp16     --gradient_accumulation_steps=1     --log_freq=1     --local_rank=1     --bert_config_path=/workspace/phase1/bert_config.j
 82: Run vars: id 394913 gpus 16 mparams
449: son --seed=18263'
 89: Run vars: id 394913 gpus 16 mparams
 67: ++ ./bind.sh --cpu=exclusive --ib=single --cluster=circe -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=3 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
 87: Run vars: id 394913 gpus 16 mparams
 73: + eval '    ./bind.sh --cpu=exclusive --ib=single --cluster=circe --     python -u /workspace/bert/run_pretraining.py         --train_batch_size=18     --learning_rate=2.0e-3     --opt_lamb_beta_1=0.86     --opt_lamb_beta_2=0.975     --warmup_proportion=0.0     --max_steps=750     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data_phase2     --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.712     --max_samples_termination=7500000     --eval_iter_start_samples=3000000 --eval_iter_samples=500000     --eval_batch_size=16 --eval_dir=/workspace/evaldata     --cache_eval_data     --output_dir=/results     --fp16 --fused_gelu_bias --dense_seq_output --fused_mha      --allreduce_post_accumulation --allreduce_post_accumulation_fp16     --gradient_accumulation_steps=1     --log_freq=1     --local_rank=9     --bert_config_path=/workspace/phase1/bert_config.j
 91: Run vars: id 394913 gpus 16 mparams
 73: son --seed=18263'
448: Run vars: id 394913 gpus 16 mparams
 73: ++ ./bind.sh --cpu=exclusive --ib=single --cluster=circe -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=9 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
456: STARTING TIMING RUN AT 2020-06-25 10:59:00 AM
 76: + eval '    ./bind.sh --cpu=exclusive --ib=single --cluster=circe --     python -u /workspace/bert/run_pretraining.py         --train_batch_size=18     --learning_rate=2.0e-3     --opt_lamb_beta_1=0.86     --opt_lamb_beta_2=0.975     --warmup_proportion=0.0     --max_steps=750     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data_phase2     --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.712     --max_samples_termination=7500000     --eval_iter_start_samples=3000000 --eval_iter_samples=500000     --eval_batch_size=16 --eval_dir=/workspace/evaldata     --cache_eval_data     --output_dir=/results     --fp16 --fused_gelu_bias --dense_seq_output --fused_mha      --allreduce_post_accumulation --allreduce_post_accumulation_fp16     --gradient_accumulation_steps=1     --log_freq=1     --local_rank=12     --bert_config_path=/workspace/phase1/bert_config.
 92: Run vars: id 394913 gpus 16 mparams
 76: json --seed=18263'
 83: STARTING TIMING RUN AT 2020-06-25 10:59:00 AM
 74: + eval '    ./bind.sh --cpu=exclusive --ib=single --cluster=circe --     python -u /workspace/bert/run_pretraining.py         --train_batch_size=18     --learning_rate=2.0e-3     --opt_lamb_beta_1=0.86     --opt_lamb_beta_2=0.975     --warmup_proportion=0.0     --max_steps=750     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data_phase2     --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.712     --max_samples_termination=7500000     --eval_iter_start_samples=3000000 --eval_iter_samples=500000     --eval_batch_size=16 --eval_dir=/workspace/evaldata     --cache_eval_data     --output_dir=/results     --fp16 --fused_gelu_bias --dense_seq_output --fused_mha      --allreduce_post_accumulation --allreduce_post_accumulation_fp16     --gradient_accumulation_steps=1     --log_freq=1     --local_rank=10     --bert_config_path=/workspace/phase1/bert_config.
 78: STARTING TIMING RUN AT 2020-06-25 10:59:00 AM
 74: json --seed=18263'
 81: STARTING TIMING RUN AT 2020-06-25 10:59:00 AM
 65: + eval '    ./bind.sh --cpu=exclusive --ib=single --cluster=circe --     python -u /workspace/bert/run_pretraining.py         --train_batch_size=18     --learning_rate=2.0e-3     --opt_lamb_beta_1=0.86     --opt_lamb_beta_2=0.975     --warmup_proportion=0.0     --max_steps=750     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data_phase2     --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.712     --max_samples_termination=7500000     --eval_iter_start_samples=3000000 --eval_iter_samples=500000     --eval_batch_size=16 --eval_dir=/workspace/evaldata     --cache_eval_data     --output_dir=/results     --fp16 --fused_gelu_bias --dense_seq_output --fused_mha      --allreduce_post_accumulation --allreduce_post_accumulation_fp16     --gradient_accumulation_steps=1     --log_freq=1     --local_rank=1     --bert_config_path=/workspace/phase1/bert_config.j
 95: Run vars: id 394913 gpus 16 mparams
 65: son --seed=18263'
 54: Run vars: id 394913 gpus 16 mparams
 74: ++ ./bind.sh --cpu=exclusive --ib=single --cluster=circe -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=10 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
 61: Run vars: id 394913 gpus 16 mparams
 76: ++ ./bind.sh --cpu=exclusive --ib=single --cluster=circe -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=12 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
 63: Run vars: id 394913 gpus 16 mparams
 65: ++ ./bind.sh --cpu=exclusive --ib=single --cluster=circe -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=1 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
 52: Run vars: id 394913 gpus 16 mparams
 72: + eval '    ./bind.sh --cpu=exclusive --ib=single --cluster=circe --     python -u /workspace/bert/run_pretraining.py         --train_batch_size=18     --learning_rate=2.0e-3     --opt_lamb_beta_1=0.86     --opt_lamb_beta_2=0.975     --warmup_proportion=0.0     --max_steps=750     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data_phase2     --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.712     --max_samples_termination=7500000     --eval_iter_start_samples=3000000 --eval_iter_samples=500000     --eval_batch_size=16 --eval_dir=/workspace/evaldata     --cache_eval_data     --output_dir=/results     --fp16 --fused_gelu_bias --dense_seq_output --fused_mha      --allreduce_post_accumulation --allreduce_post_accumulation_fp16     --gradient_accumulation_steps=1     --log_freq=1     --local_rank=8     --bert_config_path=/workspace/phase1/bert_config.j
 53: Run vars: id 394913 gpus 16 mparams
 72: son --seed=18263'
 62: Run vars: id 394913 gpus 16 mparams
 75: + eval '    ./bind.sh --cpu=exclusive --ib=single --cluster=circe --     python -u /workspace/bert/run_pretraining.py         --train_batch_size=18     --learning_rate=2.0e-3     --opt_lamb_beta_1=0.86     --opt_lamb_beta_2=0.975     --warmup_proportion=0.0     --max_steps=750     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data_phase2     --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.712     --max_samples_termination=7500000     --eval_iter_start_samples=3000000 --eval_iter_samples=500000     --eval_batch_size=16 --eval_dir=/workspace/evaldata     --cache_eval_data     --output_dir=/results     --fp16 --fused_gelu_bias --dense_seq_output --fused_mha      --allreduce_post_accumulation --allreduce_post_accumulation_fp16     --gradient_accumulation_steps=1     --log_freq=1     --local_rank=11     --bert_config_path=/workspace/phase1/bert_config.
 59: Run vars: id 394913 gpus 16 mparams
 75: json --seed=18263'
 60: Run vars: id 394913 gpus 16 mparams
176: slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
 51: Run vars: id 394913 gpus 16 mparams
 72: ++ ./bind.sh --cpu=exclusive --ib=single --cluster=circe -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=8 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
 58: Run vars: id 394913 gpus 16 mparams
 75: ++ ./bind.sh --cpu=exclusive --ib=single --cluster=circe -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=11 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
 54: STARTING TIMING RUN AT 2020-06-25 10:59:00 AM
 69: + eval '    ./bind.sh --cpu=exclusive --ib=single --cluster=circe --     python -u /workspace/bert/run_pretraining.py         --train_batch_size=18     --learning_rate=2.0e-3     --opt_lamb_beta_1=0.86     --opt_lamb_beta_2=0.975     --warmup_proportion=0.0     --max_steps=750     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data_phase2     --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.712     --max_samples_termination=7500000     --eval_iter_start_samples=3000000 --eval_iter_samples=500000     --eval_batch_size=16 --eval_dir=/workspace/evaldata     --cache_eval_data     --output_dir=/results     --fp16 --fused_gelu_bias --dense_seq_output --fused_mha      --allreduce_post_accumulation --allreduce_post_accumulation_fp16     --gradient_accumulation_steps=1     --log_freq=1     --local_rank=5     --bert_config_path=/workspace/phase1/bert_config.j
160: Run vars: id 394913 gpus 16 mparams
 69: son --seed=18263'
166: Run vars: id 394913 gpus 16 mparams
 69: ++ ./bind.sh --cpu=exclusive --ib=single --cluster=circe -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=5 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
165: STARTING TIMING RUN AT 2020-06-25 10:59:00 AM
 85: + eval '    ./bind.sh --cpu=exclusive --ib=single --cluster=circe --     python -u /workspace/bert/run_pretraining.py         --train_batch_size=18     --learning_rate=2.0e-3     --opt_lamb_beta_1=0.86     --opt_lamb_beta_2=0.975     --warmup_proportion=0.0     --max_steps=750     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data_phase2     --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.712     --max_samples_termination=7500000     --eval_iter_start_samples=3000000 --eval_iter_samples=500000     --eval_batch_size=16 --eval_dir=/workspace/evaldata     --cache_eval_data     --output_dir=/results     --fp16 --fused_gelu_bias --dense_seq_output --fused_mha      --allreduce_post_accumulation --allreduce_post_accumulation_fp16     --gradient_accumulation_steps=1     --log_freq=1     --local_rank=5     --bert_config_path=/workspace/phase1/bert_config.j
173: Run vars: id 394913 gpus 16 mparams
175: + eval '    ./bind.sh --cpu=exclusive --ib=single --cluster=circe --     python -u /workspace/bert/run_pretraining.py         --train_batch_size=18     --learning_rate=2.0e-3     --opt_lamb_beta_1=0.86     --opt_lamb_beta_2=0.975     --warmup_proportion=0.0     --max_steps=750     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data_phase2     --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.712     --max_samples_termination=7500000     --eval_iter_start_samples=3000000 --eval_iter_samples=500000     --eval_batch_size=16 --eval_dir=/workspace/evaldata     --cache_eval_data     --output_dir=/results     --fp16 --fused_gelu_bias --dense_seq_output --fused_mha      --allreduce_post_accumulation --allreduce_post_accumulation_fp16     --gradient_accumulation_steps=1     --log_freq=1     --local_rank=15     --bert_config_path=/workspace/phase1/bert_config.
163: Run vars: id 394913 gpus 16 mparams
175: json --seed=18263'
167: STARTING TIMING RUN AT 2020-06-25 10:59:00 AM
175: ++ ./bind.sh --cpu=exclusive --ib=single --cluster=circe -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=15 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
 55: Run vars: id 394913 gpus 16 mparams
 85: son --seed=18263'
164: STARTING TIMING RUN AT 2020-06-25 10:59:00 AM
 94: + eval '    ./bind.sh --cpu=exclusive --ib=single --cluster=circe --     python -u /workspace/bert/run_pretraining.py         --train_batch_size=18     --learning_rate=2.0e-3     --opt_lamb_beta_1=0.86     --opt_lamb_beta_2=0.975     --warmup_proportion=0.0     --max_steps=750     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data_phase2     --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.712     --max_samples_termination=7500000     --eval_iter_start_samples=3000000 --eval_iter_samples=500000     --eval_batch_size=16 --eval_dir=/workspace/evaldata     --cache_eval_data     --output_dir=/results     --fp16 --fused_gelu_bias --dense_seq_output --fused_mha      --allreduce_post_accumulation --allreduce_post_accumulation_fp16     --gradient_accumulation_steps=1     --log_freq=1     --local_rank=14     --bert_config_path=/workspace/phase1/bert_config.
 61: STARTING TIMING RUN AT 2020-06-25 10:59:00 AM
 94: json --seed=18263'
170: Run vars: id 394913 gpus 16 mparams
 86: + eval '    ./bind.sh --cpu=exclusive --ib=single --cluster=circe --     python -u /workspace/bert/run_pretraining.py         --train_batch_size=18     --learning_rate=2.0e-3     --opt_lamb_beta_1=0.86     --opt_lamb_beta_2=0.975     --warmup_proportion=0.0     --max_steps=750     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data_phase2     --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.712     --max_samples_termination=7500000     --eval_iter_start_samples=3000000 --eval_iter_samples=500000     --eval_batch_size=16 --eval_dir=/workspace/evaldata     --cache_eval_data     --output_dir=/results     --fp16 --fused_gelu_bias --dense_seq_output --fused_mha      --allreduce_post_accumulation --allreduce_post_accumulation_fp16     --gradient_accumulation_steps=1     --log_freq=1     --local_rank=6     --bert_config_path=/workspace/phase1/bert_config.j
 80: STARTING TIMING RUN AT 2020-06-25 10:59:00 AM
 86: son --seed=18263'
457: Run vars: id 394913 gpus 16 mparams
 85: ++ ./bind.sh --cpu=exclusive --ib=single --cluster=circe -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=5 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
458: Run vars: id 394913 gpus 16 mparams
 94: ++ ./bind.sh --cpu=exclusive --ib=single --cluster=circe -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=14 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
 56: Run vars: id 394913 gpus 16 mparams
 86: ++ ./bind.sh --cpu=exclusive --ib=single --cluster=circe -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=6 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
462: Run vars: id 394913 gpus 16 mparams
 88: + eval '    ./bind.sh --cpu=exclusive --ib=single --cluster=circe --     python -u /workspace/bert/run_pretraining.py         --train_batch_size=18     --learning_rate=2.0e-3     --opt_lamb_beta_1=0.86     --opt_lamb_beta_2=0.975     --warmup_proportion=0.0     --max_steps=750     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data_phase2     --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.712     --max_samples_termination=7500000     --eval_iter_start_samples=3000000 --eval_iter_samples=500000     --eval_batch_size=16 --eval_dir=/workspace/evaldata     --cache_eval_data     --output_dir=/results     --fp16 --fused_gelu_bias --dense_seq_output --fused_mha      --allreduce_post_accumulation --allreduce_post_accumulation_fp16     --gradient_accumulation_steps=1     --log_freq=1     --local_rank=8     --bert_config_path=/workspace/phase1/bert_config.j
 52: STARTING TIMING RUN AT 2020-06-25 10:59:00 AM
 88: son --seed=18263'
451: Run vars: id 394913 gpus 16 mparams
 88: ++ ./bind.sh --cpu=exclusive --ib=single --cluster=circe -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=8 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
 62: STARTING TIMING RUN AT 2020-06-25 10:59:00 AM
 84: + eval '    ./bind.sh --cpu=exclusive --ib=single --cluster=circe --     python -u /workspace/bert/run_pretraining.py         --train_batch_size=18     --learning_rate=2.0e-3     --opt_lamb_beta_1=0.86     --opt_lamb_beta_2=0.975     --warmup_proportion=0.0     --max_steps=750     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data_phase2     --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.712     --max_samples_termination=7500000     --eval_iter_start_samples=3000000 --eval_iter_samples=500000     --eval_batch_size=16 --eval_dir=/workspace/evaldata     --cache_eval_data     --output_dir=/results     --fp16 --fused_gelu_bias --dense_seq_output --fused_mha      --allreduce_post_accumulation --allreduce_post_accumulation_fp16     --gradient_accumulation_steps=1     --log_freq=1     --local_rank=4     --bert_config_path=/workspace/phase1/bert_config.j
453: Run vars: id 394913 gpus 16 mparams
 84: son --seed=18263'
 63: STARTING TIMING RUN AT 2020-06-25 10:59:00 AM
 84: ++ ./bind.sh --cpu=exclusive --ib=single --cluster=circe -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=4 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
455: Run vars: id 394913 gpus 16 mparams
449: ++ ./bind.sh --cpu=exclusive --ib=single --cluster=circe -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=1 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
 90: STARTING TIMING RUN AT 2020-06-25 10:59:00 AM
456: + eval '    ./bind.sh --cpu=exclusive --ib=single --cluster=circe --     python -u /workspace/bert/run_pretraining.py         --train_batch_size=18     --learning_rate=2.0e-3     --opt_lamb_beta_1=0.86     --opt_lamb_beta_2=0.975     --warmup_proportion=0.0     --max_steps=750     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data_phase2     --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.712     --max_samples_termination=7500000     --eval_iter_start_samples=3000000 --eval_iter_samples=500000     --eval_batch_size=16 --eval_dir=/workspace/evaldata     --cache_eval_data     --output_dir=/results     --fp16 --fused_gelu_bias --dense_seq_output --fused_mha      --allreduce_post_accumulation --allreduce_post_accumulation_fp16     --gradient_accumulation_steps=1     --log_freq=1     --local_rank=8     --bert_config_path=/workspace/phase1/bert_config.j
463: Run vars: id 394913 gpus 16 mparams
456: son --seed=18263'
448: STARTING TIMING RUN AT 2020-06-25 10:59:00 AM
 83: + eval '    ./bind.sh --cpu=exclusive --ib=single --cluster=circe --     python -u /workspace/bert/run_pretraining.py         --train_batch_size=18     --learning_rate=2.0e-3     --opt_lamb_beta_1=0.86     --opt_lamb_beta_2=0.975     --warmup_proportion=0.0     --max_steps=750     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data_phase2     --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.712     --max_samples_termination=7500000     --eval_iter_start_samples=3000000 --eval_iter_samples=500000     --eval_batch_size=16 --eval_dir=/workspace/evaldata     --cache_eval_data     --output_dir=/results     --fp16 --fused_gelu_bias --dense_seq_output --fused_mha      --allreduce_post_accumulation --allreduce_post_accumulation_fp16     --gradient_accumulation_steps=1     --log_freq=1     --local_rank=3     --bert_config_path=/workspace/phase1/bert_config.j
 53: STARTING TIMING RUN AT 2020-06-25 10:59:00 AM
 83: son --seed=18263'
452: Run vars: id 394913 gpus 16 mparams
 83: ++ ./bind.sh --cpu=exclusive --ib=single --cluster=circe -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=3 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
 60: STARTING TIMING RUN AT 2020-06-25 10:59:00 AM
 78: + eval '    ./bind.sh --cpu=exclusive --ib=single --cluster=circe --     python -u /workspace/bert/run_pretraining.py         --train_batch_size=18     --learning_rate=2.0e-3     --opt_lamb_beta_1=0.86     --opt_lamb_beta_2=0.975     --warmup_proportion=0.0     --max_steps=750     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data_phase2     --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.712     --max_samples_termination=7500000     --eval_iter_start_samples=3000000 --eval_iter_samples=500000     --eval_batch_size=16 --eval_dir=/workspace/evaldata     --cache_eval_data     --output_dir=/results     --fp16 --fused_gelu_bias --dense_seq_output --fused_mha      --allreduce_post_accumulation --allreduce_post_accumulation_fp16     --gradient_accumulation_steps=1     --log_freq=1     --local_rank=14     --bert_config_path=/workspace/phase1/bert_config.
 93: STARTING TIMING RUN AT 2020-06-25 10:59:00 AM
 78: json --seed=18263'
238: Run vars: id 394913 gpus 16 mparams
 81: + eval '    ./bind.sh --cpu=exclusive --ib=single --cluster=circe --     python -u /workspace/bert/run_pretraining.py         --train_batch_size=18     --learning_rate=2.0e-3     --opt_lamb_beta_1=0.86     --opt_lamb_beta_2=0.975     --warmup_proportion=0.0     --max_steps=750     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data_phase2     --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.712     --max_samples_termination=7500000     --eval_iter_start_samples=3000000 --eval_iter_samples=500000     --eval_batch_size=16 --eval_dir=/workspace/evaldata     --cache_eval_data     --output_dir=/results     --fp16 --fused_gelu_bias --dense_seq_output --fused_mha      --allreduce_post_accumulation --allreduce_post_accumulation_fp16     --gradient_accumulation_steps=1     --log_freq=1     --local_rank=1     --bert_config_path=/workspace/phase1/bert_config.j
238: STARTING TIMING RUN AT 2020-06-25 10:59:00 AM
 78: ++ ./bind.sh --cpu=exclusive --ib=single --cluster=circe -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=14 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
 57: Run vars: id 394913 gpus 16 mparams
 81: son --seed=18263'
 59: STARTING TIMING RUN AT 2020-06-25 10:59:00 AM
 81: ++ ./bind.sh --cpu=exclusive --ib=single --cluster=circe -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=1 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
 87: STARTING TIMING RUN AT 2020-06-25 10:59:00 AM
 54: + eval '    ./bind.sh --cpu=exclusive --ib=single --cluster=circe --     python -u /workspace/bert/run_pretraining.py         --train_batch_size=18     --learning_rate=2.0e-3     --opt_lamb_beta_1=0.86     --opt_lamb_beta_2=0.975     --warmup_proportion=0.0     --max_steps=750     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data_phase2     --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.712     --max_samples_termination=7500000     --eval_iter_start_samples=3000000 --eval_iter_samples=500000     --eval_batch_size=16 --eval_dir=/workspace/evaldata     --cache_eval_data     --output_dir=/results     --fp16 --fused_gelu_bias --dense_seq_output --fused_mha      --allreduce_post_accumulation --allreduce_post_accumulation_fp16     --gradient_accumulation_steps=1     --log_freq=1     --local_rank=6     --bert_config_path=/workspace/phase1/bert_config.j
 89: STARTING TIMING RUN AT 2020-06-25 10:59:00 AM
 54: son --seed=18263'
174: Run vars: id 394913 gpus 16 mparams
165: + eval '    ./bind.sh --cpu=exclusive --ib=single --cluster=circe --     python -u /workspace/bert/run_pretraining.py         --train_batch_size=18     --learning_rate=2.0e-3     --opt_lamb_beta_1=0.86     --opt_lamb_beta_2=0.975     --warmup_proportion=0.0     --max_steps=750     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data_phase2     --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.712     --max_samples_termination=7500000     --eval_iter_start_samples=3000000 --eval_iter_samples=500000     --eval_batch_size=16 --eval_dir=/workspace/evaldata     --cache_eval_data     --output_dir=/results     --fp16 --fused_gelu_bias --dense_seq_output --fused_mha      --allreduce_post_accumulation --allreduce_post_accumulation_fp16     --gradient_accumulation_steps=1     --log_freq=1     --local_rank=5     --bert_config_path=/workspace/phase1/bert_config.j
168: Run vars: id 394913 gpus 16 mparams
165: son --seed=18263'
169: Run vars: id 394913 gpus 16 mparams
165: ++ ./bind.sh --cpu=exclusive --ib=single --cluster=circe -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=5 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
171: Run vars: id 394913 gpus 16 mparams
167: + eval '    ./bind.sh --cpu=exclusive --ib=single --cluster=circe --     python -u /workspace/bert/run_pretraining.py         --train_batch_size=18     --learning_rate=2.0e-3     --opt_lamb_beta_1=0.86     --opt_lamb_beta_2=0.975     --warmup_proportion=0.0     --max_steps=750     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data_phase2     --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.712     --max_samples_termination=7500000     --eval_iter_start_samples=3000000 --eval_iter_samples=500000     --eval_batch_size=16 --eval_dir=/workspace/evaldata     --cache_eval_data     --output_dir=/results     --fp16 --fused_gelu_bias --dense_seq_output --fused_mha      --allreduce_post_accumulation --allreduce_post_accumulation_fp16     --gradient_accumulation_steps=1     --log_freq=1     --local_rank=7     --bert_config_path=/workspace/phase1/bert_config.j
172: Run vars: id 394913 gpus 16 mparams
 54: ++ ./bind.sh --cpu=exclusive --ib=single --cluster=circe -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=6 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
 92: STARTING TIMING RUN AT 2020-06-25 10:59:00 AM
167: son --seed=18263'
166: STARTING TIMING RUN AT 2020-06-25 10:59:00 AM
167: ++ ./bind.sh --cpu=exclusive --ib=single --cluster=circe -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=7 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
160: STARTING TIMING RUN AT 2020-06-25 10:59:00 AM
 61: + eval '    ./bind.sh --cpu=exclusive --ib=single --cluster=circe --     python -u /workspace/bert/run_pretraining.py         --train_batch_size=18     --learning_rate=2.0e-3     --opt_lamb_beta_1=0.86     --opt_lamb_beta_2=0.975     --warmup_proportion=0.0     --max_steps=750     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data_phase2     --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.712     --max_samples_termination=7500000     --eval_iter_start_samples=3000000 --eval_iter_samples=500000     --eval_batch_size=16 --eval_dir=/workspace/evaldata     --cache_eval_data     --output_dir=/results     --fp16 --fused_gelu_bias --dense_seq_output --fused_mha      --allreduce_post_accumulation --allreduce_post_accumulation_fp16     --gradient_accumulation_steps=1     --log_freq=1     --local_rank=13     --bert_config_path=/workspace/phase1/bert_config.
 91: STARTING TIMING RUN AT 2020-06-25 10:59:00 AM
 61: json --seed=18263'
162: Run vars: id 394913 gpus 16 mparams
164: + eval '    ./bind.sh --cpu=exclusive --ib=single --cluster=circe --     python -u /workspace/bert/run_pretraining.py         --train_batch_size=18     --learning_rate=2.0e-3     --opt_lamb_beta_1=0.86     --opt_lamb_beta_2=0.975     --warmup_proportion=0.0     --max_steps=750     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data_phase2     --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.712     --max_samples_termination=7500000     --eval_iter_start_samples=3000000 --eval_iter_samples=500000     --eval_batch_size=16 --eval_dir=/workspace/evaldata     --cache_eval_data     --output_dir=/results     --fp16 --fused_gelu_bias --dense_seq_output --fused_mha      --allreduce_post_accumulation --allreduce_post_accumulation_fp16     --gradient_accumulation_steps=1     --log_freq=1     --local_rank=4     --bert_config_path=/workspace/phase1/bert_config.j
 82: STARTING TIMING RUN AT 2020-06-25 10:59:00 AM
164: son --seed=18263'
 95: STARTING TIMING RUN AT 2020-06-25 10:59:00 AM
 80: + eval '    ./bind.sh --cpu=exclusive --ib=single --cluster=circe --     python -u /workspace/bert/run_pretraining.py         --train_batch_size=18     --learning_rate=2.0e-3     --opt_lamb_beta_1=0.86     --opt_lamb_beta_2=0.975     --warmup_proportion=0.0     --max_steps=750     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data_phase2     --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.712     --max_samples_termination=7500000     --eval_iter_start_samples=3000000 --eval_iter_samples=500000     --eval_batch_size=16 --eval_dir=/workspace/evaldata     --cache_eval_data     --output_dir=/results     --fp16 --fused_gelu_bias --dense_seq_output --fused_mha      --allreduce_post_accumulation --allreduce_post_accumulation_fp16     --gradient_accumulation_steps=1     --log_freq=1     --local_rank=0     --bert_config_path=/workspace/phase1/bert_config.j
461: Run vars: id 394913 gpus 16 mparams
 80: son --seed=18263'
460: Run vars: id 394913 gpus 16 mparams
 80: ++ ./bind.sh --cpu=exclusive --ib=single --cluster=circe -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=0 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
457: STARTING TIMING RUN AT 2020-06-25 10:59:00 AM
456: ++ ./bind.sh --cpu=exclusive --ib=single --cluster=circe -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=8 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
454: Run vars: id 394913 gpus 16 mparams
 61: ++ ./bind.sh --cpu=exclusive --ib=single --cluster=circe -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=13 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
173: STARTING TIMING RUN AT 2020-06-25 10:59:00 AM
 52: + eval '    ./bind.sh --cpu=exclusive --ib=single --cluster=circe --     python -u /workspace/bert/run_pretraining.py         --train_batch_size=18     --learning_rate=2.0e-3     --opt_lamb_beta_1=0.86     --opt_lamb_beta_2=0.975     --warmup_proportion=0.0     --max_steps=750     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data_phase2     --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.712     --max_samples_termination=7500000     --eval_iter_start_samples=3000000 --eval_iter_samples=500000     --eval_batch_size=16 --eval_dir=/workspace/evaldata     --cache_eval_data     --output_dir=/results     --fp16 --fused_gelu_bias --dense_seq_output --fused_mha      --allreduce_post_accumulation --allreduce_post_accumulation_fp16     --gradient_accumulation_steps=1     --log_freq=1     --local_rank=4     --bert_config_path=/workspace/phase1/bert_config.j
458: STARTING TIMING RUN AT 2020-06-25 10:59:00 AM
 52: son --seed=18263'
453: STARTING TIMING RUN AT 2020-06-25 10:59:00 AM
448: + eval '    ./bind.sh --cpu=exclusive --ib=single --cluster=circe --     python -u /workspace/bert/run_pretraining.py         --train_batch_size=18     --learning_rate=2.0e-3     --opt_lamb_beta_1=0.86     --opt_lamb_beta_2=0.975     --warmup_proportion=0.0     --max_steps=750     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data_phase2     --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.712     --max_samples_termination=7500000     --eval_iter_start_samples=3000000 --eval_iter_samples=500000     --eval_batch_size=16 --eval_dir=/workspace/evaldata     --cache_eval_data     --output_dir=/results     --fp16 --fused_gelu_bias --dense_seq_output --fused_mha      --allreduce_post_accumulation --allreduce_post_accumulation_fp16     --gradient_accumulation_steps=1     --log_freq=1     --local_rank=0     --bert_config_path=/workspace/phase1/bert_config.j
163: STARTING TIMING RUN AT 2020-06-25 10:59:00 AM
 62: + eval '    ./bind.sh --cpu=exclusive --ib=single --cluster=circe --     python -u /workspace/bert/run_pretraining.py         --train_batch_size=18     --learning_rate=2.0e-3     --opt_lamb_beta_1=0.86     --opt_lamb_beta_2=0.975     --warmup_proportion=0.0     --max_steps=750     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data_phase2     --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.712     --max_samples_termination=7500000     --eval_iter_start_samples=3000000 --eval_iter_samples=500000     --eval_batch_size=16 --eval_dir=/workspace/evaldata     --cache_eval_data     --output_dir=/results     --fp16 --fused_gelu_bias --dense_seq_output --fused_mha      --allreduce_post_accumulation --allreduce_post_accumulation_fp16     --gradient_accumulation_steps=1     --log_freq=1     --local_rank=14     --bert_config_path=/workspace/phase1/bert_config.
 51: STARTING TIMING RUN AT 2020-06-25 10:59:00 AM
448: son --seed=18263'
 55: STARTING TIMING RUN AT 2020-06-25 10:59:00 AM
 62: json --seed=18263'
459: Run vars: id 394913 gpus 16 mparams
 63: + eval '    ./bind.sh --cpu=exclusive --ib=single --cluster=circe --     python -u /workspace/bert/run_pretraining.py         --train_batch_size=18     --learning_rate=2.0e-3     --opt_lamb_beta_1=0.86     --opt_lamb_beta_2=0.975     --warmup_proportion=0.0     --max_steps=750     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data_phase2     --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.712     --max_samples_termination=7500000     --eval_iter_start_samples=3000000 --eval_iter_samples=500000     --eval_batch_size=16 --eval_dir=/workspace/evaldata     --cache_eval_data     --output_dir=/results     --fp16 --fused_gelu_bias --dense_seq_output --fused_mha      --allreduce_post_accumulation --allreduce_post_accumulation_fp16     --gradient_accumulation_steps=1     --log_freq=1     --local_rank=15     --bert_config_path=/workspace/phase1/bert_config.
170: STARTING TIMING RUN AT 2020-06-25 10:59:00 AM
 63: json --seed=18263'
462: STARTING TIMING RUN AT 2020-06-25 10:59:00 AM
 53: + eval '    ./bind.sh --cpu=exclusive --ib=single --cluster=circe --     python -u /workspace/bert/run_pretraining.py         --train_batch_size=18     --learning_rate=2.0e-3     --opt_lamb_beta_1=0.86     --opt_lamb_beta_2=0.975     --warmup_proportion=0.0     --max_steps=750     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data_phase2     --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.712     --max_samples_termination=7500000     --eval_iter_start_samples=3000000 --eval_iter_samples=500000     --eval_batch_size=16 --eval_dir=/workspace/evaldata     --cache_eval_data     --output_dir=/results     --fp16 --fused_gelu_bias --dense_seq_output --fused_mha      --allreduce_post_accumulation --allreduce_post_accumulation_fp16     --gradient_accumulation_steps=1     --log_freq=1     --local_rank=5     --bert_config_path=/workspace/phase1/bert_config.j
451: STARTING TIMING RUN AT 2020-06-25 10:59:00 AM
 53: son --seed=18263'
 58: STARTING TIMING RUN AT 2020-06-25 10:59:00 AM
 52: ++ ./bind.sh --cpu=exclusive --ib=single --cluster=circe -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=4 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
 56: STARTING TIMING RUN AT 2020-06-25 10:59:00 AM
 62: ++ ./bind.sh --cpu=exclusive --ib=single --cluster=circe -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=14 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
174: STARTING TIMING RUN AT 2020-06-25 10:59:00 AM
 63: ++ ./bind.sh --cpu=exclusive --ib=single --cluster=circe -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=15 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
455: STARTING TIMING RUN AT 2020-06-25 10:59:00 AM
 53: ++ ./bind.sh --cpu=exclusive --ib=single --cluster=circe -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=5 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
463: STARTING TIMING RUN AT 2020-06-25 10:59:00 AM
 90: + eval '    ./bind.sh --cpu=exclusive --ib=single --cluster=circe --     python -u /workspace/bert/run_pretraining.py         --train_batch_size=18     --learning_rate=2.0e-3     --opt_lamb_beta_1=0.86     --opt_lamb_beta_2=0.975     --warmup_proportion=0.0     --max_steps=750     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data_phase2     --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.712     --max_samples_termination=7500000     --eval_iter_start_samples=3000000 --eval_iter_samples=500000     --eval_batch_size=16 --eval_dir=/workspace/evaldata     --cache_eval_data     --output_dir=/results     --fp16 --fused_gelu_bias --dense_seq_output --fused_mha      --allreduce_post_accumulation --allreduce_post_accumulation_fp16     --gradient_accumulation_steps=1     --log_freq=1     --local_rank=10     --bert_config_path=/workspace/phase1/bert_config.
460: STARTING TIMING RUN AT 2020-06-25 10:59:00 AM
229: slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
168: STARTING TIMING RUN AT 2020-06-25 10:59:00 AM
 60: + eval '    ./bind.sh --cpu=exclusive --ib=single --cluster=circe --     python -u /workspace/bert/run_pretraining.py         --train_batch_size=18     --learning_rate=2.0e-3     --opt_lamb_beta_1=0.86     --opt_lamb_beta_2=0.975     --warmup_proportion=0.0     --max_steps=750     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data_phase2     --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.712     --max_samples_termination=7500000     --eval_iter_start_samples=3000000 --eval_iter_samples=500000     --eval_batch_size=16 --eval_dir=/workspace/evaldata     --cache_eval_data     --output_dir=/results     --fp16 --fused_gelu_bias --dense_seq_output --fused_mha      --allreduce_post_accumulation --allreduce_post_accumulation_fp16     --gradient_accumulation_steps=1     --log_freq=1     --local_rank=12     --bert_config_path=/workspace/phase1/bert_config.
452: STARTING TIMING RUN AT 2020-06-25 10:59:00 AM
239: slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
169: STARTING TIMING RUN AT 2020-06-25 10:59:00 AM
 60: json --seed=18263'
324: Run vars: id 394913 gpus 16 mparams
227: slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
322: Run vars: id 394913 gpus 16 mparams
235: slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
330: Run vars: id 394913 gpus 16 mparams
 90: json --seed=18263'
 57: STARTING TIMING RUN AT 2020-06-25 10:59:00 AM
237: slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
331: Run vars: id 394913 gpus 16 mparams
238: slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
324: STARTING TIMING RUN AT 2020-06-25 10:59:00 AM
226: slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
171: STARTING TIMING RUN AT 2020-06-25 10:59:00 AM
228: slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
172: STARTING TIMING RUN AT 2020-06-25 10:59:00 AM
236: slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
461: STARTING TIMING RUN AT 2020-06-25 10:59:00 AM
233: slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
229: Run vars: id 394913 gpus 16 mparams
232: slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
239: Run vars: id 394913 gpus 16 mparams
 93: + eval '    ./bind.sh --cpu=exclusive --ib=single --cluster=circe --     python -u /workspace/bert/run_pretraining.py         --train_batch_size=18     --learning_rate=2.0e-3     --opt_lamb_beta_1=0.86     --opt_lamb_beta_2=0.975     --warmup_proportion=0.0     --max_steps=750     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data_phase2     --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.712     --max_samples_termination=7500000     --eval_iter_start_samples=3000000 --eval_iter_samples=500000     --eval_batch_size=16 --eval_dir=/workspace/evaldata     --cache_eval_data     --output_dir=/results     --fp16 --fused_gelu_bias --dense_seq_output --fused_mha      --allreduce_post_accumulation --allreduce_post_accumulation_fp16     --gradient_accumulation_steps=1     --log_freq=1     --local_rank=13     --bert_config_path=/workspace/phase1/bert_config.
232: Run vars: id 394913 gpus 16 mparams
230: slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
224: Run vars: id 394913 gpus 16 mparams
231: slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
239: STARTING TIMING RUN AT 2020-06-25 10:59:00 AM
225: slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
229: STARTING TIMING RUN AT 2020-06-25 10:59:00 AM
224: slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
162: STARTING TIMING RUN AT 2020-06-25 10:59:00 AM
 93: json --seed=18263'
 40: Run vars: id 394913 gpus 16 mparams
 90: ++ ./bind.sh --cpu=exclusive --ib=single --cluster=circe -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=10 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
 38: Run vars: id 394913 gpus 16 mparams
238: + eval '    ./bind.sh --cpu=exclusive --ib=single --cluster=circe --     python -u /workspace/bert/run_pretraining.py         --train_batch_size=18     --learning_rate=2.0e-3     --opt_lamb_beta_1=0.86     --opt_lamb_beta_2=0.975     --warmup_proportion=0.0     --max_steps=750     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data_phase2     --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.712     --max_samples_termination=7500000     --eval_iter_start_samples=3000000 --eval_iter_samples=500000     --eval_batch_size=16 --eval_dir=/workspace/evaldata     --cache_eval_data     --output_dir=/results     --fp16 --fused_gelu_bias --dense_seq_output --fused_mha      --allreduce_post_accumulation --allreduce_post_accumulation_fp16     --gradient_accumulation_steps=1     --log_freq=1     --local_rank=14     --bert_config_path=/workspace/phase1/bert_config.
 47: Run vars: id 394913 gpus 16 mparams
238: json --seed=18263'
 40: STARTING TIMING RUN AT 2020-06-25 10:59:00 AM
 93: ++ ./bind.sh --cpu=exclusive --ib=single --cluster=circe -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=13 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
 46: Run vars: id 394913 gpus 16 mparams
 60: ++ ./bind.sh --cpu=exclusive --ib=single --cluster=circe -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=12 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
 41: Run vars: id 394913 gpus 16 mparams
 59: + eval '    ./bind.sh --cpu=exclusive --ib=single --cluster=circe --     python -u /workspace/bert/run_pretraining.py         --train_batch_size=18     --learning_rate=2.0e-3     --opt_lamb_beta_1=0.86     --opt_lamb_beta_2=0.975     --warmup_proportion=0.0     --max_steps=750     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data_phase2     --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.712     --max_samples_termination=7500000     --eval_iter_start_samples=3000000 --eval_iter_samples=500000     --eval_batch_size=16 --eval_dir=/workspace/evaldata     --cache_eval_data     --output_dir=/results     --fp16 --fused_gelu_bias --dense_seq_output --fused_mha      --allreduce_post_accumulation --allreduce_post_accumulation_fp16     --gradient_accumulation_steps=1     --log_freq=1     --local_rank=11     --bert_config_path=/workspace/phase1/bert_config.
454: STARTING TIMING RUN AT 2020-06-25 10:59:00 AM
 59: json --seed=18263'
328: Run vars: id 394913 gpus 16 mparams
 87: + eval '    ./bind.sh --cpu=exclusive --ib=single --cluster=circe --     python -u /workspace/bert/run_pretraining.py         --train_batch_size=18     --learning_rate=2.0e-3     --opt_lamb_beta_1=0.86     --opt_lamb_beta_2=0.975     --warmup_proportion=0.0     --max_steps=750     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data_phase2     --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.712     --max_samples_termination=7500000     --eval_iter_start_samples=3000000 --eval_iter_samples=500000     --eval_batch_size=16 --eval_dir=/workspace/evaldata     --cache_eval_data     --output_dir=/results     --fp16 --fused_gelu_bias --dense_seq_output --fused_mha      --allreduce_post_accumulation --allreduce_post_accumulation_fp16     --gradient_accumulation_steps=1     --log_freq=1     --local_rank=7     --bert_config_path=/workspace/phase1/bert_config.j
332: Run vars: id 394913 gpus 16 mparams
 87: son --seed=18263'
322: STARTING TIMING RUN AT 2020-06-25 10:59:00 AM
 87: ++ ./bind.sh --cpu=exclusive --ib=single --cluster=circe -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=7 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
459: STARTING TIMING RUN AT 2020-06-25 10:59:00 AM
 89: + eval '    ./bind.sh --cpu=exclusive --ib=single --cluster=circe --     python -u /workspace/bert/run_pretraining.py         --train_batch_size=18     --learning_rate=2.0e-3     --opt_lamb_beta_1=0.86     --opt_lamb_beta_2=0.975     --warmup_proportion=0.0     --max_steps=750     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data_phase2     --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.712     --max_samples_termination=7500000     --eval_iter_start_samples=3000000 --eval_iter_samples=500000     --eval_batch_size=16 --eval_dir=/workspace/evaldata     --cache_eval_data     --output_dir=/results     --fp16 --fused_gelu_bias --dense_seq_output --fused_mha      --allreduce_post_accumulation --allreduce_post_accumulation_fp16     --gradient_accumulation_steps=1     --log_freq=1     --local_rank=9     --bert_config_path=/workspace/phase1/bert_config.j
330: STARTING TIMING RUN AT 2020-06-25 10:59:00 AM
 89: son --seed=18263'
321: Run vars: id 394913 gpus 16 mparams
 89: ++ ./bind.sh --cpu=exclusive --ib=single --cluster=circe -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=9 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
226: Run vars: id 394913 gpus 16 mparams
164: ++ ./bind.sh --cpu=exclusive --ib=single --cluster=circe -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=4 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
231: Run vars: id 394913 gpus 16 mparams
 92: + eval '    ./bind.sh --cpu=exclusive --ib=single --cluster=circe --     python -u /workspace/bert/run_pretraining.py         --train_batch_size=18     --learning_rate=2.0e-3     --opt_lamb_beta_1=0.86     --opt_lamb_beta_2=0.975     --warmup_proportion=0.0     --max_steps=750     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data_phase2     --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.712     --max_samples_termination=7500000     --eval_iter_start_samples=3000000 --eval_iter_samples=500000     --eval_batch_size=16 --eval_dir=/workspace/evaldata     --cache_eval_data     --output_dir=/results     --fp16 --fused_gelu_bias --dense_seq_output --fused_mha      --allreduce_post_accumulation --allreduce_post_accumulation_fp16     --gradient_accumulation_steps=1     --log_freq=1     --local_rank=12     --bert_config_path=/workspace/phase1/bert_config.
227: Run vars: id 394913 gpus 16 mparams
166: + eval '    ./bind.sh --cpu=exclusive --ib=single --cluster=circe --     python -u /workspace/bert/run_pretraining.py         --train_batch_size=18     --learning_rate=2.0e-3     --opt_lamb_beta_1=0.86     --opt_lamb_beta_2=0.975     --warmup_proportion=0.0     --max_steps=750     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data_phase2     --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.712     --max_samples_termination=7500000     --eval_iter_start_samples=3000000 --eval_iter_samples=500000     --eval_batch_size=16 --eval_dir=/workspace/evaldata     --cache_eval_data     --output_dir=/results     --fp16 --fused_gelu_bias --dense_seq_output --fused_mha      --allreduce_post_accumulation --allreduce_post_accumulation_fp16     --gradient_accumulation_steps=1     --log_freq=1     --local_rank=6     --bert_config_path=/workspace/phase1/bert_config.j
234: Run vars: id 394913 gpus 16 mparams
 92: json --seed=18263'
228: Run vars: id 394913 gpus 16 mparams
166: son --seed=18263'
233: Run vars: id 394913 gpus 16 mparams
160: + eval '    ./bind.sh --cpu=exclusive --ib=single --cluster=circe --     python -u /workspace/bert/run_pretraining.py         --train_batch_size=18     --learning_rate=2.0e-3     --opt_lamb_beta_1=0.86     --opt_lamb_beta_2=0.975     --warmup_proportion=0.0     --max_steps=750     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data_phase2     --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.712     --max_samples_termination=7500000     --eval_iter_start_samples=3000000 --eval_iter_samples=500000     --eval_batch_size=16 --eval_dir=/workspace/evaldata     --cache_eval_data     --output_dir=/results     --fp16 --fused_gelu_bias --dense_seq_output --fused_mha      --allreduce_post_accumulation --allreduce_post_accumulation_fp16     --gradient_accumulation_steps=1     --log_freq=1     --local_rank=0     --bert_config_path=/workspace/phase1/bert_config.j
232: STARTING TIMING RUN AT 2020-06-25 10:59:00 AM
160: son --seed=18263'
137: Run vars: id 394913 gpus 16 mparams
166: ++ ./bind.sh --cpu=exclusive --ib=single --cluster=circe -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=6 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
237: Run vars: id 394913 gpus 16 mparams
 91: + eval '    ./bind.sh --cpu=exclusive --ib=single --cluster=circe --     python -u /workspace/bert/run_pretraining.py         --train_batch_size=18     --learning_rate=2.0e-3     --opt_lamb_beta_1=0.86     --opt_lamb_beta_2=0.975     --warmup_proportion=0.0     --max_steps=750     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data_phase2     --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.712     --max_samples_termination=7500000     --eval_iter_start_samples=3000000 --eval_iter_samples=500000     --eval_batch_size=16 --eval_dir=/workspace/evaldata     --cache_eval_data     --output_dir=/results     --fp16 --fused_gelu_bias --dense_seq_output --fused_mha      --allreduce_post_accumulation --allreduce_post_accumulation_fp16     --gradient_accumulation_steps=1     --log_freq=1     --local_rank=11     --bert_config_path=/workspace/phase1/bert_config.
325: Run vars: id 394913 gpus 16 mparams
 91: json --seed=18263'
331: STARTING TIMING RUN AT 2020-06-25 10:59:00 AM
 92: ++ ./bind.sh --cpu=exclusive --ib=single --cluster=circe -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=12 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
 32: Run vars: id 394913 gpus 16 mparams
 91: ++ ./bind.sh --cpu=exclusive --ib=single --cluster=circe -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=11 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
 37: Run vars: id 394913 gpus 16 mparams
 82: + eval '    ./bind.sh --cpu=exclusive --ib=single --cluster=circe --     python -u /workspace/bert/run_pretraining.py         --train_batch_size=18     --learning_rate=2.0e-3     --opt_lamb_beta_1=0.86     --opt_lamb_beta_2=0.975     --warmup_proportion=0.0     --max_steps=750     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data_phase2     --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.712     --max_samples_termination=7500000     --eval_iter_start_samples=3000000 --eval_iter_samples=500000     --eval_batch_size=16 --eval_dir=/workspace/evaldata     --cache_eval_data     --output_dir=/results     --fp16 --fused_gelu_bias --dense_seq_output --fused_mha      --allreduce_post_accumulation --allreduce_post_accumulation_fp16     --gradient_accumulation_steps=1     --log_freq=1     --local_rank=2     --bert_config_path=/workspace/phase1/bert_config.j
 43: Run vars: id 394913 gpus 16 mparams
 82: son --seed=18263'
326: Run vars: id 394913 gpus 16 mparams
448: ++ ./bind.sh --cpu=exclusive --ib=single --cluster=circe -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=0 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
 33: Run vars: id 394913 gpus 16 mparams
 82: ++ ./bind.sh --cpu=exclusive --ib=single --cluster=circe -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=2 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
 39: Run vars: id 394913 gpus 16 mparams
 95: + eval '    ./bind.sh --cpu=exclusive --ib=single --cluster=circe --     python -u /workspace/bert/run_pretraining.py         --train_batch_size=18     --learning_rate=2.0e-3     --opt_lamb_beta_1=0.86     --opt_lamb_beta_2=0.975     --warmup_proportion=0.0     --max_steps=750     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data_phase2     --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.712     --max_samples_termination=7500000     --eval_iter_start_samples=3000000 --eval_iter_samples=500000     --eval_batch_size=16 --eval_dir=/workspace/evaldata     --cache_eval_data     --output_dir=/results     --fp16 --fused_gelu_bias --dense_seq_output --fused_mha      --allreduce_post_accumulation --allreduce_post_accumulation_fp16     --gradient_accumulation_steps=1     --log_freq=1     --local_rank=15     --bert_config_path=/workspace/phase1/bert_config.
 38: STARTING TIMING RUN AT 2020-06-25 10:59:00 AM
457: + eval '    ./bind.sh --cpu=exclusive --ib=single --cluster=circe --     python -u /workspace/bert/run_pretraining.py         --train_batch_size=18     --learning_rate=2.0e-3     --opt_lamb_beta_1=0.86     --opt_lamb_beta_2=0.975     --warmup_proportion=0.0     --max_steps=750     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data_phase2     --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.712     --max_samples_termination=7500000     --eval_iter_start_samples=3000000 --eval_iter_samples=500000     --eval_batch_size=16 --eval_dir=/workspace/evaldata     --cache_eval_data     --output_dir=/results     --fp16 --fused_gelu_bias --dense_seq_output --fused_mha      --allreduce_post_accumulation --allreduce_post_accumulation_fp16     --gradient_accumulation_steps=1     --log_freq=1     --local_rank=9     --bert_config_path=/workspace/phase1/bert_config.j
224: STARTING TIMING RUN AT 2020-06-25 10:59:00 AM
457: son --seed=18263'
 47: STARTING TIMING RUN AT 2020-06-25 10:59:00 AM
 95: json --seed=18263'
 41: STARTING TIMING RUN AT 2020-06-25 10:59:00 AM
 95: ++ ./bind.sh --cpu=exclusive --ib=single --cluster=circe -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=15 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
335: Run vars: id 394913 gpus 16 mparams
160: ++ ./bind.sh --cpu=exclusive --ib=single --cluster=circe -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=0 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
328: STARTING TIMING RUN AT 2020-06-25 10:59:00 AM
457: ++ ./bind.sh --cpu=exclusive --ib=single --cluster=circe -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=9 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
 34: Run vars: id 394913 gpus 16 mparams
173: + eval '    ./bind.sh --cpu=exclusive --ib=single --cluster=circe --     python -u /workspace/bert/run_pretraining.py         --train_batch_size=18     --learning_rate=2.0e-3     --opt_lamb_beta_1=0.86     --opt_lamb_beta_2=0.975     --warmup_proportion=0.0     --max_steps=750     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data_phase2     --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.712     --max_samples_termination=7500000     --eval_iter_start_samples=3000000 --eval_iter_samples=500000     --eval_batch_size=16 --eval_dir=/workspace/evaldata     --cache_eval_data     --output_dir=/results     --fp16 --fused_gelu_bias --dense_seq_output --fused_mha      --allreduce_post_accumulation --allreduce_post_accumulation_fp16     --gradient_accumulation_steps=1     --log_freq=1     --local_rank=13     --bert_config_path=/workspace/phase1/bert_config.
136: Run vars: id 394913 gpus 16 mparams
458: + eval '    ./bind.sh --cpu=exclusive --ib=single --cluster=circe --     python -u /workspace/bert/run_pretraining.py         --train_batch_size=18     --learning_rate=2.0e-3     --opt_lamb_beta_1=0.86     --opt_lamb_beta_2=0.975     --warmup_proportion=0.0     --max_steps=750     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data_phase2     --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.712     --max_samples_termination=7500000     --eval_iter_start_samples=3000000 --eval_iter_samples=500000     --eval_batch_size=16 --eval_dir=/workspace/evaldata     --cache_eval_data     --output_dir=/results     --fp16 --fused_gelu_bias --dense_seq_output --fused_mha      --allreduce_post_accumulation --allreduce_post_accumulation_fp16     --gradient_accumulation_steps=1     --log_freq=1     --local_rank=10     --bert_config_path=/workspace/phase1/bert_config.
332: STARTING TIMING RUN AT 2020-06-25 10:59:00 AM
173: json --seed=18263'
235: Run vars: id 394913 gpus 16 mparams
458: json --seed=18263'
236: Run vars: id 394913 gpus 16 mparams
 59: ++ ./bind.sh --cpu=exclusive --ib=single --cluster=circe -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=11 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
226: STARTING TIMING RUN AT 2020-06-25 10:59:00 AM
458: ++ ./bind.sh --cpu=exclusive --ib=single --cluster=circe -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=10 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
231: STARTING TIMING RUN AT 2020-06-25 10:59:00 AM
453: + eval '    ./bind.sh --cpu=exclusive --ib=single --cluster=circe --     python -u /workspace/bert/run_pretraining.py         --train_batch_size=18     --learning_rate=2.0e-3     --opt_lamb_beta_1=0.86     --opt_lamb_beta_2=0.975     --warmup_proportion=0.0     --max_steps=750     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data_phase2     --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.712     --max_samples_termination=7500000     --eval_iter_start_samples=3000000 --eval_iter_samples=500000     --eval_batch_size=16 --eval_dir=/workspace/evaldata     --cache_eval_data     --output_dir=/results     --fp16 --fused_gelu_bias --dense_seq_output --fused_mha      --allreduce_post_accumulation --allreduce_post_accumulation_fp16     --gradient_accumulation_steps=1     --log_freq=1     --local_rank=5     --bert_config_path=/workspace/phase1/bert_config.j
 45: Run vars: id 394913 gpus 16 mparams
173: ++ ./bind.sh --cpu=exclusive --ib=single --cluster=circe -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=13 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
 44: Run vars: id 394913 gpus 16 mparams
453: son --seed=18263'
 46: STARTING TIMING RUN AT 2020-06-25 10:59:00 AM
163: + eval '    ./bind.sh --cpu=exclusive --ib=single --cluster=circe --     python -u /workspace/bert/run_pretraining.py         --train_batch_size=18     --learning_rate=2.0e-3     --opt_lamb_beta_1=0.86     --opt_lamb_beta_2=0.975     --warmup_proportion=0.0     --max_steps=750     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data_phase2     --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.712     --max_samples_termination=7500000     --eval_iter_start_samples=3000000 --eval_iter_samples=500000     --eval_batch_size=16 --eval_dir=/workspace/evaldata     --cache_eval_data     --output_dir=/results     --fp16 --fused_gelu_bias --dense_seq_output --fused_mha      --allreduce_post_accumulation --allreduce_post_accumulation_fp16     --gradient_accumulation_steps=1     --log_freq=1     --local_rank=3     --bert_config_path=/workspace/phase1/bert_config.j
 42: Run vars: id 394913 gpus 16 mparams
 51: + eval '    ./bind.sh --cpu=exclusive --ib=single --cluster=circe --     python -u /workspace/bert/run_pretraining.py         --train_batch_size=18     --learning_rate=2.0e-3     --opt_lamb_beta_1=0.86     --opt_lamb_beta_2=0.975     --warmup_proportion=0.0     --max_steps=750     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data_phase2     --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.712     --max_samples_termination=7500000     --eval_iter_start_samples=3000000 --eval_iter_samples=500000     --eval_batch_size=16 --eval_dir=/workspace/evaldata     --cache_eval_data     --output_dir=/results     --fp16 --fused_gelu_bias --dense_seq_output --fused_mha      --allreduce_post_accumulation --allreduce_post_accumulation_fp16     --gradient_accumulation_steps=1     --log_freq=1     --local_rank=3     --bert_config_path=/workspace/phase1/bert_config.j
234: STARTING TIMING RUN AT 2020-06-25 10:59:00 AM
163: son --seed=18263'
327: Run vars: id 394913 gpus 16 mparams
 51: son --seed=18263'
137: STARTING TIMING RUN AT 2020-06-25 10:59:00 AM
453: ++ ./bind.sh --cpu=exclusive --ib=single --cluster=circe -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=5 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
230: Run vars: id 394913 gpus 16 mparams
 55: + eval '    ./bind.sh --cpu=exclusive --ib=single --cluster=circe --     python -u /workspace/bert/run_pretraining.py         --train_batch_size=18     --learning_rate=2.0e-3     --opt_lamb_beta_1=0.86     --opt_lamb_beta_2=0.975     --warmup_proportion=0.0     --max_steps=750     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data_phase2     --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.712     --max_samples_termination=7500000     --eval_iter_start_samples=3000000 --eval_iter_samples=500000     --eval_batch_size=16 --eval_dir=/workspace/evaldata     --cache_eval_data     --output_dir=/results     --fp16 --fused_gelu_bias --dense_seq_output --fused_mha      --allreduce_post_accumulation --allreduce_post_accumulation_fp16     --gradient_accumulation_steps=1     --log_freq=1     --local_rank=7     --bert_config_path=/workspace/phase1/bert_config.j
323: Run vars: id 394913 gpus 16 mparams
163: ++ ./bind.sh --cpu=exclusive --ib=single --cluster=circe -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=3 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
225: Run vars: id 394913 gpus 16 mparams
 55: son --seed=18263'
333: Run vars: id 394913 gpus 16 mparams
462: + eval '    ./bind.sh --cpu=exclusive --ib=single --cluster=circe --     python -u /workspace/bert/run_pretraining.py         --train_batch_size=18     --learning_rate=2.0e-3     --opt_lamb_beta_1=0.86     --opt_lamb_beta_2=0.975     --warmup_proportion=0.0     --max_steps=750     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data_phase2     --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.712     --max_samples_termination=7500000     --eval_iter_start_samples=3000000 --eval_iter_samples=500000     --eval_batch_size=16 --eval_dir=/workspace/evaldata     --cache_eval_data     --output_dir=/results     --fp16 --fused_gelu_bias --dense_seq_output --fused_mha      --allreduce_post_accumulation --allreduce_post_accumulation_fp16     --gradient_accumulation_steps=1     --log_freq=1     --local_rank=14     --bert_config_path=/workspace/phase1/bert_config.
228: STARTING TIMING RUN AT 2020-06-25 10:59:00 AM
170: + eval '    ./bind.sh --cpu=exclusive --ib=single --cluster=circe --     python -u /workspace/bert/run_pretraining.py         --train_batch_size=18     --learning_rate=2.0e-3     --opt_lamb_beta_1=0.86     --opt_lamb_beta_2=0.975     --warmup_proportion=0.0     --max_steps=750     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data_phase2     --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.712     --max_samples_termination=7500000     --eval_iter_start_samples=3000000 --eval_iter_samples=500000     --eval_batch_size=16 --eval_dir=/workspace/evaldata     --cache_eval_data     --output_dir=/results     --fp16 --fused_gelu_bias --dense_seq_output --fused_mha      --allreduce_post_accumulation --allreduce_post_accumulation_fp16     --gradient_accumulation_steps=1     --log_freq=1     --local_rank=10     --bert_config_path=/workspace/phase1/bert_config.
321: STARTING TIMING RUN AT 2020-06-25 10:59:00 AM
 51: ++ ./bind.sh --cpu=exclusive --ib=single --cluster=circe -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=3 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
233: STARTING TIMING RUN AT 2020-06-25 10:59:00 AM
462: json --seed=18263'
329: Run vars: id 394913 gpus 16 mparams
170: json --seed=18263'
 35: Run vars: id 394913 gpus 16 mparams
 55: ++ ./bind.sh --cpu=exclusive --ib=single --cluster=circe -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=7 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
 37: STARTING TIMING RUN AT 2020-06-25 10:59:00 AM
462: ++ ./bind.sh --cpu=exclusive --ib=single --cluster=circe -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=14 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
 36: Run vars: id 394913 gpus 16 mparams
451: + eval '    ./bind.sh --cpu=exclusive --ib=single --cluster=circe --     python -u /workspace/bert/run_pretraining.py         --train_batch_size=18     --learning_rate=2.0e-3     --opt_lamb_beta_1=0.86     --opt_lamb_beta_2=0.975     --warmup_proportion=0.0     --max_steps=750     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data_phase2     --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.712     --max_samples_termination=7500000     --eval_iter_start_samples=3000000 --eval_iter_samples=500000     --eval_batch_size=16 --eval_dir=/workspace/evaldata     --cache_eval_data     --output_dir=/results     --fp16 --fused_gelu_bias --dense_seq_output --fused_mha      --allreduce_post_accumulation --allreduce_post_accumulation_fp16     --gradient_accumulation_steps=1     --log_freq=1     --local_rank=3     --bert_config_path=/workspace/phase1/bert_config.j
227: STARTING TIMING RUN AT 2020-06-25 10:59:00 AM
451: son --seed=18263'
334: Run vars: id 394913 gpus 16 mparams
 58: + eval '    ./bind.sh --cpu=exclusive --ib=single --cluster=circe --     python -u /workspace/bert/run_pretraining.py         --train_batch_size=18     --learning_rate=2.0e-3     --opt_lamb_beta_1=0.86     --opt_lamb_beta_2=0.975     --warmup_proportion=0.0     --max_steps=750     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data_phase2     --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.712     --max_samples_termination=7500000     --eval_iter_start_samples=3000000 --eval_iter_samples=500000     --eval_batch_size=16 --eval_dir=/workspace/evaldata     --cache_eval_data     --output_dir=/results     --fp16 --fused_gelu_bias --dense_seq_output --fused_mha      --allreduce_post_accumulation --allreduce_post_accumulation_fp16     --gradient_accumulation_steps=1     --log_freq=1     --local_rank=10     --bert_config_path=/workspace/phase1/bert_config.
325: STARTING TIMING RUN AT 2020-06-25 10:59:00 AM
 58: json --seed=18263'
320: Run vars: id 394913 gpus 16 mparams
170: ++ ./bind.sh --cpu=exclusive --ib=single --cluster=circe -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=10 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
 32: STARTING TIMING RUN AT 2020-06-25 10:59:00 AM
 58: ++ ./bind.sh --cpu=exclusive --ib=single --cluster=circe -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=10 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
508: Run vars: id 394913 gpus 16 mparams
174: + eval '    ./bind.sh --cpu=exclusive --ib=single --cluster=circe --     python -u /workspace/bert/run_pretraining.py         --train_batch_size=18     --learning_rate=2.0e-3     --opt_lamb_beta_1=0.86     --opt_lamb_beta_2=0.975     --warmup_proportion=0.0     --max_steps=750     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data_phase2     --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.712     --max_samples_termination=7500000     --eval_iter_start_samples=3000000 --eval_iter_samples=500000     --eval_batch_size=16 --eval_dir=/workspace/evaldata     --cache_eval_data     --output_dir=/results     --fp16 --fused_gelu_bias --dense_seq_output --fused_mha      --allreduce_post_accumulation --allreduce_post_accumulation_fp16     --gradient_accumulation_steps=1     --log_freq=1     --local_rank=14     --bert_config_path=/workspace/phase1/bert_config.
499: Run vars: id 394913 gpus 16 mparams
174: json --seed=18263'
496: Run vars: id 394913 gpus 16 mparams
451: ++ ./bind.sh --cpu=exclusive --ib=single --cluster=circe -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=3 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
501: Run vars: id 394913 gpus 16 mparams
455: + eval '    ./bind.sh --cpu=exclusive --ib=single --cluster=circe --     python -u /workspace/bert/run_pretraining.py         --train_batch_size=18     --learning_rate=2.0e-3     --opt_lamb_beta_1=0.86     --opt_lamb_beta_2=0.975     --warmup_proportion=0.0     --max_steps=750     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data_phase2     --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.712     --max_samples_termination=7500000     --eval_iter_start_samples=3000000 --eval_iter_samples=500000     --eval_batch_size=16 --eval_dir=/workspace/evaldata     --cache_eval_data     --output_dir=/results     --fp16 --fused_gelu_bias --dense_seq_output --fused_mha      --allreduce_post_accumulation --allreduce_post_accumulation_fp16     --gradient_accumulation_steps=1     --log_freq=1     --local_rank=7     --bert_config_path=/workspace/phase1/bert_config.j
505: Run vars: id 394913 gpus 16 mparams
 56: + eval '    ./bind.sh --cpu=exclusive --ib=single --cluster=circe --     python -u /workspace/bert/run_pretraining.py         --train_batch_size=18     --learning_rate=2.0e-3     --opt_lamb_beta_1=0.86     --opt_lamb_beta_2=0.975     --warmup_proportion=0.0     --max_steps=750     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data_phase2     --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.712     --max_samples_termination=7500000     --eval_iter_start_samples=3000000 --eval_iter_samples=500000     --eval_batch_size=16 --eval_dir=/workspace/evaldata     --cache_eval_data     --output_dir=/results     --fp16 --fused_gelu_bias --dense_seq_output --fused_mha      --allreduce_post_accumulation --allreduce_post_accumulation_fp16     --gradient_accumulation_steps=1     --log_freq=1     --local_rank=8     --bert_config_path=/workspace/phase1/bert_config.j
503: Run vars: id 394913 gpus 16 mparams
455: son --seed=18263'
500: Run vars: id 394913 gpus 16 mparams
 56: son --seed=18263'
 43: STARTING TIMING RUN AT 2020-06-25 10:59:00 AM
 56: ++ ./bind.sh --cpu=exclusive --ib=single --cluster=circe -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=8 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
502: Run vars: id 394913 gpus 16 mparams
463: + eval '    ./bind.sh --cpu=exclusive --ib=single --cluster=circe --     python -u /workspace/bert/run_pretraining.py         --train_batch_size=18     --learning_rate=2.0e-3     --opt_lamb_beta_1=0.86     --opt_lamb_beta_2=0.975     --warmup_proportion=0.0     --max_steps=750     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data_phase2     --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.712     --max_samples_termination=7500000     --eval_iter_start_samples=3000000 --eval_iter_samples=500000     --eval_batch_size=16 --eval_dir=/workspace/evaldata     --cache_eval_data     --output_dir=/results     --fp16 --fused_gelu_bias --dense_seq_output --fused_mha      --allreduce_post_accumulation --allreduce_post_accumulation_fp16     --gradient_accumulation_steps=1     --log_freq=1     --local_rank=15     --bert_config_path=/workspace/phase1/bert_config.
 39: STARTING TIMING RUN AT 2020-06-25 10:59:00 AM
463: json --seed=18263'
237: STARTING TIMING RUN AT 2020-06-25 10:59:00 AM
455: ++ ./bind.sh --cpu=exclusive --ib=single --cluster=circe -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=7 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
236: STARTING TIMING RUN AT 2020-06-25 10:59:00 AM
463: ++ ./bind.sh --cpu=exclusive --ib=single --cluster=circe -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=15 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
 33: STARTING TIMING RUN AT 2020-06-25 10:59:00 AM
295: slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
498: Run vars: id 394913 gpus 16 mparams
174: ++ ./bind.sh --cpu=exclusive --ib=single --cluster=circe -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=14 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
509: Run vars: id 394913 gpus 16 mparams
168: + eval '    ./bind.sh --cpu=exclusive --ib=single --cluster=circe --     python -u /workspace/bert/run_pretraining.py         --train_batch_size=18     --learning_rate=2.0e-3     --opt_lamb_beta_1=0.86     --opt_lamb_beta_2=0.975     --warmup_proportion=0.0     --max_steps=750     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data_phase2     --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.712     --max_samples_termination=7500000     --eval_iter_start_samples=3000000 --eval_iter_samples=500000     --eval_batch_size=16 --eval_dir=/workspace/evaldata     --cache_eval_data     --output_dir=/results     --fp16 --fused_gelu_bias --dense_seq_output --fused_mha      --allreduce_post_accumulation --allreduce_post_accumulation_fp16     --gradient_accumulation_steps=1     --log_freq=1     --local_rank=8     --bert_config_path=/workspace/phase1/bert_config.j
497: Run vars: id 394913 gpus 16 mparams
168: son --seed=18263'
508: STARTING TIMING RUN AT 2020-06-25 10:59:00 AM
460: + eval '    ./bind.sh --cpu=exclusive --ib=single --cluster=circe --     python -u /workspace/bert/run_pretraining.py         --train_batch_size=18     --learning_rate=2.0e-3     --opt_lamb_beta_1=0.86     --opt_lamb_beta_2=0.975     --warmup_proportion=0.0     --max_steps=750     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data_phase2     --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.712     --max_samples_termination=7500000     --eval_iter_start_samples=3000000 --eval_iter_samples=500000     --eval_batch_size=16 --eval_dir=/workspace/evaldata     --cache_eval_data     --output_dir=/results     --fp16 --fused_gelu_bias --dense_seq_output --fused_mha      --allreduce_post_accumulation --allreduce_post_accumulation_fp16     --gradient_accumulation_steps=1     --log_freq=1     --local_rank=12     --bert_config_path=/workspace/phase1/bert_config.
235: STARTING TIMING RUN AT 2020-06-25 10:59:00 AM
460: json --seed=18263'
335: STARTING TIMING RUN AT 2020-06-25 10:59:00 AM
460: ++ ./bind.sh --cpu=exclusive --ib=single --cluster=circe -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=12 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
 45: STARTING TIMING RUN AT 2020-06-25 10:59:00 AM
324: slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
 34: STARTING TIMING RUN AT 2020-06-25 10:59:00 AM
323: slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
499: STARTING TIMING RUN AT 2020-06-25 10:59:00 AM
330: slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
506: Run vars: id 394913 gpus 16 mparams
321: slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
496: STARTING TIMING RUN AT 2020-06-25 10:59:00 AM
328: slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
326: STARTING TIMING RUN AT 2020-06-25 10:59:00 AM
326: slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
327: STARTING TIMING RUN AT 2020-06-25 10:59:00 AM
325: slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
333: STARTING TIMING RUN AT 2020-06-25 10:59:00 AM
322: slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
128: Run vars: id 394913 gpus 16 mparams
335: slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
141: Run vars: id 394913 gpus 16 mparams
452: + eval '    ./bind.sh --cpu=exclusive --ib=single --cluster=circe --     python -u /workspace/bert/run_pretraining.py         --train_batch_size=18     --learning_rate=2.0e-3     --opt_lamb_beta_1=0.86     --opt_lamb_beta_2=0.975     --warmup_proportion=0.0     --max_steps=750     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data_phase2     --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.712     --max_samples_termination=7500000     --eval_iter_start_samples=3000000 --eval_iter_samples=500000     --eval_batch_size=16 --eval_dir=/workspace/evaldata     --cache_eval_data     --output_dir=/results     --fp16 --fused_gelu_bias --dense_seq_output --fused_mha      --allreduce_post_accumulation --allreduce_post_accumulation_fp16     --gradient_accumulation_steps=1     --log_freq=1     --local_rank=4     --bert_config_path=/workspace/phase1/bert_config.j
134: Run vars: id 394913 gpus 16 mparams
332: slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
138: Run vars: id 394913 gpus 16 mparams
168: ++ ./bind.sh --cpu=exclusive --ib=single --cluster=circe -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=8 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
129: Run vars: id 394913 gpus 16 mparams
452: son --seed=18263'
130: Run vars: id 394913 gpus 16 mparams
331: slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
140: Run vars: id 394913 gpus 16 mparams
334: slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
136: STARTING TIMING RUN AT 2020-06-25 10:59:00 AM
169: + eval '    ./bind.sh --cpu=exclusive --ib=single --cluster=circe --     python -u /workspace/bert/run_pretraining.py         --train_batch_size=18     --learning_rate=2.0e-3     --opt_lamb_beta_1=0.86     --opt_lamb_beta_2=0.975     --warmup_proportion=0.0     --max_steps=750     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data_phase2     --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.712     --max_samples_termination=7500000     --eval_iter_start_samples=3000000 --eval_iter_samples=500000     --eval_batch_size=16 --eval_dir=/workspace/evaldata     --cache_eval_data     --output_dir=/results     --fp16 --fused_gelu_bias --dense_seq_output --fused_mha      --allreduce_post_accumulation --allreduce_post_accumulation_fp16     --gradient_accumulation_steps=1     --log_freq=1     --local_rank=9     --bert_config_path=/workspace/phase1/bert_config.j
142: Run vars: id 394913 gpus 16 mparams
452: ++ ./bind.sh --cpu=exclusive --ib=single --cluster=circe -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=4 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
135: Run vars: id 394913 gpus 16 mparams
333: slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
323: STARTING TIMING RUN AT 2020-06-25 10:59:00 AM
169: son --seed=18263'
504: Run vars: id 394913 gpus 16 mparams
327: slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
511: Run vars: id 394913 gpus 16 mparams
329: slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
501: STARTING TIMING RUN AT 2020-06-25 10:59:00 AM
 57: + eval '    ./bind.sh --cpu=exclusive --ib=single --cluster=circe --     python -u /workspace/bert/run_pretraining.py         --train_batch_size=18     --learning_rate=2.0e-3     --opt_lamb_beta_1=0.86     --opt_lamb_beta_2=0.975     --warmup_proportion=0.0     --max_steps=750     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data_phase2     --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.712     --max_samples_termination=7500000     --eval_iter_start_samples=3000000 --eval_iter_samples=500000     --eval_batch_size=16 --eval_dir=/workspace/evaldata     --cache_eval_data     --output_dir=/results     --fp16 --fused_gelu_bias --dense_seq_output --fused_mha      --allreduce_post_accumulation --allreduce_post_accumulation_fp16     --gradient_accumulation_steps=1     --log_freq=1     --local_rank=9     --bert_config_path=/workspace/phase1/bert_config.j
507: Run vars: id 394913 gpus 16 mparams
 57: son --seed=18263'
505: STARTING TIMING RUN AT 2020-06-25 10:59:00 AM
 57: ++ ./bind.sh --cpu=exclusive --ib=single --cluster=circe -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=9 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
141: STARTING TIMING RUN AT 2020-06-25 10:59:00 AM
324: + eval '    ./bind.sh --cpu=exclusive --ib=single --cluster=circe --     python -u /workspace/bert/run_pretraining.py         --train_batch_size=18     --learning_rate=2.0e-3     --opt_lamb_beta_1=0.86     --opt_lamb_beta_2=0.975     --warmup_proportion=0.0     --max_steps=750     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data_phase2     --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.712     --max_samples_termination=7500000     --eval_iter_start_samples=3000000 --eval_iter_samples=500000     --eval_batch_size=16 --eval_dir=/workspace/evaldata     --cache_eval_data     --output_dir=/results     --fp16 --fused_gelu_bias --dense_seq_output --fused_mha      --allreduce_post_accumulation --allreduce_post_accumulation_fp16     --gradient_accumulation_steps=1     --log_freq=1     --local_rank=4     --bert_config_path=/workspace/phase1/bert_config.j
230: STARTING TIMING RUN AT 2020-06-25 10:59:00 AM
324: son --seed=18263'
510: Run vars: id 394913 gpus 16 mparams
169: ++ ./bind.sh --cpu=exclusive --ib=single --cluster=circe -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=9 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
503: STARTING TIMING RUN AT 2020-06-25 10:59:00 AM
171: + eval '    ./bind.sh --cpu=exclusive --ib=single --cluster=circe --     python -u /workspace/bert/run_pretraining.py         --train_batch_size=18     --learning_rate=2.0e-3     --opt_lamb_beta_1=0.86     --opt_lamb_beta_2=0.975     --warmup_proportion=0.0     --max_steps=750     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data_phase2     --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.712     --max_samples_termination=7500000     --eval_iter_start_samples=3000000 --eval_iter_samples=500000     --eval_batch_size=16 --eval_dir=/workspace/evaldata     --cache_eval_data     --output_dir=/results     --fp16 --fused_gelu_bias --dense_seq_output --fused_mha      --allreduce_post_accumulation --allreduce_post_accumulation_fp16     --gradient_accumulation_steps=1     --log_freq=1     --local_rank=11     --bert_config_path=/workspace/phase1/bert_config.
225: STARTING TIMING RUN AT 2020-06-25 10:59:00 AM
171: json --seed=18263'
 44: STARTING TIMING RUN AT 2020-06-25 10:59:00 AM
171: ++ ./bind.sh --cpu=exclusive --ib=single --cluster=circe -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=11 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
329: STARTING TIMING RUN AT 2020-06-25 10:59:00 AM
172: + eval '    ./bind.sh --cpu=exclusive --ib=single --cluster=circe --     python -u /workspace/bert/run_pretraining.py         --train_batch_size=18     --learning_rate=2.0e-3     --opt_lamb_beta_1=0.86     --opt_lamb_beta_2=0.975     --warmup_proportion=0.0     --max_steps=750     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data_phase2     --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.712     --max_samples_termination=7500000     --eval_iter_start_samples=3000000 --eval_iter_samples=500000     --eval_batch_size=16 --eval_dir=/workspace/evaldata     --cache_eval_data     --output_dir=/results     --fp16 --fused_gelu_bias --dense_seq_output --fused_mha      --allreduce_post_accumulation --allreduce_post_accumulation_fp16     --gradient_accumulation_steps=1     --log_freq=1     --local_rank=12     --bert_config_path=/workspace/phase1/bert_config.
128: STARTING TIMING RUN AT 2020-06-25 10:59:00 AM
172: json --seed=18263'
 35: STARTING TIMING RUN AT 2020-06-25 10:59:00 AM
461: + eval '    ./bind.sh --cpu=exclusive --ib=single --cluster=circe --     python -u /workspace/bert/run_pretraining.py         --train_batch_size=18     --learning_rate=2.0e-3     --opt_lamb_beta_1=0.86     --opt_lamb_beta_2=0.975     --warmup_proportion=0.0     --max_steps=750     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data_phase2     --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.712     --max_samples_termination=7500000     --eval_iter_start_samples=3000000 --eval_iter_samples=500000     --eval_batch_size=16 --eval_dir=/workspace/evaldata     --cache_eval_data     --output_dir=/results     --fp16 --fused_gelu_bias --dense_seq_output --fused_mha      --allreduce_post_accumulation --allreduce_post_accumulation_fp16     --gradient_accumulation_steps=1     --log_freq=1     --local_rank=13     --bert_config_path=/workspace/phase1/bert_config.
500: STARTING TIMING RUN AT 2020-06-25 10:59:00 AM
461: json --seed=18263'
320: STARTING TIMING RUN AT 2020-06-25 10:59:00 AM
461: ++ ./bind.sh --cpu=exclusive --ib=single --cluster=circe -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=13 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
 36: STARTING TIMING RUN AT 2020-06-25 10:59:00 AM
238: ++ ./bind.sh --cpu=exclusive --ib=single --cluster=circe -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=14 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
 42: STARTING TIMING RUN AT 2020-06-25 10:59:00 AM
239: + eval '    ./bind.sh --cpu=exclusive --ib=single --cluster=circe --     python -u /workspace/bert/run_pretraining.py         --train_batch_size=18     --learning_rate=2.0e-3     --opt_lamb_beta_1=0.86     --opt_lamb_beta_2=0.975     --warmup_proportion=0.0     --max_steps=750     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data_phase2     --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.712     --max_samples_termination=7500000     --eval_iter_start_samples=3000000 --eval_iter_samples=500000     --eval_batch_size=16 --eval_dir=/workspace/evaldata     --cache_eval_data     --output_dir=/results     --fp16 --fused_gelu_bias --dense_seq_output --fused_mha      --allreduce_post_accumulation --allreduce_post_accumulation_fp16     --gradient_accumulation_steps=1     --log_freq=1     --local_rank=15     --bert_config_path=/workspace/phase1/bert_config.
502: STARTING TIMING RUN AT 2020-06-25 10:59:00 AM
239: json --seed=18263'
334: STARTING TIMING RUN AT 2020-06-25 10:59:00 AM
229: + eval '    ./bind.sh --cpu=exclusive --ib=single --cluster=circe --     python -u /workspace/bert/run_pretraining.py         --train_batch_size=18     --learning_rate=2.0e-3     --opt_lamb_beta_1=0.86     --opt_lamb_beta_2=0.975     --warmup_proportion=0.0     --max_steps=750     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data_phase2     --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.712     --max_samples_termination=7500000     --eval_iter_start_samples=3000000 --eval_iter_samples=500000     --eval_batch_size=16 --eval_dir=/workspace/evaldata     --cache_eval_data     --output_dir=/results     --fp16 --fused_gelu_bias --dense_seq_output --fused_mha      --allreduce_post_accumulation --allreduce_post_accumulation_fp16     --gradient_accumulation_steps=1     --log_freq=1     --local_rank=5     --bert_config_path=/workspace/phase1/bert_config.j
138: STARTING TIMING RUN AT 2020-06-25 10:59:00 AM
229: son --seed=18263'
143: Run vars: id 394913 gpus 16 mparams
239: ++ ./bind.sh --cpu=exclusive --ib=single --cluster=circe -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=15 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
498: STARTING TIMING RUN AT 2020-06-25 10:59:00 AM
172: ++ ./bind.sh --cpu=exclusive --ib=single --cluster=circe -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=12 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
134: STARTING TIMING RUN AT 2020-06-25 10:59:00 AM
162: + eval '    ./bind.sh --cpu=exclusive --ib=single --cluster=circe --     python -u /workspace/bert/run_pretraining.py         --train_batch_size=18     --learning_rate=2.0e-3     --opt_lamb_beta_1=0.86     --opt_lamb_beta_2=0.975     --warmup_proportion=0.0     --max_steps=750     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data_phase2     --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.712     --max_samples_termination=7500000     --eval_iter_start_samples=3000000 --eval_iter_samples=500000     --eval_batch_size=16 --eval_dir=/workspace/evaldata     --cache_eval_data     --output_dir=/results     --fp16 --fused_gelu_bias --dense_seq_output --fused_mha      --allreduce_post_accumulation --allreduce_post_accumulation_fp16     --gradient_accumulation_steps=1     --log_freq=1     --local_rank=2     --bert_config_path=/workspace/phase1/bert_config.j
133: Run vars: id 394913 gpus 16 mparams
162: son --seed=18263'
509: STARTING TIMING RUN AT 2020-06-25 10:59:00 AM
 37: slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
139: Run vars: id 394913 gpus 16 mparams
 33: slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
497: STARTING TIMING RUN AT 2020-06-25 10:59:00 AM
 36: slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
130: STARTING TIMING RUN AT 2020-06-25 10:59:00 AM
 39: slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
129: STARTING TIMING RUN AT 2020-06-25 10:59:00 AM
 40: slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
504: STARTING TIMING RUN AT 2020-06-25 10:59:00 AM
 38: slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
132: Run vars: id 394913 gpus 16 mparams
 35: slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
135: STARTING TIMING RUN AT 2020-06-25 10:59:00 AM
 34: slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
140: STARTING TIMING RUN AT 2020-06-25 10:59:00 AM
 41: slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
131: Run vars: id 394913 gpus 16 mparams
 44: slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
142: STARTING TIMING RUN AT 2020-06-25 10:59:00 AM
 46: slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
506: STARTING TIMING RUN AT 2020-06-25 10:59:00 AM
 43: slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
511: STARTING TIMING RUN AT 2020-06-25 10:59:00 AM
 47: slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
510: STARTING TIMING RUN AT 2020-06-25 10:59:00 AM
 45: slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
507: STARTING TIMING RUN AT 2020-06-25 10:59:00 AM
 42: slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
143: STARTING TIMING RUN AT 2020-06-25 10:59:00 AM
 40: + eval '    ./bind.sh --cpu=exclusive --ib=single --cluster=circe --     python -u /workspace/bert/run_pretraining.py         --train_batch_size=18     --learning_rate=2.0e-3     --opt_lamb_beta_1=0.86     --opt_lamb_beta_2=0.975     --warmup_proportion=0.0     --max_steps=750     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data_phase2     --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.712     --max_samples_termination=7500000     --eval_iter_start_samples=3000000 --eval_iter_samples=500000     --eval_batch_size=16 --eval_dir=/workspace/evaldata     --cache_eval_data     --output_dir=/results     --fp16 --fused_gelu_bias --dense_seq_output --fused_mha      --allreduce_post_accumulation --allreduce_post_accumulation_fp16     --gradient_accumulation_steps=1     --log_freq=1     --local_rank=8     --bert_config_path=/workspace/phase1/bert_config.j
133: STARTING TIMING RUN AT 2020-06-25 10:59:00 AM
 40: son --seed=18263'
139: STARTING TIMING RUN AT 2020-06-25 10:59:00 AM
324: ++ ./bind.sh --cpu=exclusive --ib=single --cluster=circe -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=4 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
257: Run vars: id 394913 gpus 16 mparams
454: + eval '    ./bind.sh --cpu=exclusive --ib=single --cluster=circe --     python -u /workspace/bert/run_pretraining.py         --train_batch_size=18     --learning_rate=2.0e-3     --opt_lamb_beta_1=0.86     --opt_lamb_beta_2=0.975     --warmup_proportion=0.0     --max_steps=750     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data_phase2     --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.712     --max_samples_termination=7500000     --eval_iter_start_samples=3000000 --eval_iter_samples=500000     --eval_batch_size=16 --eval_dir=/workspace/evaldata     --cache_eval_data     --output_dir=/results     --fp16 --fused_gelu_bias --dense_seq_output --fused_mha      --allreduce_post_accumulation --allreduce_post_accumulation_fp16     --gradient_accumulation_steps=1     --log_freq=1     --local_rank=6     --bert_config_path=/workspace/phase1/bert_config.j
263: Run vars: id 394913 gpus 16 mparams
454: son --seed=18263'
270: Run vars: id 394913 gpus 16 mparams
322: + eval '    ./bind.sh --cpu=exclusive --ib=single --cluster=circe --     python -u /workspace/bert/run_pretraining.py         --train_batch_size=18     --learning_rate=2.0e-3     --opt_lamb_beta_1=0.86     --opt_lamb_beta_2=0.975     --warmup_proportion=0.0     --max_steps=750     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data_phase2     --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.712     --max_samples_termination=7500000     --eval_iter_start_samples=3000000 --eval_iter_samples=500000     --eval_batch_size=16 --eval_dir=/workspace/evaldata     --cache_eval_data     --output_dir=/results     --fp16 --fused_gelu_bias --dense_seq_output --fused_mha      --allreduce_post_accumulation --allreduce_post_accumulation_fp16     --gradient_accumulation_steps=1     --log_freq=1     --local_rank=2     --bert_config_path=/workspace/phase1/bert_config.j
257: STARTING TIMING RUN AT 2020-06-25 10:59:00 AM
459: + eval '    ./bind.sh --cpu=exclusive --ib=single --cluster=circe --     python -u /workspace/bert/run_pretraining.py         --train_batch_size=18     --learning_rate=2.0e-3     --opt_lamb_beta_1=0.86     --opt_lamb_beta_2=0.975     --warmup_proportion=0.0     --max_steps=750     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data_phase2     --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.712     --max_samples_termination=7500000     --eval_iter_start_samples=3000000 --eval_iter_samples=500000     --eval_batch_size=16 --eval_dir=/workspace/evaldata     --cache_eval_data     --output_dir=/results     --fp16 --fused_gelu_bias --dense_seq_output --fused_mha      --allreduce_post_accumulation --allreduce_post_accumulation_fp16     --gradient_accumulation_steps=1     --log_freq=1     --local_rank=11     --bert_config_path=/workspace/phase1/bert_config.
132: STARTING TIMING RUN AT 2020-06-25 10:59:00 AM
322: son --seed=18263'
259: Run vars: id 394913 gpus 16 mparams
459: json --seed=18263'
263: STARTING TIMING RUN AT 2020-06-25 10:59:00 AM
454: ++ ./bind.sh --cpu=exclusive --ib=single --cluster=circe -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=6 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
256: Run vars: id 394913 gpus 16 mparams
322: ++ ./bind.sh --cpu=exclusive --ib=single --cluster=circe -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=2 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
261: Run vars: id 394913 gpus 16 mparams
330: + eval '    ./bind.sh --cpu=exclusive --ib=single --cluster=circe --     python -u /workspace/bert/run_pretraining.py         --train_batch_size=18     --learning_rate=2.0e-3     --opt_lamb_beta_1=0.86     --opt_lamb_beta_2=0.975     --warmup_proportion=0.0     --max_steps=750     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data_phase2     --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.712     --max_samples_termination=7500000     --eval_iter_start_samples=3000000 --eval_iter_samples=500000     --eval_batch_size=16 --eval_dir=/workspace/evaldata     --cache_eval_data     --output_dir=/results     --fp16 --fused_gelu_bias --dense_seq_output --fused_mha      --allreduce_post_accumulation --allreduce_post_accumulation_fp16     --gradient_accumulation_steps=1     --log_freq=1     --local_rank=10     --bert_config_path=/workspace/phase1/bert_config.
131: STARTING TIMING RUN AT 2020-06-25 10:59:00 AM
330: json --seed=18263'
330: ++ ./bind.sh --cpu=exclusive --ib=single --cluster=circe -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=10 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
202: Run vars: id 394913 gpus 16 mparams
229: ++ ./bind.sh --cpu=exclusive --ib=single --cluster=circe -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=5 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
194: Run vars: id 394913 gpus 16 mparams
232: + eval '    ./bind.sh --cpu=exclusive --ib=single --cluster=circe --     python -u /workspace/bert/run_pretraining.py         --train_batch_size=18     --learning_rate=2.0e-3     --opt_lamb_beta_1=0.86     --opt_lamb_beta_2=0.975     --warmup_proportion=0.0     --max_steps=750     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data_phase2     --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.712     --max_samples_termination=7500000     --eval_iter_start_samples=3000000 --eval_iter_samples=500000     --eval_batch_size=16 --eval_dir=/workspace/evaldata     --cache_eval_data     --output_dir=/results     --fp16 --fused_gelu_bias --dense_seq_output --fused_mha      --allreduce_post_accumulation --allreduce_post_accumulation_fp16     --gradient_accumulation_steps=1     --log_freq=1     --local_rank=8     --bert_config_path=/workspace/phase1/bert_config.j
196: Run vars: id 394913 gpus 16 mparams
232: son --seed=18263'
202: STARTING TIMING RUN AT 2020-06-25 10:59:00 AM
232: ++ ./bind.sh --cpu=exclusive --ib=single --cluster=circe -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=8 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
194: STARTING TIMING RUN AT 2020-06-25 10:59:00 AM
 40: ++ ./bind.sh --cpu=exclusive --ib=single --cluster=circe -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=8 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
260: Run vars: id 394913 gpus 16 mparams
331: + eval '    ./bind.sh --cpu=exclusive --ib=single --cluster=circe --     python -u /workspace/bert/run_pretraining.py         --train_batch_size=18     --learning_rate=2.0e-3     --opt_lamb_beta_1=0.86     --opt_lamb_beta_2=0.975     --warmup_proportion=0.0     --max_steps=750     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data_phase2     --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.712     --max_samples_termination=7500000     --eval_iter_start_samples=3000000 --eval_iter_samples=500000     --eval_batch_size=16 --eval_dir=/workspace/evaldata     --cache_eval_data     --output_dir=/results     --fp16 --fused_gelu_bias --dense_seq_output --fused_mha      --allreduce_post_accumulation --allreduce_post_accumulation_fp16     --gradient_accumulation_steps=1     --log_freq=1     --local_rank=11     --bert_config_path=/workspace/phase1/bert_config.
267: Run vars: id 394913 gpus 16 mparams
331: json --seed=18263'
265: Run vars: id 394913 gpus 16 mparams
331: ++ ./bind.sh --cpu=exclusive --ib=single --cluster=circe -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=11 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
262: Run vars: id 394913 gpus 16 mparams
 38: + eval '    ./bind.sh --cpu=exclusive --ib=single --cluster=circe --     python -u /workspace/bert/run_pretraining.py         --train_batch_size=18     --learning_rate=2.0e-3     --opt_lamb_beta_1=0.86     --opt_lamb_beta_2=0.975     --warmup_proportion=0.0     --max_steps=750     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data_phase2     --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.712     --max_samples_termination=7500000     --eval_iter_start_samples=3000000 --eval_iter_samples=500000     --eval_batch_size=16 --eval_dir=/workspace/evaldata     --cache_eval_data     --output_dir=/results     --fp16 --fused_gelu_bias --dense_seq_output --fused_mha      --allreduce_post_accumulation --allreduce_post_accumulation_fp16     --gradient_accumulation_steps=1     --log_freq=1     --local_rank=6     --bert_config_path=/workspace/phase1/bert_config.j
258: Run vars: id 394913 gpus 16 mparams
 38: son --seed=18263'
271: Run vars: id 394913 gpus 16 mparams
 38: ++ ./bind.sh --cpu=exclusive --ib=single --cluster=circe -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=6 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
270: STARTING TIMING RUN AT 2020-06-25 10:59:00 AM
224: + eval '    ./bind.sh --cpu=exclusive --ib=single --cluster=circe --     python -u /workspace/bert/run_pretraining.py         --train_batch_size=18     --learning_rate=2.0e-3     --opt_lamb_beta_1=0.86     --opt_lamb_beta_2=0.975     --warmup_proportion=0.0     --max_steps=750     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data_phase2     --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.712     --max_samples_termination=7500000     --eval_iter_start_samples=3000000 --eval_iter_samples=500000     --eval_batch_size=16 --eval_dir=/workspace/evaldata     --cache_eval_data     --output_dir=/results     --fp16 --fused_gelu_bias --dense_seq_output --fused_mha      --allreduce_post_accumulation --allreduce_post_accumulation_fp16     --gradient_accumulation_steps=1     --log_freq=1     --local_rank=0     --bert_config_path=/workspace/phase1/bert_config.j
264: Run vars: id 394913 gpus 16 mparams
224: son --seed=18263'
266: Run vars: id 394913 gpus 16 mparams
224: ++ ./bind.sh --cpu=exclusive --ib=single --cluster=circe -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=0 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
259: STARTING TIMING RUN AT 2020-06-25 10:59:00 AM
 47: + eval '    ./bind.sh --cpu=exclusive --ib=single --cluster=circe --     python -u /workspace/bert/run_pretraining.py         --train_batch_size=18     --learning_rate=2.0e-3     --opt_lamb_beta_1=0.86     --opt_lamb_beta_2=0.975     --warmup_proportion=0.0     --max_steps=750     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data_phase2     --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.712     --max_samples_termination=7500000     --eval_iter_start_samples=3000000 --eval_iter_samples=500000     --eval_batch_size=16 --eval_dir=/workspace/evaldata     --cache_eval_data     --output_dir=/results     --fp16 --fused_gelu_bias --dense_seq_output --fused_mha      --allreduce_post_accumulation --allreduce_post_accumulation_fp16     --gradient_accumulation_steps=1     --log_freq=1     --local_rank=15     --bert_config_path=/workspace/phase1/bert_config.
256: STARTING TIMING RUN AT 2020-06-25 10:59:00 AM
 47: json --seed=18263'
192: Run vars: id 394913 gpus 16 mparams
 47: ++ ./bind.sh --cpu=exclusive --ib=single --cluster=circe -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=15 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
193: Run vars: id 394913 gpus 16 mparams
328: + eval '    ./bind.sh --cpu=exclusive --ib=single --cluster=circe --     python -u /workspace/bert/run_pretraining.py         --train_batch_size=18     --learning_rate=2.0e-3     --opt_lamb_beta_1=0.86     --opt_lamb_beta_2=0.975     --warmup_proportion=0.0     --max_steps=750     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data_phase2     --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.712     --max_samples_termination=7500000     --eval_iter_start_samples=3000000 --eval_iter_samples=500000     --eval_batch_size=16 --eval_dir=/workspace/evaldata     --cache_eval_data     --output_dir=/results     --fp16 --fused_gelu_bias --dense_seq_output --fused_mha      --allreduce_post_accumulation --allreduce_post_accumulation_fp16     --gradient_accumulation_steps=1     --log_freq=1     --local_rank=8     --bert_config_path=/workspace/phase1/bert_config.j
203: Run vars: id 394913 gpus 16 mparams
 41: + eval '    ./bind.sh --cpu=exclusive --ib=single --cluster=circe --     python -u /workspace/bert/run_pretraining.py         --train_batch_size=18     --learning_rate=2.0e-3     --opt_lamb_beta_1=0.86     --opt_lamb_beta_2=0.975     --warmup_proportion=0.0     --max_steps=750     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data_phase2     --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.712     --max_samples_termination=7500000     --eval_iter_start_samples=3000000 --eval_iter_samples=500000     --eval_batch_size=16 --eval_dir=/workspace/evaldata     --cache_eval_data     --output_dir=/results     --fp16 --fused_gelu_bias --dense_seq_output --fused_mha      --allreduce_post_accumulation --allreduce_post_accumulation_fp16     --gradient_accumulation_steps=1     --log_freq=1     --local_rank=9     --bert_config_path=/workspace/phase1/bert_config.j
197: Run vars: id 394913 gpus 16 mparams
328: son --seed=18263'
196: STARTING TIMING RUN AT 2020-06-25 10:59:00 AM
 41: son --seed=18263'
207: Run vars: id 394913 gpus 16 mparams
328: ++ ./bind.sh --cpu=exclusive --ib=single --cluster=circe -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=8 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
198: Run vars: id 394913 gpus 16 mparams
 41: ++ ./bind.sh --cpu=exclusive --ib=single --cluster=circe -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=9 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
269: Run vars: id 394913 gpus 16 mparams
332: + eval '    ./bind.sh --cpu=exclusive --ib=single --cluster=circe --     python -u /workspace/bert/run_pretraining.py         --train_batch_size=18     --learning_rate=2.0e-3     --opt_lamb_beta_1=0.86     --opt_lamb_beta_2=0.975     --warmup_proportion=0.0     --max_steps=750     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data_phase2     --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.712     --max_samples_termination=7500000     --eval_iter_start_samples=3000000 --eval_iter_samples=500000     --eval_batch_size=16 --eval_dir=/workspace/evaldata     --cache_eval_data     --output_dir=/results     --fp16 --fused_gelu_bias --dense_seq_output --fused_mha      --allreduce_post_accumulation --allreduce_post_accumulation_fp16     --gradient_accumulation_steps=1     --log_freq=1     --local_rank=12     --bert_config_path=/workspace/phase1/bert_config.
260: STARTING TIMING RUN AT 2020-06-25 10:59:00 AM
332: json --seed=18263'
268: Run vars: id 394913 gpus 16 mparams
332: ++ ./bind.sh --cpu=exclusive --ib=single --cluster=circe -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=12 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
267: STARTING TIMING RUN AT 2020-06-25 10:59:00 AM
231: + eval '    ./bind.sh --cpu=exclusive --ib=single --cluster=circe --     python -u /workspace/bert/run_pretraining.py         --train_batch_size=18     --learning_rate=2.0e-3     --opt_lamb_beta_1=0.86     --opt_lamb_beta_2=0.975     --warmup_proportion=0.0     --max_steps=750     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data_phase2     --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.712     --max_samples_termination=7500000     --eval_iter_start_samples=3000000 --eval_iter_samples=500000     --eval_batch_size=16 --eval_dir=/workspace/evaldata     --cache_eval_data     --output_dir=/results     --fp16 --fused_gelu_bias --dense_seq_output --fused_mha      --allreduce_post_accumulation --allreduce_post_accumulation_fp16     --gradient_accumulation_steps=1     --log_freq=1     --local_rank=7     --bert_config_path=/workspace/phase1/bert_config.j
261: STARTING TIMING RUN AT 2020-06-25 10:59:00 AM
231: son --seed=18263'
265: STARTING TIMING RUN AT 2020-06-25 10:59:00 AM
226: + eval '    ./bind.sh --cpu=exclusive --ib=single --cluster=circe --     python -u /workspace/bert/run_pretraining.py         --train_batch_size=18     --learning_rate=2.0e-3     --opt_lamb_beta_1=0.86     --opt_lamb_beta_2=0.975     --warmup_proportion=0.0     --max_steps=750     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data_phase2     --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.712     --max_samples_termination=7500000     --eval_iter_start_samples=3000000 --eval_iter_samples=500000     --eval_batch_size=16 --eval_dir=/workspace/evaldata     --cache_eval_data     --output_dir=/results     --fp16 --fused_gelu_bias --dense_seq_output --fused_mha      --allreduce_post_accumulation --allreduce_post_accumulation_fp16     --gradient_accumulation_steps=1     --log_freq=1     --local_rank=2     --bert_config_path=/workspace/phase1/bert_config.j
262: STARTING TIMING RUN AT 2020-06-25 10:59:00 AM
226: son --seed=18263'
271: STARTING TIMING RUN AT 2020-06-25 10:59:00 AM
231: ++ ./bind.sh --cpu=exclusive --ib=single --cluster=circe -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=7 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
258: STARTING TIMING RUN AT 2020-06-25 10:59:00 AM
226: ++ ./bind.sh --cpu=exclusive --ib=single --cluster=circe -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=2 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
479: Run vars: id 394913 gpus 16 mparams
 46: + eval '    ./bind.sh --cpu=exclusive --ib=single --cluster=circe --     python -u /workspace/bert/run_pretraining.py         --train_batch_size=18     --learning_rate=2.0e-3     --opt_lamb_beta_1=0.86     --opt_lamb_beta_2=0.975     --warmup_proportion=0.0     --max_steps=750     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data_phase2     --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.712     --max_samples_termination=7500000     --eval_iter_start_samples=3000000 --eval_iter_samples=500000     --eval_batch_size=16 --eval_dir=/workspace/evaldata     --cache_eval_data     --output_dir=/results     --fp16 --fused_gelu_bias --dense_seq_output --fused_mha      --allreduce_post_accumulation --allreduce_post_accumulation_fp16     --gradient_accumulation_steps=1     --log_freq=1     --local_rank=14     --bert_config_path=/workspace/phase1/bert_config.
201: Run vars: id 394913 gpus 16 mparams
 46: json --seed=18263'
195: Run vars: id 394913 gpus 16 mparams
 46: ++ ./bind.sh --cpu=exclusive --ib=single --cluster=circe -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=14 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
205: Run vars: id 394913 gpus 16 mparams
416: slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
206: Run vars: id 394913 gpus 16 mparams
234: + eval '    ./bind.sh --cpu=exclusive --ib=single --cluster=circe --     python -u /workspace/bert/run_pretraining.py         --train_batch_size=18     --learning_rate=2.0e-3     --opt_lamb_beta_1=0.86     --opt_lamb_beta_2=0.975     --warmup_proportion=0.0     --max_steps=750     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data_phase2     --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.712     --max_samples_termination=7500000     --eval_iter_start_samples=3000000 --eval_iter_samples=500000     --eval_batch_size=16 --eval_dir=/workspace/evaldata     --cache_eval_data     --output_dir=/results     --fp16 --fused_gelu_bias --dense_seq_output --fused_mha      --allreduce_post_accumulation --allreduce_post_accumulation_fp16     --gradient_accumulation_steps=1     --log_freq=1     --local_rank=10     --bert_config_path=/workspace/phase1/bert_config.
199: Run vars: id 394913 gpus 16 mparams
234: json --seed=18263'
204: Run vars: id 394913 gpus 16 mparams
321: + eval '    ./bind.sh --cpu=exclusive --ib=single --cluster=circe --     python -u /workspace/bert/run_pretraining.py         --train_batch_size=18     --learning_rate=2.0e-3     --opt_lamb_beta_1=0.86     --opt_lamb_beta_2=0.975     --warmup_proportion=0.0     --max_steps=750     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data_phase2     --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.712     --max_samples_termination=7500000     --eval_iter_start_samples=3000000 --eval_iter_samples=500000     --eval_batch_size=16 --eval_dir=/workspace/evaldata     --cache_eval_data     --output_dir=/results     --fp16 --fused_gelu_bias --dense_seq_output --fused_mha      --allreduce_post_accumulation --allreduce_post_accumulation_fp16     --gradient_accumulation_steps=1     --log_freq=1     --local_rank=1     --bert_config_path=/workspace/phase1/bert_config.j
193: STARTING TIMING RUN AT 2020-06-25 10:59:00 AM
228: + eval '    ./bind.sh --cpu=exclusive --ib=single --cluster=circe --     python -u /workspace/bert/run_pretraining.py         --train_batch_size=18     --learning_rate=2.0e-3     --opt_lamb_beta_1=0.86     --opt_lamb_beta_2=0.975     --warmup_proportion=0.0     --max_steps=750     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data_phase2     --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.712     --max_samples_termination=7500000     --eval_iter_start_samples=3000000 --eval_iter_samples=500000     --eval_batch_size=16 --eval_dir=/workspace/evaldata     --cache_eval_data     --output_dir=/results     --fp16 --fused_gelu_bias --dense_seq_output --fused_mha      --allreduce_post_accumulation --allreduce_post_accumulation_fp16     --gradient_accumulation_steps=1     --log_freq=1     --local_rank=4     --bert_config_path=/workspace/phase1/bert_config.j
266: STARTING TIMING RUN AT 2020-06-25 10:59:00 AM
321: son --seed=18263'
200: Run vars: id 394913 gpus 16 mparams
137: + eval '    ./bind.sh --cpu=exclusive --ib=single --cluster=circe --     python -u /workspace/bert/run_pretraining.py         --train_batch_size=18     --learning_rate=2.0e-3     --opt_lamb_beta_1=0.86     --opt_lamb_beta_2=0.975     --warmup_proportion=0.0     --max_steps=750     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data_phase2     --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.712     --max_samples_termination=7500000     --eval_iter_start_samples=3000000 --eval_iter_samples=500000     --eval_batch_size=16 --eval_dir=/workspace/evaldata     --cache_eval_data     --output_dir=/results     --fp16 --fused_gelu_bias --dense_seq_output --fused_mha      --allreduce_post_accumulation --allreduce_post_accumulation_fp16     --gradient_accumulation_steps=1     --log_freq=1     --local_rank=9     --bert_config_path=/workspace/phase1/bert_config.j
264: STARTING TIMING RUN AT 2020-06-25 10:59:00 AM
228: son --seed=18263'
192: STARTING TIMING RUN AT 2020-06-25 10:59:00 AM
321: ++ ./bind.sh --cpu=exclusive --ib=single --cluster=circe -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=1 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
203: STARTING TIMING RUN AT 2020-06-25 10:59:00 AM
137: son --seed=18263'
197: STARTING TIMING RUN AT 2020-06-25 10:59:00 AM
233: + eval '    ./bind.sh --cpu=exclusive --ib=single --cluster=circe --     python -u /workspace/bert/run_pretraining.py         --train_batch_size=18     --learning_rate=2.0e-3     --opt_lamb_beta_1=0.86     --opt_lamb_beta_2=0.975     --warmup_proportion=0.0     --max_steps=750     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data_phase2     --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.712     --max_samples_termination=7500000     --eval_iter_start_samples=3000000 --eval_iter_samples=500000     --eval_batch_size=16 --eval_dir=/workspace/evaldata     --cache_eval_data     --output_dir=/results     --fp16 --fused_gelu_bias --dense_seq_output --fused_mha      --allreduce_post_accumulation --allreduce_post_accumulation_fp16     --gradient_accumulation_steps=1     --log_freq=1     --local_rank=9     --bert_config_path=/workspace/phase1/bert_config.j
207: STARTING TIMING RUN AT 2020-06-25 10:59:00 AM
233: son --seed=18263'
269: STARTING TIMING RUN AT 2020-06-25 10:59:00 AM
234: ++ ./bind.sh --cpu=exclusive --ib=single --cluster=circe -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=10 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
268: STARTING TIMING RUN AT 2020-06-25 10:59:00 AM
 37: + eval '    ./bind.sh --cpu=exclusive --ib=single --cluster=circe --     python -u /workspace/bert/run_pretraining.py         --train_batch_size=18     --learning_rate=2.0e-3     --opt_lamb_beta_1=0.86     --opt_lamb_beta_2=0.975     --warmup_proportion=0.0     --max_steps=750     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data_phase2     --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.712     --max_samples_termination=7500000     --eval_iter_start_samples=3000000 --eval_iter_samples=500000     --eval_batch_size=16 --eval_dir=/workspace/evaldata     --cache_eval_data     --output_dir=/results     --fp16 --fused_gelu_bias --dense_seq_output --fused_mha      --allreduce_post_accumulation --allreduce_post_accumulation_fp16     --gradient_accumulation_steps=1     --log_freq=1     --local_rank=5     --bert_config_path=/workspace/phase1/bert_config.j
475: Run vars: id 394913 gpus 16 mparams
228: ++ ./bind.sh --cpu=exclusive --ib=single --cluster=circe -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=4 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
477: Run vars: id 394913 gpus 16 mparams
 37: son --seed=18263'
198: STARTING TIMING RUN AT 2020-06-25 10:59:00 AM
233: ++ ./bind.sh --cpu=exclusive --ib=single --cluster=circe -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=9 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
201: STARTING TIMING RUN AT 2020-06-25 10:59:00 AM
 37: ++ ./bind.sh --cpu=exclusive --ib=single --cluster=circe -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=5 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
205: STARTING TIMING RUN AT 2020-06-25 10:59:00 AM
325: + eval '    ./bind.sh --cpu=exclusive --ib=single --cluster=circe --     python -u /workspace/bert/run_pretraining.py         --train_batch_size=18     --learning_rate=2.0e-3     --opt_lamb_beta_1=0.86     --opt_lamb_beta_2=0.975     --warmup_proportion=0.0     --max_steps=750     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data_phase2     --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.712     --max_samples_termination=7500000     --eval_iter_start_samples=3000000 --eval_iter_samples=500000     --eval_batch_size=16 --eval_dir=/workspace/evaldata     --cache_eval_data     --output_dir=/results     --fp16 --fused_gelu_bias --dense_seq_output --fused_mha      --allreduce_post_accumulation --allreduce_post_accumulation_fp16     --gradient_accumulation_steps=1     --log_freq=1     --local_rank=5     --bert_config_path=/workspace/phase1/bert_config.j
206: STARTING TIMING RUN AT 2020-06-25 10:59:00 AM
325: son --seed=18263'
195: STARTING TIMING RUN AT 2020-06-25 10:59:00 AM
325: ++ ./bind.sh --cpu=exclusive --ib=single --cluster=circe -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=5 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
204: STARTING TIMING RUN AT 2020-06-25 10:59:00 AM
 32: + eval '    ./bind.sh --cpu=exclusive --ib=single --cluster=circe --     python -u /workspace/bert/run_pretraining.py         --train_batch_size=18     --learning_rate=2.0e-3     --opt_lamb_beta_1=0.86     --opt_lamb_beta_2=0.975     --warmup_proportion=0.0     --max_steps=750     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data_phase2     --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.712     --max_samples_termination=7500000     --eval_iter_start_samples=3000000 --eval_iter_samples=500000     --eval_batch_size=16 --eval_dir=/workspace/evaldata     --cache_eval_data     --output_dir=/results     --fp16 --fused_gelu_bias --dense_seq_output --fused_mha      --allreduce_post_accumulation --allreduce_post_accumulation_fp16     --gradient_accumulation_steps=1     --log_freq=1     --local_rank=0     --bert_config_path=/workspace/phase1/bert_config.j
479: STARTING TIMING RUN AT 2020-06-25 10:59:00 AM
 32: son --seed=18263'
199: STARTING TIMING RUN AT 2020-06-25 10:59:00 AM
 32: ++ ./bind.sh --cpu=exclusive --ib=single --cluster=circe -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=0 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
476: Run vars: id 394913 gpus 16 mparams
227: + eval '    ./bind.sh --cpu=exclusive --ib=single --cluster=circe --     python -u /workspace/bert/run_pretraining.py         --train_batch_size=18     --learning_rate=2.0e-3     --opt_lamb_beta_1=0.86     --opt_lamb_beta_2=0.975     --warmup_proportion=0.0     --max_steps=750     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data_phase2     --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.712     --max_samples_termination=7500000     --eval_iter_start_samples=3000000 --eval_iter_samples=500000     --eval_batch_size=16 --eval_dir=/workspace/evaldata     --cache_eval_data     --output_dir=/results     --fp16 --fused_gelu_bias --dense_seq_output --fused_mha      --allreduce_post_accumulation --allreduce_post_accumulation_fp16     --gradient_accumulation_steps=1     --log_freq=1     --local_rank=3     --bert_config_path=/workspace/phase1/bert_config.j
200: STARTING TIMING RUN AT 2020-06-25 10:59:00 AM
227: son --seed=18263'
178: Run vars: id 394913 gpus 16 mparams
227: ++ ./bind.sh --cpu=exclusive --ib=single --cluster=circe -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=3 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
185: Run vars: id 394913 gpus 16 mparams
500: slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
178: STARTING TIMING RUN AT 2020-06-25 10:59:00 AM
497: slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
475: STARTING TIMING RUN AT 2020-06-25 10:59:00 AM
502: slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
465: Run vars: id 394913 gpus 16 mparams
501: slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
477: STARTING TIMING RUN AT 2020-06-25 10:59:00 AM
503: slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
471: Run vars: id 394913 gpus 16 mparams
498: slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
469: Run vars: id 394913 gpus 16 mparams
499: slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
478: Run vars: id 394913 gpus 16 mparams
509: slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
470: Run vars: id 394913 gpus 16 mparams
511: slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
182: Run vars: id 394913 gpus 16 mparams
510: slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
474: Run vars: id 394913 gpus 16 mparams
507: slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
185: STARTING TIMING RUN AT 2020-06-25 10:59:00 AM
508: slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
464: Run vars: id 394913 gpus 16 mparams
504: slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
476: STARTING TIMING RUN AT 2020-06-25 10:59:00 AM
505: slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
472: Run vars: id 394913 gpus 16 mparams
506: slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
468: Run vars: id 394913 gpus 16 mparams
 43: + eval '    ./bind.sh --cpu=exclusive --ib=single --cluster=circe --     python -u /workspace/bert/run_pretraining.py         --train_batch_size=18     --learning_rate=2.0e-3     --opt_lamb_beta_1=0.86     --opt_lamb_beta_2=0.975     --warmup_proportion=0.0     --max_steps=750     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data_phase2     --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.712     --max_samples_termination=7500000     --eval_iter_start_samples=3000000 --eval_iter_samples=500000     --eval_batch_size=16 --eval_dir=/workspace/evaldata     --cache_eval_data     --output_dir=/results     --fp16 --fused_gelu_bias --dense_seq_output --fused_mha      --allreduce_post_accumulation --allreduce_post_accumulation_fp16     --gradient_accumulation_steps=1     --log_freq=1     --local_rank=11     --bert_config_path=/workspace/phase1/bert_config.
466: Run vars: id 394913 gpus 16 mparams
 43: json --seed=18263'
470: STARTING TIMING RUN AT 2020-06-25 10:59:01 AM
 43: ++ ./bind.sh --cpu=exclusive --ib=single --cluster=circe -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=11 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
471: STARTING TIMING RUN AT 2020-06-25 10:59:01 AM
 39: + eval '    ./bind.sh --cpu=exclusive --ib=single --cluster=circe --     python -u /workspace/bert/run_pretraining.py         --train_batch_size=18     --learning_rate=2.0e-3     --opt_lamb_beta_1=0.86     --opt_lamb_beta_2=0.975     --warmup_proportion=0.0     --max_steps=750     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data_phase2     --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.712     --max_samples_termination=7500000     --eval_iter_start_samples=3000000 --eval_iter_samples=500000     --eval_batch_size=16 --eval_dir=/workspace/evaldata     --cache_eval_data     --output_dir=/results     --fp16 --fused_gelu_bias --dense_seq_output --fused_mha      --allreduce_post_accumulation --allreduce_post_accumulation_fp16     --gradient_accumulation_steps=1     --log_freq=1     --local_rank=7     --bert_config_path=/workspace/phase1/bert_config.j
467: Run vars: id 394913 gpus 16 mparams
 39: son --seed=18263'
473: Run vars: id 394913 gpus 16 mparams
 39: ++ ./bind.sh --cpu=exclusive --ib=single --cluster=circe -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=7 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
469: STARTING TIMING RUN AT 2020-06-25 10:59:01 AM
237: + eval '    ./bind.sh --cpu=exclusive --ib=single --cluster=circe --     python -u /workspace/bert/run_pretraining.py         --train_batch_size=18     --learning_rate=2.0e-3     --opt_lamb_beta_1=0.86     --opt_lamb_beta_2=0.975     --warmup_proportion=0.0     --max_steps=750     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data_phase2     --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.712     --max_samples_termination=7500000     --eval_iter_start_samples=3000000 --eval_iter_samples=500000     --eval_batch_size=16 --eval_dir=/workspace/evaldata     --cache_eval_data     --output_dir=/results     --fp16 --fused_gelu_bias --dense_seq_output --fused_mha      --allreduce_post_accumulation --allreduce_post_accumulation_fp16     --gradient_accumulation_steps=1     --log_freq=1     --local_rank=13     --bert_config_path=/workspace/phase1/bert_config.
478: STARTING TIMING RUN AT 2020-06-25 10:59:01 AM
237: json --seed=18263'
465: STARTING TIMING RUN AT 2020-06-25 10:59:01 AM
237: ++ ./bind.sh --cpu=exclusive --ib=single --cluster=circe -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=13 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
474: STARTING TIMING RUN AT 2020-06-25 10:59:01 AM
236: + eval '    ./bind.sh --cpu=exclusive --ib=single --cluster=circe --     python -u /workspace/bert/run_pretraining.py         --train_batch_size=18     --learning_rate=2.0e-3     --opt_lamb_beta_1=0.86     --opt_lamb_beta_2=0.975     --warmup_proportion=0.0     --max_steps=750     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data_phase2     --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.712     --max_samples_termination=7500000     --eval_iter_start_samples=3000000 --eval_iter_samples=500000     --eval_batch_size=16 --eval_dir=/workspace/evaldata     --cache_eval_data     --output_dir=/results     --fp16 --fused_gelu_bias --dense_seq_output --fused_mha      --allreduce_post_accumulation --allreduce_post_accumulation_fp16     --gradient_accumulation_steps=1     --log_freq=1     --local_rank=12     --bert_config_path=/workspace/phase1/bert_config.
181: Run vars: id 394913 gpus 16 mparams
236: json --seed=18263'
191: Run vars: id 394913 gpus 16 mparams
236: ++ ./bind.sh --cpu=exclusive --ib=single --cluster=circe -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=12 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
180: Run vars: id 394913 gpus 16 mparams
 33: + eval '    ./bind.sh --cpu=exclusive --ib=single --cluster=circe --     python -u /workspace/bert/run_pretraining.py         --train_batch_size=18     --learning_rate=2.0e-3     --opt_lamb_beta_1=0.86     --opt_lamb_beta_2=0.975     --warmup_proportion=0.0     --max_steps=750     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data_phase2     --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.712     --max_samples_termination=7500000     --eval_iter_start_samples=3000000 --eval_iter_samples=500000     --eval_batch_size=16 --eval_dir=/workspace/evaldata     --cache_eval_data     --output_dir=/results     --fp16 --fused_gelu_bias --dense_seq_output --fused_mha      --allreduce_post_accumulation --allreduce_post_accumulation_fp16     --gradient_accumulation_steps=1     --log_freq=1     --local_rank=1     --bert_config_path=/workspace/phase1/bert_config.j
464: STARTING TIMING RUN AT 2020-06-25 10:59:01 AM
 33: son --seed=18263'
176: Run vars: id 394913 gpus 16 mparams
 33: ++ ./bind.sh --cpu=exclusive --ib=single --cluster=circe -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=1 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
187: Run vars: id 394913 gpus 16 mparams
508: + eval '    ./bind.sh --cpu=exclusive --ib=single --cluster=circe --     python -u /workspace/bert/run_pretraining.py         --train_batch_size=18     --learning_rate=2.0e-3     --opt_lamb_beta_1=0.86     --opt_lamb_beta_2=0.975     --warmup_proportion=0.0     --max_steps=750     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data_phase2     --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.712     --max_samples_termination=7500000     --eval_iter_start_samples=3000000 --eval_iter_samples=500000     --eval_batch_size=16 --eval_dir=/workspace/evaldata     --cache_eval_data     --output_dir=/results     --fp16 --fused_gelu_bias --dense_seq_output --fused_mha      --allreduce_post_accumulation --allreduce_post_accumulation_fp16     --gradient_accumulation_steps=1     --log_freq=1     --local_rank=12     --bert_config_path=/workspace/phase1/bert_config.
182: STARTING TIMING RUN AT 2020-06-25 10:59:01 AM
508: json --seed=18263'
472: STARTING TIMING RUN AT 2020-06-25 10:59:01 AM
508: ++ ./bind.sh --cpu=exclusive --ib=single --cluster=circe -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=12 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
294: Run vars: id 394913 gpus 16 mparams
235: + eval '    ./bind.sh --cpu=exclusive --ib=single --cluster=circe --     python -u /workspace/bert/run_pretraining.py         --train_batch_size=18     --learning_rate=2.0e-3     --opt_lamb_beta_1=0.86     --opt_lamb_beta_2=0.975     --warmup_proportion=0.0     --max_steps=750     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data_phase2     --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.712     --max_samples_termination=7500000     --eval_iter_start_samples=3000000 --eval_iter_samples=500000     --eval_batch_size=16 --eval_dir=/workspace/evaldata     --cache_eval_data     --output_dir=/results     --fp16 --fused_gelu_bias --dense_seq_output --fused_mha      --allreduce_post_accumulation --allreduce_post_accumulation_fp16     --gradient_accumulation_steps=1     --log_freq=1     --local_rank=11     --bert_config_path=/workspace/phase1/bert_config.
302: Run vars: id 394913 gpus 16 mparams
235: json --seed=18263'
302: STARTING TIMING RUN AT 2020-06-25 10:59:01 AM
235: ++ ./bind.sh --cpu=exclusive --ib=single --cluster=circe -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=11 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
294: STARTING TIMING RUN AT 2020-06-25 10:59:01 AM
335: + eval '    ./bind.sh --cpu=exclusive --ib=single --cluster=circe --     python -u /workspace/bert/run_pretraining.py         --train_batch_size=18     --learning_rate=2.0e-3     --opt_lamb_beta_1=0.86     --opt_lamb_beta_2=0.975     --warmup_proportion=0.0     --max_steps=750     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data_phase2     --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.712     --max_samples_termination=7500000     --eval_iter_start_samples=3000000 --eval_iter_samples=500000     --eval_batch_size=16 --eval_dir=/workspace/evaldata     --cache_eval_data     --output_dir=/results     --fp16 --fused_gelu_bias --dense_seq_output --fused_mha      --allreduce_post_accumulation --allreduce_post_accumulation_fp16     --gradient_accumulation_steps=1     --log_freq=1     --local_rank=15     --bert_config_path=/workspace/phase1/bert_config.
179: Run vars: id 394913 gpus 16 mparams
335: json --seed=18263'
468: STARTING TIMING RUN AT 2020-06-25 10:59:01 AM
335: ++ ./bind.sh --cpu=exclusive --ib=single --cluster=circe -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=15 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
186: Run vars: id 394913 gpus 16 mparams
 45: + eval '    ./bind.sh --cpu=exclusive --ib=single --cluster=circe --     python -u /workspace/bert/run_pretraining.py         --train_batch_size=18     --learning_rate=2.0e-3     --opt_lamb_beta_1=0.86     --opt_lamb_beta_2=0.975     --warmup_proportion=0.0     --max_steps=750     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data_phase2     --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.712     --max_samples_termination=7500000     --eval_iter_start_samples=3000000 --eval_iter_samples=500000     --eval_batch_size=16 --eval_dir=/workspace/evaldata     --cache_eval_data     --output_dir=/results     --fp16 --fused_gelu_bias --dense_seq_output --fused_mha      --allreduce_post_accumulation --allreduce_post_accumulation_fp16     --gradient_accumulation_steps=1     --log_freq=1     --local_rank=13     --bert_config_path=/workspace/phase1/bert_config.
183: Run vars: id 394913 gpus 16 mparams
 45: json --seed=18263'
184: Run vars: id 394913 gpus 16 mparams
 34: + eval '    ./bind.sh --cpu=exclusive --ib=single --cluster=circe --     python -u /workspace/bert/run_pretraining.py         --train_batch_size=18     --learning_rate=2.0e-3     --opt_lamb_beta_1=0.86     --opt_lamb_beta_2=0.975     --warmup_proportion=0.0     --max_steps=750     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data_phase2     --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.712     --max_samples_termination=7500000     --eval_iter_start_samples=3000000 --eval_iter_samples=500000     --eval_batch_size=16 --eval_dir=/workspace/evaldata     --cache_eval_data     --output_dir=/results     --fp16 --fused_gelu_bias --dense_seq_output --fused_mha      --allreduce_post_accumulation --allreduce_post_accumulation_fp16     --gradient_accumulation_steps=1     --log_freq=1     --local_rank=2     --bert_config_path=/workspace/phase1/bert_config.j
177: Run vars: id 394913 gpus 16 mparams
 34: son --seed=18263'
181: STARTING TIMING RUN AT 2020-06-25 10:59:01 AM
 45: ++ ./bind.sh --cpu=exclusive --ib=single --cluster=circe -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=13 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
466: STARTING TIMING RUN AT 2020-06-25 10:59:01 AM
 34: ++ ./bind.sh --cpu=exclusive --ib=single --cluster=circe -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=2 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
190: Run vars: id 394913 gpus 16 mparams
499: + eval '    ./bind.sh --cpu=exclusive --ib=single --cluster=circe --     python -u /workspace/bert/run_pretraining.py         --train_batch_size=18     --learning_rate=2.0e-3     --opt_lamb_beta_1=0.86     --opt_lamb_beta_2=0.975     --warmup_proportion=0.0     --max_steps=750     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data_phase2     --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.712     --max_samples_termination=7500000     --eval_iter_start_samples=3000000 --eval_iter_samples=500000     --eval_batch_size=16 --eval_dir=/workspace/evaldata     --cache_eval_data     --output_dir=/results     --fp16 --fused_gelu_bias --dense_seq_output --fused_mha      --allreduce_post_accumulation --allreduce_post_accumulation_fp16     --gradient_accumulation_steps=1     --log_freq=1     --local_rank=3     --bert_config_path=/workspace/phase1/bert_config.j
191: STARTING TIMING RUN AT 2020-06-25 10:59:01 AM
499: son --seed=18263'
188: Run vars: id 394913 gpus 16 mparams
496: + eval '    ./bind.sh --cpu=exclusive --ib=single --cluster=circe --     python -u /workspace/bert/run_pretraining.py         --train_batch_size=18     --learning_rate=2.0e-3     --opt_lamb_beta_1=0.86     --opt_lamb_beta_2=0.975     --warmup_proportion=0.0     --max_steps=750     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data_phase2     --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.712     --max_samples_termination=7500000     --eval_iter_start_samples=3000000 --eval_iter_samples=500000     --eval_batch_size=16 --eval_dir=/workspace/evaldata     --cache_eval_data     --output_dir=/results     --fp16 --fused_gelu_bias --dense_seq_output --fused_mha      --allreduce_post_accumulation --allreduce_post_accumulation_fp16     --gradient_accumulation_steps=1     --log_freq=1     --local_rank=0     --bert_config_path=/workspace/phase1/bert_config.j
189: Run vars: id 394913 gpus 16 mparams
496: son --seed=18263'
473: STARTING TIMING RUN AT 2020-06-25 10:59:01 AM
499: ++ ./bind.sh --cpu=exclusive --ib=single --cluster=circe -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=3 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
467: STARTING TIMING RUN AT 2020-06-25 10:59:01 AM
326: + eval '    ./bind.sh --cpu=exclusive --ib=single --cluster=circe --     python -u /workspace/bert/run_pretraining.py         --train_batch_size=18     --learning_rate=2.0e-3     --opt_lamb_beta_1=0.86     --opt_lamb_beta_2=0.975     --warmup_proportion=0.0     --max_steps=750     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data_phase2     --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.712     --max_samples_termination=7500000     --eval_iter_start_samples=3000000 --eval_iter_samples=500000     --eval_batch_size=16 --eval_dir=/workspace/evaldata     --cache_eval_data     --output_dir=/results     --fp16 --fused_gelu_bias --dense_seq_output --fused_mha      --allreduce_post_accumulation --allreduce_post_accumulation_fp16     --gradient_accumulation_steps=1     --log_freq=1     --local_rank=6     --bert_config_path=/workspace/phase1/bert_config.j
180: STARTING TIMING RUN AT 2020-06-25 10:59:01 AM
496: ++ ./bind.sh --cpu=exclusive --ib=single --cluster=circe -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=0 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
187: STARTING TIMING RUN AT 2020-06-25 10:59:01 AM
326: son --seed=18263'
176: STARTING TIMING RUN AT 2020-06-25 10:59:01 AM
326: ++ ./bind.sh --cpu=exclusive --ib=single --cluster=circe -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=6 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
186: STARTING TIMING RUN AT 2020-06-25 10:59:01 AM
327: + eval '    ./bind.sh --cpu=exclusive --ib=single --cluster=circe --     python -u /workspace/bert/run_pretraining.py         --train_batch_size=18     --learning_rate=2.0e-3     --opt_lamb_beta_1=0.86     --opt_lamb_beta_2=0.975     --warmup_proportion=0.0     --max_steps=750     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data_phase2     --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.712     --max_samples_termination=7500000     --eval_iter_start_samples=3000000 --eval_iter_samples=500000     --eval_batch_size=16 --eval_dir=/workspace/evaldata     --cache_eval_data     --output_dir=/results     --fp16 --fused_gelu_bias --dense_seq_output --fused_mha      --allreduce_post_accumulation --allreduce_post_accumulation_fp16     --gradient_accumulation_steps=1     --log_freq=1     --local_rank=7     --bert_config_path=/workspace/phase1/bert_config.j
179: STARTING TIMING RUN AT 2020-06-25 10:59:01 AM
327: son --seed=18263'
183: STARTING TIMING RUN AT 2020-06-25 10:59:01 AM
327: ++ ./bind.sh --cpu=exclusive --ib=single --cluster=circe -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=7 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
290: Run vars: id 394913 gpus 16 mparams
137: ++ ./bind.sh --cpu=exclusive --ib=single --cluster=circe -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=9 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
296: Run vars: id 394913 gpus 16 mparams
333: + eval '    ./bind.sh --cpu=exclusive --ib=single --cluster=circe --     python -u /workspace/bert/run_pretraining.py         --train_batch_size=18     --learning_rate=2.0e-3     --opt_lamb_beta_1=0.86     --opt_lamb_beta_2=0.975     --warmup_proportion=0.0     --max_steps=750     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data_phase2     --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.712     --max_samples_termination=7500000     --eval_iter_start_samples=3000000 --eval_iter_samples=500000     --eval_batch_size=16 --eval_dir=/workspace/evaldata     --cache_eval_data     --output_dir=/results     --fp16 --fused_gelu_bias --dense_seq_output --fused_mha      --allreduce_post_accumulation --allreduce_post_accumulation_fp16     --gradient_accumulation_steps=1     --log_freq=1     --local_rank=13     --bert_config_path=/workspace/phase1/bert_config.
291: Run vars: id 394913 gpus 16 mparams
333: json --seed=18263'
300: Run vars: id 394913 gpus 16 mparams
136: + eval '    ./bind.sh --cpu=exclusive --ib=single --cluster=circe --     python -u /workspace/bert/run_pretraining.py         --train_batch_size=18     --learning_rate=2.0e-3     --opt_lamb_beta_1=0.86     --opt_lamb_beta_2=0.975     --warmup_proportion=0.0     --max_steps=750     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data_phase2     --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.712     --max_samples_termination=7500000     --eval_iter_start_samples=3000000 --eval_iter_samples=500000     --eval_batch_size=16 --eval_dir=/workspace/evaldata     --cache_eval_data     --output_dir=/results     --fp16 --fused_gelu_bias --dense_seq_output --fused_mha      --allreduce_post_accumulation --allreduce_post_accumulation_fp16     --gradient_accumulation_steps=1     --log_freq=1     --local_rank=8     --bert_config_path=/workspace/phase1/bert_config.j
288: Run vars: id 394913 gpus 16 mparams
136: son --seed=18263'
295: Run vars: id 394913 gpus 16 mparams
 99: slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
293: Run vars: id 394913 gpus 16 mparams
136: ++ ./bind.sh --cpu=exclusive --ib=single --cluster=circe -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=8 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
301: Run vars: id 394913 gpus 16 mparams
333: ++ ./bind.sh --cpu=exclusive --ib=single --cluster=circe -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=13 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
303: Run vars: id 394913 gpus 16 mparams
323: + eval '    ./bind.sh --cpu=exclusive --ib=single --cluster=circe --     python -u /workspace/bert/run_pretraining.py         --train_batch_size=18     --learning_rate=2.0e-3     --opt_lamb_beta_1=0.86     --opt_lamb_beta_2=0.975     --warmup_proportion=0.0     --max_steps=750     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data_phase2     --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.712     --max_samples_termination=7500000     --eval_iter_start_samples=3000000 --eval_iter_samples=500000     --eval_batch_size=16 --eval_dir=/workspace/evaldata     --cache_eval_data     --output_dir=/results     --fp16 --fused_gelu_bias --dense_seq_output --fused_mha      --allreduce_post_accumulation --allreduce_post_accumulation_fp16     --gradient_accumulation_steps=1     --log_freq=1     --local_rank=3     --bert_config_path=/workspace/phase1/bert_config.j
297: Run vars: id 394913 gpus 16 mparams
501: + eval '    ./bind.sh --cpu=exclusive --ib=single --cluster=circe --     python -u /workspace/bert/run_pretraining.py         --train_batch_size=18     --learning_rate=2.0e-3     --opt_lamb_beta_1=0.86     --opt_lamb_beta_2=0.975     --warmup_proportion=0.0     --max_steps=750     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data_phase2     --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.712     --max_samples_termination=7500000     --eval_iter_start_samples=3000000 --eval_iter_samples=500000     --eval_batch_size=16 --eval_dir=/workspace/evaldata     --cache_eval_data     --output_dir=/results     --fp16 --fused_gelu_bias --dense_seq_output --fused_mha      --allreduce_post_accumulation --allreduce_post_accumulation_fp16     --gradient_accumulation_steps=1     --log_freq=1     --local_rank=5     --bert_config_path=/workspace/phase1/bert_config.j
298: Run vars: id 394913 gpus 16 mparams
501: son --seed=18263'
190: STARTING TIMING RUN AT 2020-06-25 10:59:01 AM
505: + eval '    ./bind.sh --cpu=exclusive --ib=single --cluster=circe --     python -u /workspace/bert/run_pretraining.py         --train_batch_size=18     --learning_rate=2.0e-3     --opt_lamb_beta_1=0.86     --opt_lamb_beta_2=0.975     --warmup_proportion=0.0     --max_steps=750     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data_phase2     --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.712     --max_samples_termination=7500000     --eval_iter_start_samples=3000000 --eval_iter_samples=500000     --eval_batch_size=16 --eval_dir=/workspace/evaldata     --cache_eval_data     --output_dir=/results     --fp16 --fused_gelu_bias --dense_seq_output --fused_mha      --allreduce_post_accumulation --allreduce_post_accumulation_fp16     --gradient_accumulation_steps=1     --log_freq=1     --local_rank=9     --bert_config_path=/workspace/phase1/bert_config.j
299: Run vars: id 394913 gpus 16 mparams
505: son --seed=18263'
177: STARTING TIMING RUN AT 2020-06-25 10:59:01 AM
501: ++ ./bind.sh --cpu=exclusive --ib=single --cluster=circe -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=5 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
184: STARTING TIMING RUN AT 2020-06-25 10:59:01 AM
505: ++ ./bind.sh --cpu=exclusive --ib=single --cluster=circe -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=9 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
189: STARTING TIMING RUN AT 2020-06-25 10:59:01 AM
230: + eval '    ./bind.sh --cpu=exclusive --ib=single --cluster=circe --     python -u /workspace/bert/run_pretraining.py         --train_batch_size=18     --learning_rate=2.0e-3     --opt_lamb_beta_1=0.86     --opt_lamb_beta_2=0.975     --warmup_proportion=0.0     --max_steps=750     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data_phase2     --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.712     --max_samples_termination=7500000     --eval_iter_start_samples=3000000 --eval_iter_samples=500000     --eval_batch_size=16 --eval_dir=/workspace/evaldata     --cache_eval_data     --output_dir=/results     --fp16 --fused_gelu_bias --dense_seq_output --fused_mha      --allreduce_post_accumulation --allreduce_post_accumulation_fp16     --gradient_accumulation_steps=1     --log_freq=1     --local_rank=6     --bert_config_path=/workspace/phase1/bert_config.j
188: STARTING TIMING RUN AT 2020-06-25 10:59:01 AM
141: + eval '    ./bind.sh --cpu=exclusive --ib=single --cluster=circe --     python -u /workspace/bert/run_pretraining.py         --train_batch_size=18     --learning_rate=2.0e-3     --opt_lamb_beta_1=0.86     --opt_lamb_beta_2=0.975     --warmup_proportion=0.0     --max_steps=750     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data_phase2     --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.712     --max_samples_termination=7500000     --eval_iter_start_samples=3000000 --eval_iter_samples=500000     --eval_batch_size=16 --eval_dir=/workspace/evaldata     --cache_eval_data     --output_dir=/results     --fp16 --fused_gelu_bias --dense_seq_output --fused_mha      --allreduce_post_accumulation --allreduce_post_accumulation_fp16     --gradient_accumulation_steps=1     --log_freq=1     --local_rank=13     --bert_config_path=/workspace/phase1/bert_config.
290: STARTING TIMING RUN AT 2020-06-25 10:59:01 AM
230: son --seed=18263'
291: STARTING TIMING RUN AT 2020-06-25 10:59:01 AM
503: + eval '    ./bind.sh --cpu=exclusive --ib=single --cluster=circe --     python -u /workspace/bert/run_pretraining.py         --train_batch_size=18     --learning_rate=2.0e-3     --opt_lamb_beta_1=0.86     --opt_lamb_beta_2=0.975     --warmup_proportion=0.0     --max_steps=750     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data_phase2     --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.712     --max_samples_termination=7500000     --eval_iter_start_samples=3000000 --eval_iter_samples=500000     --eval_batch_size=16 --eval_dir=/workspace/evaldata     --cache_eval_data     --output_dir=/results     --fp16 --fused_gelu_bias --dense_seq_output --fused_mha      --allreduce_post_accumulation --allreduce_post_accumulation_fp16     --gradient_accumulation_steps=1     --log_freq=1     --local_rank=7     --bert_config_path=/workspace/phase1/bert_config.j
289: Run vars: id 394913 gpus 16 mparams
225: + eval '    ./bind.sh --cpu=exclusive --ib=single --cluster=circe --     python -u /workspace/bert/run_pretraining.py         --train_batch_size=18     --learning_rate=2.0e-3     --opt_lamb_beta_1=0.86     --opt_lamb_beta_2=0.975     --warmup_proportion=0.0     --max_steps=750     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data_phase2     --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.712     --max_samples_termination=7500000     --eval_iter_start_samples=3000000 --eval_iter_samples=500000     --eval_batch_size=16 --eval_dir=/workspace/evaldata     --cache_eval_data     --output_dir=/results     --fp16 --fused_gelu_bias --dense_seq_output --fused_mha      --allreduce_post_accumulation --allreduce_post_accumulation_fp16     --gradient_accumulation_steps=1     --log_freq=1     --local_rank=1     --bert_config_path=/workspace/phase1/bert_config.j
292: Run vars: id 394913 gpus 16 mparams
503: son --seed=18263'
288: STARTING TIMING RUN AT 2020-06-25 10:59:01 AM
141: json --seed=18263'
296: STARTING TIMING RUN AT 2020-06-25 10:59:01 AM
225: son --seed=18263'
295: STARTING TIMING RUN AT 2020-06-25 10:59:01 AM
503: ++ ./bind.sh --cpu=exclusive --ib=single --cluster=circe -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=7 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
300: STARTING TIMING RUN AT 2020-06-25 10:59:01 AM
230: ++ ./bind.sh --cpu=exclusive --ib=single --cluster=circe -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=6 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
301: STARTING TIMING RUN AT 2020-06-25 10:59:01 AM
323: son --seed=18263'
293: STARTING TIMING RUN AT 2020-06-25 10:59:01 AM
 44: + eval '    ./bind.sh --cpu=exclusive --ib=single --cluster=circe --     python -u /workspace/bert/run_pretraining.py         --train_batch_size=18     --learning_rate=2.0e-3     --opt_lamb_beta_1=0.86     --opt_lamb_beta_2=0.975     --warmup_proportion=0.0     --max_steps=750     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data_phase2     --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.712     --max_samples_termination=7500000     --eval_iter_start_samples=3000000 --eval_iter_samples=500000     --eval_batch_size=16 --eval_dir=/workspace/evaldata     --cache_eval_data     --output_dir=/results     --fp16 --fused_gelu_bias --dense_seq_output --fused_mha      --allreduce_post_accumulation --allreduce_post_accumulation_fp16     --gradient_accumulation_steps=1     --log_freq=1     --local_rank=12     --bert_config_path=/workspace/phase1/bert_config.
297: STARTING TIMING RUN AT 2020-06-25 10:59:01 AM
225: ++ ./bind.sh --cpu=exclusive --ib=single --cluster=circe -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=1 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
299: STARTING TIMING RUN AT 2020-06-25 10:59:01 AM
323: ++ ./bind.sh --cpu=exclusive --ib=single --cluster=circe -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=3 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
303: STARTING TIMING RUN AT 2020-06-25 10:59:01 AM
141: ++ ./bind.sh --cpu=exclusive --ib=single --cluster=circe -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=13 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
421: Run vars: id 394913 gpus 16 mparams
 44: json --seed=18263'
418: Run vars: id 394913 gpus 16 mparams
 44: ++ ./bind.sh --cpu=exclusive --ib=single --cluster=circe -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=12 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
430: Run vars: id 394913 gpus 16 mparams
329: + eval '    ./bind.sh --cpu=exclusive --ib=single --cluster=circe --     python -u /workspace/bert/run_pretraining.py         --train_batch_size=18     --learning_rate=2.0e-3     --opt_lamb_beta_1=0.86     --opt_lamb_beta_2=0.975     --warmup_proportion=0.0     --max_steps=750     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data_phase2     --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.712     --max_samples_termination=7500000     --eval_iter_start_samples=3000000 --eval_iter_samples=500000     --eval_batch_size=16 --eval_dir=/workspace/evaldata     --cache_eval_data     --output_dir=/results     --fp16 --fused_gelu_bias --dense_seq_output --fused_mha      --allreduce_post_accumulation --allreduce_post_accumulation_fp16     --gradient_accumulation_steps=1     --log_freq=1     --local_rank=9     --bert_config_path=/workspace/phase1/bert_config.j
424: Run vars: id 394913 gpus 16 mparams
329: son --seed=18263'
426: Run vars: id 394913 gpus 16 mparams
128: + eval '    ./bind.sh --cpu=exclusive --ib=single --cluster=circe --     python -u /workspace/bert/run_pretraining.py         --train_batch_size=18     --learning_rate=2.0e-3     --opt_lamb_beta_1=0.86     --opt_lamb_beta_2=0.975     --warmup_proportion=0.0     --max_steps=750     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data_phase2     --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.712     --max_samples_termination=7500000     --eval_iter_start_samples=3000000 --eval_iter_samples=500000     --eval_batch_size=16 --eval_dir=/workspace/evaldata     --cache_eval_data     --output_dir=/results     --fp16 --fused_gelu_bias --dense_seq_output --fused_mha      --allreduce_post_accumulation --allreduce_post_accumulation_fp16     --gradient_accumulation_steps=1     --log_freq=1     --local_rank=0     --bert_config_path=/workspace/phase1/bert_config.j
417: Run vars: id 394913 gpus 16 mparams
 35: + eval '    ./bind.sh --cpu=exclusive --ib=single --cluster=circe --     python -u /workspace/bert/run_pretraining.py         --train_batch_size=18     --learning_rate=2.0e-3     --opt_lamb_beta_1=0.86     --opt_lamb_beta_2=0.975     --warmup_proportion=0.0     --max_steps=750     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data_phase2     --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.712     --max_samples_termination=7500000     --eval_iter_start_samples=3000000 --eval_iter_samples=500000     --eval_batch_size=16 --eval_dir=/workspace/evaldata     --cache_eval_data     --output_dir=/results     --fp16 --fused_gelu_bias --dense_seq_output --fused_mha      --allreduce_post_accumulation --allreduce_post_accumulation_fp16     --gradient_accumulation_steps=1     --log_freq=1     --local_rank=3     --bert_config_path=/workspace/phase1/bert_config.j
423: Run vars: id 394913 gpus 16 mparams
329: ++ ./bind.sh --cpu=exclusive --ib=single --cluster=circe -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=9 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
422: Run vars: id 394913 gpus 16 mparams
128: son --seed=18263'
289: STARTING TIMING RUN AT 2020-06-25 10:59:01 AM
 35: son --seed=18263'
298: STARTING TIMING RUN AT 2020-06-25 10:59:01 AM
500: + eval '    ./bind.sh --cpu=exclusive --ib=single --cluster=circe --     python -u /workspace/bert/run_pretraining.py         --train_batch_size=18     --learning_rate=2.0e-3     --opt_lamb_beta_1=0.86     --opt_lamb_beta_2=0.975     --warmup_proportion=0.0     --max_steps=750     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data_phase2     --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.712     --max_samples_termination=7500000     --eval_iter_start_samples=3000000 --eval_iter_samples=500000     --eval_batch_size=16 --eval_dir=/workspace/evaldata     --cache_eval_data     --output_dir=/results     --fp16 --fused_gelu_bias --dense_seq_output --fused_mha      --allreduce_post_accumulation --allreduce_post_accumulation_fp16     --gradient_accumulation_steps=1     --log_freq=1     --local_rank=4     --bert_config_path=/workspace/phase1/bert_config.j
292: STARTING TIMING RUN AT 2020-06-25 10:59:01 AM
500: son --seed=18263'
419: Run vars: id 394913 gpus 16 mparams
320: + eval '    ./bind.sh --cpu=exclusive --ib=single --cluster=circe --     python -u /workspace/bert/run_pretraining.py         --train_batch_size=18     --learning_rate=2.0e-3     --opt_lamb_beta_1=0.86     --opt_lamb_beta_2=0.975     --warmup_proportion=0.0     --max_steps=750     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data_phase2     --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.712     --max_samples_termination=7500000     --eval_iter_start_samples=3000000 --eval_iter_samples=500000     --eval_batch_size=16 --eval_dir=/workspace/evaldata     --cache_eval_data     --output_dir=/results     --fp16 --fused_gelu_bias --dense_seq_output --fused_mha      --allreduce_post_accumulation --allreduce_post_accumulation_fp16     --gradient_accumulation_steps=1     --log_freq=1     --local_rank=0     --bert_config_path=/workspace/phase1/bert_config.j
421: STARTING TIMING RUN AT 2020-06-25 10:59:01 AM
320: son --seed=18263'
427: Run vars: id 394913 gpus 16 mparams
 35: ++ ./bind.sh --cpu=exclusive --ib=single --cluster=circe -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=3 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
420: Run vars: id 394913 gpus 16 mparams
502: + eval '    ./bind.sh --cpu=exclusive --ib=single --cluster=circe --     python -u /workspace/bert/run_pretraining.py         --train_batch_size=18     --learning_rate=2.0e-3     --opt_lamb_beta_1=0.86     --opt_lamb_beta_2=0.975     --warmup_proportion=0.0     --max_steps=750     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data_phase2     --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.712     --max_samples_termination=7500000     --eval_iter_start_samples=3000000 --eval_iter_samples=500000     --eval_batch_size=16 --eval_dir=/workspace/evaldata     --cache_eval_data     --output_dir=/results     --fp16 --fused_gelu_bias --dense_seq_output --fused_mha      --allreduce_post_accumulation --allreduce_post_accumulation_fp16     --gradient_accumulation_steps=1     --log_freq=1     --local_rank=6     --bert_config_path=/workspace/phase1/bert_config.j
418: STARTING TIMING RUN AT 2020-06-25 10:59:01 AM
 36: + eval '    ./bind.sh --cpu=exclusive --ib=single --cluster=circe --     python -u /workspace/bert/run_pretraining.py         --train_batch_size=18     --learning_rate=2.0e-3     --opt_lamb_beta_1=0.86     --opt_lamb_beta_2=0.975     --warmup_proportion=0.0     --max_steps=750     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data_phase2     --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.712     --max_samples_termination=7500000     --eval_iter_start_samples=3000000 --eval_iter_samples=500000     --eval_batch_size=16 --eval_dir=/workspace/evaldata     --cache_eval_data     --output_dir=/results     --fp16 --fused_gelu_bias --dense_seq_output --fused_mha      --allreduce_post_accumulation --allreduce_post_accumulation_fp16     --gradient_accumulation_steps=1     --log_freq=1     --local_rank=4     --bert_config_path=/workspace/phase1/bert_config.j
429: Run vars: id 394913 gpus 16 mparams
502: son --seed=18263'
430: STARTING TIMING RUN AT 2020-06-25 10:59:01 AM
 36: son --seed=18263'
 36: ++ ./bind.sh --cpu=exclusive --ib=single --cluster=circe -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=4 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
416: Run vars: id 394913 gpus 16 mparams
 42: + eval '    ./bind.sh --cpu=exclusive --ib=single --cluster=circe --     python -u /workspace/bert/run_pretraining.py         --train_batch_size=18     --learning_rate=2.0e-3     --opt_lamb_beta_1=0.86     --opt_lamb_beta_2=0.975     --warmup_proportion=0.0     --max_steps=750     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data_phase2     --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.712     --max_samples_termination=7500000     --eval_iter_start_samples=3000000 --eval_iter_samples=500000     --eval_batch_size=16 --eval_dir=/workspace/evaldata     --cache_eval_data     --output_dir=/results     --fp16 --fused_gelu_bias --dense_seq_output --fused_mha      --allreduce_post_accumulation --allreduce_post_accumulation_fp16     --gradient_accumulation_steps=1     --log_freq=1     --local_rank=10     --bert_config_path=/workspace/phase1/bert_config.
425: Run vars: id 394913 gpus 16 mparams
 42: json --seed=18263'
 42: ++ ./bind.sh --cpu=exclusive --ib=single --cluster=circe -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=10 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
428: Run vars: id 394913 gpus 16 mparams
500: ++ ./bind.sh --cpu=exclusive --ib=single --cluster=circe -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=4 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
424: STARTING TIMING RUN AT 2020-06-25 10:59:01 AM
128: ++ ./bind.sh --cpu=exclusive --ib=single --cluster=circe -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=0 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
417: STARTING TIMING RUN AT 2020-06-25 10:59:01 AM
502: ++ ./bind.sh --cpu=exclusive --ib=single --cluster=circe -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=6 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
426: STARTING TIMING RUN AT 2020-06-25 10:59:01 AM
320: ++ ./bind.sh --cpu=exclusive --ib=single --cluster=circe -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=0 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
423: STARTING TIMING RUN AT 2020-06-25 10:59:01 AM
334: + eval '    ./bind.sh --cpu=exclusive --ib=single --cluster=circe --     python -u /workspace/bert/run_pretraining.py         --train_batch_size=18     --learning_rate=2.0e-3     --opt_lamb_beta_1=0.86     --opt_lamb_beta_2=0.975     --warmup_proportion=0.0     --max_steps=750     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data_phase2     --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.712     --max_samples_termination=7500000     --eval_iter_start_samples=3000000 --eval_iter_samples=500000     --eval_batch_size=16 --eval_dir=/workspace/evaldata     --cache_eval_data     --output_dir=/results     --fp16 --fused_gelu_bias --dense_seq_output --fused_mha      --allreduce_post_accumulation --allreduce_post_accumulation_fp16     --gradient_accumulation_steps=1     --log_freq=1     --local_rank=14     --bert_config_path=/workspace/phase1/bert_config.
419: STARTING TIMING RUN AT 2020-06-25 10:59:01 AM
334: json --seed=18263'
422: STARTING TIMING RUN AT 2020-06-25 10:59:01 AM
138: + eval '    ./bind.sh --cpu=exclusive --ib=single --cluster=circe --     python -u /workspace/bert/run_pretraining.py         --train_batch_size=18     --learning_rate=2.0e-3     --opt_lamb_beta_1=0.86     --opt_lamb_beta_2=0.975     --warmup_proportion=0.0     --max_steps=750     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data_phase2     --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.712     --max_samples_termination=7500000     --eval_iter_start_samples=3000000 --eval_iter_samples=500000     --eval_batch_size=16 --eval_dir=/workspace/evaldata     --cache_eval_data     --output_dir=/results     --fp16 --fused_gelu_bias --dense_seq_output --fused_mha      --allreduce_post_accumulation --allreduce_post_accumulation_fp16     --gradient_accumulation_steps=1     --log_freq=1     --local_rank=10     --bert_config_path=/workspace/phase1/bert_config.
431: Run vars: id 394913 gpus 16 mparams
138: json --seed=18263'
420: STARTING TIMING RUN AT 2020-06-25 10:59:01 AM
138: ++ ./bind.sh --cpu=exclusive --ib=single --cluster=circe -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=10 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
427: STARTING TIMING RUN AT 2020-06-25 10:59:01 AM
498: + eval '    ./bind.sh --cpu=exclusive --ib=single --cluster=circe --     python -u /workspace/bert/run_pretraining.py         --train_batch_size=18     --learning_rate=2.0e-3     --opt_lamb_beta_1=0.86     --opt_lamb_beta_2=0.975     --warmup_proportion=0.0     --max_steps=750     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data_phase2     --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.712     --max_samples_termination=7500000     --eval_iter_start_samples=3000000 --eval_iter_samples=500000     --eval_batch_size=16 --eval_dir=/workspace/evaldata     --cache_eval_data     --output_dir=/results     --fp16 --fused_gelu_bias --dense_seq_output --fused_mha      --allreduce_post_accumulation --allreduce_post_accumulation_fp16     --gradient_accumulation_steps=1     --log_freq=1     --local_rank=2     --bert_config_path=/workspace/phase1/bert_config.j
416: STARTING TIMING RUN AT 2020-06-25 10:59:01 AM
498: son --seed=18263'
425: STARTING TIMING RUN AT 2020-06-25 10:59:01 AM
498: ++ ./bind.sh --cpu=exclusive --ib=single --cluster=circe -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=2 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
428: STARTING TIMING RUN AT 2020-06-25 10:59:01 AM
134: + eval '    ./bind.sh --cpu=exclusive --ib=single --cluster=circe --     python -u /workspace/bert/run_pretraining.py         --train_batch_size=18     --learning_rate=2.0e-3     --opt_lamb_beta_1=0.86     --opt_lamb_beta_2=0.975     --warmup_proportion=0.0     --max_steps=750     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data_phase2     --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.712     --max_samples_termination=7500000     --eval_iter_start_samples=3000000 --eval_iter_samples=500000     --eval_batch_size=16 --eval_dir=/workspace/evaldata     --cache_eval_data     --output_dir=/results     --fp16 --fused_gelu_bias --dense_seq_output --fused_mha      --allreduce_post_accumulation --allreduce_post_accumulation_fp16     --gradient_accumulation_steps=1     --log_freq=1     --local_rank=6     --bert_config_path=/workspace/phase1/bert_config.j
429: STARTING TIMING RUN AT 2020-06-25 10:59:01 AM
134: son --seed=18263'
102: Run vars: id 394913 gpus 16 mparams
134: ++ ./bind.sh --cpu=exclusive --ib=single --cluster=circe -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=6 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
111: Run vars: id 394913 gpus 16 mparams
509: + eval '    ./bind.sh --cpu=exclusive --ib=single --cluster=circe --     python -u /workspace/bert/run_pretraining.py         --train_batch_size=18     --learning_rate=2.0e-3     --opt_lamb_beta_1=0.86     --opt_lamb_beta_2=0.975     --warmup_proportion=0.0     --max_steps=750     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data_phase2     --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.712     --max_samples_termination=7500000     --eval_iter_start_samples=3000000 --eval_iter_samples=500000     --eval_batch_size=16 --eval_dir=/workspace/evaldata     --cache_eval_data     --output_dir=/results     --fp16 --fused_gelu_bias --dense_seq_output --fused_mha      --allreduce_post_accumulation --allreduce_post_accumulation_fp16     --gradient_accumulation_steps=1     --log_freq=1     --local_rank=13     --bert_config_path=/workspace/phase1/bert_config.
106: Run vars: id 394913 gpus 16 mparams
509: json --seed=18263'
103: Run vars: id 394913 gpus 16 mparams
509: ++ ./bind.sh --cpu=exclusive --ib=single --cluster=circe -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=13 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
 99: Run vars: id 394913 gpus 16 mparams
497: + eval '    ./bind.sh --cpu=exclusive --ib=single --cluster=circe --     python -u /workspace/bert/run_pretraining.py         --train_batch_size=18     --learning_rate=2.0e-3     --opt_lamb_beta_1=0.86     --opt_lamb_beta_2=0.975     --warmup_proportion=0.0     --max_steps=750     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data_phase2     --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.712     --max_samples_termination=7500000     --eval_iter_start_samples=3000000 --eval_iter_samples=500000     --eval_batch_size=16 --eval_dir=/workspace/evaldata     --cache_eval_data     --output_dir=/results     --fp16 --fused_gelu_bias --dense_seq_output --fused_mha      --allreduce_post_accumulation --allreduce_post_accumulation_fp16     --gradient_accumulation_steps=1     --log_freq=1     --local_rank=1     --bert_config_path=/workspace/phase1/bert_config.j
107: Run vars: id 394913 gpus 16 mparams
497: son --seed=18263'
105: Run vars: id 394913 gpus 16 mparams
497: ++ ./bind.sh --cpu=exclusive --ib=single --cluster=circe -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=1 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
431: STARTING TIMING RUN AT 2020-06-25 10:59:01 AM
130: + eval '    ./bind.sh --cpu=exclusive --ib=single --cluster=circe --     python -u /workspace/bert/run_pretraining.py         --train_batch_size=18     --learning_rate=2.0e-3     --opt_lamb_beta_1=0.86     --opt_lamb_beta_2=0.975     --warmup_proportion=0.0     --max_steps=750     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data_phase2     --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.712     --max_samples_termination=7500000     --eval_iter_start_samples=3000000 --eval_iter_samples=500000     --eval_batch_size=16 --eval_dir=/workspace/evaldata     --cache_eval_data     --output_dir=/results     --fp16 --fused_gelu_bias --dense_seq_output --fused_mha      --allreduce_post_accumulation --allreduce_post_accumulation_fp16     --gradient_accumulation_steps=1     --log_freq=1     --local_rank=2     --bert_config_path=/workspace/phase1/bert_config.j
100: Run vars: id 394913 gpus 16 mparams
130: son --seed=18263'
104: Run vars: id 394913 gpus 16 mparams
129: + eval '    ./bind.sh --cpu=exclusive --ib=single --cluster=circe --     python -u /workspace/bert/run_pretraining.py         --train_batch_size=18     --learning_rate=2.0e-3     --opt_lamb_beta_1=0.86     --opt_lamb_beta_2=0.975     --warmup_proportion=0.0     --max_steps=750     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data_phase2     --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.712     --max_samples_termination=7500000     --eval_iter_start_samples=3000000 --eval_iter_samples=500000     --eval_batch_size=16 --eval_dir=/workspace/evaldata     --cache_eval_data     --output_dir=/results     --fp16 --fused_gelu_bias --dense_seq_output --fused_mha      --allreduce_post_accumulation --allreduce_post_accumulation_fp16     --gradient_accumulation_steps=1     --log_freq=1     --local_rank=1     --bert_config_path=/workspace/phase1/bert_config.j
101: Run vars: id 394913 gpus 16 mparams
129: son --seed=18263'
 98: Run vars: id 394913 gpus 16 mparams
130: ++ ./bind.sh --cpu=exclusive --ib=single --cluster=circe -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=2 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
108: Run vars: id 394913 gpus 16 mparams
129: ++ ./bind.sh --cpu=exclusive --ib=single --cluster=circe -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=1 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
 97: Run vars: id 394913 gpus 16 mparams
504: + eval '    ./bind.sh --cpu=exclusive --ib=single --cluster=circe --     python -u /workspace/bert/run_pretraining.py         --train_batch_size=18     --learning_rate=2.0e-3     --opt_lamb_beta_1=0.86     --opt_lamb_beta_2=0.975     --warmup_proportion=0.0     --max_steps=750     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data_phase2     --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.712     --max_samples_termination=7500000     --eval_iter_start_samples=3000000 --eval_iter_samples=500000     --eval_batch_size=16 --eval_dir=/workspace/evaldata     --cache_eval_data     --output_dir=/results     --fp16 --fused_gelu_bias --dense_seq_output --fused_mha      --allreduce_post_accumulation --allreduce_post_accumulation_fp16     --gradient_accumulation_steps=1     --log_freq=1     --local_rank=8     --bert_config_path=/workspace/phase1/bert_config.j
102: STARTING TIMING RUN AT 2020-06-25 10:59:01 AM
504: son --seed=18263'
110: Run vars: id 394913 gpus 16 mparams
504: ++ ./bind.sh --cpu=exclusive --ib=single --cluster=circe -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=8 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
 26: Run vars: id 394913 gpus 16 mparams
135: + eval '    ./bind.sh --cpu=exclusive --ib=single --cluster=circe --     python -u /workspace/bert/run_pretraining.py         --train_batch_size=18     --learning_rate=2.0e-3     --opt_lamb_beta_1=0.86     --opt_lamb_beta_2=0.975     --warmup_proportion=0.0     --max_steps=750     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data_phase2     --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.712     --max_samples_termination=7500000     --eval_iter_start_samples=3000000 --eval_iter_samples=500000     --eval_batch_size=16 --eval_dir=/workspace/evaldata     --cache_eval_data     --output_dir=/results     --fp16 --fused_gelu_bias --dense_seq_output --fused_mha      --allreduce_post_accumulation --allreduce_post_accumulation_fp16     --gradient_accumulation_steps=1     --log_freq=1     --local_rank=7     --bert_config_path=/workspace/phase1/bert_config.j
 17: Run vars: id 394913 gpus 16 mparams
135: son --seed=18263'
 24: Run vars: id 394913 gpus 16 mparams
135: ++ ./bind.sh --cpu=exclusive --ib=single --cluster=circe -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=7 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
 18: Run vars: id 394913 gpus 16 mparams
140: + eval '    ./bind.sh --cpu=exclusive --ib=single --cluster=circe --     python -u /workspace/bert/run_pretraining.py         --train_batch_size=18     --learning_rate=2.0e-3     --opt_lamb_beta_1=0.86     --opt_lamb_beta_2=0.975     --warmup_proportion=0.0     --max_steps=750     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data_phase2     --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.712     --max_samples_termination=7500000     --eval_iter_start_samples=3000000 --eval_iter_samples=500000     --eval_batch_size=16 --eval_dir=/workspace/evaldata     --cache_eval_data     --output_dir=/results     --fp16 --fused_gelu_bias --dense_seq_output --fused_mha      --allreduce_post_accumulation --allreduce_post_accumulation_fp16     --gradient_accumulation_steps=1     --log_freq=1     --local_rank=12     --bert_config_path=/workspace/phase1/bert_config.
 26: STARTING TIMING RUN AT 2020-06-25 10:59:01 AM
140: json --seed=18263'
 17: STARTING TIMING RUN AT 2020-06-25 10:59:01 AM
140: ++ ./bind.sh --cpu=exclusive --ib=single --cluster=circe -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=12 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
103: STARTING TIMING RUN AT 2020-06-25 10:59:01 AM
142: + eval '    ./bind.sh --cpu=exclusive --ib=single --cluster=circe --     python -u /workspace/bert/run_pretraining.py         --train_batch_size=18     --learning_rate=2.0e-3     --opt_lamb_beta_1=0.86     --opt_lamb_beta_2=0.975     --warmup_proportion=0.0     --max_steps=750     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data_phase2     --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.712     --max_samples_termination=7500000     --eval_iter_start_samples=3000000 --eval_iter_samples=500000     --eval_batch_size=16 --eval_dir=/workspace/evaldata     --cache_eval_data     --output_dir=/results     --fp16 --fused_gelu_bias --dense_seq_output --fused_mha      --allreduce_post_accumulation --allreduce_post_accumulation_fp16     --gradient_accumulation_steps=1     --log_freq=1     --local_rank=14     --bert_config_path=/workspace/phase1/bert_config.
 24: STARTING TIMING RUN AT 2020-06-25 10:59:01 AM
142: json --seed=18263'
111: STARTING TIMING RUN AT 2020-06-25 10:59:01 AM
506: + eval '    ./bind.sh --cpu=exclusive --ib=single --cluster=circe --     python -u /workspace/bert/run_pretraining.py         --train_batch_size=18     --learning_rate=2.0e-3     --opt_lamb_beta_1=0.86     --opt_lamb_beta_2=0.975     --warmup_proportion=0.0     --max_steps=750     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data_phase2     --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.712     --max_samples_termination=7500000     --eval_iter_start_samples=3000000 --eval_iter_samples=500000     --eval_batch_size=16 --eval_dir=/workspace/evaldata     --cache_eval_data     --output_dir=/results     --fp16 --fused_gelu_bias --dense_seq_output --fused_mha      --allreduce_post_accumulation --allreduce_post_accumulation_fp16     --gradient_accumulation_steps=1     --log_freq=1     --local_rank=10     --bert_config_path=/workspace/phase1/bert_config.
 20: Run vars: id 394913 gpus 16 mparams
506: json --seed=18263'
106: STARTING TIMING RUN AT 2020-06-25 10:59:01 AM
511: + eval '    ./bind.sh --cpu=exclusive --ib=single --cluster=circe --     python -u /workspace/bert/run_pretraining.py         --train_batch_size=18     --learning_rate=2.0e-3     --opt_lamb_beta_1=0.86     --opt_lamb_beta_2=0.975     --warmup_proportion=0.0     --max_steps=750     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data_phase2     --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.712     --max_samples_termination=7500000     --eval_iter_start_samples=3000000 --eval_iter_samples=500000     --eval_batch_size=16 --eval_dir=/workspace/evaldata     --cache_eval_data     --output_dir=/results     --fp16 --fused_gelu_bias --dense_seq_output --fused_mha      --allreduce_post_accumulation --allreduce_post_accumulation_fp16     --gradient_accumulation_steps=1     --log_freq=1     --local_rank=15     --bert_config_path=/workspace/phase1/bert_config.
109: Run vars: id 394913 gpus 16 mparams
511: json --seed=18263'
 99: STARTING TIMING RUN AT 2020-06-25 10:59:01 AM
142: ++ ./bind.sh --cpu=exclusive --ib=single --cluster=circe -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=14 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
104: STARTING TIMING RUN AT 2020-06-25 10:59:01 AM
511: ++ ./bind.sh --cpu=exclusive --ib=single --cluster=circe -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=15 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
105: STARTING TIMING RUN AT 2020-06-25 10:59:01 AM
506: ++ ./bind.sh --cpu=exclusive --ib=single --cluster=circe -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=10 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
 96: Run vars: id 394913 gpus 16 mparams
510: + eval '    ./bind.sh --cpu=exclusive --ib=single --cluster=circe --     python -u /workspace/bert/run_pretraining.py         --train_batch_size=18     --learning_rate=2.0e-3     --opt_lamb_beta_1=0.86     --opt_lamb_beta_2=0.975     --warmup_proportion=0.0     --max_steps=750     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data_phase2     --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.712     --max_samples_termination=7500000     --eval_iter_start_samples=3000000 --eval_iter_samples=500000     --eval_batch_size=16 --eval_dir=/workspace/evaldata     --cache_eval_data     --output_dir=/results     --fp16 --fused_gelu_bias --dense_seq_output --fused_mha      --allreduce_post_accumulation --allreduce_post_accumulation_fp16     --gradient_accumulation_steps=1     --log_freq=1     --local_rank=14     --bert_config_path=/workspace/phase1/bert_config.
100: STARTING TIMING RUN AT 2020-06-25 10:59:01 AM
510: json --seed=18263'
107: STARTING TIMING RUN AT 2020-06-25 10:59:01 AM
510: ++ ./bind.sh --cpu=exclusive --ib=single --cluster=circe -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=14 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
 98: STARTING TIMING RUN AT 2020-06-25 10:59:01 AM
 17: slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
101: STARTING TIMING RUN AT 2020-06-25 10:59:01 AM
507: + eval '    ./bind.sh --cpu=exclusive --ib=single --cluster=circe --     python -u /workspace/bert/run_pretraining.py         --train_batch_size=18     --learning_rate=2.0e-3     --opt_lamb_beta_1=0.86     --opt_lamb_beta_2=0.975     --warmup_proportion=0.0     --max_steps=750     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data_phase2     --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.712     --max_samples_termination=7500000     --eval_iter_start_samples=3000000 --eval_iter_samples=500000     --eval_batch_size=16 --eval_dir=/workspace/evaldata     --cache_eval_data     --output_dir=/results     --fp16 --fused_gelu_bias --dense_seq_output --fused_mha      --allreduce_post_accumulation --allreduce_post_accumulation_fp16     --gradient_accumulation_steps=1     --log_freq=1     --local_rank=11     --bert_config_path=/workspace/phase1/bert_config.
108: STARTING TIMING RUN AT 2020-06-25 10:59:01 AM
507: json --seed=18263'
 97: STARTING TIMING RUN AT 2020-06-25 10:59:01 AM
507: ++ ./bind.sh --cpu=exclusive --ib=single --cluster=circe -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=11 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
 30: Run vars: id 394913 gpus 16 mparams
143: + eval '    ./bind.sh --cpu=exclusive --ib=single --cluster=circe --     python -u /workspace/bert/run_pretraining.py         --train_batch_size=18     --learning_rate=2.0e-3     --opt_lamb_beta_1=0.86     --opt_lamb_beta_2=0.975     --warmup_proportion=0.0     --max_steps=750     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data_phase2     --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.712     --max_samples_termination=7500000     --eval_iter_start_samples=3000000 --eval_iter_samples=500000     --eval_batch_size=16 --eval_dir=/workspace/evaldata     --cache_eval_data     --output_dir=/results     --fp16 --fused_gelu_bias --dense_seq_output --fused_mha      --allreduce_post_accumulation --allreduce_post_accumulation_fp16     --gradient_accumulation_steps=1     --log_freq=1     --local_rank=15     --bert_config_path=/workspace/phase1/bert_config.
 19: Run vars: id 394913 gpus 16 mparams
143: json --seed=18263'
 27: Run vars: id 394913 gpus 16 mparams
143: ++ ./bind.sh --cpu=exclusive --ib=single --cluster=circe -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=15 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
 25: Run vars: id 394913 gpus 16 mparams
263: slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
 18: STARTING TIMING RUN AT 2020-06-25 10:59:01 AM
257: slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
 29: Run vars: id 394913 gpus 16 mparams
262: slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
110: STARTING TIMING RUN AT 2020-06-25 10:59:01 AM
260: slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
 28: Run vars: id 394913 gpus 16 mparams
256: slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
 31: Run vars: id 394913 gpus 16 mparams
261: slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
 20: STARTING TIMING RUN AT 2020-06-25 10:59:01 AM
271: slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
 96: STARTING TIMING RUN AT 2020-06-25 10:59:01 AM
133: + eval '    ./bind.sh --cpu=exclusive --ib=single --cluster=circe --     python -u /workspace/bert/run_pretraining.py         --train_batch_size=18     --learning_rate=2.0e-3     --opt_lamb_beta_1=0.86     --opt_lamb_beta_2=0.975     --warmup_proportion=0.0     --max_steps=750     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data_phase2     --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.712     --max_samples_termination=7500000     --eval_iter_start_samples=3000000 --eval_iter_samples=500000     --eval_batch_size=16 --eval_dir=/workspace/evaldata     --cache_eval_data     --output_dir=/results     --fp16 --fused_gelu_bias --dense_seq_output --fused_mha      --allreduce_post_accumulation --allreduce_post_accumulation_fp16     --gradient_accumulation_steps=1     --log_freq=1     --local_rank=5     --bert_config_path=/workspace/phase1/bert_config.j
109: STARTING TIMING RUN AT 2020-06-25 10:59:01 AM
258: slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
 30: STARTING TIMING RUN AT 2020-06-25 10:59:01 AM
133: son --seed=18263'
 16: Run vars: id 394913 gpus 16 mparams
267: slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
 19: STARTING TIMING RUN AT 2020-06-25 10:59:01 AM
270: slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
 21: Run vars: id 394913 gpus 16 mparams
268: slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
 22: Run vars: id 394913 gpus 16 mparams
269: slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
 27: STARTING TIMING RUN AT 2020-06-25 10:59:01 AM
264: slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
 23: Run vars: id 394913 gpus 16 mparams
266: slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
 25: STARTING TIMING RUN AT 2020-06-25 10:59:01 AM
265: slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
 31: STARTING TIMING RUN AT 2020-06-25 10:59:01 AM
257: + eval '    ./bind.sh --cpu=exclusive --ib=single --cluster=circe --     python -u /workspace/bert/run_pretraining.py         --train_batch_size=18     --learning_rate=2.0e-3     --opt_lamb_beta_1=0.86     --opt_lamb_beta_2=0.975     --warmup_proportion=0.0     --max_steps=750     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data_phase2     --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.712     --max_samples_termination=7500000     --eval_iter_start_samples=3000000 --eval_iter_samples=500000     --eval_batch_size=16 --eval_dir=/workspace/evaldata     --cache_eval_data     --output_dir=/results     --fp16 --fused_gelu_bias --dense_seq_output --fused_mha      --allreduce_post_accumulation --allreduce_post_accumulation_fp16     --gradient_accumulation_steps=1     --log_freq=1     --local_rank=1     --bert_config_path=/workspace/phase1/bert_config.j
 29: STARTING TIMING RUN AT 2020-06-25 10:59:01 AM
257: son --seed=18263'
 28: STARTING TIMING RUN AT 2020-06-25 10:59:01 AM
139: + eval '    ./bind.sh --cpu=exclusive --ib=single --cluster=circe --     python -u /workspace/bert/run_pretraining.py         --train_batch_size=18     --learning_rate=2.0e-3     --opt_lamb_beta_1=0.86     --opt_lamb_beta_2=0.975     --warmup_proportion=0.0     --max_steps=750     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data_phase2     --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.712     --max_samples_termination=7500000     --eval_iter_start_samples=3000000 --eval_iter_samples=500000     --eval_batch_size=16 --eval_dir=/workspace/evaldata     --cache_eval_data     --output_dir=/results     --fp16 --fused_gelu_bias --dense_seq_output --fused_mha      --allreduce_post_accumulation --allreduce_post_accumulation_fp16     --gradient_accumulation_steps=1     --log_freq=1     --local_rank=11     --bert_config_path=/workspace/phase1/bert_config.
 16: STARTING TIMING RUN AT 2020-06-25 10:59:01 AM
257: ++ ./bind.sh --cpu=exclusive --ib=single --cluster=circe -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=1 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
 22: STARTING TIMING RUN AT 2020-06-25 10:59:01 AM
139: json --seed=18263'
 23: STARTING TIMING RUN AT 2020-06-25 10:59:01 AM
133: ++ ./bind.sh --cpu=exclusive --ib=single --cluster=circe -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=5 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
 21: STARTING TIMING RUN AT 2020-06-25 10:59:01 AM
139: ++ ./bind.sh --cpu=exclusive --ib=single --cluster=circe -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=11 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
150: Run vars: id 394913 gpus 16 mparams
132: + eval '    ./bind.sh --cpu=exclusive --ib=single --cluster=circe --     python -u /workspace/bert/run_pretraining.py         --train_batch_size=18     --learning_rate=2.0e-3     --opt_lamb_beta_1=0.86     --opt_lamb_beta_2=0.975     --warmup_proportion=0.0     --max_steps=750     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data_phase2     --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.712     --max_samples_termination=7500000     --eval_iter_start_samples=3000000 --eval_iter_samples=500000     --eval_batch_size=16 --eval_dir=/workspace/evaldata     --cache_eval_data     --output_dir=/results     --fp16 --fused_gelu_bias --dense_seq_output --fused_mha      --allreduce_post_accumulation --allreduce_post_accumulation_fp16     --gradient_accumulation_steps=1     --log_freq=1     --local_rank=4     --bert_config_path=/workspace/phase1/bert_config.j
158: Run vars: id 394913 gpus 16 mparams
132: son --seed=18263'
150: STARTING TIMING RUN AT 2020-06-25 10:59:01 AM
132: ++ ./bind.sh --cpu=exclusive --ib=single --cluster=circe -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=4 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
147: Run vars: id 394913 gpus 16 mparams
263: + eval '    ./bind.sh --cpu=exclusive --ib=single --cluster=circe --     python -u /workspace/bert/run_pretraining.py         --train_batch_size=18     --learning_rate=2.0e-3     --opt_lamb_beta_1=0.86     --opt_lamb_beta_2=0.975     --warmup_proportion=0.0     --max_steps=750     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data_phase2     --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.712     --max_samples_termination=7500000     --eval_iter_start_samples=3000000 --eval_iter_samples=500000     --eval_batch_size=16 --eval_dir=/workspace/evaldata     --cache_eval_data     --output_dir=/results     --fp16 --fused_gelu_bias --dense_seq_output --fused_mha      --allreduce_post_accumulation --allreduce_post_accumulation_fp16     --gradient_accumulation_steps=1     --log_freq=1     --local_rank=7     --bert_config_path=/workspace/phase1/bert_config.j
157: Run vars: id 394913 gpus 16 mparams
263: son --seed=18263'
151: Run vars: id 394913 gpus 16 mparams
263: ++ ./bind.sh --cpu=exclusive --ib=single --cluster=circe -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=7 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
149: Run vars: id 394913 gpus 16 mparams
131: + eval '    ./bind.sh --cpu=exclusive --ib=single --cluster=circe --     python -u /workspace/bert/run_pretraining.py         --train_batch_size=18     --learning_rate=2.0e-3     --opt_lamb_beta_1=0.86     --opt_lamb_beta_2=0.975     --warmup_proportion=0.0     --max_steps=750     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data_phase2     --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.712     --max_samples_termination=7500000     --eval_iter_start_samples=3000000 --eval_iter_samples=500000     --eval_batch_size=16 --eval_dir=/workspace/evaldata     --cache_eval_data     --output_dir=/results     --fp16 --fused_gelu_bias --dense_seq_output --fused_mha      --allreduce_post_accumulation --allreduce_post_accumulation_fp16     --gradient_accumulation_steps=1     --log_freq=1     --local_rank=3     --bert_config_path=/workspace/phase1/bert_config.j
148: Run vars: id 394913 gpus 16 mparams
131: son --seed=18263'
159: Run vars: id 394913 gpus 16 mparams
131: ++ ./bind.sh --cpu=exclusive --ib=single --cluster=circe -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=3 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
158: STARTING TIMING RUN AT 2020-06-25 10:59:01 AM
198: slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
146: Run vars: id 394913 gpus 16 mparams
192: slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
153: Run vars: id 394913 gpus 16 mparams
197: slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
144: Run vars: id 394913 gpus 16 mparams
194: slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
155: Run vars: id 394913 gpus 16 mparams
196: slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
156: Run vars: id 394913 gpus 16 mparams
199: slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
157: STARTING TIMING RUN AT 2020-06-25 10:59:01 AM
200: slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
147: STARTING TIMING RUN AT 2020-06-25 10:59:01 AM
193: slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
149: STARTING TIMING RUN AT 2020-06-25 10:59:01 AM
202: slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
151: STARTING TIMING RUN AT 2020-06-25 10:59:01 AM
203: slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
145: Run vars: id 394913 gpus 16 mparams
207: slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
148: STARTING TIMING RUN AT 2020-06-25 10:59:01 AM
201: slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
159: STARTING TIMING RUN AT 2020-06-25 10:59:01 AM
206: slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
152: Run vars: id 394913 gpus 16 mparams
204: slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
153: STARTING TIMING RUN AT 2020-06-25 10:59:01 AM
205: slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
144: STARTING TIMING RUN AT 2020-06-25 10:59:01 AM
202: + eval '    ./bind.sh --cpu=exclusive --ib=single --cluster=circe --     python -u /workspace/bert/run_pretraining.py         --train_batch_size=18     --learning_rate=2.0e-3     --opt_lamb_beta_1=0.86     --opt_lamb_beta_2=0.975     --warmup_proportion=0.0     --max_steps=750     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data_phase2     --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.712     --max_samples_termination=7500000     --eval_iter_start_samples=3000000 --eval_iter_samples=500000     --eval_batch_size=16 --eval_dir=/workspace/evaldata     --cache_eval_data     --output_dir=/results     --fp16 --fused_gelu_bias --dense_seq_output --fused_mha      --allreduce_post_accumulation --allreduce_post_accumulation_fp16     --gradient_accumulation_steps=1     --log_freq=1     --local_rank=10     --bert_config_path=/workspace/phase1/bert_config.
146: STARTING TIMING RUN AT 2020-06-25 10:59:01 AM
202: json --seed=18263'
155: STARTING TIMING RUN AT 2020-06-25 10:59:01 AM
202: ++ ./bind.sh --cpu=exclusive --ib=single --cluster=circe -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=10 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
156: STARTING TIMING RUN AT 2020-06-25 10:59:01 AM
194: + eval '    ./bind.sh --cpu=exclusive --ib=single --cluster=circe --     python -u /workspace/bert/run_pretraining.py         --train_batch_size=18     --learning_rate=2.0e-3     --opt_lamb_beta_1=0.86     --opt_lamb_beta_2=0.975     --warmup_proportion=0.0     --max_steps=750     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data_phase2     --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.712     --max_samples_termination=7500000     --eval_iter_start_samples=3000000 --eval_iter_samples=500000     --eval_batch_size=16 --eval_dir=/workspace/evaldata     --cache_eval_data     --output_dir=/results     --fp16 --fused_gelu_bias --dense_seq_output --fused_mha      --allreduce_post_accumulation --allreduce_post_accumulation_fp16     --gradient_accumulation_steps=1     --log_freq=1     --local_rank=2     --bert_config_path=/workspace/phase1/bert_config.j
154: Run vars: id 394913 gpus 16 mparams
194: son --seed=18263'
145: STARTING TIMING RUN AT 2020-06-25 10:59:01 AM
270: + eval '    ./bind.sh --cpu=exclusive --ib=single --cluster=circe --     python -u /workspace/bert/run_pretraining.py         --train_batch_size=18     --learning_rate=2.0e-3     --opt_lamb_beta_1=0.86     --opt_lamb_beta_2=0.975     --warmup_proportion=0.0     --max_steps=750     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data_phase2     --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.712     --max_samples_termination=7500000     --eval_iter_start_samples=3000000 --eval_iter_samples=500000     --eval_batch_size=16 --eval_dir=/workspace/evaldata     --cache_eval_data     --output_dir=/results     --fp16 --fused_gelu_bias --dense_seq_output --fused_mha      --allreduce_post_accumulation --allreduce_post_accumulation_fp16     --gradient_accumulation_steps=1     --log_freq=1     --local_rank=14     --bert_config_path=/workspace/phase1/bert_config.
152: STARTING TIMING RUN AT 2020-06-25 10:59:01 AM
270: json --seed=18263'
154: STARTING TIMING RUN AT 2020-06-25 10:59:01 AM
270: ++ ./bind.sh --cpu=exclusive --ib=single --cluster=circe -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=14 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
371: Run vars: id 394913 gpus 16 mparams
259: + eval '    ./bind.sh --cpu=exclusive --ib=single --cluster=circe --     python -u /workspace/bert/run_pretraining.py         --train_batch_size=18     --learning_rate=2.0e-3     --opt_lamb_beta_1=0.86     --opt_lamb_beta_2=0.975     --warmup_proportion=0.0     --max_steps=750     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data_phase2     --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.712     --max_samples_termination=7500000     --eval_iter_start_samples=3000000 --eval_iter_samples=500000     --eval_batch_size=16 --eval_dir=/workspace/evaldata     --cache_eval_data     --output_dir=/results     --fp16 --fused_gelu_bias --dense_seq_output --fused_mha      --allreduce_post_accumulation --allreduce_post_accumulation_fp16     --gradient_accumulation_steps=1     --log_freq=1     --local_rank=3     --bert_config_path=/workspace/phase1/bert_config.j
338: Run vars: id 394913 gpus 16 mparams
259: son --seed=18263'
337: Run vars: id 394913 gpus 16 mparams
259: ++ ./bind.sh --cpu=exclusive --ib=single --cluster=circe -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=3 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
338: STARTING TIMING RUN AT 2020-06-25 10:59:01 AM
256: + eval '    ./bind.sh --cpu=exclusive --ib=single --cluster=circe --     python -u /workspace/bert/run_pretraining.py         --train_batch_size=18     --learning_rate=2.0e-3     --opt_lamb_beta_1=0.86     --opt_lamb_beta_2=0.975     --warmup_proportion=0.0     --max_steps=750     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data_phase2     --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.712     --max_samples_termination=7500000     --eval_iter_start_samples=3000000 --eval_iter_samples=500000     --eval_batch_size=16 --eval_dir=/workspace/evaldata     --cache_eval_data     --output_dir=/results     --fp16 --fused_gelu_bias --dense_seq_output --fused_mha      --allreduce_post_accumulation --allreduce_post_accumulation_fp16     --gradient_accumulation_steps=1     --log_freq=1     --local_rank=0     --bert_config_path=/workspace/phase1/bert_config.j
337: STARTING TIMING RUN AT 2020-06-25 10:59:01 AM
256: son --seed=18263'
373: Run vars: id 394913 gpus 16 mparams
256: ++ ./bind.sh --cpu=exclusive --ib=single --cluster=circe -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=0 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
371: STARTING TIMING RUN AT 2020-06-25 10:59:01 AM
194: ++ ./bind.sh --cpu=exclusive --ib=single --cluster=circe -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=2 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
340: Run vars: id 394913 gpus 16 mparams
196: + eval '    ./bind.sh --cpu=exclusive --ib=single --cluster=circe --     python -u /workspace/bert/run_pretraining.py         --train_batch_size=18     --learning_rate=2.0e-3     --opt_lamb_beta_1=0.86     --opt_lamb_beta_2=0.975     --warmup_proportion=0.0     --max_steps=750     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data_phase2     --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.712     --max_samples_termination=7500000     --eval_iter_start_samples=3000000 --eval_iter_samples=500000     --eval_batch_size=16 --eval_dir=/workspace/evaldata     --cache_eval_data     --output_dir=/results     --fp16 --fused_gelu_bias --dense_seq_output --fused_mha      --allreduce_post_accumulation --allreduce_post_accumulation_fp16     --gradient_accumulation_steps=1     --log_freq=1     --local_rank=4     --bert_config_path=/workspace/phase1/bert_config.j
336: Run vars: id 394913 gpus 16 mparams
196: son --seed=18263'
348: Run vars: id 394913 gpus 16 mparams
260: + eval '    ./bind.sh --cpu=exclusive --ib=single --cluster=circe --     python -u /workspace/bert/run_pretraining.py         --train_batch_size=18     --learning_rate=2.0e-3     --opt_lamb_beta_1=0.86     --opt_lamb_beta_2=0.975     --warmup_proportion=0.0     --max_steps=750     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data_phase2     --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.712     --max_samples_termination=7500000     --eval_iter_start_samples=3000000 --eval_iter_samples=500000     --eval_batch_size=16 --eval_dir=/workspace/evaldata     --cache_eval_data     --output_dir=/results     --fp16 --fused_gelu_bias --dense_seq_output --fused_mha      --allreduce_post_accumulation --allreduce_post_accumulation_fp16     --gradient_accumulation_steps=1     --log_freq=1     --local_rank=4     --bert_config_path=/workspace/phase1/bert_config.j
351: Run vars: id 394913 gpus 16 mparams
260: son --seed=18263'
347: Run vars: id 394913 gpus 16 mparams
260: ++ ./bind.sh --cpu=exclusive --ib=single --cluster=circe -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=4 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
339: Run vars: id 394913 gpus 16 mparams
267: + eval '    ./bind.sh --cpu=exclusive --ib=single --cluster=circe --     python -u /workspace/bert/run_pretraining.py         --train_batch_size=18     --learning_rate=2.0e-3     --opt_lamb_beta_1=0.86     --opt_lamb_beta_2=0.975     --warmup_proportion=0.0     --max_steps=750     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data_phase2     --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.712     --max_samples_termination=7500000     --eval_iter_start_samples=3000000 --eval_iter_samples=500000     --eval_batch_size=16 --eval_dir=/workspace/evaldata     --cache_eval_data     --output_dir=/results     --fp16 --fused_gelu_bias --dense_seq_output --fused_mha      --allreduce_post_accumulation --allreduce_post_accumulation_fp16     --gradient_accumulation_steps=1     --log_freq=1     --local_rank=11     --bert_config_path=/workspace/phase1/bert_config.
340: STARTING TIMING RUN AT 2020-06-25 10:59:01 AM
267: json --seed=18263'
344: Run vars: id 394913 gpus 16 mparams
261: + eval '    ./bind.sh --cpu=exclusive --ib=single --cluster=circe --     python -u /workspace/bert/run_pretraining.py         --train_batch_size=18     --learning_rate=2.0e-3     --opt_lamb_beta_1=0.86     --opt_lamb_beta_2=0.975     --warmup_proportion=0.0     --max_steps=750     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data_phase2     --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.712     --max_samples_termination=7500000     --eval_iter_start_samples=3000000 --eval_iter_samples=500000     --eval_batch_size=16 --eval_dir=/workspace/evaldata     --cache_eval_data     --output_dir=/results     --fp16 --fused_gelu_bias --dense_seq_output --fused_mha      --allreduce_post_accumulation --allreduce_post_accumulation_fp16     --gradient_accumulation_steps=1     --log_freq=1     --local_rank=5     --bert_config_path=/workspace/phase1/bert_config.j
345: Run vars: id 394913 gpus 16 mparams
261: son --seed=18263'
346: Run vars: id 394913 gpus 16 mparams
265: + eval '    ./bind.sh --cpu=exclusive --ib=single --cluster=circe --     python -u /workspace/bert/run_pretraining.py         --train_batch_size=18     --learning_rate=2.0e-3     --opt_lamb_beta_1=0.86     --opt_lamb_beta_2=0.975     --warmup_proportion=0.0     --max_steps=750     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data_phase2     --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.712     --max_samples_termination=7500000     --eval_iter_start_samples=3000000 --eval_iter_samples=500000     --eval_batch_size=16 --eval_dir=/workspace/evaldata     --cache_eval_data     --output_dir=/results     --fp16 --fused_gelu_bias --dense_seq_output --fused_mha      --allreduce_post_accumulation --allreduce_post_accumulation_fp16     --gradient_accumulation_steps=1     --log_freq=1     --local_rank=9     --bert_config_path=/workspace/phase1/bert_config.j
336: STARTING TIMING RUN AT 2020-06-25 10:59:01 AM
265: son --seed=18263'
349: Run vars: id 394913 gpus 16 mparams
267: ++ ./bind.sh --cpu=exclusive --ib=single --cluster=circe -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=11 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
377: Run vars: id 394913 gpus 16 mparams
261: ++ ./bind.sh --cpu=exclusive --ib=single --cluster=circe -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=5 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
382: Run vars: id 394913 gpus 16 mparams
265: ++ ./bind.sh --cpu=exclusive --ib=single --cluster=circe -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=9 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
368: Run vars: id 394913 gpus 16 mparams
262: + eval '    ./bind.sh --cpu=exclusive --ib=single --cluster=circe --     python -u /workspace/bert/run_pretraining.py         --train_batch_size=18     --learning_rate=2.0e-3     --opt_lamb_beta_1=0.86     --opt_lamb_beta_2=0.975     --warmup_proportion=0.0     --max_steps=750     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data_phase2     --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.712     --max_samples_termination=7500000     --eval_iter_start_samples=3000000 --eval_iter_samples=500000     --eval_batch_size=16 --eval_dir=/workspace/evaldata     --cache_eval_data     --output_dir=/results     --fp16 --fused_gelu_bias --dense_seq_output --fused_mha      --allreduce_post_accumulation --allreduce_post_accumulation_fp16     --gradient_accumulation_steps=1     --log_freq=1     --local_rank=6     --bert_config_path=/workspace/phase1/bert_config.j
374: Run vars: id 394913 gpus 16 mparams
262: son --seed=18263'
376: Run vars: id 394913 gpus 16 mparams
262: ++ ./bind.sh --cpu=exclusive --ib=single --cluster=circe -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=6 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
378: Run vars: id 394913 gpus 16 mparams
271: + eval '    ./bind.sh --cpu=exclusive --ib=single --cluster=circe --     python -u /workspace/bert/run_pretraining.py         --train_batch_size=18     --learning_rate=2.0e-3     --opt_lamb_beta_1=0.86     --opt_lamb_beta_2=0.975     --warmup_proportion=0.0     --max_steps=750     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data_phase2     --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.712     --max_samples_termination=7500000     --eval_iter_start_samples=3000000 --eval_iter_samples=500000     --eval_batch_size=16 --eval_dir=/workspace/evaldata     --cache_eval_data     --output_dir=/results     --fp16 --fused_gelu_bias --dense_seq_output --fused_mha      --allreduce_post_accumulation --allreduce_post_accumulation_fp16     --gradient_accumulation_steps=1     --log_freq=1     --local_rank=15     --bert_config_path=/workspace/phase1/bert_config.
375: Run vars: id 394913 gpus 16 mparams
271: json --seed=18263'
381: Run vars: id 394913 gpus 16 mparams
271: ++ ./bind.sh --cpu=exclusive --ib=single --cluster=circe -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=15 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
373: STARTING TIMING RUN AT 2020-06-25 10:59:01 AM
258: + eval '    ./bind.sh --cpu=exclusive --ib=single --cluster=circe --     python -u /workspace/bert/run_pretraining.py         --train_batch_size=18     --learning_rate=2.0e-3     --opt_lamb_beta_1=0.86     --opt_lamb_beta_2=0.975     --warmup_proportion=0.0     --max_steps=750     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data_phase2     --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.712     --max_samples_termination=7500000     --eval_iter_start_samples=3000000 --eval_iter_samples=500000     --eval_batch_size=16 --eval_dir=/workspace/evaldata     --cache_eval_data     --output_dir=/results     --fp16 --fused_gelu_bias --dense_seq_output --fused_mha      --allreduce_post_accumulation --allreduce_post_accumulation_fp16     --gradient_accumulation_steps=1     --log_freq=1     --local_rank=2     --bert_config_path=/workspace/phase1/bert_config.j
377: STARTING TIMING RUN AT 2020-06-25 10:59:01 AM
258: son --seed=18263'
379: Run vars: id 394913 gpus 16 mparams
258: ++ ./bind.sh --cpu=exclusive --ib=single --cluster=circe -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=2 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
342: Run vars: id 394913 gpus 16 mparams
196: ++ ./bind.sh --cpu=exclusive --ib=single --cluster=circe -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=4 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
343: Run vars: id 394913 gpus 16 mparams
193: + eval '    ./bind.sh --cpu=exclusive --ib=single --cluster=circe --     python -u /workspace/bert/run_pretraining.py         --train_batch_size=18     --learning_rate=2.0e-3     --opt_lamb_beta_1=0.86     --opt_lamb_beta_2=0.975     --warmup_proportion=0.0     --max_steps=750     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data_phase2     --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.712     --max_samples_termination=7500000     --eval_iter_start_samples=3000000 --eval_iter_samples=500000     --eval_batch_size=16 --eval_dir=/workspace/evaldata     --cache_eval_data     --output_dir=/results     --fp16 --fused_gelu_bias --dense_seq_output --fused_mha      --allreduce_post_accumulation --allreduce_post_accumulation_fp16     --gradient_accumulation_steps=1     --log_freq=1     --local_rank=1     --bert_config_path=/workspace/phase1/bert_config.j
350: Run vars: id 394913 gpus 16 mparams
193: son --seed=18263'
341: Run vars: id 394913 gpus 16 mparams
266: + eval '    ./bind.sh --cpu=exclusive --ib=single --cluster=circe --     python -u /workspace/bert/run_pretraining.py         --train_batch_size=18     --learning_rate=2.0e-3     --opt_lamb_beta_1=0.86     --opt_lamb_beta_2=0.975     --warmup_proportion=0.0     --max_steps=750     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data_phase2     --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.712     --max_samples_termination=7500000     --eval_iter_start_samples=3000000 --eval_iter_samples=500000     --eval_batch_size=16 --eval_dir=/workspace/evaldata     --cache_eval_data     --output_dir=/results     --fp16 --fused_gelu_bias --dense_seq_output --fused_mha      --allreduce_post_accumulation --allreduce_post_accumulation_fp16     --gradient_accumulation_steps=1     --log_freq=1     --local_rank=10     --bert_config_path=/workspace/phase1/bert_config.
348: STARTING TIMING RUN AT 2020-06-25 10:59:01 AM
266: json --seed=18263'
370: Run vars: id 394913 gpus 16 mparams
193: ++ ./bind.sh --cpu=exclusive --ib=single --cluster=circe -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=1 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
351: STARTING TIMING RUN AT 2020-06-25 10:59:01 AM
264: + eval '    ./bind.sh --cpu=exclusive --ib=single --cluster=circe --     python -u /workspace/bert/run_pretraining.py         --train_batch_size=18     --learning_rate=2.0e-3     --opt_lamb_beta_1=0.86     --opt_lamb_beta_2=0.975     --warmup_proportion=0.0     --max_steps=750     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data_phase2     --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.712     --max_samples_termination=7500000     --eval_iter_start_samples=3000000 --eval_iter_samples=500000     --eval_batch_size=16 --eval_dir=/workspace/evaldata     --cache_eval_data     --output_dir=/results     --fp16 --fused_gelu_bias --dense_seq_output --fused_mha      --allreduce_post_accumulation --allreduce_post_accumulation_fp16     --gradient_accumulation_steps=1     --log_freq=1     --local_rank=8     --bert_config_path=/workspace/phase1/bert_config.j
380: Run vars: id 394913 gpus 16 mparams
264: son --seed=18263'
383: Run vars: id 394913 gpus 16 mparams
192: + eval '    ./bind.sh --cpu=exclusive --ib=single --cluster=circe --     python -u /workspace/bert/run_pretraining.py         --train_batch_size=18     --learning_rate=2.0e-3     --opt_lamb_beta_1=0.86     --opt_lamb_beta_2=0.975     --warmup_proportion=0.0     --max_steps=750     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data_phase2     --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.712     --max_samples_termination=7500000     --eval_iter_start_samples=3000000 --eval_iter_samples=500000     --eval_batch_size=16 --eval_dir=/workspace/evaldata     --cache_eval_data     --output_dir=/results     --fp16 --fused_gelu_bias --dense_seq_output --fused_mha      --allreduce_post_accumulation --allreduce_post_accumulation_fp16     --gradient_accumulation_steps=1     --log_freq=1     --local_rank=0     --bert_config_path=/workspace/phase1/bert_config.j
372: Run vars: id 394913 gpus 16 mparams
266: ++ ./bind.sh --cpu=exclusive --ib=single --cluster=circe -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=10 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
382: STARTING TIMING RUN AT 2020-06-25 10:59:01 AM
192: son --seed=18263'
374: STARTING TIMING RUN AT 2020-06-25 10:59:01 AM
192: ++ ./bind.sh --cpu=exclusive --ib=single --cluster=circe -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=0 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
369: Run vars: id 394913 gpus 16 mparams
203: + eval '    ./bind.sh --cpu=exclusive --ib=single --cluster=circe --     python -u /workspace/bert/run_pretraining.py         --train_batch_size=18     --learning_rate=2.0e-3     --opt_lamb_beta_1=0.86     --opt_lamb_beta_2=0.975     --warmup_proportion=0.0     --max_steps=750     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data_phase2     --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.712     --max_samples_termination=7500000     --eval_iter_start_samples=3000000 --eval_iter_samples=500000     --eval_batch_size=16 --eval_dir=/workspace/evaldata     --cache_eval_data     --output_dir=/results     --fp16 --fused_gelu_bias --dense_seq_output --fused_mha      --allreduce_post_accumulation --allreduce_post_accumulation_fp16     --gradient_accumulation_steps=1     --log_freq=1     --local_rank=11     --bert_config_path=/workspace/phase1/bert_config.
376: STARTING TIMING RUN AT 2020-06-25 10:59:01 AM
203: json --seed=18263'
344: STARTING TIMING RUN AT 2020-06-25 10:59:01 AM
197: + eval '    ./bind.sh --cpu=exclusive --ib=single --cluster=circe --     python -u /workspace/bert/run_pretraining.py         --train_batch_size=18     --learning_rate=2.0e-3     --opt_lamb_beta_1=0.86     --opt_lamb_beta_2=0.975     --warmup_proportion=0.0     --max_steps=750     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data_phase2     --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.712     --max_samples_termination=7500000     --eval_iter_start_samples=3000000 --eval_iter_samples=500000     --eval_batch_size=16 --eval_dir=/workspace/evaldata     --cache_eval_data     --output_dir=/results     --fp16 --fused_gelu_bias --dense_seq_output --fused_mha      --allreduce_post_accumulation --allreduce_post_accumulation_fp16     --gradient_accumulation_steps=1     --log_freq=1     --local_rank=5     --bert_config_path=/workspace/phase1/bert_config.j
346: STARTING TIMING RUN AT 2020-06-25 10:59:01 AM
197: son --seed=18263'
339: STARTING TIMING RUN AT 2020-06-25 10:59:01 AM
197: ++ ./bind.sh --cpu=exclusive --ib=single --cluster=circe -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=5 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
368: STARTING TIMING RUN AT 2020-06-25 10:59:01 AM
203: ++ ./bind.sh --cpu=exclusive --ib=single --cluster=circe -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=11 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
347: STARTING TIMING RUN AT 2020-06-25 10:59:01 AM
264: ++ ./bind.sh --cpu=exclusive --ib=single --cluster=circe -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=8 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
375: STARTING TIMING RUN AT 2020-06-25 10:59:01 AM
269: + eval '    ./bind.sh --cpu=exclusive --ib=single --cluster=circe --     python -u /workspace/bert/run_pretraining.py         --train_batch_size=18     --learning_rate=2.0e-3     --opt_lamb_beta_1=0.86     --opt_lamb_beta_2=0.975     --warmup_proportion=0.0     --max_steps=750     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data_phase2     --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.712     --max_samples_termination=7500000     --eval_iter_start_samples=3000000 --eval_iter_samples=500000     --eval_batch_size=16 --eval_dir=/workspace/evaldata     --cache_eval_data     --output_dir=/results     --fp16 --fused_gelu_bias --dense_seq_output --fused_mha      --allreduce_post_accumulation --allreduce_post_accumulation_fp16     --gradient_accumulation_steps=1     --log_freq=1     --local_rank=13     --bert_config_path=/workspace/phase1/bert_config.
378: STARTING TIMING RUN AT 2020-06-25 10:59:01 AM
269: json --seed=18263'
381: STARTING TIMING RUN AT 2020-06-25 10:59:01 AM
207: + eval '    ./bind.sh --cpu=exclusive --ib=single --cluster=circe --     python -u /workspace/bert/run_pretraining.py         --train_batch_size=18     --learning_rate=2.0e-3     --opt_lamb_beta_1=0.86     --opt_lamb_beta_2=0.975     --warmup_proportion=0.0     --max_steps=750     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data_phase2     --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.712     --max_samples_termination=7500000     --eval_iter_start_samples=3000000 --eval_iter_samples=500000     --eval_batch_size=16 --eval_dir=/workspace/evaldata     --cache_eval_data     --output_dir=/results     --fp16 --fused_gelu_bias --dense_seq_output --fused_mha      --allreduce_post_accumulation --allreduce_post_accumulation_fp16     --gradient_accumulation_steps=1     --log_freq=1     --local_rank=15     --bert_config_path=/workspace/phase1/bert_config.
343: STARTING TIMING RUN AT 2020-06-25 10:59:01 AM
268: + eval '    ./bind.sh --cpu=exclusive --ib=single --cluster=circe --     python -u /workspace/bert/run_pretraining.py         --train_batch_size=18     --learning_rate=2.0e-3     --opt_lamb_beta_1=0.86     --opt_lamb_beta_2=0.975     --warmup_proportion=0.0     --max_steps=750     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data_phase2     --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.712     --max_samples_termination=7500000     --eval_iter_start_samples=3000000 --eval_iter_samples=500000     --eval_batch_size=16 --eval_dir=/workspace/evaldata     --cache_eval_data     --output_dir=/results     --fp16 --fused_gelu_bias --dense_seq_output --fused_mha      --allreduce_post_accumulation --allreduce_post_accumulation_fp16     --gradient_accumulation_steps=1     --log_freq=1     --local_rank=12     --bert_config_path=/workspace/phase1/bert_config.
342: STARTING TIMING RUN AT 2020-06-25 10:59:01 AM
207: json --seed=18263'
379: STARTING TIMING RUN AT 2020-06-25 10:59:01 AM
268: json --seed=18263'
349: STARTING TIMING RUN AT 2020-06-25 10:59:01 AM
207: ++ ./bind.sh --cpu=exclusive --ib=single --cluster=circe -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=15 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
345: STARTING TIMING RUN AT 2020-06-25 10:59:01 AM
268: ++ ./bind.sh --cpu=exclusive --ib=single --cluster=circe -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=12 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
341: STARTING TIMING RUN AT 2020-06-25 10:59:01 AM
269: ++ ./bind.sh --cpu=exclusive --ib=single --cluster=circe -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=13 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
370: STARTING TIMING RUN AT 2020-06-25 10:59:01 AM
198: + eval '    ./bind.sh --cpu=exclusive --ib=single --cluster=circe --     python -u /workspace/bert/run_pretraining.py         --train_batch_size=18     --learning_rate=2.0e-3     --opt_lamb_beta_1=0.86     --opt_lamb_beta_2=0.975     --warmup_proportion=0.0     --max_steps=750     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data_phase2     --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.712     --max_samples_termination=7500000     --eval_iter_start_samples=3000000 --eval_iter_samples=500000     --eval_batch_size=16 --eval_dir=/workspace/evaldata     --cache_eval_data     --output_dir=/results     --fp16 --fused_gelu_bias --dense_seq_output --fused_mha      --allreduce_post_accumulation --allreduce_post_accumulation_fp16     --gradient_accumulation_steps=1     --log_freq=1     --local_rank=6     --bert_config_path=/workspace/phase1/bert_config.j
350: STARTING TIMING RUN AT 2020-06-25 10:59:01 AM
198: son --seed=18263'
380: STARTING TIMING RUN AT 2020-06-25 10:59:01 AM
198: ++ ./bind.sh --cpu=exclusive --ib=single --cluster=circe -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=6 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
369: STARTING TIMING RUN AT 2020-06-25 10:59:01 AM
201: + eval '    ./bind.sh --cpu=exclusive --ib=single --cluster=circe --     python -u /workspace/bert/run_pretraining.py         --train_batch_size=18     --learning_rate=2.0e-3     --opt_lamb_beta_1=0.86     --opt_lamb_beta_2=0.975     --warmup_proportion=0.0     --max_steps=750     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data_phase2     --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.712     --max_samples_termination=7500000     --eval_iter_start_samples=3000000 --eval_iter_samples=500000     --eval_batch_size=16 --eval_dir=/workspace/evaldata     --cache_eval_data     --output_dir=/results     --fp16 --fused_gelu_bias --dense_seq_output --fused_mha      --allreduce_post_accumulation --allreduce_post_accumulation_fp16     --gradient_accumulation_steps=1     --log_freq=1     --local_rank=9     --bert_config_path=/workspace/phase1/bert_config.j
372: STARTING TIMING RUN AT 2020-06-25 10:59:01 AM
201: son --seed=18263'
383: STARTING TIMING RUN AT 2020-06-25 10:59:01 AM
201: ++ ./bind.sh --cpu=exclusive --ib=single --cluster=circe -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=9 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
385: Run vars: id 394913 gpus 16 mparams
205: + eval '    ./bind.sh --cpu=exclusive --ib=single --cluster=circe --     python -u /workspace/bert/run_pretraining.py         --train_batch_size=18     --learning_rate=2.0e-3     --opt_lamb_beta_1=0.86     --opt_lamb_beta_2=0.975     --warmup_proportion=0.0     --max_steps=750     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data_phase2     --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.712     --max_samples_termination=7500000     --eval_iter_start_samples=3000000 --eval_iter_samples=500000     --eval_batch_size=16 --eval_dir=/workspace/evaldata     --cache_eval_data     --output_dir=/results     --fp16 --fused_gelu_bias --dense_seq_output --fused_mha      --allreduce_post_accumulation --allreduce_post_accumulation_fp16     --gradient_accumulation_steps=1     --log_freq=1     --local_rank=13     --bert_config_path=/workspace/phase1/bert_config.
387: Run vars: id 394913 gpus 16 mparams
205: json --seed=18263'
399: Run vars: id 394913 gpus 16 mparams
205: ++ ./bind.sh --cpu=exclusive --ib=single --cluster=circe -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=13 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
385: STARTING TIMING RUN AT 2020-06-25 10:59:01 AM
206: + eval '    ./bind.sh --cpu=exclusive --ib=single --cluster=circe --     python -u /workspace/bert/run_pretraining.py         --train_batch_size=18     --learning_rate=2.0e-3     --opt_lamb_beta_1=0.86     --opt_lamb_beta_2=0.975     --warmup_proportion=0.0     --max_steps=750     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data_phase2     --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.712     --max_samples_termination=7500000     --eval_iter_start_samples=3000000 --eval_iter_samples=500000     --eval_batch_size=16 --eval_dir=/workspace/evaldata     --cache_eval_data     --output_dir=/results     --fp16 --fused_gelu_bias --dense_seq_output --fused_mha      --allreduce_post_accumulation --allreduce_post_accumulation_fp16     --gradient_accumulation_steps=1     --log_freq=1     --local_rank=14     --bert_config_path=/workspace/phase1/bert_config.
388: Run vars: id 394913 gpus 16 mparams
206: json --seed=18263'
397: Run vars: id 394913 gpus 16 mparams
206: ++ ./bind.sh --cpu=exclusive --ib=single --cluster=circe -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=14 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
387: STARTING TIMING RUN AT 2020-06-25 10:59:01 AM
195: + eval '    ./bind.sh --cpu=exclusive --ib=single --cluster=circe --     python -u /workspace/bert/run_pretraining.py         --train_batch_size=18     --learning_rate=2.0e-3     --opt_lamb_beta_1=0.86     --opt_lamb_beta_2=0.975     --warmup_proportion=0.0     --max_steps=750     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data_phase2     --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.712     --max_samples_termination=7500000     --eval_iter_start_samples=3000000 --eval_iter_samples=500000     --eval_batch_size=16 --eval_dir=/workspace/evaldata     --cache_eval_data     --output_dir=/results     --fp16 --fused_gelu_bias --dense_seq_output --fused_mha      --allreduce_post_accumulation --allreduce_post_accumulation_fp16     --gradient_accumulation_steps=1     --log_freq=1     --local_rank=3     --bert_config_path=/workspace/phase1/bert_config.j
398: Run vars: id 394913 gpus 16 mparams
195: son --seed=18263'
384: Run vars: id 394913 gpus 16 mparams
195: ++ ./bind.sh --cpu=exclusive --ib=single --cluster=circe -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=3 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
386: Run vars: id 394913 gpus 16 mparams
204: + eval '    ./bind.sh --cpu=exclusive --ib=single --cluster=circe --     python -u /workspace/bert/run_pretraining.py         --train_batch_size=18     --learning_rate=2.0e-3     --opt_lamb_beta_1=0.86     --opt_lamb_beta_2=0.975     --warmup_proportion=0.0     --max_steps=750     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data_phase2     --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.712     --max_samples_termination=7500000     --eval_iter_start_samples=3000000 --eval_iter_samples=500000     --eval_batch_size=16 --eval_dir=/workspace/evaldata     --cache_eval_data     --output_dir=/results     --fp16 --fused_gelu_bias --dense_seq_output --fused_mha      --allreduce_post_accumulation --allreduce_post_accumulation_fp16     --gradient_accumulation_steps=1     --log_freq=1     --local_rank=12     --bert_config_path=/workspace/phase1/bert_config.
395: Run vars: id 394913 gpus 16 mparams
479: + eval '    ./bind.sh --cpu=exclusive --ib=single --cluster=circe --     python -u /workspace/bert/run_pretraining.py         --train_batch_size=18     --learning_rate=2.0e-3     --opt_lamb_beta_1=0.86     --opt_lamb_beta_2=0.975     --warmup_proportion=0.0     --max_steps=750     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data_phase2     --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.712     --max_samples_termination=7500000     --eval_iter_start_samples=3000000 --eval_iter_samples=500000     --eval_batch_size=16 --eval_dir=/workspace/evaldata     --cache_eval_data     --output_dir=/results     --fp16 --fused_gelu_bias --dense_seq_output --fused_mha      --allreduce_post_accumulation --allreduce_post_accumulation_fp16     --gradient_accumulation_steps=1     --log_freq=1     --local_rank=15     --bert_config_path=/workspace/phase1/bert_config.
389: Run vars: id 394913 gpus 16 mparams
204: json --seed=18263'
399: STARTING TIMING RUN AT 2020-06-25 10:59:01 AM
479: json --seed=18263'
390: Run vars: id 394913 gpus 16 mparams
479: ++ ./bind.sh --cpu=exclusive --ib=single --cluster=circe -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=15 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
391: Run vars: id 394913 gpus 16 mparams
204: ++ ./bind.sh --cpu=exclusive --ib=single --cluster=circe -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=12 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
388: STARTING TIMING RUN AT 2020-06-25 10:59:01 AM
199: + eval '    ./bind.sh --cpu=exclusive --ib=single --cluster=circe --     python -u /workspace/bert/run_pretraining.py         --train_batch_size=18     --learning_rate=2.0e-3     --opt_lamb_beta_1=0.86     --opt_lamb_beta_2=0.975     --warmup_proportion=0.0     --max_steps=750     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data_phase2     --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.712     --max_samples_termination=7500000     --eval_iter_start_samples=3000000 --eval_iter_samples=500000     --eval_batch_size=16 --eval_dir=/workspace/evaldata     --cache_eval_data     --output_dir=/results     --fp16 --fused_gelu_bias --dense_seq_output --fused_mha      --allreduce_post_accumulation --allreduce_post_accumulation_fp16     --gradient_accumulation_steps=1     --log_freq=1     --local_rank=7     --bert_config_path=/workspace/phase1/bert_config.j
392: Run vars: id 394913 gpus 16 mparams
199: son --seed=18263'
393: Run vars: id 394913 gpus 16 mparams
199: ++ ./bind.sh --cpu=exclusive --ib=single --cluster=circe -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=7 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
397: STARTING TIMING RUN AT 2020-06-25 10:59:01 AM
200: + eval '    ./bind.sh --cpu=exclusive --ib=single --cluster=circe --     python -u /workspace/bert/run_pretraining.py         --train_batch_size=18     --learning_rate=2.0e-3     --opt_lamb_beta_1=0.86     --opt_lamb_beta_2=0.975     --warmup_proportion=0.0     --max_steps=750     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data_phase2     --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.712     --max_samples_termination=7500000     --eval_iter_start_samples=3000000 --eval_iter_samples=500000     --eval_batch_size=16 --eval_dir=/workspace/evaldata     --cache_eval_data     --output_dir=/results     --fp16 --fused_gelu_bias --dense_seq_output --fused_mha      --allreduce_post_accumulation --allreduce_post_accumulation_fp16     --gradient_accumulation_steps=1     --log_freq=1     --local_rank=8     --bert_config_path=/workspace/phase1/bert_config.j
396: Run vars: id 394913 gpus 16 mparams
200: son --seed=18263'
394: Run vars: id 394913 gpus 16 mparams
200: ++ ./bind.sh --cpu=exclusive --ib=single --cluster=circe -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=8 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
398: STARTING TIMING RUN AT 2020-06-25 10:59:01 AM
180: slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
384: STARTING TIMING RUN AT 2020-06-25 10:59:01 AM
182: slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
395: STARTING TIMING RUN AT 2020-06-25 10:59:01 AM
181: slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
386: STARTING TIMING RUN AT 2020-06-25 10:59:01 AM
179: slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
389: STARTING TIMING RUN AT 2020-06-25 10:59:01 AM
183: slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
391: STARTING TIMING RUN AT 2020-06-25 10:59:01 AM
185: slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
392: STARTING TIMING RUN AT 2020-06-25 10:59:01 AM
178: slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
241: Run vars: id 394913 gpus 16 mparams
177: slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
242: Run vars: id 394913 gpus 16 mparams
184: slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
242: STARTING TIMING RUN AT 2020-06-25 10:59:01 AM
190: slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
241: STARTING TIMING RUN AT 2020-06-25 10:59:01 AM
187: slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
393: STARTING TIMING RUN AT 2020-06-25 10:59:01 AM
191: slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
396: STARTING TIMING RUN AT 2020-06-25 10:59:01 AM
186: slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
238: num_sockets = 2 num_nodes=2 cores_per_socket=24
189: slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
390: STARTING TIMING RUN AT 2020-06-25 10:59:01 AM
188: slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
394: STARTING TIMING RUN AT 2020-06-25 10:59:01 AM
178: + eval '    ./bind.sh --cpu=exclusive --ib=single --cluster=circe --     python -u /workspace/bert/run_pretraining.py         --train_batch_size=18     --learning_rate=2.0e-3     --opt_lamb_beta_1=0.86     --opt_lamb_beta_2=0.975     --warmup_proportion=0.0     --max_steps=750     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data_phase2     --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.712     --max_samples_termination=7500000     --eval_iter_start_samples=3000000 --eval_iter_samples=500000     --eval_batch_size=16 --eval_dir=/workspace/evaldata     --cache_eval_data     --output_dir=/results     --fp16 --fused_gelu_bias --dense_seq_output --fused_mha      --allreduce_post_accumulation --allreduce_post_accumulation_fp16     --gradient_accumulation_steps=1     --log_freq=1     --local_rank=2     --bert_config_path=/workspace/phase1/bert_config.j
402: Run vars: id 394913 gpus 16 mparams
178: son --seed=18263'
406: Run vars: id 394913 gpus 16 mparams
178: ++ ./bind.sh --cpu=exclusive --ib=single --cluster=circe -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=2 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
403: Run vars: id 394913 gpus 16 mparams
475: + eval '    ./bind.sh --cpu=exclusive --ib=single --cluster=circe --     python -u /workspace/bert/run_pretraining.py         --train_batch_size=18     --learning_rate=2.0e-3     --opt_lamb_beta_1=0.86     --opt_lamb_beta_2=0.975     --warmup_proportion=0.0     --max_steps=750     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data_phase2     --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.712     --max_samples_termination=7500000     --eval_iter_start_samples=3000000 --eval_iter_samples=500000     --eval_batch_size=16 --eval_dir=/workspace/evaldata     --cache_eval_data     --output_dir=/results     --fp16 --fused_gelu_bias --dense_seq_output --fused_mha      --allreduce_post_accumulation --allreduce_post_accumulation_fp16     --gradient_accumulation_steps=1     --log_freq=1     --local_rank=11     --bert_config_path=/workspace/phase1/bert_config.
217: Run vars: id 394913 gpus 16 mparams
475: json --seed=18263'
413: Run vars: id 394913 gpus 16 mparams
477: + eval '    ./bind.sh --cpu=exclusive --ib=single --cluster=circe --     python -u /workspace/bert/run_pretraining.py         --train_batch_size=18     --learning_rate=2.0e-3     --opt_lamb_beta_1=0.86     --opt_lamb_beta_2=0.975     --warmup_proportion=0.0     --max_steps=750     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data_phase2     --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.712     --max_samples_termination=7500000     --eval_iter_start_samples=3000000 --eval_iter_samples=500000     --eval_batch_size=16 --eval_dir=/workspace/evaldata     --cache_eval_data     --output_dir=/results     --fp16 --fused_gelu_bias --dense_seq_output --fused_mha      --allreduce_post_accumulation --allreduce_post_accumulation_fp16     --gradient_accumulation_steps=1     --log_freq=1     --local_rank=13     --bert_config_path=/workspace/phase1/bert_config.
219: Run vars: id 394913 gpus 16 mparams
477: json --seed=18263'
402: STARTING TIMING RUN AT 2020-06-25 10:59:01 AM
475: ++ ./bind.sh --cpu=exclusive --ib=single --cluster=circe -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=11 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
214: Run vars: id 394913 gpus 16 mparams
477: ++ ./bind.sh --cpu=exclusive --ib=single --cluster=circe -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=13 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
406: STARTING TIMING RUN AT 2020-06-25 10:59:01 AM
185: + eval '    ./bind.sh --cpu=exclusive --ib=single --cluster=circe --     python -u /workspace/bert/run_pretraining.py         --train_batch_size=18     --learning_rate=2.0e-3     --opt_lamb_beta_1=0.86     --opt_lamb_beta_2=0.975     --warmup_proportion=0.0     --max_steps=750     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data_phase2     --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.712     --max_samples_termination=7500000     --eval_iter_start_samples=3000000 --eval_iter_samples=500000     --eval_batch_size=16 --eval_dir=/workspace/evaldata     --cache_eval_data     --output_dir=/results     --fp16 --fused_gelu_bias --dense_seq_output --fused_mha      --allreduce_post_accumulation --allreduce_post_accumulation_fp16     --gradient_accumulation_steps=1     --log_freq=1     --local_rank=9     --bert_config_path=/workspace/phase1/bert_config.j
217: STARTING TIMING RUN AT 2020-06-25 10:59:01 AM
185: son --seed=18263'
403: STARTING TIMING RUN AT 2020-06-25 10:59:01 AM
185: ++ ./bind.sh --cpu=exclusive --ib=single --cluster=circe -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=9 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
401: Run vars: id 394913 gpus 16 mparams
445: ++ ./bind.sh --cpu=exclusive --ib=single --cluster=circe -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=13 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
409: Run vars: id 394913 gpus 16 mparams
476: + eval '    ./bind.sh --cpu=exclusive --ib=single --cluster=circe --     python -u /workspace/bert/run_pretraining.py         --train_batch_size=18     --learning_rate=2.0e-3     --opt_lamb_beta_1=0.86     --opt_lamb_beta_2=0.975     --warmup_proportion=0.0     --max_steps=750     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data_phase2     --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.712     --max_samples_termination=7500000     --eval_iter_start_samples=3000000 --eval_iter_samples=500000     --eval_batch_size=16 --eval_dir=/workspace/evaldata     --cache_eval_data     --output_dir=/results     --fp16 --fused_gelu_bias --dense_seq_output --fused_mha      --allreduce_post_accumulation --allreduce_post_accumulation_fp16     --gradient_accumulation_steps=1     --log_freq=1     --local_rank=12     --bert_config_path=/workspace/phase1/bert_config.
413: STARTING TIMING RUN AT 2020-06-25 10:59:01 AM
476: json --seed=18263'
219: STARTING TIMING RUN AT 2020-06-25 10:59:01 AM
476: ++ ./bind.sh --cpu=exclusive --ib=single --cluster=circe -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=12 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
248: Run vars: id 394913 gpus 16 mparams
470: + eval '    ./bind.sh --cpu=exclusive --ib=single --cluster=circe --     python -u /workspace/bert/run_pretraining.py         --train_batch_size=18     --learning_rate=2.0e-3     --opt_lamb_beta_1=0.86     --opt_lamb_beta_2=0.975     --warmup_proportion=0.0     --max_steps=750     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data_phase2     --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.712     --max_samples_termination=7500000     --eval_iter_start_samples=3000000 --eval_iter_samples=500000     --eval_batch_size=16 --eval_dir=/workspace/evaldata     --cache_eval_data     --output_dir=/results     --fp16 --fused_gelu_bias --dense_seq_output --fused_mha      --allreduce_post_accumulation --allreduce_post_accumulation_fp16     --gradient_accumulation_steps=1     --log_freq=1     --local_rank=6     --bert_config_path=/workspace/phase1/bert_config.j
254: Run vars: id 394913 gpus 16 mparams
470: son --seed=18263'
249: Run vars: id 394913 gpus 16 mparams
470: ++ ./bind.sh --cpu=exclusive --ib=single --cluster=circe -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=6 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
246: Run vars: id 394913 gpus 16 mparams
471: + eval '    ./bind.sh --cpu=exclusive --ib=single --cluster=circe --     python -u /workspace/bert/run_pretraining.py         --train_batch_size=18     --learning_rate=2.0e-3     --opt_lamb_beta_1=0.86     --opt_lamb_beta_2=0.975     --warmup_proportion=0.0     --max_steps=750     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data_phase2     --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.712     --max_samples_termination=7500000     --eval_iter_start_samples=3000000 --eval_iter_samples=500000     --eval_batch_size=16 --eval_dir=/workspace/evaldata     --cache_eval_data     --output_dir=/results     --fp16 --fused_gelu_bias --dense_seq_output --fused_mha      --allreduce_post_accumulation --allreduce_post_accumulation_fp16     --gradient_accumulation_steps=1     --log_freq=1     --local_rank=7     --bert_config_path=/workspace/phase1/bert_config.j
253: Run vars: id 394913 gpus 16 mparams
471: son --seed=18263'
252: Run vars: id 394913 gpus 16 mparams
471: ++ ./bind.sh --cpu=exclusive --ib=single --cluster=circe -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=7 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
243: Run vars: id 394913 gpus 16 mparams
469: + eval '    ./bind.sh --cpu=exclusive --ib=single --cluster=circe --     python -u /workspace/bert/run_pretraining.py         --train_batch_size=18     --learning_rate=2.0e-3     --opt_lamb_beta_1=0.86     --opt_lamb_beta_2=0.975     --warmup_proportion=0.0     --max_steps=750     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data_phase2     --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.712     --max_samples_termination=7500000     --eval_iter_start_samples=3000000 --eval_iter_samples=500000     --eval_batch_size=16 --eval_dir=/workspace/evaldata     --cache_eval_data     --output_dir=/results     --fp16 --fused_gelu_bias --dense_seq_output --fused_mha      --allreduce_post_accumulation --allreduce_post_accumulation_fp16     --gradient_accumulation_steps=1     --log_freq=1     --local_rank=5     --bert_config_path=/workspace/phase1/bert_config.j
240: Run vars: id 394913 gpus 16 mparams
469: son --seed=18263'
245: Run vars: id 394913 gpus 16 mparams
469: ++ ./bind.sh --cpu=exclusive --ib=single --cluster=circe -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=5 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
248: STARTING TIMING RUN AT 2020-06-25 10:59:01 AM
478: + eval '    ./bind.sh --cpu=exclusive --ib=single --cluster=circe --     python -u /workspace/bert/run_pretraining.py         --train_batch_size=18     --learning_rate=2.0e-3     --opt_lamb_beta_1=0.86     --opt_lamb_beta_2=0.975     --warmup_proportion=0.0     --max_steps=750     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data_phase2     --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.712     --max_samples_termination=7500000     --eval_iter_start_samples=3000000 --eval_iter_samples=500000     --eval_batch_size=16 --eval_dir=/workspace/evaldata     --cache_eval_data     --output_dir=/results     --fp16 --fused_gelu_bias --dense_seq_output --fused_mha      --allreduce_post_accumulation --allreduce_post_accumulation_fp16     --gradient_accumulation_steps=1     --log_freq=1     --local_rank=14     --bert_config_path=/workspace/phase1/bert_config.
255: Run vars: id 394913 gpus 16 mparams
478: json --seed=18263'
209: Run vars: id 394913 gpus 16 mparams
465: + eval '    ./bind.sh --cpu=exclusive --ib=single --cluster=circe --     python -u /workspace/bert/run_pretraining.py         --train_batch_size=18     --learning_rate=2.0e-3     --opt_lamb_beta_1=0.86     --opt_lamb_beta_2=0.975     --warmup_proportion=0.0     --max_steps=750     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data_phase2     --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.712     --max_samples_termination=7500000     --eval_iter_start_samples=3000000 --eval_iter_samples=500000     --eval_batch_size=16 --eval_dir=/workspace/evaldata     --cache_eval_data     --output_dir=/results     --fp16 --fused_gelu_bias --dense_seq_output --fused_mha      --allreduce_post_accumulation --allreduce_post_accumulation_fp16     --gradient_accumulation_steps=1     --log_freq=1     --local_rank=1     --bert_config_path=/workspace/phase1/bert_config.j
214: STARTING TIMING RUN AT 2020-06-25 10:59:01 AM
465: son --seed=18263'
251: Run vars: id 394913 gpus 16 mparams
478: ++ ./bind.sh --cpu=exclusive --ib=single --cluster=circe -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=14 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
254: STARTING TIMING RUN AT 2020-06-25 10:59:01 AM
465: ++ ./bind.sh --cpu=exclusive --ib=single --cluster=circe -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=1 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
400: Run vars: id 394913 gpus 16 mparams
474: + eval '    ./bind.sh --cpu=exclusive --ib=single --cluster=circe --     python -u /workspace/bert/run_pretraining.py         --train_batch_size=18     --learning_rate=2.0e-3     --opt_lamb_beta_1=0.86     --opt_lamb_beta_2=0.975     --warmup_proportion=0.0     --max_steps=750     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data_phase2     --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.712     --max_samples_termination=7500000     --eval_iter_start_samples=3000000 --eval_iter_samples=500000     --eval_batch_size=16 --eval_dir=/workspace/evaldata     --cache_eval_data     --output_dir=/results     --fp16 --fused_gelu_bias --dense_seq_output --fused_mha      --allreduce_post_accumulation --allreduce_post_accumulation_fp16     --gradient_accumulation_steps=1     --log_freq=1     --local_rank=10     --bert_config_path=/workspace/phase1/bert_config.
407: Run vars: id 394913 gpus 16 mparams
474: json --seed=18263'
408: Run vars: id 394913 gpus 16 mparams
474: ++ ./bind.sh --cpu=exclusive --ib=single --cluster=circe -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=10 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
404: Run vars: id 394913 gpus 16 mparams
182: + eval '    ./bind.sh --cpu=exclusive --ib=single --cluster=circe --     python -u /workspace/bert/run_pretraining.py         --train_batch_size=18     --learning_rate=2.0e-3     --opt_lamb_beta_1=0.86     --opt_lamb_beta_2=0.975     --warmup_proportion=0.0     --max_steps=750     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data_phase2     --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.712     --max_samples_termination=7500000     --eval_iter_start_samples=3000000 --eval_iter_samples=500000     --eval_batch_size=16 --eval_dir=/workspace/evaldata     --cache_eval_data     --output_dir=/results     --fp16 --fused_gelu_bias --dense_seq_output --fused_mha      --allreduce_post_accumulation --allreduce_post_accumulation_fp16     --gradient_accumulation_steps=1     --log_freq=1     --local_rank=6     --bert_config_path=/workspace/phase1/bert_config.j
410: Run vars: id 394913 gpus 16 mparams
464: + eval '    ./bind.sh --cpu=exclusive --ib=single --cluster=circe --     python -u /workspace/bert/run_pretraining.py         --train_batch_size=18     --learning_rate=2.0e-3     --opt_lamb_beta_1=0.86     --opt_lamb_beta_2=0.975     --warmup_proportion=0.0     --max_steps=750     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data_phase2     --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.712     --max_samples_termination=7500000     --eval_iter_start_samples=3000000 --eval_iter_samples=500000     --eval_batch_size=16 --eval_dir=/workspace/evaldata     --cache_eval_data     --output_dir=/results     --fp16 --fused_gelu_bias --dense_seq_output --fused_mha      --allreduce_post_accumulation --allreduce_post_accumulation_fp16     --gradient_accumulation_steps=1     --log_freq=1     --local_rank=0     --bert_config_path=/workspace/phase1/bert_config.j
401: STARTING TIMING RUN AT 2020-06-25 10:59:01 AM
182: son --seed=18263'
405: Run vars: id 394913 gpus 16 mparams
182: ++ ./bind.sh --cpu=exclusive --ib=single --cluster=circe -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=6 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
411: Run vars: id 394913 gpus 16 mparams
464: son --seed=18263'
412: Run vars: id 394913 gpus 16 mparams
464: ++ ./bind.sh --cpu=exclusive --ib=single --cluster=circe -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=0 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
250: Run vars: id 394913 gpus 16 mparams
472: + eval '    ./bind.sh --cpu=exclusive --ib=single --cluster=circe --     python -u /workspace/bert/run_pretraining.py         --train_batch_size=18     --learning_rate=2.0e-3     --opt_lamb_beta_1=0.86     --opt_lamb_beta_2=0.975     --warmup_proportion=0.0     --max_steps=750     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data_phase2     --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.712     --max_samples_termination=7500000     --eval_iter_start_samples=3000000 --eval_iter_samples=500000     --eval_batch_size=16 --eval_dir=/workspace/evaldata     --cache_eval_data     --output_dir=/results     --fp16 --fused_gelu_bias --dense_seq_output --fused_mha      --allreduce_post_accumulation --allreduce_post_accumulation_fp16     --gradient_accumulation_steps=1     --log_freq=1     --local_rank=8     --bert_config_path=/workspace/phase1/bert_config.j
247: Run vars: id 394913 gpus 16 mparams
472: son --seed=18263'
244: Run vars: id 394913 gpus 16 mparams
472: ++ ./bind.sh --cpu=exclusive --ib=single --cluster=circe -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=8 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
249: STARTING TIMING RUN AT 2020-06-25 10:59:01 AM
288: slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
415: Run vars: id 394913 gpus 16 mparams
290: slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
252: STARTING TIMING RUN AT 2020-06-25 10:59:01 AM
292: slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
409: STARTING TIMING RUN AT 2020-06-25 10:59:01 AM
291: slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
246: STARTING TIMING RUN AT 2020-06-25 10:59:01 AM
289: slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
253: STARTING TIMING RUN AT 2020-06-25 10:59:01 AM
293: slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
221: Run vars: id 394913 gpus 16 mparams
294: slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
218: Run vars: id 394913 gpus 16 mparams
297: slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
213: Run vars: id 394913 gpus 16 mparams
296: slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
209: STARTING TIMING RUN AT 2020-06-25 10:59:01 AM
302: slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
212: Run vars: id 394913 gpus 16 mparams
298: slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
245: STARTING TIMING RUN AT 2020-06-25 10:59:01 AM
303: slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
240: STARTING TIMING RUN AT 2020-06-25 10:59:01 AM
301: slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
243: STARTING TIMING RUN AT 2020-06-25 10:59:01 AM
299: slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
414: Run vars: id 394913 gpus 16 mparams
300: slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
407: STARTING TIMING RUN AT 2020-06-25 10:59:01 AM
302: + eval '    ./bind.sh --cpu=exclusive --ib=single --cluster=circe --     python -u /workspace/bert/run_pretraining.py         --train_batch_size=18     --learning_rate=2.0e-3     --opt_lamb_beta_1=0.86     --opt_lamb_beta_2=0.975     --warmup_proportion=0.0     --max_steps=750     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data_phase2     --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.712     --max_samples_termination=7500000     --eval_iter_start_samples=3000000 --eval_iter_samples=500000     --eval_batch_size=16 --eval_dir=/workspace/evaldata     --cache_eval_data     --output_dir=/results     --fp16 --fused_gelu_bias --dense_seq_output --fused_mha      --allreduce_post_accumulation --allreduce_post_accumulation_fp16     --gradient_accumulation_steps=1     --log_freq=1     --local_rank=14     --bert_config_path=/workspace/phase1/bert_config.
400: STARTING TIMING RUN AT 2020-06-25 10:59:01 AM
302: json --seed=18263'
255: STARTING TIMING RUN AT 2020-06-25 10:59:01 AM
294: + eval '    ./bind.sh --cpu=exclusive --ib=single --cluster=circe --     python -u /workspace/bert/run_pretraining.py         --train_batch_size=18     --learning_rate=2.0e-3     --opt_lamb_beta_1=0.86     --opt_lamb_beta_2=0.975     --warmup_proportion=0.0     --max_steps=750     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data_phase2     --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.712     --max_samples_termination=7500000     --eval_iter_start_samples=3000000 --eval_iter_samples=500000     --eval_batch_size=16 --eval_dir=/workspace/evaldata     --cache_eval_data     --output_dir=/results     --fp16 --fused_gelu_bias --dense_seq_output --fused_mha      --allreduce_post_accumulation --allreduce_post_accumulation_fp16     --gradient_accumulation_steps=1     --log_freq=1     --local_rank=6     --bert_config_path=/workspace/phase1/bert_config.j
251: STARTING TIMING RUN AT 2020-06-25 10:59:01 AM
294: son --seed=18263'
250: STARTING TIMING RUN AT 2020-06-25 10:59:01 AM
302: ++ ./bind.sh --cpu=exclusive --ib=single --cluster=circe -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=14 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
404: STARTING TIMING RUN AT 2020-06-25 10:59:01 AM
181: + eval '    ./bind.sh --cpu=exclusive --ib=single --cluster=circe --     python -u /workspace/bert/run_pretraining.py         --train_batch_size=18     --learning_rate=2.0e-3     --opt_lamb_beta_1=0.86     --opt_lamb_beta_2=0.975     --warmup_proportion=0.0     --max_steps=750     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data_phase2     --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.712     --max_samples_termination=7500000     --eval_iter_start_samples=3000000 --eval_iter_samples=500000     --eval_batch_size=16 --eval_dir=/workspace/evaldata     --cache_eval_data     --output_dir=/results     --fp16 --fused_gelu_bias --dense_seq_output --fused_mha      --allreduce_post_accumulation --allreduce_post_accumulation_fp16     --gradient_accumulation_steps=1     --log_freq=1     --local_rank=5     --bert_config_path=/workspace/phase1/bert_config.j
408: STARTING TIMING RUN AT 2020-06-25 10:59:01 AM
181: son --seed=18263'
410: STARTING TIMING RUN AT 2020-06-25 10:59:01 AM
181: ++ ./bind.sh --cpu=exclusive --ib=single --cluster=circe -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=5 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
210: Run vars: id 394913 gpus 16 mparams
468: + eval '    ./bind.sh --cpu=exclusive --ib=single --cluster=circe --     python -u /workspace/bert/run_pretraining.py         --train_batch_size=18     --learning_rate=2.0e-3     --opt_lamb_beta_1=0.86     --opt_lamb_beta_2=0.975     --warmup_proportion=0.0     --max_steps=750     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data_phase2     --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.712     --max_samples_termination=7500000     --eval_iter_start_samples=3000000 --eval_iter_samples=500000     --eval_batch_size=16 --eval_dir=/workspace/evaldata     --cache_eval_data     --output_dir=/results     --fp16 --fused_gelu_bias --dense_seq_output --fused_mha      --allreduce_post_accumulation --allreduce_post_accumulation_fp16     --gradient_accumulation_steps=1     --log_freq=1     --local_rank=4     --bert_config_path=/workspace/phase1/bert_config.j
216: Run vars: id 394913 gpus 16 mparams
468: son --seed=18263'
215: Run vars: id 394913 gpus 16 mparams
466: + eval '    ./bind.sh --cpu=exclusive --ib=single --cluster=circe --     python -u /workspace/bert/run_pretraining.py         --train_batch_size=18     --learning_rate=2.0e-3     --opt_lamb_beta_1=0.86     --opt_lamb_beta_2=0.975     --warmup_proportion=0.0     --max_steps=750     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data_phase2     --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.712     --max_samples_termination=7500000     --eval_iter_start_samples=3000000 --eval_iter_samples=500000     --eval_batch_size=16 --eval_dir=/workspace/evaldata     --cache_eval_data     --output_dir=/results     --fp16 --fused_gelu_bias --dense_seq_output --fused_mha      --allreduce_post_accumulation --allreduce_post_accumulation_fp16     --gradient_accumulation_steps=1     --log_freq=1     --local_rank=2     --bert_config_path=/workspace/phase1/bert_config.j
221: STARTING TIMING RUN AT 2020-06-25 10:59:01 AM
466: son --seed=18263'
218: STARTING TIMING RUN AT 2020-06-25 10:59:01 AM
468: ++ ./bind.sh --cpu=exclusive --ib=single --cluster=circe -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=4 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
244: STARTING TIMING RUN AT 2020-06-25 10:59:01 AM
466: ++ ./bind.sh --cpu=exclusive --ib=single --cluster=circe -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=2 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
411: STARTING TIMING RUN AT 2020-06-25 10:59:01 AM
191: + eval '    ./bind.sh --cpu=exclusive --ib=single --cluster=circe --     python -u /workspace/bert/run_pretraining.py         --train_batch_size=18     --learning_rate=2.0e-3     --opt_lamb_beta_1=0.86     --opt_lamb_beta_2=0.975     --warmup_proportion=0.0     --max_steps=750     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data_phase2     --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.712     --max_samples_termination=7500000     --eval_iter_start_samples=3000000 --eval_iter_samples=500000     --eval_batch_size=16 --eval_dir=/workspace/evaldata     --cache_eval_data     --output_dir=/results     --fp16 --fused_gelu_bias --dense_seq_output --fused_mha      --allreduce_post_accumulation --allreduce_post_accumulation_fp16     --gradient_accumulation_steps=1     --log_freq=1     --local_rank=15     --bert_config_path=/workspace/phase1/bert_config.
405: STARTING TIMING RUN AT 2020-06-25 10:59:01 AM
191: json --seed=18263'
415: STARTING TIMING RUN AT 2020-06-25 10:59:01 AM
191: ++ ./bind.sh --cpu=exclusive --ib=single --cluster=circe -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=15 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
220: Run vars: id 394913 gpus 16 mparams
473: + eval '    ./bind.sh --cpu=exclusive --ib=single --cluster=circe --     python -u /workspace/bert/run_pretraining.py         --train_batch_size=18     --learning_rate=2.0e-3     --opt_lamb_beta_1=0.86     --opt_lamb_beta_2=0.975     --warmup_proportion=0.0     --max_steps=750     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data_phase2     --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.712     --max_samples_termination=7500000     --eval_iter_start_samples=3000000 --eval_iter_samples=500000     --eval_batch_size=16 --eval_dir=/workspace/evaldata     --cache_eval_data     --output_dir=/results     --fp16 --fused_gelu_bias --dense_seq_output --fused_mha      --allreduce_post_accumulation --allreduce_post_accumulation_fp16     --gradient_accumulation_steps=1     --log_freq=1     --local_rank=9     --bert_config_path=/workspace/phase1/bert_config.j
213: STARTING TIMING RUN AT 2020-06-25 10:59:01 AM
473: son --seed=18263'
222: Run vars: id 394913 gpus 16 mparams
473: ++ ./bind.sh --cpu=exclusive --ib=single --cluster=circe -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=9 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
223: Run vars: id 394913 gpus 16 mparams
180: + eval '    ./bind.sh --cpu=exclusive --ib=single --cluster=circe --     python -u /workspace/bert/run_pretraining.py         --train_batch_size=18     --learning_rate=2.0e-3     --opt_lamb_beta_1=0.86     --opt_lamb_beta_2=0.975     --warmup_proportion=0.0     --max_steps=750     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data_phase2     --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.712     --max_samples_termination=7500000     --eval_iter_start_samples=3000000 --eval_iter_samples=500000     --eval_batch_size=16 --eval_dir=/workspace/evaldata     --cache_eval_data     --output_dir=/results     --fp16 --fused_gelu_bias --dense_seq_output --fused_mha      --allreduce_post_accumulation --allreduce_post_accumulation_fp16     --gradient_accumulation_steps=1     --log_freq=1     --local_rank=4     --bert_config_path=/workspace/phase1/bert_config.j
247: STARTING TIMING RUN AT 2020-06-25 10:59:01 AM
180: son --seed=18263'
412: STARTING TIMING RUN AT 2020-06-25 10:59:01 AM
180: ++ ./bind.sh --cpu=exclusive --ib=single --cluster=circe -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=4 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
212: STARTING TIMING RUN AT 2020-06-25 10:59:01 AM
187: + eval '    ./bind.sh --cpu=exclusive --ib=single --cluster=circe --     python -u /workspace/bert/run_pretraining.py         --train_batch_size=18     --learning_rate=2.0e-3     --opt_lamb_beta_1=0.86     --opt_lamb_beta_2=0.975     --warmup_proportion=0.0     --max_steps=750     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data_phase2     --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.712     --max_samples_termination=7500000     --eval_iter_start_samples=3000000 --eval_iter_samples=500000     --eval_batch_size=16 --eval_dir=/workspace/evaldata     --cache_eval_data     --output_dir=/results     --fp16 --fused_gelu_bias --dense_seq_output --fused_mha      --allreduce_post_accumulation --allreduce_post_accumulation_fp16     --gradient_accumulation_steps=1     --log_freq=1     --local_rank=11     --bert_config_path=/workspace/phase1/bert_config.
414: STARTING TIMING RUN AT 2020-06-25 10:59:01 AM
467: + eval '    ./bind.sh --cpu=exclusive --ib=single --cluster=circe --     python -u /workspace/bert/run_pretraining.py         --train_batch_size=18     --learning_rate=2.0e-3     --opt_lamb_beta_1=0.86     --opt_lamb_beta_2=0.975     --warmup_proportion=0.0     --max_steps=750     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data_phase2     --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.712     --max_samples_termination=7500000     --eval_iter_start_samples=3000000 --eval_iter_samples=500000     --eval_batch_size=16 --eval_dir=/workspace/evaldata     --cache_eval_data     --output_dir=/results     --fp16 --fused_gelu_bias --dense_seq_output --fused_mha      --allreduce_post_accumulation --allreduce_post_accumulation_fp16     --gradient_accumulation_steps=1     --log_freq=1     --local_rank=3     --bert_config_path=/workspace/phase1/bert_config.j
208: Run vars: id 394913 gpus 16 mparams
187: json --seed=18263'
211: Run vars: id 394913 gpus 16 mparams
467: son --seed=18263'
467: ++ ./bind.sh --cpu=exclusive --ib=single --cluster=circe -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=3 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
210: STARTING TIMING RUN AT 2020-06-25 10:59:01 AM
176: + eval '    ./bind.sh --cpu=exclusive --ib=single --cluster=circe --     python -u /workspace/bert/run_pretraining.py         --train_batch_size=18     --learning_rate=2.0e-3     --opt_lamb_beta_1=0.86     --opt_lamb_beta_2=0.975     --warmup_proportion=0.0     --max_steps=750     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data_phase2     --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.712     --max_samples_termination=7500000     --eval_iter_start_samples=3000000 --eval_iter_samples=500000     --eval_batch_size=16 --eval_dir=/workspace/evaldata     --cache_eval_data     --output_dir=/results     --fp16 --fused_gelu_bias --dense_seq_output --fused_mha      --allreduce_post_accumulation --allreduce_post_accumulation_fp16     --gradient_accumulation_steps=1     --log_freq=1     --local_rank=0     --bert_config_path=/workspace/phase1/bert_config.j
216: STARTING TIMING RUN AT 2020-06-25 10:59:01 AM
176: son --seed=18263'
215: STARTING TIMING RUN AT 2020-06-25 10:59:01 AM
187: ++ ./bind.sh --cpu=exclusive --ib=single --cluster=circe -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=11 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
220: STARTING TIMING RUN AT 2020-06-25 10:59:01 AM
176: ++ ./bind.sh --cpu=exclusive --ib=single --cluster=circe -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=0 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
223: STARTING TIMING RUN AT 2020-06-25 10:59:01 AM
186: + eval '    ./bind.sh --cpu=exclusive --ib=single --cluster=circe --     python -u /workspace/bert/run_pretraining.py         --train_batch_size=18     --learning_rate=2.0e-3     --opt_lamb_beta_1=0.86     --opt_lamb_beta_2=0.975     --warmup_proportion=0.0     --max_steps=750     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data_phase2     --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.712     --max_samples_termination=7500000     --eval_iter_start_samples=3000000 --eval_iter_samples=500000     --eval_batch_size=16 --eval_dir=/workspace/evaldata     --cache_eval_data     --output_dir=/results     --fp16 --fused_gelu_bias --dense_seq_output --fused_mha      --allreduce_post_accumulation --allreduce_post_accumulation_fp16     --gradient_accumulation_steps=1     --log_freq=1     --local_rank=10     --bert_config_path=/workspace/phase1/bert_config.
222: STARTING TIMING RUN AT 2020-06-25 10:59:01 AM
186: json --seed=18263'
355: Run vars: id 394913 gpus 16 mparams
179: + eval '    ./bind.sh --cpu=exclusive --ib=single --cluster=circe --     python -u /workspace/bert/run_pretraining.py         --train_batch_size=18     --learning_rate=2.0e-3     --opt_lamb_beta_1=0.86     --opt_lamb_beta_2=0.975     --warmup_proportion=0.0     --max_steps=750     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data_phase2     --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.712     --max_samples_termination=7500000     --eval_iter_start_samples=3000000 --eval_iter_samples=500000     --eval_batch_size=16 --eval_dir=/workspace/evaldata     --cache_eval_data     --output_dir=/results     --fp16 --fused_gelu_bias --dense_seq_output --fused_mha      --allreduce_post_accumulation --allreduce_post_accumulation_fp16     --gradient_accumulation_steps=1     --log_freq=1     --local_rank=3     --bert_config_path=/workspace/phase1/bert_config.j
359: Run vars: id 394913 gpus 16 mparams
179: son --seed=18263'
363: Run vars: id 394913 gpus 16 mparams
186: ++ ./bind.sh --cpu=exclusive --ib=single --cluster=circe -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=10 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
360: Run vars: id 394913 gpus 16 mparams
179: ++ ./bind.sh --cpu=exclusive --ib=single --cluster=circe -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=3 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
355: STARTING TIMING RUN AT 2020-06-25 10:59:01 AM
144: slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
359: STARTING TIMING RUN AT 2020-06-25 10:59:01 AM
183: + eval '    ./bind.sh --cpu=exclusive --ib=single --cluster=circe --     python -u /workspace/bert/run_pretraining.py         --train_batch_size=18     --learning_rate=2.0e-3     --opt_lamb_beta_1=0.86     --opt_lamb_beta_2=0.975     --warmup_proportion=0.0     --max_steps=750     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data_phase2     --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.712     --max_samples_termination=7500000     --eval_iter_start_samples=3000000 --eval_iter_samples=500000     --eval_batch_size=16 --eval_dir=/workspace/evaldata     --cache_eval_data     --output_dir=/results     --fp16 --fused_gelu_bias --dense_seq_output --fused_mha      --allreduce_post_accumulation --allreduce_post_accumulation_fp16     --gradient_accumulation_steps=1     --log_freq=1     --local_rank=7     --bert_config_path=/workspace/phase1/bert_config.j
363: STARTING TIMING RUN AT 2020-06-25 10:59:01 AM
183: son --seed=18263'
358: Run vars: id 394913 gpus 16 mparams
183: ++ ./bind.sh --cpu=exclusive --ib=single --cluster=circe -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=7 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
208: STARTING TIMING RUN AT 2020-06-25 10:59:01 AM
294: ++ ./bind.sh --cpu=exclusive --ib=single --cluster=circe -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=6 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
356: Run vars: id 394913 gpus 16 mparams
190: + eval '    ./bind.sh --cpu=exclusive --ib=single --cluster=circe --     python -u /workspace/bert/run_pretraining.py         --train_batch_size=18     --learning_rate=2.0e-3     --opt_lamb_beta_1=0.86     --opt_lamb_beta_2=0.975     --warmup_proportion=0.0     --max_steps=750     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data_phase2     --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.712     --max_samples_termination=7500000     --eval_iter_start_samples=3000000 --eval_iter_samples=500000     --eval_batch_size=16 --eval_dir=/workspace/evaldata     --cache_eval_data     --output_dir=/results     --fp16 --fused_gelu_bias --dense_seq_output --fused_mha      --allreduce_post_accumulation --allreduce_post_accumulation_fp16     --gradient_accumulation_steps=1     --log_freq=1     --local_rank=14     --bert_config_path=/workspace/phase1/bert_config.
362: Run vars: id 394913 gpus 16 mparams
190: json --seed=18263'
365: Run vars: id 394913 gpus 16 mparams
177: + eval '    ./bind.sh --cpu=exclusive --ib=single --cluster=circe --     python -u /workspace/bert/run_pretraining.py         --train_batch_size=18     --learning_rate=2.0e-3     --opt_lamb_beta_1=0.86     --opt_lamb_beta_2=0.975     --warmup_proportion=0.0     --max_steps=750     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data_phase2     --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.712     --max_samples_termination=7500000     --eval_iter_start_samples=3000000 --eval_iter_samples=500000     --eval_batch_size=16 --eval_dir=/workspace/evaldata     --cache_eval_data     --output_dir=/results     --fp16 --fused_gelu_bias --dense_seq_output --fused_mha      --allreduce_post_accumulation --allreduce_post_accumulation_fp16     --gradient_accumulation_steps=1     --log_freq=1     --local_rank=1     --bert_config_path=/workspace/phase1/bert_config.j
352: Run vars: id 394913 gpus 16 mparams
177: son --seed=18263'
354: Run vars: id 394913 gpus 16 mparams
184: + eval '    ./bind.sh --cpu=exclusive --ib=single --cluster=circe --     python -u /workspace/bert/run_pretraining.py         --train_batch_size=18     --learning_rate=2.0e-3     --opt_lamb_beta_1=0.86     --opt_lamb_beta_2=0.975     --warmup_proportion=0.0     --max_steps=750     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data_phase2     --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.712     --max_samples_termination=7500000     --eval_iter_start_samples=3000000 --eval_iter_samples=500000     --eval_batch_size=16 --eval_dir=/workspace/evaldata     --cache_eval_data     --output_dir=/results     --fp16 --fused_gelu_bias --dense_seq_output --fused_mha      --allreduce_post_accumulation --allreduce_post_accumulation_fp16     --gradient_accumulation_steps=1     --log_freq=1     --local_rank=8     --bert_config_path=/workspace/phase1/bert_config.j
360: STARTING TIMING RUN AT 2020-06-25 10:59:01 AM
184: son --seed=18263'
211: STARTING TIMING RUN AT 2020-06-25 10:59:01 AM
189: + eval '    ./bind.sh --cpu=exclusive --ib=single --cluster=circe --     python -u /workspace/bert/run_pretraining.py         --train_batch_size=18     --learning_rate=2.0e-3     --opt_lamb_beta_1=0.86     --opt_lamb_beta_2=0.975     --warmup_proportion=0.0     --max_steps=750     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data_phase2     --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.712     --max_samples_termination=7500000     --eval_iter_start_samples=3000000 --eval_iter_samples=500000     --eval_batch_size=16 --eval_dir=/workspace/evaldata     --cache_eval_data     --output_dir=/results     --fp16 --fused_gelu_bias --dense_seq_output --fused_mha      --allreduce_post_accumulation --allreduce_post_accumulation_fp16     --gradient_accumulation_steps=1     --log_freq=1     --local_rank=13     --bert_config_path=/workspace/phase1/bert_config.
367: Run vars: id 394913 gpus 16 mparams
189: json --seed=18263'
353: Run vars: id 394913 gpus 16 mparams
177: ++ ./bind.sh --cpu=exclusive --ib=single --cluster=circe -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=1 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
357: Run vars: id 394913 gpus 16 mparams
190: ++ ./bind.sh --cpu=exclusive --ib=single --cluster=circe -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=14 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
366: Run vars: id 394913 gpus 16 mparams
189: ++ ./bind.sh --cpu=exclusive --ib=single --cluster=circe -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=13 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
358: STARTING TIMING RUN AT 2020-06-25 10:59:01 AM
184: ++ ./bind.sh --cpu=exclusive --ib=single --cluster=circe -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=8 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
356: STARTING TIMING RUN AT 2020-06-25 10:59:01 AM
188: + eval '    ./bind.sh --cpu=exclusive --ib=single --cluster=circe --     python -u /workspace/bert/run_pretraining.py         --train_batch_size=18     --learning_rate=2.0e-3     --opt_lamb_beta_1=0.86     --opt_lamb_beta_2=0.975     --warmup_proportion=0.0     --max_steps=750     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data_phase2     --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.712     --max_samples_termination=7500000     --eval_iter_start_samples=3000000 --eval_iter_samples=500000     --eval_batch_size=16 --eval_dir=/workspace/evaldata     --cache_eval_data     --output_dir=/results     --fp16 --fused_gelu_bias --dense_seq_output --fused_mha      --allreduce_post_accumulation --allreduce_post_accumulation_fp16     --gradient_accumulation_steps=1     --log_freq=1     --local_rank=12     --bert_config_path=/workspace/phase1/bert_config.
361: Run vars: id 394913 gpus 16 mparams
188: json --seed=18263'
364: Run vars: id 394913 gpus 16 mparams
290: + eval '    ./bind.sh --cpu=exclusive --ib=single --cluster=circe --     python -u /workspace/bert/run_pretraining.py         --train_batch_size=18     --learning_rate=2.0e-3     --opt_lamb_beta_1=0.86     --opt_lamb_beta_2=0.975     --warmup_proportion=0.0     --max_steps=750     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data_phase2     --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.712     --max_samples_termination=7500000     --eval_iter_start_samples=3000000 --eval_iter_samples=500000     --eval_batch_size=16 --eval_dir=/workspace/evaldata     --cache_eval_data     --output_dir=/results     --fp16 --fused_gelu_bias --dense_seq_output --fused_mha      --allreduce_post_accumulation --allreduce_post_accumulation_fp16     --gradient_accumulation_steps=1     --log_freq=1     --local_rank=2     --bert_config_path=/workspace/phase1/bert_config.j
365: STARTING TIMING RUN AT 2020-06-25 10:59:01 AM
290: son --seed=18263'
362: STARTING TIMING RUN AT 2020-06-25 10:59:01 AM
290: ++ ./bind.sh --cpu=exclusive --ib=single --cluster=circe -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=2 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
354: STARTING TIMING RUN AT 2020-06-25 10:59:01 AM
291: + eval '    ./bind.sh --cpu=exclusive --ib=single --cluster=circe --     python -u /workspace/bert/run_pretraining.py         --train_batch_size=18     --learning_rate=2.0e-3     --opt_lamb_beta_1=0.86     --opt_lamb_beta_2=0.975     --warmup_proportion=0.0     --max_steps=750     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data_phase2     --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.712     --max_samples_termination=7500000     --eval_iter_start_samples=3000000 --eval_iter_samples=500000     --eval_batch_size=16 --eval_dir=/workspace/evaldata     --cache_eval_data     --output_dir=/results     --fp16 --fused_gelu_bias --dense_seq_output --fused_mha      --allreduce_post_accumulation --allreduce_post_accumulation_fp16     --gradient_accumulation_steps=1     --log_freq=1     --local_rank=3     --bert_config_path=/workspace/phase1/bert_config.j
352: STARTING TIMING RUN AT 2020-06-25 10:59:01 AM
291: son --seed=18263'
367: STARTING TIMING RUN AT 2020-06-25 10:59:01 AM
291: ++ ./bind.sh --cpu=exclusive --ib=single --cluster=circe -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=3 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
353: STARTING TIMING RUN AT 2020-06-25 10:59:01 AM
288: + eval '    ./bind.sh --cpu=exclusive --ib=single --cluster=circe --     python -u /workspace/bert/run_pretraining.py         --train_batch_size=18     --learning_rate=2.0e-3     --opt_lamb_beta_1=0.86     --opt_lamb_beta_2=0.975     --warmup_proportion=0.0     --max_steps=750     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data_phase2     --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.712     --max_samples_termination=7500000     --eval_iter_start_samples=3000000 --eval_iter_samples=500000     --eval_batch_size=16 --eval_dir=/workspace/evaldata     --cache_eval_data     --output_dir=/results     --fp16 --fused_gelu_bias --dense_seq_output --fused_mha      --allreduce_post_accumulation --allreduce_post_accumulation_fp16     --gradient_accumulation_steps=1     --log_freq=1     --local_rank=0     --bert_config_path=/workspace/phase1/bert_config.j
366: STARTING TIMING RUN AT 2020-06-25 10:59:01 AM
288: son --seed=18263'
357: STARTING TIMING RUN AT 2020-06-25 10:59:01 AM
288: ++ ./bind.sh --cpu=exclusive --ib=single --cluster=circe -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=0 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
364: STARTING TIMING RUN AT 2020-06-25 10:59:01 AM
296: + eval '    ./bind.sh --cpu=exclusive --ib=single --cluster=circe --     python -u /workspace/bert/run_pretraining.py         --train_batch_size=18     --learning_rate=2.0e-3     --opt_lamb_beta_1=0.86     --opt_lamb_beta_2=0.975     --warmup_proportion=0.0     --max_steps=750     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data_phase2     --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.712     --max_samples_termination=7500000     --eval_iter_start_samples=3000000 --eval_iter_samples=500000     --eval_batch_size=16 --eval_dir=/workspace/evaldata     --cache_eval_data     --output_dir=/results     --fp16 --fused_gelu_bias --dense_seq_output --fused_mha      --allreduce_post_accumulation --allreduce_post_accumulation_fp16     --gradient_accumulation_steps=1     --log_freq=1     --local_rank=8     --bert_config_path=/workspace/phase1/bert_config.j
361: STARTING TIMING RUN AT 2020-06-25 10:59:01 AM
296: son --seed=18263'
482: Run vars: id 394913 gpus 16 mparams
296: ++ ./bind.sh --cpu=exclusive --ib=single --cluster=circe -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=8 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
486: Run vars: id 394913 gpus 16 mparams
162: ++ ./bind.sh --cpu=exclusive --ib=single --cluster=circe -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=2 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
482: STARTING TIMING RUN AT 2020-06-25 10:59:01 AM
300: + eval '    ./bind.sh --cpu=exclusive --ib=single --cluster=circe --     python -u /workspace/bert/run_pretraining.py         --train_batch_size=18     --learning_rate=2.0e-3     --opt_lamb_beta_1=0.86     --opt_lamb_beta_2=0.975     --warmup_proportion=0.0     --max_steps=750     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data_phase2     --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.712     --max_samples_termination=7500000     --eval_iter_start_samples=3000000 --eval_iter_samples=500000     --eval_batch_size=16 --eval_dir=/workspace/evaldata     --cache_eval_data     --output_dir=/results     --fp16 --fused_gelu_bias --dense_seq_output --fused_mha      --allreduce_post_accumulation --allreduce_post_accumulation_fp16     --gradient_accumulation_steps=1     --log_freq=1     --local_rank=12     --bert_config_path=/workspace/phase1/bert_config.
486: STARTING TIMING RUN AT 2020-06-25 10:59:01 AM
300: json --seed=18263'
488: Run vars: id 394913 gpus 16 mparams
295: + eval '    ./bind.sh --cpu=exclusive --ib=single --cluster=circe --     python -u /workspace/bert/run_pretraining.py         --train_batch_size=18     --learning_rate=2.0e-3     --opt_lamb_beta_1=0.86     --opt_lamb_beta_2=0.975     --warmup_proportion=0.0     --max_steps=750     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data_phase2     --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.712     --max_samples_termination=7500000     --eval_iter_start_samples=3000000 --eval_iter_samples=500000     --eval_batch_size=16 --eval_dir=/workspace/evaldata     --cache_eval_data     --output_dir=/results     --fp16 --fused_gelu_bias --dense_seq_output --fused_mha      --allreduce_post_accumulation --allreduce_post_accumulation_fp16     --gradient_accumulation_steps=1     --log_freq=1     --local_rank=7     --bert_config_path=/workspace/phase1/bert_config.j
494: Run vars: id 394913 gpus 16 mparams
295: son --seed=18263'
484: Run vars: id 394913 gpus 16 mparams
300: ++ ./bind.sh --cpu=exclusive --ib=single --cluster=circe -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=12 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
495: Run vars: id 394913 gpus 16 mparams
295: ++ ./bind.sh --cpu=exclusive --ib=single --cluster=circe -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=7 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
490: Run vars: id 394913 gpus 16 mparams
301: + eval '    ./bind.sh --cpu=exclusive --ib=single --cluster=circe --     python -u /workspace/bert/run_pretraining.py         --train_batch_size=18     --learning_rate=2.0e-3     --opt_lamb_beta_1=0.86     --opt_lamb_beta_2=0.975     --warmup_proportion=0.0     --max_steps=750     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data_phase2     --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.712     --max_samples_termination=7500000     --eval_iter_start_samples=3000000 --eval_iter_samples=500000     --eval_batch_size=16 --eval_dir=/workspace/evaldata     --cache_eval_data     --output_dir=/results     --fp16 --fused_gelu_bias --dense_seq_output --fused_mha      --allreduce_post_accumulation --allreduce_post_accumulation_fp16     --gradient_accumulation_steps=1     --log_freq=1     --local_rank=13     --bert_config_path=/workspace/phase1/bert_config.
485: Run vars: id 394913 gpus 16 mparams
301: json --seed=18263'
480: Run vars: id 394913 gpus 16 mparams
293: + eval '    ./bind.sh --cpu=exclusive --ib=single --cluster=circe --     python -u /workspace/bert/run_pretraining.py         --train_batch_size=18     --learning_rate=2.0e-3     --opt_lamb_beta_1=0.86     --opt_lamb_beta_2=0.975     --warmup_proportion=0.0     --max_steps=750     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data_phase2     --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.712     --max_samples_termination=7500000     --eval_iter_start_samples=3000000 --eval_iter_samples=500000     --eval_batch_size=16 --eval_dir=/workspace/evaldata     --cache_eval_data     --output_dir=/results     --fp16 --fused_gelu_bias --dense_seq_output --fused_mha      --allreduce_post_accumulation --allreduce_post_accumulation_fp16     --gradient_accumulation_steps=1     --log_freq=1     --local_rank=5     --bert_config_path=/workspace/phase1/bert_config.j
483: Run vars: id 394913 gpus 16 mparams
293: son --seed=18263'
481: Run vars: id 394913 gpus 16 mparams
301: ++ ./bind.sh --cpu=exclusive --ib=single --cluster=circe -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=13 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
491: Run vars: id 394913 gpus 16 mparams
293: ++ ./bind.sh --cpu=exclusive --ib=single --cluster=circe -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=5 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
492: Run vars: id 394913 gpus 16 mparams
297: + eval '    ./bind.sh --cpu=exclusive --ib=single --cluster=circe --     python -u /workspace/bert/run_pretraining.py         --train_batch_size=18     --learning_rate=2.0e-3     --opt_lamb_beta_1=0.86     --opt_lamb_beta_2=0.975     --warmup_proportion=0.0     --max_steps=750     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data_phase2     --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.712     --max_samples_termination=7500000     --eval_iter_start_samples=3000000 --eval_iter_samples=500000     --eval_batch_size=16 --eval_dir=/workspace/evaldata     --cache_eval_data     --output_dir=/results     --fp16 --fused_gelu_bias --dense_seq_output --fused_mha      --allreduce_post_accumulation --allreduce_post_accumulation_fp16     --gradient_accumulation_steps=1     --log_freq=1     --local_rank=9     --bert_config_path=/workspace/phase1/bert_config.j
489: Run vars: id 394913 gpus 16 mparams
297: son --seed=18263'
493: Run vars: id 394913 gpus 16 mparams
299: + eval '    ./bind.sh --cpu=exclusive --ib=single --cluster=circe --     python -u /workspace/bert/run_pretraining.py         --train_batch_size=18     --learning_rate=2.0e-3     --opt_lamb_beta_1=0.86     --opt_lamb_beta_2=0.975     --warmup_proportion=0.0     --max_steps=750     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data_phase2     --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.712     --max_samples_termination=7500000     --eval_iter_start_samples=3000000 --eval_iter_samples=500000     --eval_batch_size=16 --eval_dir=/workspace/evaldata     --cache_eval_data     --output_dir=/results     --fp16 --fused_gelu_bias --dense_seq_output --fused_mha      --allreduce_post_accumulation --allreduce_post_accumulation_fp16     --gradient_accumulation_steps=1     --log_freq=1     --local_rank=11     --bert_config_path=/workspace/phase1/bert_config.
494: STARTING TIMING RUN AT 2020-06-25 10:59:01 AM
299: json --seed=18263'
487: Run vars: id 394913 gpus 16 mparams
303: + eval '    ./bind.sh --cpu=exclusive --ib=single --cluster=circe --     python -u /workspace/bert/run_pretraining.py         --train_batch_size=18     --learning_rate=2.0e-3     --opt_lamb_beta_1=0.86     --opt_lamb_beta_2=0.975     --warmup_proportion=0.0     --max_steps=750     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data_phase2     --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.712     --max_samples_termination=7500000     --eval_iter_start_samples=3000000 --eval_iter_samples=500000     --eval_batch_size=16 --eval_dir=/workspace/evaldata     --cache_eval_data     --output_dir=/results     --fp16 --fused_gelu_bias --dense_seq_output --fused_mha      --allreduce_post_accumulation --allreduce_post_accumulation_fp16     --gradient_accumulation_steps=1     --log_freq=1     --local_rank=15     --bert_config_path=/workspace/phase1/bert_config.
488: STARTING TIMING RUN AT 2020-06-25 10:59:01 AM
303: json --seed=18263'
484: STARTING TIMING RUN AT 2020-06-25 10:59:01 AM
297: ++ ./bind.sh --cpu=exclusive --ib=single --cluster=circe -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=9 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
495: STARTING TIMING RUN AT 2020-06-25 10:59:01 AM
299: ++ ./bind.sh --cpu=exclusive --ib=single --cluster=circe -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=11 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
490: STARTING TIMING RUN AT 2020-06-25 10:59:01 AM
417: slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
485: STARTING TIMING RUN AT 2020-06-25 10:59:01 AM
421: slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
480: STARTING TIMING RUN AT 2020-06-25 10:59:01 AM
418: slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
483: STARTING TIMING RUN AT 2020-06-25 10:59:01 AM
422: slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
491: STARTING TIMING RUN AT 2020-06-25 10:59:01 AM
423: slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
492: STARTING TIMING RUN AT 2020-06-25 10:59:01 AM
429: slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
481: STARTING TIMING RUN AT 2020-06-25 10:59:01 AM
431: slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
493: STARTING TIMING RUN AT 2020-06-25 10:59:01 AM
425: slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
489: STARTING TIMING RUN AT 2020-06-25 10:59:01 AM
303: ++ ./bind.sh --cpu=exclusive --ib=single --cluster=circe -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=15 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
487: STARTING TIMING RUN AT 2020-06-25 10:59:01 AM
427: slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
 40: num_sockets = 2 num_nodes=2 cores_per_socket=24
420: slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
161: num_sockets = 2 num_nodes=2 cores_per_socket=24
419: slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
115: num_sockets = 2 num_nodes=2 cores_per_socket=24
426: slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
287: Run vars: id 394913 gpus 16 mparams
430: slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
277: Run vars: id 394913 gpus 16 mparams
428: slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
272: Run vars: id 394913 gpus 16 mparams
424: slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
275: Run vars: id 394913 gpus 16 mparams
289: + eval '    ./bind.sh --cpu=exclusive --ib=single --cluster=circe --     python -u /workspace/bert/run_pretraining.py         --train_batch_size=18     --learning_rate=2.0e-3     --opt_lamb_beta_1=0.86     --opt_lamb_beta_2=0.975     --warmup_proportion=0.0     --max_steps=750     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data_phase2     --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.712     --max_samples_termination=7500000     --eval_iter_start_samples=3000000 --eval_iter_samples=500000     --eval_batch_size=16 --eval_dir=/workspace/evaldata     --cache_eval_data     --output_dir=/results     --fp16 --fused_gelu_bias --dense_seq_output --fused_mha      --allreduce_post_accumulation --allreduce_post_accumulation_fp16     --gradient_accumulation_steps=1     --log_freq=1     --local_rank=1     --bert_config_path=/workspace/phase1/bert_config.j
279: Run vars: id 394913 gpus 16 mparams
289: son --seed=18263'
276: Run vars: id 394913 gpus 16 mparams
289: ++ ./bind.sh --cpu=exclusive --ib=single --cluster=circe -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=1 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
286: Run vars: id 394913 gpus 16 mparams
298: + eval '    ./bind.sh --cpu=exclusive --ib=single --cluster=circe --     python -u /workspace/bert/run_pretraining.py         --train_batch_size=18     --learning_rate=2.0e-3     --opt_lamb_beta_1=0.86     --opt_lamb_beta_2=0.975     --warmup_proportion=0.0     --max_steps=750     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data_phase2     --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.712     --max_samples_termination=7500000     --eval_iter_start_samples=3000000 --eval_iter_samples=500000     --eval_batch_size=16 --eval_dir=/workspace/evaldata     --cache_eval_data     --output_dir=/results     --fp16 --fused_gelu_bias --dense_seq_output --fused_mha      --allreduce_post_accumulation --allreduce_post_accumulation_fp16     --gradient_accumulation_steps=1     --log_freq=1     --local_rank=10     --bert_config_path=/workspace/phase1/bert_config.
282: Run vars: id 394913 gpus 16 mparams
298: json --seed=18263'
278: Run vars: id 394913 gpus 16 mparams
298: ++ ./bind.sh --cpu=exclusive --ib=single --cluster=circe -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=10 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
281: Run vars: id 394913 gpus 16 mparams
292: + eval '    ./bind.sh --cpu=exclusive --ib=single --cluster=circe --     python -u /workspace/bert/run_pretraining.py         --train_batch_size=18     --learning_rate=2.0e-3     --opt_lamb_beta_1=0.86     --opt_lamb_beta_2=0.975     --warmup_proportion=0.0     --max_steps=750     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data_phase2     --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.712     --max_samples_termination=7500000     --eval_iter_start_samples=3000000 --eval_iter_samples=500000     --eval_batch_size=16 --eval_dir=/workspace/evaldata     --cache_eval_data     --output_dir=/results     --fp16 --fused_gelu_bias --dense_seq_output --fused_mha      --allreduce_post_accumulation --allreduce_post_accumulation_fp16     --gradient_accumulation_steps=1     --log_freq=1     --local_rank=4     --bert_config_path=/workspace/phase1/bert_config.j
284: Run vars: id 394913 gpus 16 mparams
292: son --seed=18263'
273: Run vars: id 394913 gpus 16 mparams
292: ++ ./bind.sh --cpu=exclusive --ib=single --cluster=circe -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=4 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
274: Run vars: id 394913 gpus 16 mparams
421: + eval '    ./bind.sh --cpu=exclusive --ib=single --cluster=circe --     python -u /workspace/bert/run_pretraining.py         --train_batch_size=18     --learning_rate=2.0e-3     --opt_lamb_beta_1=0.86     --opt_lamb_beta_2=0.975     --warmup_proportion=0.0     --max_steps=750     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data_phase2     --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.712     --max_samples_termination=7500000     --eval_iter_start_samples=3000000 --eval_iter_samples=500000     --eval_batch_size=16 --eval_dir=/workspace/evaldata     --cache_eval_data     --output_dir=/results     --fp16 --fused_gelu_bias --dense_seq_output --fused_mha      --allreduce_post_accumulation --allreduce_post_accumulation_fp16     --gradient_accumulation_steps=1     --log_freq=1     --local_rank=5     --bert_config_path=/workspace/phase1/bert_config.j
287: STARTING TIMING RUN AT 2020-06-25 10:59:01 AM
421: son --seed=18263'
277: STARTING TIMING RUN AT 2020-06-25 10:59:01 AM
421: ++ ./bind.sh --cpu=exclusive --ib=single --cluster=circe -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=5 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
279: STARTING TIMING RUN AT 2020-06-25 10:59:01 AM
418: + eval '    ./bind.sh --cpu=exclusive --ib=single --cluster=circe --     python -u /workspace/bert/run_pretraining.py         --train_batch_size=18     --learning_rate=2.0e-3     --opt_lamb_beta_1=0.86     --opt_lamb_beta_2=0.975     --warmup_proportion=0.0     --max_steps=750     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data_phase2     --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.712     --max_samples_termination=7500000     --eval_iter_start_samples=3000000 --eval_iter_samples=500000     --eval_batch_size=16 --eval_dir=/workspace/evaldata     --cache_eval_data     --output_dir=/results     --fp16 --fused_gelu_bias --dense_seq_output --fused_mha      --allreduce_post_accumulation --allreduce_post_accumulation_fp16     --gradient_accumulation_steps=1     --log_freq=1     --local_rank=2     --bert_config_path=/workspace/phase1/bert_config.j
276: STARTING TIMING RUN AT 2020-06-25 10:59:01 AM
418: son --seed=18263'
280: Run vars: id 394913 gpus 16 mparams
459: ++ ./bind.sh --cpu=exclusive --ib=single --cluster=circe -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=11 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
272: STARTING TIMING RUN AT 2020-06-25 10:59:01 AM
418: ++ ./bind.sh --cpu=exclusive --ib=single --cluster=circe -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=2 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
275: STARTING TIMING RUN AT 2020-06-25 10:59:01 AM
430: + eval '    ./bind.sh --cpu=exclusive --ib=single --cluster=circe --     python -u /workspace/bert/run_pretraining.py         --train_batch_size=18     --learning_rate=2.0e-3     --opt_lamb_beta_1=0.86     --opt_lamb_beta_2=0.975     --warmup_proportion=0.0     --max_steps=750     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data_phase2     --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.712     --max_samples_termination=7500000     --eval_iter_start_samples=3000000 --eval_iter_samples=500000     --eval_batch_size=16 --eval_dir=/workspace/evaldata     --cache_eval_data     --output_dir=/results     --fp16 --fused_gelu_bias --dense_seq_output --fused_mha      --allreduce_post_accumulation --allreduce_post_accumulation_fp16     --gradient_accumulation_steps=1     --log_freq=1     --local_rank=14     --bert_config_path=/workspace/phase1/bert_config.
283: Run vars: id 394913 gpus 16 mparams
430: json --seed=18263'
285: Run vars: id 394913 gpus 16 mparams
430: ++ ./bind.sh --cpu=exclusive --ib=single --cluster=circe -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=14 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
286: STARTING TIMING RUN AT 2020-06-25 10:59:01 AM
424: + eval '    ./bind.sh --cpu=exclusive --ib=single --cluster=circe --     python -u /workspace/bert/run_pretraining.py         --train_batch_size=18     --learning_rate=2.0e-3     --opt_lamb_beta_1=0.86     --opt_lamb_beta_2=0.975     --warmup_proportion=0.0     --max_steps=750     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data_phase2     --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.712     --max_samples_termination=7500000     --eval_iter_start_samples=3000000 --eval_iter_samples=500000     --eval_batch_size=16 --eval_dir=/workspace/evaldata     --cache_eval_data     --output_dir=/results     --fp16 --fused_gelu_bias --dense_seq_output --fused_mha      --allreduce_post_accumulation --allreduce_post_accumulation_fp16     --gradient_accumulation_steps=1     --log_freq=1     --local_rank=8     --bert_config_path=/workspace/phase1/bert_config.j
278: STARTING TIMING RUN AT 2020-06-25 10:59:01 AM
424: son --seed=18263'
281: STARTING TIMING RUN AT 2020-06-25 10:59:01 AM
417: + eval '    ./bind.sh --cpu=exclusive --ib=single --cluster=circe --     python -u /workspace/bert/run_pretraining.py         --train_batch_size=18     --learning_rate=2.0e-3     --opt_lamb_beta_1=0.86     --opt_lamb_beta_2=0.975     --warmup_proportion=0.0     --max_steps=750     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data_phase2     --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.712     --max_samples_termination=7500000     --eval_iter_start_samples=3000000 --eval_iter_samples=500000     --eval_batch_size=16 --eval_dir=/workspace/evaldata     --cache_eval_data     --output_dir=/results     --fp16 --fused_gelu_bias --dense_seq_output --fused_mha      --allreduce_post_accumulation --allreduce_post_accumulation_fp16     --gradient_accumulation_steps=1     --log_freq=1     --local_rank=1     --bert_config_path=/workspace/phase1/bert_config.j
282: STARTING TIMING RUN AT 2020-06-25 10:59:01 AM
417: son --seed=18263'
284: STARTING TIMING RUN AT 2020-06-25 10:59:01 AM
424: ++ ./bind.sh --cpu=exclusive --ib=single --cluster=circe -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=8 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
273: STARTING TIMING RUN AT 2020-06-25 10:59:01 AM
417: ++ ./bind.sh --cpu=exclusive --ib=single --cluster=circe -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=1 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
274: STARTING TIMING RUN AT 2020-06-25 10:59:01 AM
426: + eval '    ./bind.sh --cpu=exclusive --ib=single --cluster=circe --     python -u /workspace/bert/run_pretraining.py         --train_batch_size=18     --learning_rate=2.0e-3     --opt_lamb_beta_1=0.86     --opt_lamb_beta_2=0.975     --warmup_proportion=0.0     --max_steps=750     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data_phase2     --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.712     --max_samples_termination=7500000     --eval_iter_start_samples=3000000 --eval_iter_samples=500000     --eval_batch_size=16 --eval_dir=/workspace/evaldata     --cache_eval_data     --output_dir=/results     --fp16 --fused_gelu_bias --dense_seq_output --fused_mha      --allreduce_post_accumulation --allreduce_post_accumulation_fp16     --gradient_accumulation_steps=1     --log_freq=1     --local_rank=10     --bert_config_path=/workspace/phase1/bert_config.
280: STARTING TIMING RUN AT 2020-06-25 10:59:01 AM
426: json --seed=18263'
285: STARTING TIMING RUN AT 2020-06-25 10:59:01 AM
426: ++ ./bind.sh --cpu=exclusive --ib=single --cluster=circe -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=10 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
283: STARTING TIMING RUN AT 2020-06-25 10:59:01 AM
423: + eval '    ./bind.sh --cpu=exclusive --ib=single --cluster=circe --     python -u /workspace/bert/run_pretraining.py         --train_batch_size=18     --learning_rate=2.0e-3     --opt_lamb_beta_1=0.86     --opt_lamb_beta_2=0.975     --warmup_proportion=0.0     --max_steps=750     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data_phase2     --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.712     --max_samples_termination=7500000     --eval_iter_start_samples=3000000 --eval_iter_samples=500000     --eval_batch_size=16 --eval_dir=/workspace/evaldata     --cache_eval_data     --output_dir=/results     --fp16 --fused_gelu_bias --dense_seq_output --fused_mha      --allreduce_post_accumulation --allreduce_post_accumulation_fp16     --gradient_accumulation_steps=1     --log_freq=1     --local_rank=7     --bert_config_path=/workspace/phase1/bert_config.j
305: Run vars: id 394913 gpus 16 mparams
423: son --seed=18263'
319: Run vars: id 394913 gpus 16 mparams
423: ++ ./bind.sh --cpu=exclusive --ib=single --cluster=circe -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=7 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
175: num_sockets = 2 num_nodes=2 cores_per_socket=24
419: + eval '    ./bind.sh --cpu=exclusive --ib=single --cluster=circe --     python -u /workspace/bert/run_pretraining.py         --train_batch_size=18     --learning_rate=2.0e-3     --opt_lamb_beta_1=0.86     --opt_lamb_beta_2=0.975     --warmup_proportion=0.0     --max_steps=750     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data_phase2     --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.712     --max_samples_termination=7500000     --eval_iter_start_samples=3000000 --eval_iter_samples=500000     --eval_batch_size=16 --eval_dir=/workspace/evaldata     --cache_eval_data     --output_dir=/results     --fp16 --fused_gelu_bias --dense_seq_output --fused_mha      --allreduce_post_accumulation --allreduce_post_accumulation_fp16     --gradient_accumulation_steps=1     --log_freq=1     --local_rank=3     --bert_config_path=/workspace/phase1/bert_config.j
305: STARTING TIMING RUN AT 2020-06-25 10:59:01 AM
419: son --seed=18263'
319: STARTING TIMING RUN AT 2020-06-25 10:59:01 AM
422: + eval '    ./bind.sh --cpu=exclusive --ib=single --cluster=circe --     python -u /workspace/bert/run_pretraining.py         --train_batch_size=18     --learning_rate=2.0e-3     --opt_lamb_beta_1=0.86     --opt_lamb_beta_2=0.975     --warmup_proportion=0.0     --max_steps=750     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data_phase2     --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.712     --max_samples_termination=7500000     --eval_iter_start_samples=3000000 --eval_iter_samples=500000     --eval_batch_size=16 --eval_dir=/workspace/evaldata     --cache_eval_data     --output_dir=/results     --fp16 --fused_gelu_bias --dense_seq_output --fused_mha      --allreduce_post_accumulation --allreduce_post_accumulation_fp16     --gradient_accumulation_steps=1     --log_freq=1     --local_rank=6     --bert_config_path=/workspace/phase1/bert_config.j
318: Run vars: id 394913 gpus 16 mparams
422: son --seed=18263'
309: Run vars: id 394913 gpus 16 mparams
419: ++ ./bind.sh --cpu=exclusive --ib=single --cluster=circe -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=3 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
308: Run vars: id 394913 gpus 16 mparams
422: ++ ./bind.sh --cpu=exclusive --ib=single --cluster=circe -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=6 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
310: Run vars: id 394913 gpus 16 mparams
420: + eval '    ./bind.sh --cpu=exclusive --ib=single --cluster=circe --     python -u /workspace/bert/run_pretraining.py         --train_batch_size=18     --learning_rate=2.0e-3     --opt_lamb_beta_1=0.86     --opt_lamb_beta_2=0.975     --warmup_proportion=0.0     --max_steps=750     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data_phase2     --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.712     --max_samples_termination=7500000     --eval_iter_start_samples=3000000 --eval_iter_samples=500000     --eval_batch_size=16 --eval_dir=/workspace/evaldata     --cache_eval_data     --output_dir=/results     --fp16 --fused_gelu_bias --dense_seq_output --fused_mha      --allreduce_post_accumulation --allreduce_post_accumulation_fp16     --gradient_accumulation_steps=1     --log_freq=1     --local_rank=4     --bert_config_path=/workspace/phase1/bert_config.j
315: Run vars: id 394913 gpus 16 mparams
420: son --seed=18263'
312: Run vars: id 394913 gpus 16 mparams
420: ++ ./bind.sh --cpu=exclusive --ib=single --cluster=circe -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=4 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
313: Run vars: id 394913 gpus 16 mparams
427: + eval '    ./bind.sh --cpu=exclusive --ib=single --cluster=circe --     python -u /workspace/bert/run_pretraining.py         --train_batch_size=18     --learning_rate=2.0e-3     --opt_lamb_beta_1=0.86     --opt_lamb_beta_2=0.975     --warmup_proportion=0.0     --max_steps=750     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data_phase2     --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.712     --max_samples_termination=7500000     --eval_iter_start_samples=3000000 --eval_iter_samples=500000     --eval_batch_size=16 --eval_dir=/workspace/evaldata     --cache_eval_data     --output_dir=/results     --fp16 --fused_gelu_bias --dense_seq_output --fused_mha      --allreduce_post_accumulation --allreduce_post_accumulation_fp16     --gradient_accumulation_steps=1     --log_freq=1     --local_rank=11     --bert_config_path=/workspace/phase1/bert_config.
307: Run vars: id 394913 gpus 16 mparams
427: json --seed=18263'
304: Run vars: id 394913 gpus 16 mparams
427: ++ ./bind.sh --cpu=exclusive --ib=single --cluster=circe -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=11 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
306: Run vars: id 394913 gpus 16 mparams
416: + eval '    ./bind.sh --cpu=exclusive --ib=single --cluster=circe --     python -u /workspace/bert/run_pretraining.py         --train_batch_size=18     --learning_rate=2.0e-3     --opt_lamb_beta_1=0.86     --opt_lamb_beta_2=0.975     --warmup_proportion=0.0     --max_steps=750     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data_phase2     --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.712     --max_samples_termination=7500000     --eval_iter_start_samples=3000000 --eval_iter_samples=500000     --eval_batch_size=16 --eval_dir=/workspace/evaldata     --cache_eval_data     --output_dir=/results     --fp16 --fused_gelu_bias --dense_seq_output --fused_mha      --allreduce_post_accumulation --allreduce_post_accumulation_fp16     --gradient_accumulation_steps=1     --log_freq=1     --local_rank=0     --bert_config_path=/workspace/phase1/bert_config.j
316: Run vars: id 394913 gpus 16 mparams
416: son --seed=18263'
317: Run vars: id 394913 gpus 16 mparams
416: ++ ./bind.sh --cpu=exclusive --ib=single --cluster=circe -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=0 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
318: STARTING TIMING RUN AT 2020-06-25 10:59:01 AM
425: + eval '    ./bind.sh --cpu=exclusive --ib=single --cluster=circe --     python -u /workspace/bert/run_pretraining.py         --train_batch_size=18     --learning_rate=2.0e-3     --opt_lamb_beta_1=0.86     --opt_lamb_beta_2=0.975     --warmup_proportion=0.0     --max_steps=750     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data_phase2     --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.712     --max_samples_termination=7500000     --eval_iter_start_samples=3000000 --eval_iter_samples=500000     --eval_batch_size=16 --eval_dir=/workspace/evaldata     --cache_eval_data     --output_dir=/results     --fp16 --fused_gelu_bias --dense_seq_output --fused_mha      --allreduce_post_accumulation --allreduce_post_accumulation_fp16     --gradient_accumulation_steps=1     --log_freq=1     --local_rank=9     --bert_config_path=/workspace/phase1/bert_config.j
  6: Run vars: id 394913 gpus 16 mparams
100: slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
  1: Run vars: id 394913 gpus 16 mparams
425: son --seed=18263'
  9: Run vars: id 394913 gpus 16 mparams
103: slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
 11: Run vars: id 394913 gpus 16 mparams
102: slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
  5: Run vars: id 394913 gpus 16 mparams
425: ++ ./bind.sh --cpu=exclusive --ib=single --cluster=circe -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=9 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
  3: Run vars: id 394913 gpus 16 mparams
104: slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
 12: Run vars: id 394913 gpus 16 mparams
428: + eval '    ./bind.sh --cpu=exclusive --ib=single --cluster=circe --     python -u /workspace/bert/run_pretraining.py         --train_batch_size=18     --learning_rate=2.0e-3     --opt_lamb_beta_1=0.86     --opt_lamb_beta_2=0.975     --warmup_proportion=0.0     --max_steps=750     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data_phase2     --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.712     --max_samples_termination=7500000     --eval_iter_start_samples=3000000 --eval_iter_samples=500000     --eval_batch_size=16 --eval_dir=/workspace/evaldata     --cache_eval_data     --output_dir=/results     --fp16 --fused_gelu_bias --dense_seq_output --fused_mha      --allreduce_post_accumulation --allreduce_post_accumulation_fp16     --gradient_accumulation_steps=1     --log_freq=1     --local_rank=12     --bert_config_path=/workspace/phase1/bert_config.
 14: Run vars: id 394913 gpus 16 mparams
111: slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
  1: STARTING TIMING RUN AT 2020-06-25 10:59:01 AM
428: json --seed=18263'
  9: STARTING TIMING RUN AT 2020-06-25 10:59:01 AM
108: slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
  5: STARTING TIMING RUN AT 2020-06-25 10:59:01 AM
 98: slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
  6: STARTING TIMING RUN AT 2020-06-25 10:59:01 AM
428: ++ ./bind.sh --cpu=exclusive --ib=single --cluster=circe -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=12 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
  0: Run vars: id 394913 gpus 16 mparams
101: slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
 12: STARTING TIMING RUN AT 2020-06-25 10:59:01 AM
429: + eval '    ./bind.sh --cpu=exclusive --ib=single --cluster=circe --     python -u /workspace/bert/run_pretraining.py         --train_batch_size=18     --learning_rate=2.0e-3     --opt_lamb_beta_1=0.86     --opt_lamb_beta_2=0.975     --warmup_proportion=0.0     --max_steps=750     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data_phase2     --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.712     --max_samples_termination=7500000     --eval_iter_start_samples=3000000 --eval_iter_samples=500000     --eval_batch_size=16 --eval_dir=/workspace/evaldata     --cache_eval_data     --output_dir=/results     --fp16 --fused_gelu_bias --dense_seq_output --fused_mha      --allreduce_post_accumulation --allreduce_post_accumulation_fp16     --gradient_accumulation_steps=1     --log_freq=1     --local_rank=13     --bert_config_path=/workspace/phase1/bert_config.
  2: Run vars: id 394913 gpus 16 mparams
107: slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
 11: STARTING TIMING RUN AT 2020-06-25 10:59:01 AM
429: json --seed=18263'
 10: Run vars: id 394913 gpus 16 mparams
 96: slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
  7: Run vars: id 394913 gpus 16 mparams
 97: slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
  3: STARTING TIMING RUN AT 2020-06-25 10:59:01 AM
106: slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
 13: Run vars: id 394913 gpus 16 mparams
109: slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
 14: STARTING TIMING RUN AT 2020-06-25 10:59:01 AM
105: slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
 15: Run vars: id 394913 gpus 16 mparams
110: slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
  4: Run vars: id 394913 gpus 16 mparams
429: ++ ./bind.sh --cpu=exclusive --ib=single --cluster=circe -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=13 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
309: STARTING TIMING RUN AT 2020-06-25 10:59:01 AM
431: + eval '    ./bind.sh --cpu=exclusive --ib=single --cluster=circe --     python -u /workspace/bert/run_pretraining.py         --train_batch_size=18     --learning_rate=2.0e-3     --opt_lamb_beta_1=0.86     --opt_lamb_beta_2=0.975     --warmup_proportion=0.0     --max_steps=750     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data_phase2     --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.712     --max_samples_termination=7500000     --eval_iter_start_samples=3000000 --eval_iter_samples=500000     --eval_batch_size=16 --eval_dir=/workspace/evaldata     --cache_eval_data     --output_dir=/results     --fp16 --fused_gelu_bias --dense_seq_output --fused_mha      --allreduce_post_accumulation --allreduce_post_accumulation_fp16     --gradient_accumulation_steps=1     --log_freq=1     --local_rank=15     --bert_config_path=/workspace/phase1/bert_config.
449: num_sockets = 2 num_nodes=2 cores_per_socket=24
431: json --seed=18263'
314: Run vars: id 394913 gpus 16 mparams
337: slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
  8: Run vars: id 394913 gpus 16 mparams
102: + eval '    ./bind.sh --cpu=exclusive --ib=single --cluster=circe --     python -u /workspace/bert/run_pretraining.py         --train_batch_size=18     --learning_rate=2.0e-3     --opt_lamb_beta_1=0.86     --opt_lamb_beta_2=0.975     --warmup_proportion=0.0     --max_steps=750     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data_phase2     --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.712     --max_samples_termination=7500000     --eval_iter_start_samples=3000000 --eval_iter_samples=500000     --eval_batch_size=16 --eval_dir=/workspace/evaldata     --cache_eval_data     --output_dir=/results     --fp16 --fused_gelu_bias --dense_seq_output --fused_mha      --allreduce_post_accumulation --allreduce_post_accumulation_fp16     --gradient_accumulation_steps=1     --log_freq=1     --local_rank=6     --bert_config_path=/workspace/phase1/bert_config.j
308: STARTING TIMING RUN AT 2020-06-25 10:59:01 AM
102: son --seed=18263'
  0: STARTING TIMING RUN AT 2020-06-25 10:59:01 AM
373: slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
310: STARTING TIMING RUN AT 2020-06-25 10:59:01 AM
372: slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
  2: STARTING TIMING RUN AT 2020-06-25 10:59:01 AM
369: slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
311: Run vars: id 394913 gpus 16 mparams
375: slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
 10: STARTING TIMING RUN AT 2020-06-25 10:59:01 AM
102: ++ ./bind.sh --cpu=exclusive --ib=single --cluster=circe -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=6 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
312: STARTING TIMING RUN AT 2020-06-25 10:59:01 AM
368: slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
313: STARTING TIMING RUN AT 2020-06-25 10:59:01 AM
370: slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
 13: STARTING TIMING RUN AT 2020-06-25 10:59:01 AM
371: slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
307: STARTING TIMING RUN AT 2020-06-25 10:59:01 AM
377: slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
315: STARTING TIMING RUN AT 2020-06-25 10:59:01 AM
376: slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
  7: STARTING TIMING RUN AT 2020-06-25 10:59:01 AM
378: slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
304: STARTING TIMING RUN AT 2020-06-25 10:59:01 AM
380: slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
306: STARTING TIMING RUN AT 2020-06-25 10:59:01 AM
382: slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
316: STARTING TIMING RUN AT 2020-06-25 10:59:01 AM
381: slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
 15: STARTING TIMING RUN AT 2020-06-25 10:59:01 AM
 24: slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
  4: STARTING TIMING RUN AT 2020-06-25 10:59:01 AM
 19: slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
317: STARTING TIMING RUN AT 2020-06-25 10:59:01 AM
 16: slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
314: STARTING TIMING RUN AT 2020-06-25 10:59:01 AM
 18: slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
  8: STARTING TIMING RUN AT 2020-06-25 10:59:01 AM
 25: slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
257: num_sockets = 2 num_nodes=2 cores_per_socket=24
383: slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
311: STARTING TIMING RUN AT 2020-06-25 10:59:01 AM
 26: slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
450: num_sockets = 2 num_nodes=2 cores_per_socket=24
 31: slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
324: num_sockets = 2 num_nodes=2 cores_per_socket=24
 20: slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
113: num_sockets = 2 num_nodes=2 cores_per_socket=24
 21: slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
121: num_sockets = 2 num_nodes=2 cores_per_socket=24
 28: slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
456: num_sockets = 2 num_nodes=2 cores_per_socket=24
 27: slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
371: num_sockets = 2 num_nodes=2 cores_per_socket=24
 22: slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
136: num_sockets = 2 num_nodes=2 cores_per_socket=24
 23: slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
263: num_sockets = 2 num_nodes=2 cores_per_socket=24
 29: slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
150: num_sockets = 2 num_nodes=2 cores_per_socket=24
 30: slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
137: num_sockets = 2 num_nodes=2 cores_per_socket=24
 26: + eval '    ./bind.sh --cpu=exclusive --ib=single --cluster=circe --     python -u /workspace/bert/run_pretraining.py         --train_batch_size=18     --learning_rate=2.0e-3     --opt_lamb_beta_1=0.86     --opt_lamb_beta_2=0.975     --warmup_proportion=0.0     --max_steps=750     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data_phase2     --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.712     --max_samples_termination=7500000     --eval_iter_start_samples=3000000 --eval_iter_samples=500000     --eval_batch_size=16 --eval_dir=/workspace/evaldata     --cache_eval_data     --output_dir=/results     --fp16 --fused_gelu_bias --dense_seq_output --fused_mha      --allreduce_post_accumulation --allreduce_post_accumulation_fp16     --gradient_accumulation_steps=1     --log_freq=1     --local_rank=10     --bert_config_path=/workspace/phase1/bert_config.
 47: num_sockets = 2 num_nodes=2 cores_per_socket=24
 26: json --seed=18263'
239: num_sockets = 2 num_nodes=2 cores_per_socket=24
 17: + eval '    ./bind.sh --cpu=exclusive --ib=single --cluster=circe --     python -u /workspace/bert/run_pretraining.py         --train_batch_size=18     --learning_rate=2.0e-3     --opt_lamb_beta_1=0.86     --opt_lamb_beta_2=0.975     --warmup_proportion=0.0     --max_steps=750     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data_phase2     --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.712     --max_samples_termination=7500000     --eval_iter_start_samples=3000000 --eval_iter_samples=500000     --eval_batch_size=16 --eval_dir=/workspace/evaldata     --cache_eval_data     --output_dir=/results     --fp16 --fused_gelu_bias --dense_seq_output --fused_mha      --allreduce_post_accumulation --allreduce_post_accumulation_fp16     --gradient_accumulation_steps=1     --log_freq=1     --local_rank=1     --bert_config_path=/workspace/phase1/bert_config.j
232: num_sockets = 2 num_nodes=2 cores_per_socket=24
 17: son --seed=18263'
338: num_sockets = 2 num_nodes=2 cores_per_socket=24
 26: ++ ./bind.sh --cpu=exclusive --ib=single --cluster=circe -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=10 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
229: num_sockets = 2 num_nodes=2 cores_per_socket=24
374: slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
337: num_sockets = 2 num_nodes=2 cores_per_socket=24
 17: ++ ./bind.sh --cpu=exclusive --ib=single --cluster=circe -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=1 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
496: num_sockets = 2 num_nodes=2 cores_per_socket=24
 24: + eval '    ./bind.sh --cpu=exclusive --ib=single --cluster=circe --     python -u /workspace/bert/run_pretraining.py         --train_batch_size=18     --learning_rate=2.0e-3     --opt_lamb_beta_1=0.86     --opt_lamb_beta_2=0.975     --warmup_proportion=0.0     --max_steps=750     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data_phase2     --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.712     --max_samples_termination=7500000     --eval_iter_start_samples=3000000 --eval_iter_samples=500000     --eval_batch_size=16 --eval_dir=/workspace/evaldata     --cache_eval_data     --output_dir=/results     --fp16 --fused_gelu_bias --dense_seq_output --fused_mha      --allreduce_post_accumulation --allreduce_post_accumulation_fp16     --gradient_accumulation_steps=1     --log_freq=1     --local_rank=8     --bert_config_path=/workspace/phase1/bert_config.j
385: num_sockets = 2 num_nodes=2 cores_per_socket=24
111: + eval '    ./bind.sh --cpu=exclusive --ib=single --cluster=circe --     python -u /workspace/bert/run_pretraining.py         --train_batch_size=18     --learning_rate=2.0e-3     --opt_lamb_beta_1=0.86     --opt_lamb_beta_2=0.975     --warmup_proportion=0.0     --max_steps=750     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data_phase2     --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.712     --max_samples_termination=7500000     --eval_iter_start_samples=3000000 --eval_iter_samples=500000     --eval_batch_size=16 --eval_dir=/workspace/evaldata     --cache_eval_data     --output_dir=/results     --fp16 --fused_gelu_bias --dense_seq_output --fused_mha      --allreduce_post_accumulation --allreduce_post_accumulation_fp16     --gradient_accumulation_steps=1     --log_freq=1     --local_rank=15     --bert_config_path=/workspace/phase1/bert_config.
479: num_sockets = 2 num_nodes=2 cores_per_socket=24
 24: son --seed=18263'
 24: num_sockets = 2 num_nodes=2 cores_per_socket=24
111: json --seed=18263'
294: num_sockets = 2 num_nodes=2 cores_per_socket=24
103: + eval '    ./bind.sh --cpu=exclusive --ib=single --cluster=circe --     python -u /workspace/bert/run_pretraining.py         --train_batch_size=18     --learning_rate=2.0e-3     --opt_lamb_beta_1=0.86     --opt_lamb_beta_2=0.975     --warmup_proportion=0.0     --max_steps=750     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data_phase2     --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.712     --max_samples_termination=7500000     --eval_iter_start_samples=3000000 --eval_iter_samples=500000     --eval_batch_size=16 --eval_dir=/workspace/evaldata     --cache_eval_data     --output_dir=/results     --fp16 --fused_gelu_bias --dense_seq_output --fused_mha      --allreduce_post_accumulation --allreduce_post_accumulation_fp16     --gradient_accumulation_steps=1     --log_freq=1     --local_rank=7     --bert_config_path=/workspace/phase1/bert_config.j
178: num_sockets = 2 num_nodes=2 cores_per_socket=24
103: son --seed=18263'
185: num_sockets = 2 num_nodes=2 cores_per_socket=24
106: + eval '    ./bind.sh --cpu=exclusive --ib=single --cluster=circe --     python -u /workspace/bert/run_pretraining.py         --train_batch_size=18     --learning_rate=2.0e-3     --opt_lamb_beta_1=0.86     --opt_lamb_beta_2=0.975     --warmup_proportion=0.0     --max_steps=750     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data_phase2     --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.712     --max_samples_termination=7500000     --eval_iter_start_samples=3000000 --eval_iter_samples=500000     --eval_batch_size=16 --eval_dir=/workspace/evaldata     --cache_eval_data     --output_dir=/results     --fp16 --fused_gelu_bias --dense_seq_output --fused_mha      --allreduce_post_accumulation --allreduce_post_accumulation_fp16     --gradient_accumulation_steps=1     --log_freq=1     --local_rank=10     --bert_config_path=/workspace/phase1/bert_config.
241: num_sockets = 2 num_nodes=2 cores_per_socket=24
106: json --seed=18263'
302: num_sockets = 2 num_nodes=2 cores_per_socket=24
106: ++ ./bind.sh --cpu=exclusive --ib=single --cluster=circe -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=10 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
182: num_sockets = 2 num_nodes=2 cores_per_socket=24
111: ++ ./bind.sh --cpu=exclusive --ib=single --cluster=circe -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=15 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
224: num_sockets = 2 num_nodes=2 cores_per_socket=24
103: ++ ./bind.sh --cpu=exclusive --ib=single --cluster=circe -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=7 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
141: num_sockets = 2 num_nodes=2 cores_per_socket=24
 99: + eval '    ./bind.sh --cpu=exclusive --ib=single --cluster=circe --     python -u /workspace/bert/run_pretraining.py         --train_batch_size=18     --learning_rate=2.0e-3     --opt_lamb_beta_1=0.86     --opt_lamb_beta_2=0.975     --warmup_proportion=0.0     --max_steps=750     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data_phase2     --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.712     --max_samples_termination=7500000     --eval_iter_start_samples=3000000 --eval_iter_samples=500000     --eval_batch_size=16 --eval_dir=/workspace/evaldata     --cache_eval_data     --output_dir=/results     --fp16 --fused_gelu_bias --dense_seq_output --fused_mha      --allreduce_post_accumulation --allreduce_post_accumulation_fp16     --gradient_accumulation_steps=1     --log_freq=1     --local_rank=3     --bert_config_path=/workspace/phase1/bert_config.j
202: num_sockets = 2 num_nodes=2 cores_per_socket=24
 99: son --seed=18263'
242: num_sockets = 2 num_nodes=2 cores_per_socket=24
104: + eval '    ./bind.sh --cpu=exclusive --ib=single --cluster=circe --     python -u /workspace/bert/run_pretraining.py         --train_batch_size=18     --learning_rate=2.0e-3     --opt_lamb_beta_1=0.86     --opt_lamb_beta_2=0.975     --warmup_proportion=0.0     --max_steps=750     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data_phase2     --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.712     --max_samples_termination=7500000     --eval_iter_start_samples=3000000 --eval_iter_samples=500000     --eval_batch_size=16 --eval_dir=/workspace/evaldata     --cache_eval_data     --output_dir=/results     --fp16 --fused_gelu_bias --dense_seq_output --fused_mha      --allreduce_post_accumulation --allreduce_post_accumulation_fp16     --gradient_accumulation_steps=1     --log_freq=1     --local_rank=8     --bert_config_path=/workspace/phase1/bert_config.j
158: num_sockets = 2 num_nodes=2 cores_per_socket=24
104: son --seed=18263'
477: num_sockets = 2 num_nodes=2 cores_per_socket=24
104: ++ ./bind.sh --cpu=exclusive --ib=single --cluster=circe -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=8 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
434: num_sockets = 2 num_nodes=2 cores_per_socket=24
105: + eval '    ./bind.sh --cpu=exclusive --ib=single --cluster=circe --     python -u /workspace/bert/run_pretraining.py         --train_batch_size=18     --learning_rate=2.0e-3     --opt_lamb_beta_1=0.86     --opt_lamb_beta_2=0.975     --warmup_proportion=0.0     --max_steps=750     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data_phase2     --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.712     --max_samples_termination=7500000     --eval_iter_start_samples=3000000 --eval_iter_samples=500000     --eval_batch_size=16 --eval_dir=/workspace/evaldata     --cache_eval_data     --output_dir=/results     --fp16 --fused_gelu_bias --dense_seq_output --fused_mha      --allreduce_post_accumulation --allreduce_post_accumulation_fp16     --gradient_accumulation_steps=1     --log_freq=1     --local_rank=9     --bert_config_path=/workspace/phase1/bert_config.j
 41: num_sockets = 2 num_nodes=2 cores_per_socket=24
379: slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
194: num_sockets = 2 num_nodes=2 cores_per_socket=24
105: son --seed=18263'
196: num_sockets = 2 num_nodes=2 cores_per_socket=24
 99: ++ ./bind.sh --cpu=exclusive --ib=single --cluster=circe -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=3 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
432: num_sockets = 2 num_nodes=2 cores_per_socket=24
105: ++ ./bind.sh --cpu=exclusive --ib=single --cluster=circe -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=9 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
387: num_sockets = 2 num_nodes=2 cores_per_socket=24
100: + eval '    ./bind.sh --cpu=exclusive --ib=single --cluster=circe --     python -u /workspace/bert/run_pretraining.py         --train_batch_size=18     --learning_rate=2.0e-3     --opt_lamb_beta_1=0.86     --opt_lamb_beta_2=0.975     --warmup_proportion=0.0     --max_steps=750     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data_phase2     --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.712     --max_samples_termination=7500000     --eval_iter_start_samples=3000000 --eval_iter_samples=500000     --eval_batch_size=16 --eval_dir=/workspace/evaldata     --cache_eval_data     --output_dir=/results     --fp16 --fused_gelu_bias --dense_seq_output --fused_mha      --allreduce_post_accumulation --allreduce_post_accumulation_fp16     --gradient_accumulation_steps=1     --log_freq=1     --local_rank=4     --bert_config_path=/workspace/phase1/bert_config.j
340: num_sockets = 2 num_nodes=2 cores_per_socket=24
100: son --seed=18263'
336: num_sockets = 2 num_nodes=2 cores_per_socket=24
107: + eval '    ./bind.sh --cpu=exclusive --ib=single --cluster=circe --     python -u /workspace/bert/run_pretraining.py         --train_batch_size=18     --learning_rate=2.0e-3     --opt_lamb_beta_1=0.86     --opt_lamb_beta_2=0.975     --warmup_proportion=0.0     --max_steps=750     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data_phase2     --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.712     --max_samples_termination=7500000     --eval_iter_start_samples=3000000 --eval_iter_samples=500000     --eval_batch_size=16 --eval_dir=/workspace/evaldata     --cache_eval_data     --output_dir=/results     --fp16 --fused_gelu_bias --dense_seq_output --fused_mha      --allreduce_post_accumulation --allreduce_post_accumulation_fp16     --gradient_accumulation_steps=1     --log_freq=1     --local_rank=11     --bert_config_path=/workspace/phase1/bert_config.
122: num_sockets = 2 num_nodes=2 cores_per_socket=24
107: json --seed=18263'
128: num_sockets = 2 num_nodes=2 cores_per_socket=24
101: + eval '    ./bind.sh --cpu=exclusive --ib=single --cluster=circe --     python -u /workspace/bert/run_pretraining.py         --train_batch_size=18     --learning_rate=2.0e-3     --opt_lamb_beta_1=0.86     --opt_lamb_beta_2=0.975     --warmup_proportion=0.0     --max_steps=750     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data_phase2     --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.712     --max_samples_termination=7500000     --eval_iter_start_samples=3000000 --eval_iter_samples=500000     --eval_batch_size=16 --eval_dir=/workspace/evaldata     --cache_eval_data     --output_dir=/results     --fp16 --fused_gelu_bias --dense_seq_output --fused_mha      --allreduce_post_accumulation --allreduce_post_accumulation_fp16     --gradient_accumulation_steps=1     --log_freq=1     --local_rank=5     --bert_config_path=/workspace/phase1/bert_config.j
 48: num_sockets = 2 num_nodes=2 cores_per_socket=24
101: son --seed=18263'
403: num_sockets = 2 num_nodes=2 cores_per_socket=24
 98: + eval '    ./bind.sh --cpu=exclusive --ib=single --cluster=circe --     python -u /workspace/bert/run_pretraining.py         --train_batch_size=18     --learning_rate=2.0e-3     --opt_lamb_beta_1=0.86     --opt_lamb_beta_2=0.975     --warmup_proportion=0.0     --max_steps=750     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data_phase2     --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.712     --max_samples_termination=7500000     --eval_iter_start_samples=3000000 --eval_iter_samples=500000     --eval_batch_size=16 --eval_dir=/workspace/evaldata     --cache_eval_data     --output_dir=/results     --fp16 --fused_gelu_bias --dense_seq_output --fused_mha      --allreduce_post_accumulation --allreduce_post_accumulation_fp16     --gradient_accumulation_steps=1     --log_freq=1     --local_rank=2     --bert_config_path=/workspace/phase1/bert_config.j
448: num_sockets = 2 num_nodes=2 cores_per_socket=24
 98: son --seed=18263'
 50: num_sockets = 2 num_nodes=2 cores_per_socket=24
107: ++ ./bind.sh --cpu=exclusive --ib=single --cluster=circe -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=11 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
 49: num_sockets = 2 num_nodes=2 cores_per_socket=24
100: ++ ./bind.sh --cpu=exclusive --ib=single --cluster=circe -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=4 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
388: num_sockets = 2 num_nodes=2 cores_per_socket=24
101: ++ ./bind.sh --cpu=exclusive --ib=single --cluster=circe -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=5 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
322: num_sockets = 2 num_nodes=2 cores_per_socket=24
108: + eval '    ./bind.sh --cpu=exclusive --ib=single --cluster=circe --     python -u /workspace/bert/run_pretraining.py         --train_batch_size=18     --learning_rate=2.0e-3     --opt_lamb_beta_1=0.86     --opt_lamb_beta_2=0.975     --warmup_proportion=0.0     --max_steps=750     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data_phase2     --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.712     --max_samples_termination=7500000     --eval_iter_start_samples=3000000 --eval_iter_samples=500000     --eval_batch_size=16 --eval_dir=/workspace/evaldata     --cache_eval_data     --output_dir=/results     --fp16 --fused_gelu_bias --dense_seq_output --fused_mha      --allreduce_post_accumulation --allreduce_post_accumulation_fp16     --gradient_accumulation_steps=1     --log_freq=1     --local_rank=12     --bert_config_path=/workspace/phase1/bert_config.
482: num_sockets = 2 num_nodes=2 cores_per_socket=24
108: json --seed=18263'
486: num_sockets = 2 num_nodes=2 cores_per_socket=24
 98: ++ ./bind.sh --cpu=exclusive --ib=single --cluster=circe -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=2 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
305: num_sockets = 2 num_nodes=2 cores_per_socket=24
108: ++ ./bind.sh --cpu=exclusive --ib=single --cluster=circe -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=12 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
 77: num_sockets = 2 num_nodes=2 cores_per_socket=24
 97: + eval '    ./bind.sh --cpu=exclusive --ib=single --cluster=circe --     python -u /workspace/bert/run_pretraining.py         --train_batch_size=18     --learning_rate=2.0e-3     --opt_lamb_beta_1=0.86     --opt_lamb_beta_2=0.975     --warmup_proportion=0.0     --max_steps=750     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data_phase2     --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.712     --max_samples_termination=7500000     --eval_iter_start_samples=3000000 --eval_iter_samples=500000     --eval_batch_size=16 --eval_dir=/workspace/evaldata     --cache_eval_data     --output_dir=/results     --fp16 --fused_gelu_bias --dense_seq_output --fused_mha      --allreduce_post_accumulation --allreduce_post_accumulation_fp16     --gradient_accumulation_steps=1     --log_freq=1     --local_rank=1     --bert_config_path=/workspace/phase1/bert_config.j
319: num_sockets = 2 num_nodes=2 cores_per_socket=24
 97: son --seed=18263'
475: num_sockets = 2 num_nodes=2 cores_per_socket=24
 97: ++ ./bind.sh --cpu=exclusive --ib=single --cluster=circe -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=1 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
499: num_sockets = 2 num_nodes=2 cores_per_socket=24
 24: ++ ./bind.sh --cpu=exclusive --ib=single --cluster=circe -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=8 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
330: num_sockets = 2 num_nodes=2 cores_per_socket=24
 18: + eval '    ./bind.sh --cpu=exclusive --ib=single --cluster=circe --     python -u /workspace/bert/run_pretraining.py         --train_batch_size=18     --learning_rate=2.0e-3     --opt_lamb_beta_1=0.86     --opt_lamb_beta_2=0.975     --warmup_proportion=0.0     --max_steps=750     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data_phase2     --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.712     --max_samples_termination=7500000     --eval_iter_start_samples=3000000 --eval_iter_samples=500000     --eval_batch_size=16 --eval_dir=/workspace/evaldata     --cache_eval_data     --output_dir=/results     --fp16 --fused_gelu_bias --dense_seq_output --fused_mha      --allreduce_post_accumulation --allreduce_post_accumulation_fp16     --gradient_accumulation_steps=1     --log_freq=1     --local_rank=2     --bert_config_path=/workspace/phase1/bert_config.j
 68: num_sockets = 2 num_nodes=2 cores_per_socket=24
 18: son --seed=18263'
 17: num_sockets = 2 num_nodes=2 cores_per_socket=24
 18: ++ ./bind.sh --cpu=exclusive --ib=single --cluster=circe -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=2 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
157: num_sockets = 2 num_nodes=2 cores_per_socket=24
110: + eval '    ./bind.sh --cpu=exclusive --ib=single --cluster=circe --     python -u /workspace/bert/run_pretraining.py         --train_batch_size=18     --learning_rate=2.0e-3     --opt_lamb_beta_1=0.86     --opt_lamb_beta_2=0.975     --warmup_proportion=0.0     --max_steps=750     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data_phase2     --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.712     --max_samples_termination=7500000     --eval_iter_start_samples=3000000 --eval_iter_samples=500000     --eval_batch_size=16 --eval_dir=/workspace/evaldata     --cache_eval_data     --output_dir=/results     --fp16 --fused_gelu_bias --dense_seq_output --fused_mha      --allreduce_post_accumulation --allreduce_post_accumulation_fp16     --gradient_accumulation_steps=1     --log_freq=1     --local_rank=14     --bert_config_path=/workspace/phase1/bert_config.
191: num_sockets = 2 num_nodes=2 cores_per_socket=24
110: json --seed=18263'
248: num_sockets = 2 num_nodes=2 cores_per_socket=24
110: ++ ./bind.sh --cpu=exclusive --ib=single --cluster=circe -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=14 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
181: num_sockets = 2 num_nodes=2 cores_per_socket=24
334: ++ ./bind.sh --cpu=exclusive --ib=single --cluster=circe -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=14 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
 26: num_sockets = 2 num_nodes=2 cores_per_socket=24
 20: + eval '    ./bind.sh --cpu=exclusive --ib=single --cluster=circe --     python -u /workspace/bert/run_pretraining.py         --train_batch_size=18     --learning_rate=2.0e-3     --opt_lamb_beta_1=0.86     --opt_lamb_beta_2=0.975     --warmup_proportion=0.0     --max_steps=750     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data_phase2     --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.712     --max_samples_termination=7500000     --eval_iter_start_samples=3000000 --eval_iter_samples=500000     --eval_batch_size=16 --eval_dir=/workspace/evaldata     --cache_eval_data     --output_dir=/results     --fp16 --fused_gelu_bias --dense_seq_output --fused_mha      --allreduce_post_accumulation --allreduce_post_accumulation_fp16     --gradient_accumulation_steps=1     --log_freq=1     --local_rank=4     --bert_config_path=/workspace/phase1/bert_config.j
331: num_sockets = 2 num_nodes=2 cores_per_socket=24
 20: son --seed=18263'
290: num_sockets = 2 num_nodes=2 cores_per_socket=24
 20: ++ ./bind.sh --cpu=exclusive --ib=single --cluster=circe -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=4 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
 66: num_sockets = 2 num_nodes=2 cores_per_socket=24
 96: + eval '    ./bind.sh --cpu=exclusive --ib=single --cluster=circe --     python -u /workspace/bert/run_pretraining.py         --train_batch_size=18     --learning_rate=2.0e-3     --opt_lamb_beta_1=0.86     --opt_lamb_beta_2=0.975     --warmup_proportion=0.0     --max_steps=750     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data_phase2     --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.712     --max_samples_termination=7500000     --eval_iter_start_samples=3000000 --eval_iter_samples=500000     --eval_batch_size=16 --eval_dir=/workspace/evaldata     --cache_eval_data     --output_dir=/results     --fp16 --fused_gelu_bias --dense_seq_output --fused_mha      --allreduce_post_accumulation --allreduce_post_accumulation_fp16     --gradient_accumulation_steps=1     --log_freq=1     --local_rank=0     --bert_config_path=/workspace/phase1/bert_config.j
476: num_sockets = 2 num_nodes=2 cores_per_socket=24
 96: son --seed=18263'
288: num_sockets = 2 num_nodes=2 cores_per_socket=24
109: + eval '    ./bind.sh --cpu=exclusive --ib=single --cluster=circe --     python -u /workspace/bert/run_pretraining.py         --train_batch_size=18     --learning_rate=2.0e-3     --opt_lamb_beta_1=0.86     --opt_lamb_beta_2=0.975     --warmup_proportion=0.0     --max_steps=750     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data_phase2     --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.712     --max_samples_termination=7500000     --eval_iter_start_samples=3000000 --eval_iter_samples=500000     --eval_batch_size=16 --eval_dir=/workspace/evaldata     --cache_eval_data     --output_dir=/results     --fp16 --fused_gelu_bias --dense_seq_output --fused_mha      --allreduce_post_accumulation --allreduce_post_accumulation_fp16     --gradient_accumulation_steps=1     --log_freq=1     --local_rank=13     --bert_config_path=/workspace/phase1/bert_config.
 18: num_sockets = 2 num_nodes=2 cores_per_socket=24
109: json --seed=18263'
399: num_sockets = 2 num_nodes=2 cores_per_socket=24
 96: ++ ./bind.sh --cpu=exclusive --ib=single --cluster=circe -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=0 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
501: num_sockets = 2 num_nodes=2 cores_per_socket=24
109: ++ ./bind.sh --cpu=exclusive --ib=single --cluster=circe -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=13 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
217: num_sockets = 2 num_nodes=2 cores_per_socket=24
 30: + eval '    ./bind.sh --cpu=exclusive --ib=single --cluster=circe --     python -u /workspace/bert/run_pretraining.py         --train_batch_size=18     --learning_rate=2.0e-3     --opt_lamb_beta_1=0.86     --opt_lamb_beta_2=0.975     --warmup_proportion=0.0     --max_steps=750     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data_phase2     --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.712     --max_samples_termination=7500000     --eval_iter_start_samples=3000000 --eval_iter_samples=500000     --eval_batch_size=16 --eval_dir=/workspace/evaldata     --cache_eval_data     --output_dir=/results     --fp16 --fused_gelu_bias --dense_seq_output --fused_mha      --allreduce_post_accumulation --allreduce_post_accumulation_fp16     --gradient_accumulation_steps=1     --log_freq=1     --local_rank=14     --bert_config_path=/workspace/phase1/bert_config.
219: num_sockets = 2 num_nodes=2 cores_per_socket=24
 30: json --seed=18263'
214: num_sockets = 2 num_nodes=2 cores_per_socket=24
 30: ++ ./bind.sh --cpu=exclusive --ib=single --cluster=circe -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=14 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
 99: num_sockets = 2 num_nodes=2 cores_per_socket=24
 19: + eval '    ./bind.sh --cpu=exclusive --ib=single --cluster=circe --     python -u /workspace/bert/run_pretraining.py         --train_batch_size=18     --learning_rate=2.0e-3     --opt_lamb_beta_1=0.86     --opt_lamb_beta_2=0.975     --warmup_proportion=0.0     --max_steps=750     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data_phase2     --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.712     --max_samples_termination=7500000     --eval_iter_start_samples=3000000 --eval_iter_samples=500000     --eval_batch_size=16 --eval_dir=/workspace/evaldata     --cache_eval_data     --output_dir=/results     --fp16 --fused_gelu_bias --dense_seq_output --fused_mha      --allreduce_post_accumulation --allreduce_post_accumulation_fp16     --gradient_accumulation_steps=1     --log_freq=1     --local_rank=3     --bert_config_path=/workspace/phase1/bert_config.j
348: num_sockets = 2 num_nodes=2 cores_per_socket=24
 19: son --seed=18263'
376: num_sockets = 2 num_nodes=2 cores_per_socket=24
 19: ++ ./bind.sh --cpu=exclusive --ib=single --cluster=circe -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=3 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
373: num_sockets = 2 num_nodes=2 cores_per_socket=24
 27: + eval '    ./bind.sh --cpu=exclusive --ib=single --cluster=circe --     python -u /workspace/bert/run_pretraining.py         --train_batch_size=18     --learning_rate=2.0e-3     --opt_lamb_beta_1=0.86     --opt_lamb_beta_2=0.975     --warmup_proportion=0.0     --max_steps=750     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data_phase2     --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.712     --max_samples_termination=7500000     --eval_iter_start_samples=3000000 --eval_iter_samples=500000     --eval_batch_size=16 --eval_dir=/workspace/evaldata     --cache_eval_data     --output_dir=/results     --fp16 --fused_gelu_bias --dense_seq_output --fused_mha      --allreduce_post_accumulation --allreduce_post_accumulation_fp16     --gradient_accumulation_steps=1     --log_freq=1     --local_rank=11     --bert_config_path=/workspace/phase1/bert_config.
254: num_sockets = 2 num_nodes=2 cores_per_socket=24
 27: json --seed=18263'
209: num_sockets = 2 num_nodes=2 cores_per_socket=24
 27: ++ ./bind.sh --cpu=exclusive --ib=single --cluster=circe -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=11 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
103: num_sockets = 2 num_nodes=2 cores_per_socket=24
 25: + eval '    ./bind.sh --cpu=exclusive --ib=single --cluster=circe --     python -u /workspace/bert/run_pretraining.py         --train_batch_size=18     --learning_rate=2.0e-3     --opt_lamb_beta_1=0.86     --opt_lamb_beta_2=0.975     --warmup_proportion=0.0     --max_steps=750     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data_phase2     --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.712     --max_samples_termination=7500000     --eval_iter_start_samples=3000000 --eval_iter_samples=500000     --eval_batch_size=16 --eval_dir=/workspace/evaldata     --cache_eval_data     --output_dir=/results     --fp16 --fused_gelu_bias --dense_seq_output --fused_mha      --allreduce_post_accumulation --allreduce_post_accumulation_fp16     --gradient_accumulation_steps=1     --log_freq=1     --local_rank=9     --bert_config_path=/workspace/phase1/bert_config.j
355: num_sockets = 2 num_nodes=2 cores_per_socket=24
 25: son --seed=18263'
359: num_sockets = 2 num_nodes=2 cores_per_socket=24
 25: ++ ./bind.sh --cpu=exclusive --ib=single --cluster=circe -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=9 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
363: num_sockets = 2 num_nodes=2 cores_per_socket=24
 31: + eval '    ./bind.sh --cpu=exclusive --ib=single --cluster=circe --     python -u /workspace/bert/run_pretraining.py         --train_batch_size=18     --learning_rate=2.0e-3     --opt_lamb_beta_1=0.86     --opt_lamb_beta_2=0.975     --warmup_proportion=0.0     --max_steps=750     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data_phase2     --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.712     --max_samples_termination=7500000     --eval_iter_start_samples=3000000 --eval_iter_samples=500000     --eval_batch_size=16 --eval_dir=/workspace/evaldata     --cache_eval_data     --output_dir=/results     --fp16 --fused_gelu_bias --dense_seq_output --fused_mha      --allreduce_post_accumulation --allreduce_post_accumulation_fp16     --gradient_accumulation_steps=1     --log_freq=1     --local_rank=15     --bert_config_path=/workspace/phase1/bert_config.
413: num_sockets = 2 num_nodes=2 cores_per_socket=24
 31: json --seed=18263'
309: num_sockets = 2 num_nodes=2 cores_per_socket=24
 31: ++ ./bind.sh --cpu=exclusive --ib=single --cluster=circe -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=15 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
377: num_sockets = 2 num_nodes=2 cores_per_socket=24
 29: + eval '    ./bind.sh --cpu=exclusive --ib=single --cluster=circe --     python -u /workspace/bert/run_pretraining.py         --train_batch_size=18     --learning_rate=2.0e-3     --opt_lamb_beta_1=0.86     --opt_lamb_beta_2=0.975     --warmup_proportion=0.0     --max_steps=750     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data_phase2     --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.712     --max_samples_termination=7500000     --eval_iter_start_samples=3000000 --eval_iter_samples=500000     --eval_batch_size=16 --eval_dir=/workspace/evaldata     --cache_eval_data     --output_dir=/results     --fp16 --fused_gelu_bias --dense_seq_output --fused_mha      --allreduce_post_accumulation --allreduce_post_accumulation_fp16     --gradient_accumulation_steps=1     --log_freq=1     --local_rank=13     --bert_config_path=/workspace/phase1/bert_config.
402: num_sockets = 2 num_nodes=2 cores_per_socket=24
 29: json --seed=18263'
218: num_sockets = 2 num_nodes=2 cores_per_socket=24
 28: + eval '    ./bind.sh --cpu=exclusive --ib=single --cluster=circe --     python -u /workspace/bert/run_pretraining.py         --train_batch_size=18     --learning_rate=2.0e-3     --opt_lamb_beta_1=0.86     --opt_lamb_beta_2=0.975     --warmup_proportion=0.0     --max_steps=750     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data_phase2     --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.712     --max_samples_termination=7500000     --eval_iter_start_samples=3000000 --eval_iter_samples=500000     --eval_batch_size=16 --eval_dir=/workspace/evaldata     --cache_eval_data     --output_dir=/results     --fp16 --fused_gelu_bias --dense_seq_output --fused_mha      --allreduce_post_accumulation --allreduce_post_accumulation_fp16     --gradient_accumulation_steps=1     --log_freq=1     --local_rank=12     --bert_config_path=/workspace/phase1/bert_config.
406: num_sockets = 2 num_nodes=2 cores_per_socket=24
 28: json --seed=18263'
382: num_sockets = 2 num_nodes=2 cores_per_socket=24
 29: ++ ./bind.sh --cpu=exclusive --ib=single --cluster=circe -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=13 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
351: num_sockets = 2 num_nodes=2 cores_per_socket=24
 28: ++ ./bind.sh --cpu=exclusive --ib=single --cluster=circe -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=12 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
 46: num_sockets = 2 num_nodes=2 cores_per_socket=24
 16: + eval '    ./bind.sh --cpu=exclusive --ib=single --cluster=circe --     python -u /workspace/bert/run_pretraining.py         --train_batch_size=18     --learning_rate=2.0e-3     --opt_lamb_beta_1=0.86     --opt_lamb_beta_2=0.975     --warmup_proportion=0.0     --max_steps=750     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data_phase2     --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.712     --max_samples_termination=7500000     --eval_iter_start_samples=3000000 --eval_iter_samples=500000     --eval_batch_size=16 --eval_dir=/workspace/evaldata     --cache_eval_data     --output_dir=/results     --fp16 --fused_gelu_bias --dense_seq_output --fused_mha      --allreduce_post_accumulation --allreduce_post_accumulation_fp16     --gradient_accumulation_steps=1     --log_freq=1     --local_rank=0     --bert_config_path=/workspace/phase1/bert_config.j
318: num_sockets = 2 num_nodes=2 cores_per_socket=24
 16: son --seed=18263'
165: num_sockets = 2 num_nodes=2 cores_per_socket=24
 16: ++ ./bind.sh --cpu=exclusive --ib=single --cluster=circe -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=0 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
147: num_sockets = 2 num_nodes=2 cores_per_socket=24
 22: + eval '    ./bind.sh --cpu=exclusive --ib=single --cluster=circe --     python -u /workspace/bert/run_pretraining.py         --train_batch_size=18     --learning_rate=2.0e-3     --opt_lamb_beta_1=0.86     --opt_lamb_beta_2=0.975     --warmup_proportion=0.0     --max_steps=750     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data_phase2     --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.712     --max_samples_termination=7500000     --eval_iter_start_samples=3000000 --eval_iter_samples=500000     --eval_batch_size=16 --eval_dir=/workspace/evaldata     --cache_eval_data     --output_dir=/results     --fp16 --fused_gelu_bias --dense_seq_output --fused_mha      --allreduce_post_accumulation --allreduce_post_accumulation_fp16     --gradient_accumulation_steps=1     --log_freq=1     --local_rank=6     --bert_config_path=/workspace/phase1/bert_config.j
  1: num_sockets = 2 num_nodes=2 cores_per_socket=24
 22: son --seed=18263'
397: num_sockets = 2 num_nodes=2 cores_per_socket=24
 22: ++ ./bind.sh --cpu=exclusive --ib=single --cluster=circe -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=6 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
 12: num_sockets = 2 num_nodes=2 cores_per_socket=24
 23: + eval '    ./bind.sh --cpu=exclusive --ib=single --cluster=circe --     python -u /workspace/bert/run_pretraining.py         --train_batch_size=18     --learning_rate=2.0e-3     --opt_lamb_beta_1=0.86     --opt_lamb_beta_2=0.975     --warmup_proportion=0.0     --max_steps=750     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data_phase2     --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.712     --max_samples_termination=7500000     --eval_iter_start_samples=3000000 --eval_iter_samples=500000     --eval_batch_size=16 --eval_dir=/workspace/evaldata     --cache_eval_data     --output_dir=/results     --fp16 --fused_gelu_bias --dense_seq_output --fused_mha      --allreduce_post_accumulation --allreduce_post_accumulation_fp16     --gradient_accumulation_steps=1     --log_freq=1     --local_rank=7     --bert_config_path=/workspace/phase1/bert_config.j
 23: son --seed=18263'
 23: ++ ./bind.sh --cpu=exclusive --ib=single --cluster=circe -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=7 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
328: num_sockets = 2 num_nodes=2 cores_per_socket=24
 21: + eval '    ./bind.sh --cpu=exclusive --ib=single --cluster=circe --     python -u /workspace/bert/run_pretraining.py         --train_batch_size=18     --learning_rate=2.0e-3     --opt_lamb_beta_1=0.86     --opt_lamb_beta_2=0.975     --warmup_proportion=0.0     --max_steps=750     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data_phase2     --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.712     --max_samples_termination=7500000     --eval_iter_start_samples=3000000 --eval_iter_samples=500000     --eval_batch_size=16 --eval_dir=/workspace/evaldata     --cache_eval_data     --output_dir=/results     --fp16 --fused_gelu_bias --dense_seq_output --fused_mha      --allreduce_post_accumulation --allreduce_post_accumulation_fp16     --gradient_accumulation_steps=1     --log_freq=1     --local_rank=5     --bert_config_path=/workspace/phase1/bert_config.j
149: slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
147: slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
148: slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
146: slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
145: slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
150: slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
157: slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
155: slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
158: slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
151: slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
386: slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
159: slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
153: slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
154: slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
152: slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
156: slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
150: + eval '    ./bind.sh --cpu=exclusive --ib=single --cluster=circe --     python -u /workspace/bert/run_pretraining.py         --train_batch_size=18     --learning_rate=2.0e-3     --opt_lamb_beta_1=0.86     --opt_lamb_beta_2=0.975     --warmup_proportion=0.0     --max_steps=750     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data_phase2     --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.712     --max_samples_termination=7500000     --eval_iter_start_samples=3000000 --eval_iter_samples=500000     --eval_batch_size=16 --eval_dir=/workspace/evaldata     --cache_eval_data     --output_dir=/results     --fp16 --fused_gelu_bias --dense_seq_output --fused_mha      --allreduce_post_accumulation --allreduce_post_accumulation_fp16     --gradient_accumulation_steps=1     --log_freq=1     --local_rank=6     --bert_config_path=/workspace/phase1/bert_config.j
150: son --seed=18263'
150: ++ ./bind.sh --cpu=exclusive --ib=single --cluster=circe -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=6 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
158: + eval '    ./bind.sh --cpu=exclusive --ib=single --cluster=circe --     python -u /workspace/bert/run_pretraining.py         --train_batch_size=18     --learning_rate=2.0e-3     --opt_lamb_beta_1=0.86     --opt_lamb_beta_2=0.975     --warmup_proportion=0.0     --max_steps=750     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data_phase2     --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.712     --max_samples_termination=7500000     --eval_iter_start_samples=3000000 --eval_iter_samples=500000     --eval_batch_size=16 --eval_dir=/workspace/evaldata     --cache_eval_data     --output_dir=/results     --fp16 --fused_gelu_bias --dense_seq_output --fused_mha      --allreduce_post_accumulation --allreduce_post_accumulation_fp16     --gradient_accumulation_steps=1     --log_freq=1     --local_rank=14     --bert_config_path=/workspace/phase1/bert_config.
241: slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
158: json --seed=18263'
158: ++ ./bind.sh --cpu=exclusive --ib=single --cluster=circe -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=14 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
157: + eval '    ./bind.sh --cpu=exclusive --ib=single --cluster=circe --     python -u /workspace/bert/run_pretraining.py         --train_batch_size=18     --learning_rate=2.0e-3     --opt_lamb_beta_1=0.86     --opt_lamb_beta_2=0.975     --warmup_proportion=0.0     --max_steps=750     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data_phase2     --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.712     --max_samples_termination=7500000     --eval_iter_start_samples=3000000 --eval_iter_samples=500000     --eval_batch_size=16 --eval_dir=/workspace/evaldata     --cache_eval_data     --output_dir=/results     --fp16 --fused_gelu_bias --dense_seq_output --fused_mha      --allreduce_post_accumulation --allreduce_post_accumulation_fp16     --gradient_accumulation_steps=1     --log_freq=1     --local_rank=13     --bert_config_path=/workspace/phase1/bert_config.
157: json --seed=18263'
157: ++ ./bind.sh --cpu=exclusive --ib=single --cluster=circe -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=13 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
147: + eval '    ./bind.sh --cpu=exclusive --ib=single --cluster=circe --     python -u /workspace/bert/run_pretraining.py         --train_batch_size=18     --learning_rate=2.0e-3     --opt_lamb_beta_1=0.86     --opt_lamb_beta_2=0.975     --warmup_proportion=0.0     --max_steps=750     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data_phase2     --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.712     --max_samples_termination=7500000     --eval_iter_start_samples=3000000 --eval_iter_samples=500000     --eval_batch_size=16 --eval_dir=/workspace/evaldata     --cache_eval_data     --output_dir=/results     --fp16 --fused_gelu_bias --dense_seq_output --fused_mha      --allreduce_post_accumulation --allreduce_post_accumulation_fp16     --gradient_accumulation_steps=1     --log_freq=1     --local_rank=3     --bert_config_path=/workspace/phase1/bert_config.j
147: son --seed=18263'
147: ++ ./bind.sh --cpu=exclusive --ib=single --cluster=circe -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=3 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
149: + eval '    ./bind.sh --cpu=exclusive --ib=single --cluster=circe --     python -u /workspace/bert/run_pretraining.py         --train_batch_size=18     --learning_rate=2.0e-3     --opt_lamb_beta_1=0.86     --opt_lamb_beta_2=0.975     --warmup_proportion=0.0     --max_steps=750     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data_phase2     --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.712     --max_samples_termination=7500000     --eval_iter_start_samples=3000000 --eval_iter_samples=500000     --eval_batch_size=16 --eval_dir=/workspace/evaldata     --cache_eval_data     --output_dir=/results     --fp16 --fused_gelu_bias --dense_seq_output --fused_mha      --allreduce_post_accumulation --allreduce_post_accumulation_fp16     --gradient_accumulation_steps=1     --log_freq=1     --local_rank=5     --bert_config_path=/workspace/phase1/bert_config.j
149: son --seed=18263'
151: + eval '    ./bind.sh --cpu=exclusive --ib=single --cluster=circe --     python -u /workspace/bert/run_pretraining.py         --train_batch_size=18     --learning_rate=2.0e-3     --opt_lamb_beta_1=0.86     --opt_lamb_beta_2=0.975     --warmup_proportion=0.0     --max_steps=750     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data_phase2     --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.712     --max_samples_termination=7500000     --eval_iter_start_samples=3000000 --eval_iter_samples=500000     --eval_batch_size=16 --eval_dir=/workspace/evaldata     --cache_eval_data     --output_dir=/results     --fp16 --fused_gelu_bias --dense_seq_output --fused_mha      --allreduce_post_accumulation --allreduce_post_accumulation_fp16     --gradient_accumulation_steps=1     --log_freq=1     --local_rank=7     --bert_config_path=/workspace/phase1/bert_config.j
151: son --seed=18263'
149: ++ ./bind.sh --cpu=exclusive --ib=single --cluster=circe -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=5 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
151: ++ ./bind.sh --cpu=exclusive --ib=single --cluster=circe -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=7 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
148: + eval '    ./bind.sh --cpu=exclusive --ib=single --cluster=circe --     python -u /workspace/bert/run_pretraining.py         --train_batch_size=18     --learning_rate=2.0e-3     --opt_lamb_beta_1=0.86     --opt_lamb_beta_2=0.975     --warmup_proportion=0.0     --max_steps=750     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data_phase2     --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.712     --max_samples_termination=7500000     --eval_iter_start_samples=3000000 --eval_iter_samples=500000     --eval_batch_size=16 --eval_dir=/workspace/evaldata     --cache_eval_data     --output_dir=/results     --fp16 --fused_gelu_bias --dense_seq_output --fused_mha      --allreduce_post_accumulation --allreduce_post_accumulation_fp16     --gradient_accumulation_steps=1     --log_freq=1     --local_rank=4     --bert_config_path=/workspace/phase1/bert_config.j
148: son --seed=18263'
148: ++ ./bind.sh --cpu=exclusive --ib=single --cluster=circe -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=4 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
159: + eval '    ./bind.sh --cpu=exclusive --ib=single --cluster=circe --     python -u /workspace/bert/run_pretraining.py         --train_batch_size=18     --learning_rate=2.0e-3     --opt_lamb_beta_1=0.86     --opt_lamb_beta_2=0.975     --warmup_proportion=0.0     --max_steps=750     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data_phase2     --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.712     --max_samples_termination=7500000     --eval_iter_start_samples=3000000 --eval_iter_samples=500000     --eval_batch_size=16 --eval_dir=/workspace/evaldata     --cache_eval_data     --output_dir=/results     --fp16 --fused_gelu_bias --dense_seq_output --fused_mha      --allreduce_post_accumulation --allreduce_post_accumulation_fp16     --gradient_accumulation_steps=1     --log_freq=1     --local_rank=15     --bert_config_path=/workspace/phase1/bert_config.
159: json --seed=18263'
159: ++ ./bind.sh --cpu=exclusive --ib=single --cluster=circe -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=15 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
153: + eval '    ./bind.sh --cpu=exclusive --ib=single --cluster=circe --     python -u /workspace/bert/run_pretraining.py         --train_batch_size=18     --learning_rate=2.0e-3     --opt_lamb_beta_1=0.86     --opt_lamb_beta_2=0.975     --warmup_proportion=0.0     --max_steps=750     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data_phase2     --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.712     --max_samples_termination=7500000     --eval_iter_start_samples=3000000 --eval_iter_samples=500000     --eval_batch_size=16 --eval_dir=/workspace/evaldata     --cache_eval_data     --output_dir=/results     --fp16 --fused_gelu_bias --dense_seq_output --fused_mha      --allreduce_post_accumulation --allreduce_post_accumulation_fp16     --gradient_accumulation_steps=1     --log_freq=1     --local_rank=9     --bert_config_path=/workspace/phase1/bert_config.j
153: son --seed=18263'
153: ++ ./bind.sh --cpu=exclusive --ib=single --cluster=circe -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=9 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
144: + eval '    ./bind.sh --cpu=exclusive --ib=single --cluster=circe --     python -u /workspace/bert/run_pretraining.py         --train_batch_size=18     --learning_rate=2.0e-3     --opt_lamb_beta_1=0.86     --opt_lamb_beta_2=0.975     --warmup_proportion=0.0     --max_steps=750     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data_phase2     --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.712     --max_samples_termination=7500000     --eval_iter_start_samples=3000000 --eval_iter_samples=500000     --eval_batch_size=16 --eval_dir=/workspace/evaldata     --cache_eval_data     --output_dir=/results     --fp16 --fused_gelu_bias --dense_seq_output --fused_mha      --allreduce_post_accumulation --allreduce_post_accumulation_fp16     --gradient_accumulation_steps=1     --log_freq=1     --local_rank=0     --bert_config_path=/workspace/phase1/bert_config.j
144: son --seed=18263'
144: ++ ./bind.sh --cpu=exclusive --ib=single --cluster=circe -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=0 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
146: + eval '    ./bind.sh --cpu=exclusive --ib=single --cluster=circe --     python -u /workspace/bert/run_pretraining.py         --train_batch_size=18     --learning_rate=2.0e-3     --opt_lamb_beta_1=0.86     --opt_lamb_beta_2=0.975     --warmup_proportion=0.0     --max_steps=750     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data_phase2     --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.712     --max_samples_termination=7500000     --eval_iter_start_samples=3000000 --eval_iter_samples=500000     --eval_batch_size=16 --eval_dir=/workspace/evaldata     --cache_eval_data     --output_dir=/results     --fp16 --fused_gelu_bias --dense_seq_output --fused_mha      --allreduce_post_accumulation --allreduce_post_accumulation_fp16     --gradient_accumulation_steps=1     --log_freq=1     --local_rank=2     --bert_config_path=/workspace/phase1/bert_config.j
146: son --seed=18263'
146: ++ ./bind.sh --cpu=exclusive --ib=single --cluster=circe -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=2 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
155: + eval '    ./bind.sh --cpu=exclusive --ib=single --cluster=circe --     python -u /workspace/bert/run_pretraining.py         --train_batch_size=18     --learning_rate=2.0e-3     --opt_lamb_beta_1=0.86     --opt_lamb_beta_2=0.975     --warmup_proportion=0.0     --max_steps=750     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data_phase2     --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.712     --max_samples_termination=7500000     --eval_iter_start_samples=3000000 --eval_iter_samples=500000     --eval_batch_size=16 --eval_dir=/workspace/evaldata     --cache_eval_data     --output_dir=/results     --fp16 --fused_gelu_bias --dense_seq_output --fused_mha      --allreduce_post_accumulation --allreduce_post_accumulation_fp16     --gradient_accumulation_steps=1     --log_freq=1     --local_rank=11     --bert_config_path=/workspace/phase1/bert_config.
155: json --seed=18263'
156: + eval '    ./bind.sh --cpu=exclusive --ib=single --cluster=circe --     python -u /workspace/bert/run_pretraining.py         --train_batch_size=18     --learning_rate=2.0e-3     --opt_lamb_beta_1=0.86     --opt_lamb_beta_2=0.975     --warmup_proportion=0.0     --max_steps=750     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data_phase2     --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.712     --max_samples_termination=7500000     --eval_iter_start_samples=3000000 --eval_iter_samples=500000     --eval_batch_size=16 --eval_dir=/workspace/evaldata     --cache_eval_data     --output_dir=/results     --fp16 --fused_gelu_bias --dense_seq_output --fused_mha      --allreduce_post_accumulation --allreduce_post_accumulation_fp16     --gradient_accumulation_steps=1     --log_freq=1     --local_rank=12     --bert_config_path=/workspace/phase1/bert_config.
156: json --seed=18263'
155: ++ ./bind.sh --cpu=exclusive --ib=single --cluster=circe -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=11 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
508: num_sockets = 2 num_nodes=2 cores_per_socket=24
156: ++ ./bind.sh --cpu=exclusive --ib=single --cluster=circe -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=12 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
407: slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
145: + eval '    ./bind.sh --cpu=exclusive --ib=single --cluster=circe --     python -u /workspace/bert/run_pretraining.py         --train_batch_size=18     --learning_rate=2.0e-3     --opt_lamb_beta_1=0.86     --opt_lamb_beta_2=0.975     --warmup_proportion=0.0     --max_steps=750     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data_phase2     --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.712     --max_samples_termination=7500000     --eval_iter_start_samples=3000000 --eval_iter_samples=500000     --eval_batch_size=16 --eval_dir=/workspace/evaldata     --cache_eval_data     --output_dir=/results     --fp16 --fused_gelu_bias --dense_seq_output --fused_mha      --allreduce_post_accumulation --allreduce_post_accumulation_fp16     --gradient_accumulation_steps=1     --log_freq=1     --local_rank=1     --bert_config_path=/workspace/phase1/bert_config.j
145: son --seed=18263'
145: ++ ./bind.sh --cpu=exclusive --ib=single --cluster=circe -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=1 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
152: + eval '    ./bind.sh --cpu=exclusive --ib=single --cluster=circe --     python -u /workspace/bert/run_pretraining.py         --train_batch_size=18     --learning_rate=2.0e-3     --opt_lamb_beta_1=0.86     --opt_lamb_beta_2=0.975     --warmup_proportion=0.0     --max_steps=750     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data_phase2     --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.712     --max_samples_termination=7500000     --eval_iter_start_samples=3000000 --eval_iter_samples=500000     --eval_batch_size=16 --eval_dir=/workspace/evaldata     --cache_eval_data     --output_dir=/results     --fp16 --fused_gelu_bias --dense_seq_output --fused_mha      --allreduce_post_accumulation --allreduce_post_accumulation_fp16     --gradient_accumulation_steps=1     --log_freq=1     --local_rank=8     --bert_config_path=/workspace/phase1/bert_config.j
152: son --seed=18263'
152: ++ ./bind.sh --cpu=exclusive --ib=single --cluster=circe -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=8 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
208: slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
154: + eval '    ./bind.sh --cpu=exclusive --ib=single --cluster=circe --     python -u /workspace/bert/run_pretraining.py         --train_batch_size=18     --learning_rate=2.0e-3     --opt_lamb_beta_1=0.86     --opt_lamb_beta_2=0.975     --warmup_proportion=0.0     --max_steps=750     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data_phase2     --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.712     --max_samples_termination=7500000     --eval_iter_start_samples=3000000 --eval_iter_samples=500000     --eval_batch_size=16 --eval_dir=/workspace/evaldata     --cache_eval_data     --output_dir=/results     --fp16 --fused_gelu_bias --dense_seq_output --fused_mha      --allreduce_post_accumulation --allreduce_post_accumulation_fp16     --gradient_accumulation_steps=1     --log_freq=1     --local_rank=10     --bert_config_path=/workspace/phase1/bert_config.
188: ++ ./bind.sh --cpu=exclusive --ib=single --cluster=circe -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=12 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
154: json --seed=18263'
154: ++ ./bind.sh --cpu=exclusive --ib=single --cluster=circe -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=10 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
357: slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
339: slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
342: slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
341: slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
340: slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
338: slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
336: slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
345: slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
343: slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
347: slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
346: slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
344: slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
350: slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
351: slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
348: slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
349: slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
338: + eval '    ./bind.sh --cpu=exclusive --ib=single --cluster=circe --     python -u /workspace/bert/run_pretraining.py         --train_batch_size=18     --learning_rate=2.0e-3     --opt_lamb_beta_1=0.86     --opt_lamb_beta_2=0.975     --warmup_proportion=0.0     --max_steps=750     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data_phase2     --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.712     --max_samples_termination=7500000     --eval_iter_start_samples=3000000 --eval_iter_samples=500000     --eval_batch_size=16 --eval_dir=/workspace/evaldata     --cache_eval_data     --output_dir=/results     --fp16 --fused_gelu_bias --dense_seq_output --fused_mha      --allreduce_post_accumulation --allreduce_post_accumulation_fp16     --gradient_accumulation_steps=1     --log_freq=1     --local_rank=2     --bert_config_path=/workspace/phase1/bert_config.j
338: son --seed=18263'
338: ++ ./bind.sh --cpu=exclusive --ib=single --cluster=circe -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=2 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
337: + eval '    ./bind.sh --cpu=exclusive --ib=single --cluster=circe --     python -u /workspace/bert/run_pretraining.py         --train_batch_size=18     --learning_rate=2.0e-3     --opt_lamb_beta_1=0.86     --opt_lamb_beta_2=0.975     --warmup_proportion=0.0     --max_steps=750     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data_phase2     --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.712     --max_samples_termination=7500000     --eval_iter_start_samples=3000000 --eval_iter_samples=500000     --eval_batch_size=16 --eval_dir=/workspace/evaldata     --cache_eval_data     --output_dir=/results     --fp16 --fused_gelu_bias --dense_seq_output --fused_mha      --allreduce_post_accumulation --allreduce_post_accumulation_fp16     --gradient_accumulation_steps=1     --log_freq=1     --local_rank=1     --bert_config_path=/workspace/phase1/bert_config.j
252: num_sockets = 2 num_nodes=2 cores_per_socket=24
337: son --seed=18263'
337: ++ ./bind.sh --cpu=exclusive --ib=single --cluster=circe -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=1 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
371: + eval '    ./bind.sh --cpu=exclusive --ib=single --cluster=circe --     python -u /workspace/bert/run_pretraining.py         --train_batch_size=18     --learning_rate=2.0e-3     --opt_lamb_beta_1=0.86     --opt_lamb_beta_2=0.975     --warmup_proportion=0.0     --max_steps=750     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data_phase2     --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.712     --max_samples_termination=7500000     --eval_iter_start_samples=3000000 --eval_iter_samples=500000     --eval_batch_size=16 --eval_dir=/workspace/evaldata     --cache_eval_data     --output_dir=/results     --fp16 --fused_gelu_bias --dense_seq_output --fused_mha      --allreduce_post_accumulation --allreduce_post_accumulation_fp16     --gradient_accumulation_steps=1     --log_freq=1     --local_rank=3     --bert_config_path=/workspace/phase1/bert_config.j
371: son --seed=18263'
431: ++ ./bind.sh --cpu=exclusive --ib=single --cluster=circe -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=15 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
340: + eval '    ./bind.sh --cpu=exclusive --ib=single --cluster=circe --     python -u /workspace/bert/run_pretraining.py         --train_batch_size=18     --learning_rate=2.0e-3     --opt_lamb_beta_1=0.86     --opt_lamb_beta_2=0.975     --warmup_proportion=0.0     --max_steps=750     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data_phase2     --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.712     --max_samples_termination=7500000     --eval_iter_start_samples=3000000 --eval_iter_samples=500000     --eval_batch_size=16 --eval_dir=/workspace/evaldata     --cache_eval_data     --output_dir=/results     --fp16 --fused_gelu_bias --dense_seq_output --fused_mha      --allreduce_post_accumulation --allreduce_post_accumulation_fp16     --gradient_accumulation_steps=1     --log_freq=1     --local_rank=4     --bert_config_path=/workspace/phase1/bert_config.j
340: son --seed=18263'
340: ++ ./bind.sh --cpu=exclusive --ib=single --cluster=circe -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=4 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
336: + eval '    ./bind.sh --cpu=exclusive --ib=single --cluster=circe --     python -u /workspace/bert/run_pretraining.py         --train_batch_size=18     --learning_rate=2.0e-3     --opt_lamb_beta_1=0.86     --opt_lamb_beta_2=0.975     --warmup_proportion=0.0     --max_steps=750     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data_phase2     --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.712     --max_samples_termination=7500000     --eval_iter_start_samples=3000000 --eval_iter_samples=500000     --eval_batch_size=16 --eval_dir=/workspace/evaldata     --cache_eval_data     --output_dir=/results     --fp16 --fused_gelu_bias --dense_seq_output --fused_mha      --allreduce_post_accumulation --allreduce_post_accumulation_fp16     --gradient_accumulation_steps=1     --log_freq=1     --local_rank=0     --bert_config_path=/workspace/phase1/bert_config.j
336: son --seed=18263'
336: ++ ./bind.sh --cpu=exclusive --ib=single --cluster=circe -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=0 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
371: ++ ./bind.sh --cpu=exclusive --ib=single --cluster=circe -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=3 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
373: + eval '    ./bind.sh --cpu=exclusive --ib=single --cluster=circe --     python -u /workspace/bert/run_pretraining.py         --train_batch_size=18     --learning_rate=2.0e-3     --opt_lamb_beta_1=0.86     --opt_lamb_beta_2=0.975     --warmup_proportion=0.0     --max_steps=750     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data_phase2     --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.712     --max_samples_termination=7500000     --eval_iter_start_samples=3000000 --eval_iter_samples=500000     --eval_batch_size=16 --eval_dir=/workspace/evaldata     --cache_eval_data     --output_dir=/results     --fp16 --fused_gelu_bias --dense_seq_output --fused_mha      --allreduce_post_accumulation --allreduce_post_accumulation_fp16     --gradient_accumulation_steps=1     --log_freq=1     --local_rank=5     --bert_config_path=/workspace/phase1/bert_config.j
373: son --seed=18263'
373: ++ ./bind.sh --cpu=exclusive --ib=single --cluster=circe -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=5 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
377: + eval '    ./bind.sh --cpu=exclusive --ib=single --cluster=circe --     python -u /workspace/bert/run_pretraining.py         --train_batch_size=18     --learning_rate=2.0e-3     --opt_lamb_beta_1=0.86     --opt_lamb_beta_2=0.975     --warmup_proportion=0.0     --max_steps=750     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data_phase2     --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.712     --max_samples_termination=7500000     --eval_iter_start_samples=3000000 --eval_iter_samples=500000     --eval_batch_size=16 --eval_dir=/workspace/evaldata     --cache_eval_data     --output_dir=/results     --fp16 --fused_gelu_bias --dense_seq_output --fused_mha      --allreduce_post_accumulation --allreduce_post_accumulation_fp16     --gradient_accumulation_steps=1     --log_freq=1     --local_rank=9     --bert_config_path=/workspace/phase1/bert_config.j
377: son --seed=18263'
348: + eval '    ./bind.sh --cpu=exclusive --ib=single --cluster=circe --     python -u /workspace/bert/run_pretraining.py         --train_batch_size=18     --learning_rate=2.0e-3     --opt_lamb_beta_1=0.86     --opt_lamb_beta_2=0.975     --warmup_proportion=0.0     --max_steps=750     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data_phase2     --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.712     --max_samples_termination=7500000     --eval_iter_start_samples=3000000 --eval_iter_samples=500000     --eval_batch_size=16 --eval_dir=/workspace/evaldata     --cache_eval_data     --output_dir=/results     --fp16 --fused_gelu_bias --dense_seq_output --fused_mha      --allreduce_post_accumulation --allreduce_post_accumulation_fp16     --gradient_accumulation_steps=1     --log_freq=1     --local_rank=12     --bert_config_path=/workspace/phase1/bert_config.
 63: num_sockets = 2 num_nodes=2 cores_per_socket=24
348: json --seed=18263'
348: ++ ./bind.sh --cpu=exclusive --ib=single --cluster=circe -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=12 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
377: ++ ./bind.sh --cpu=exclusive --ib=single --cluster=circe -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=9 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
351: + eval '    ./bind.sh --cpu=exclusive --ib=single --cluster=circe --     python -u /workspace/bert/run_pretraining.py         --train_batch_size=18     --learning_rate=2.0e-3     --opt_lamb_beta_1=0.86     --opt_lamb_beta_2=0.975     --warmup_proportion=0.0     --max_steps=750     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data_phase2     --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.712     --max_samples_termination=7500000     --eval_iter_start_samples=3000000 --eval_iter_samples=500000     --eval_batch_size=16 --eval_dir=/workspace/evaldata     --cache_eval_data     --output_dir=/results     --fp16 --fused_gelu_bias --dense_seq_output --fused_mha      --allreduce_post_accumulation --allreduce_post_accumulation_fp16     --gradient_accumulation_steps=1     --log_freq=1     --local_rank=15     --bert_config_path=/workspace/phase1/bert_config.
351: json --seed=18263'
351: ++ ./bind.sh --cpu=exclusive --ib=single --cluster=circe -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=15 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
382: + eval '    ./bind.sh --cpu=exclusive --ib=single --cluster=circe --     python -u /workspace/bert/run_pretraining.py         --train_batch_size=18     --learning_rate=2.0e-3     --opt_lamb_beta_1=0.86     --opt_lamb_beta_2=0.975     --warmup_proportion=0.0     --max_steps=750     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data_phase2     --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.712     --max_samples_termination=7500000     --eval_iter_start_samples=3000000 --eval_iter_samples=500000     --eval_batch_size=16 --eval_dir=/workspace/evaldata     --cache_eval_data     --output_dir=/results     --fp16 --fused_gelu_bias --dense_seq_output --fused_mha      --allreduce_post_accumulation --allreduce_post_accumulation_fp16     --gradient_accumulation_steps=1     --log_freq=1     --local_rank=14     --bert_config_path=/workspace/phase1/bert_config.
382: json --seed=18263'
374: + eval '    ./bind.sh --cpu=exclusive --ib=single --cluster=circe --     python -u /workspace/bert/run_pretraining.py         --train_batch_size=18     --learning_rate=2.0e-3     --opt_lamb_beta_1=0.86     --opt_lamb_beta_2=0.975     --warmup_proportion=0.0     --max_steps=750     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data_phase2     --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.712     --max_samples_termination=7500000     --eval_iter_start_samples=3000000 --eval_iter_samples=500000     --eval_batch_size=16 --eval_dir=/workspace/evaldata     --cache_eval_data     --output_dir=/results     --fp16 --fused_gelu_bias --dense_seq_output --fused_mha      --allreduce_post_accumulation --allreduce_post_accumulation_fp16     --gradient_accumulation_steps=1     --log_freq=1     --local_rank=6     --bert_config_path=/workspace/phase1/bert_config.j
374: son --seed=18263'
374: ++ ./bind.sh --cpu=exclusive --ib=single --cluster=circe -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=6 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
382: ++ ./bind.sh --cpu=exclusive --ib=single --cluster=circe -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=14 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
256: num_sockets = 2 num_nodes=2 cores_per_socket=24
376: + eval '    ./bind.sh --cpu=exclusive --ib=single --cluster=circe --     python -u /workspace/bert/run_pretraining.py         --train_batch_size=18     --learning_rate=2.0e-3     --opt_lamb_beta_1=0.86     --opt_lamb_beta_2=0.975     --warmup_proportion=0.0     --max_steps=750     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data_phase2     --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.712     --max_samples_termination=7500000     --eval_iter_start_samples=3000000 --eval_iter_samples=500000     --eval_batch_size=16 --eval_dir=/workspace/evaldata     --cache_eval_data     --output_dir=/results     --fp16 --fused_gelu_bias --dense_seq_output --fused_mha      --allreduce_post_accumulation --allreduce_post_accumulation_fp16     --gradient_accumulation_steps=1     --log_freq=1     --local_rank=8     --bert_config_path=/workspace/phase1/bert_config.j
344: + eval '    ./bind.sh --cpu=exclusive --ib=single --cluster=circe --     python -u /workspace/bert/run_pretraining.py         --train_batch_size=18     --learning_rate=2.0e-3     --opt_lamb_beta_1=0.86     --opt_lamb_beta_2=0.975     --warmup_proportion=0.0     --max_steps=750     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data_phase2     --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.712     --max_samples_termination=7500000     --eval_iter_start_samples=3000000 --eval_iter_samples=500000     --eval_batch_size=16 --eval_dir=/workspace/evaldata     --cache_eval_data     --output_dir=/results     --fp16 --fused_gelu_bias --dense_seq_output --fused_mha      --allreduce_post_accumulation --allreduce_post_accumulation_fp16     --gradient_accumulation_steps=1     --log_freq=1     --local_rank=8     --bert_config_path=/workspace/phase1/bert_config.j
376: son --seed=18263'
344: son --seed=18263'
344: ++ ./bind.sh --cpu=exclusive --ib=single --cluster=circe -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=8 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
376: ++ ./bind.sh --cpu=exclusive --ib=single --cluster=circe -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=8 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
346: + eval '    ./bind.sh --cpu=exclusive --ib=single --cluster=circe --     python -u /workspace/bert/run_pretraining.py         --train_batch_size=18     --learning_rate=2.0e-3     --opt_lamb_beta_1=0.86     --opt_lamb_beta_2=0.975     --warmup_proportion=0.0     --max_steps=750     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data_phase2     --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.712     --max_samples_termination=7500000     --eval_iter_start_samples=3000000 --eval_iter_samples=500000     --eval_batch_size=16 --eval_dir=/workspace/evaldata     --cache_eval_data     --output_dir=/results     --fp16 --fused_gelu_bias --dense_seq_output --fused_mha      --allreduce_post_accumulation --allreduce_post_accumulation_fp16     --gradient_accumulation_steps=1     --log_freq=1     --local_rank=10     --bert_config_path=/workspace/phase1/bert_config.
346: json --seed=18263'
346: ++ ./bind.sh --cpu=exclusive --ib=single --cluster=circe -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=10 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
339: + eval '    ./bind.sh --cpu=exclusive --ib=single --cluster=circe --     python -u /workspace/bert/run_pretraining.py         --train_batch_size=18     --learning_rate=2.0e-3     --opt_lamb_beta_1=0.86     --opt_lamb_beta_2=0.975     --warmup_proportion=0.0     --max_steps=750     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data_phase2     --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.712     --max_samples_termination=7500000     --eval_iter_start_samples=3000000 --eval_iter_samples=500000     --eval_batch_size=16 --eval_dir=/workspace/evaldata     --cache_eval_data     --output_dir=/results     --fp16 --fused_gelu_bias --dense_seq_output --fused_mha      --allreduce_post_accumulation --allreduce_post_accumulation_fp16     --gradient_accumulation_steps=1     --log_freq=1     --local_rank=3     --bert_config_path=/workspace/phase1/bert_config.j
368: + eval '    ./bind.sh --cpu=exclusive --ib=single --cluster=circe --     python -u /workspace/bert/run_pretraining.py         --train_batch_size=18     --learning_rate=2.0e-3     --opt_lamb_beta_1=0.86     --opt_lamb_beta_2=0.975     --warmup_proportion=0.0     --max_steps=750     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data_phase2     --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.712     --max_samples_termination=7500000     --eval_iter_start_samples=3000000 --eval_iter_samples=500000     --eval_batch_size=16 --eval_dir=/workspace/evaldata     --cache_eval_data     --output_dir=/results     --fp16 --fused_gelu_bias --dense_seq_output --fused_mha      --allreduce_post_accumulation --allreduce_post_accumulation_fp16     --gradient_accumulation_steps=1     --log_freq=1     --local_rank=0     --bert_config_path=/workspace/phase1/bert_config.j
339: son --seed=18263'
368: son --seed=18263'
339: ++ ./bind.sh --cpu=exclusive --ib=single --cluster=circe -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=3 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
347: + eval '    ./bind.sh --cpu=exclusive --ib=single --cluster=circe --     python -u /workspace/bert/run_pretraining.py         --train_batch_size=18     --learning_rate=2.0e-3     --opt_lamb_beta_1=0.86     --opt_lamb_beta_2=0.975     --warmup_proportion=0.0     --max_steps=750     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data_phase2     --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.712     --max_samples_termination=7500000     --eval_iter_start_samples=3000000 --eval_iter_samples=500000     --eval_batch_size=16 --eval_dir=/workspace/evaldata     --cache_eval_data     --output_dir=/results     --fp16 --fused_gelu_bias --dense_seq_output --fused_mha      --allreduce_post_accumulation --allreduce_post_accumulation_fp16     --gradient_accumulation_steps=1     --log_freq=1     --local_rank=11     --bert_config_path=/workspace/phase1/bert_config.
368: ++ ./bind.sh --cpu=exclusive --ib=single --cluster=circe -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=0 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
347: json --seed=18263'
375: + eval '    ./bind.sh --cpu=exclusive --ib=single --cluster=circe --     python -u /workspace/bert/run_pretraining.py         --train_batch_size=18     --learning_rate=2.0e-3     --opt_lamb_beta_1=0.86     --opt_lamb_beta_2=0.975     --warmup_proportion=0.0     --max_steps=750     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data_phase2     --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.712     --max_samples_termination=7500000     --eval_iter_start_samples=3000000 --eval_iter_samples=500000     --eval_batch_size=16 --eval_dir=/workspace/evaldata     --cache_eval_data     --output_dir=/results     --fp16 --fused_gelu_bias --dense_seq_output --fused_mha      --allreduce_post_accumulation --allreduce_post_accumulation_fp16     --gradient_accumulation_steps=1     --log_freq=1     --local_rank=7     --bert_config_path=/workspace/phase1/bert_config.j
375: son --seed=18263'
378: + eval '    ./bind.sh --cpu=exclusive --ib=single --cluster=circe --     python -u /workspace/bert/run_pretraining.py         --train_batch_size=18     --learning_rate=2.0e-3     --opt_lamb_beta_1=0.86     --opt_lamb_beta_2=0.975     --warmup_proportion=0.0     --max_steps=750     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data_phase2     --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.712     --max_samples_termination=7500000     --eval_iter_start_samples=3000000 --eval_iter_samples=500000     --eval_batch_size=16 --eval_dir=/workspace/evaldata     --cache_eval_data     --output_dir=/results     --fp16 --fused_gelu_bias --dense_seq_output --fused_mha      --allreduce_post_accumulation --allreduce_post_accumulation_fp16     --gradient_accumulation_steps=1     --log_freq=1     --local_rank=10     --bert_config_path=/workspace/phase1/bert_config.
378: json --seed=18263'
375: ++ ./bind.sh --cpu=exclusive --ib=single --cluster=circe -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=7 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
381: + eval '    ./bind.sh --cpu=exclusive --ib=single --cluster=circe --     python -u /workspace/bert/run_pretraining.py         --train_batch_size=18     --learning_rate=2.0e-3     --opt_lamb_beta_1=0.86     --opt_lamb_beta_2=0.975     --warmup_proportion=0.0     --max_steps=750     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data_phase2     --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.712     --max_samples_termination=7500000     --eval_iter_start_samples=3000000 --eval_iter_samples=500000     --eval_batch_size=16 --eval_dir=/workspace/evaldata     --cache_eval_data     --output_dir=/results     --fp16 --fused_gelu_bias --dense_seq_output --fused_mha      --allreduce_post_accumulation --allreduce_post_accumulation_fp16     --gradient_accumulation_steps=1     --log_freq=1     --local_rank=13     --bert_config_path=/workspace/phase1/bert_config.
381: json --seed=18263'
381: ++ ./bind.sh --cpu=exclusive --ib=single --cluster=circe -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=13 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
378: ++ ./bind.sh --cpu=exclusive --ib=single --cluster=circe -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=10 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
347: ++ ./bind.sh --cpu=exclusive --ib=single --cluster=circe -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=11 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
343: + eval '    ./bind.sh --cpu=exclusive --ib=single --cluster=circe --     python -u /workspace/bert/run_pretraining.py         --train_batch_size=18     --learning_rate=2.0e-3     --opt_lamb_beta_1=0.86     --opt_lamb_beta_2=0.975     --warmup_proportion=0.0     --max_steps=750     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data_phase2     --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.712     --max_samples_termination=7500000     --eval_iter_start_samples=3000000 --eval_iter_samples=500000     --eval_batch_size=16 --eval_dir=/workspace/evaldata     --cache_eval_data     --output_dir=/results     --fp16 --fused_gelu_bias --dense_seq_output --fused_mha      --allreduce_post_accumulation --allreduce_post_accumulation_fp16     --gradient_accumulation_steps=1     --log_freq=1     --local_rank=7     --bert_config_path=/workspace/phase1/bert_config.j
343: son --seed=18263'
342: + eval '    ./bind.sh --cpu=exclusive --ib=single --cluster=circe --     python -u /workspace/bert/run_pretraining.py         --train_batch_size=18     --learning_rate=2.0e-3     --opt_lamb_beta_1=0.86     --opt_lamb_beta_2=0.975     --warmup_proportion=0.0     --max_steps=750     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data_phase2     --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.712     --max_samples_termination=7500000     --eval_iter_start_samples=3000000 --eval_iter_samples=500000     --eval_batch_size=16 --eval_dir=/workspace/evaldata     --cache_eval_data     --output_dir=/results     --fp16 --fused_gelu_bias --dense_seq_output --fused_mha      --allreduce_post_accumulation --allreduce_post_accumulation_fp16     --gradient_accumulation_steps=1     --log_freq=1     --local_rank=6     --bert_config_path=/workspace/phase1/bert_config.j
342: son --seed=18263'
343: ++ ./bind.sh --cpu=exclusive --ib=single --cluster=circe -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=7 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
342: ++ ./bind.sh --cpu=exclusive --ib=single --cluster=circe -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=6 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
349: + eval '    ./bind.sh --cpu=exclusive --ib=single --cluster=circe --     python -u /workspace/bert/run_pretraining.py         --train_batch_size=18     --learning_rate=2.0e-3     --opt_lamb_beta_1=0.86     --opt_lamb_beta_2=0.975     --warmup_proportion=0.0     --max_steps=750     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data_phase2     --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.712     --max_samples_termination=7500000     --eval_iter_start_samples=3000000 --eval_iter_samples=500000     --eval_batch_size=16 --eval_dir=/workspace/evaldata     --cache_eval_data     --output_dir=/results     --fp16 --fused_gelu_bias --dense_seq_output --fused_mha      --allreduce_post_accumulation --allreduce_post_accumulation_fp16     --gradient_accumulation_steps=1     --log_freq=1     --local_rank=13     --bert_config_path=/workspace/phase1/bert_config.
349: json --seed=18263'
379: + eval '    ./bind.sh --cpu=exclusive --ib=single --cluster=circe --     python -u /workspace/bert/run_pretraining.py         --train_batch_size=18     --learning_rate=2.0e-3     --opt_lamb_beta_1=0.86     --opt_lamb_beta_2=0.975     --warmup_proportion=0.0     --max_steps=750     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data_phase2     --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.712     --max_samples_termination=7500000     --eval_iter_start_samples=3000000 --eval_iter_samples=500000     --eval_batch_size=16 --eval_dir=/workspace/evaldata     --cache_eval_data     --output_dir=/results     --fp16 --fused_gelu_bias --dense_seq_output --fused_mha      --allreduce_post_accumulation --allreduce_post_accumulation_fp16     --gradient_accumulation_steps=1     --log_freq=1     --local_rank=11     --bert_config_path=/workspace/phase1/bert_config.
379: json --seed=18263'
379: ++ ./bind.sh --cpu=exclusive --ib=single --cluster=circe -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=11 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
349: ++ ./bind.sh --cpu=exclusive --ib=single --cluster=circe -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=13 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
345: + eval '    ./bind.sh --cpu=exclusive --ib=single --cluster=circe --     python -u /workspace/bert/run_pretraining.py         --train_batch_size=18     --learning_rate=2.0e-3     --opt_lamb_beta_1=0.86     --opt_lamb_beta_2=0.975     --warmup_proportion=0.0     --max_steps=750     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data_phase2     --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.712     --max_samples_termination=7500000     --eval_iter_start_samples=3000000 --eval_iter_samples=500000     --eval_batch_size=16 --eval_dir=/workspace/evaldata     --cache_eval_data     --output_dir=/results     --fp16 --fused_gelu_bias --dense_seq_output --fused_mha      --allreduce_post_accumulation --allreduce_post_accumulation_fp16     --gradient_accumulation_steps=1     --log_freq=1     --local_rank=9     --bert_config_path=/workspace/phase1/bert_config.j
345: son --seed=18263'
345: ++ ./bind.sh --cpu=exclusive --ib=single --cluster=circe -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=9 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
341: + eval '    ./bind.sh --cpu=exclusive --ib=single --cluster=circe --     python -u /workspace/bert/run_pretraining.py         --train_batch_size=18     --learning_rate=2.0e-3     --opt_lamb_beta_1=0.86     --opt_lamb_beta_2=0.975     --warmup_proportion=0.0     --max_steps=750     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data_phase2     --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.712     --max_samples_termination=7500000     --eval_iter_start_samples=3000000 --eval_iter_samples=500000     --eval_batch_size=16 --eval_dir=/workspace/evaldata     --cache_eval_data     --output_dir=/results     --fp16 --fused_gelu_bias --dense_seq_output --fused_mha      --allreduce_post_accumulation --allreduce_post_accumulation_fp16     --gradient_accumulation_steps=1     --log_freq=1     --local_rank=5     --bert_config_path=/workspace/phase1/bert_config.j
341: son --seed=18263'
370: + eval '    ./bind.sh --cpu=exclusive --ib=single --cluster=circe --     python -u /workspace/bert/run_pretraining.py         --train_batch_size=18     --learning_rate=2.0e-3     --opt_lamb_beta_1=0.86     --opt_lamb_beta_2=0.975     --warmup_proportion=0.0     --max_steps=750     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data_phase2     --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.712     --max_samples_termination=7500000     --eval_iter_start_samples=3000000 --eval_iter_samples=500000     --eval_batch_size=16 --eval_dir=/workspace/evaldata     --cache_eval_data     --output_dir=/results     --fp16 --fused_gelu_bias --dense_seq_output --fused_mha      --allreduce_post_accumulation --allreduce_post_accumulation_fp16     --gradient_accumulation_steps=1     --log_freq=1     --local_rank=2     --bert_config_path=/workspace/phase1/bert_config.j
370: son --seed=18263'
370: ++ ./bind.sh --cpu=exclusive --ib=single --cluster=circe -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=2 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
341: ++ ./bind.sh --cpu=exclusive --ib=single --cluster=circe -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=5 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
350: + eval '    ./bind.sh --cpu=exclusive --ib=single --cluster=circe --     python -u /workspace/bert/run_pretraining.py         --train_batch_size=18     --learning_rate=2.0e-3     --opt_lamb_beta_1=0.86     --opt_lamb_beta_2=0.975     --warmup_proportion=0.0     --max_steps=750     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data_phase2     --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.712     --max_samples_termination=7500000     --eval_iter_start_samples=3000000 --eval_iter_samples=500000     --eval_batch_size=16 --eval_dir=/workspace/evaldata     --cache_eval_data     --output_dir=/results     --fp16 --fused_gelu_bias --dense_seq_output --fused_mha      --allreduce_post_accumulation --allreduce_post_accumulation_fp16     --gradient_accumulation_steps=1     --log_freq=1     --local_rank=14     --bert_config_path=/workspace/phase1/bert_config.
350: json --seed=18263'
350: ++ ./bind.sh --cpu=exclusive --ib=single --cluster=circe -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=14 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
380: + eval '    ./bind.sh --cpu=exclusive --ib=single --cluster=circe --     python -u /workspace/bert/run_pretraining.py         --train_batch_size=18     --learning_rate=2.0e-3     --opt_lamb_beta_1=0.86     --opt_lamb_beta_2=0.975     --warmup_proportion=0.0     --max_steps=750     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data_phase2     --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.712     --max_samples_termination=7500000     --eval_iter_start_samples=3000000 --eval_iter_samples=500000     --eval_batch_size=16 --eval_dir=/workspace/evaldata     --cache_eval_data     --output_dir=/results     --fp16 --fused_gelu_bias --dense_seq_output --fused_mha      --allreduce_post_accumulation --allreduce_post_accumulation_fp16     --gradient_accumulation_steps=1     --log_freq=1     --local_rank=12     --bert_config_path=/workspace/phase1/bert_config.
380: json --seed=18263'
369: + eval '    ./bind.sh --cpu=exclusive --ib=single --cluster=circe --     python -u /workspace/bert/run_pretraining.py         --train_batch_size=18     --learning_rate=2.0e-3     --opt_lamb_beta_1=0.86     --opt_lamb_beta_2=0.975     --warmup_proportion=0.0     --max_steps=750     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data_phase2     --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.712     --max_samples_termination=7500000     --eval_iter_start_samples=3000000 --eval_iter_samples=500000     --eval_batch_size=16 --eval_dir=/workspace/evaldata     --cache_eval_data     --output_dir=/results     --fp16 --fused_gelu_bias --dense_seq_output --fused_mha      --allreduce_post_accumulation --allreduce_post_accumulation_fp16     --gradient_accumulation_steps=1     --log_freq=1     --local_rank=1     --bert_config_path=/workspace/phase1/bert_config.j
369: son --seed=18263'
380: ++ ./bind.sh --cpu=exclusive --ib=single --cluster=circe -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=12 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
372: + eval '    ./bind.sh --cpu=exclusive --ib=single --cluster=circe --     python -u /workspace/bert/run_pretraining.py         --train_batch_size=18     --learning_rate=2.0e-3     --opt_lamb_beta_1=0.86     --opt_lamb_beta_2=0.975     --warmup_proportion=0.0     --max_steps=750     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data_phase2     --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.712     --max_samples_termination=7500000     --eval_iter_start_samples=3000000 --eval_iter_samples=500000     --eval_batch_size=16 --eval_dir=/workspace/evaldata     --cache_eval_data     --output_dir=/results     --fp16 --fused_gelu_bias --dense_seq_output --fused_mha      --allreduce_post_accumulation --allreduce_post_accumulation_fp16     --gradient_accumulation_steps=1     --log_freq=1     --local_rank=4     --bert_config_path=/workspace/phase1/bert_config.j
372: son --seed=18263'
383: + eval '    ./bind.sh --cpu=exclusive --ib=single --cluster=circe --     python -u /workspace/bert/run_pretraining.py         --train_batch_size=18     --learning_rate=2.0e-3     --opt_lamb_beta_1=0.86     --opt_lamb_beta_2=0.975     --warmup_proportion=0.0     --max_steps=750     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data_phase2     --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.712     --max_samples_termination=7500000     --eval_iter_start_samples=3000000 --eval_iter_samples=500000     --eval_batch_size=16 --eval_dir=/workspace/evaldata     --cache_eval_data     --output_dir=/results     --fp16 --fused_gelu_bias --dense_seq_output --fused_mha      --allreduce_post_accumulation --allreduce_post_accumulation_fp16     --gradient_accumulation_steps=1     --log_freq=1     --local_rank=15     --bert_config_path=/workspace/phase1/bert_config.
383: json --seed=18263'
369: ++ ./bind.sh --cpu=exclusive --ib=single --cluster=circe -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=1 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
372: ++ ./bind.sh --cpu=exclusive --ib=single --cluster=circe -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=4 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
383: ++ ./bind.sh --cpu=exclusive --ib=single --cluster=circe -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=15 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
 21: son --seed=18263'
 21: ++ ./bind.sh --cpu=exclusive --ib=single --cluster=circe -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=5 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
483: slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
384: slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
388: slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
393: slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
391: slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
385: slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
389: slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
390: slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
392: slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
387: slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
399: slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
397: slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
395: slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
394: slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
398: slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
396: slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
385: + eval '    ./bind.sh --cpu=exclusive --ib=single --cluster=circe --     python -u /workspace/bert/run_pretraining.py         --train_batch_size=18     --learning_rate=2.0e-3     --opt_lamb_beta_1=0.86     --opt_lamb_beta_2=0.975     --warmup_proportion=0.0     --max_steps=750     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data_phase2     --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.712     --max_samples_termination=7500000     --eval_iter_start_samples=3000000 --eval_iter_samples=500000     --eval_batch_size=16 --eval_dir=/workspace/evaldata     --cache_eval_data     --output_dir=/results     --fp16 --fused_gelu_bias --dense_seq_output --fused_mha      --allreduce_post_accumulation --allreduce_post_accumulation_fp16     --gradient_accumulation_steps=1     --log_freq=1     --local_rank=1     --bert_config_path=/workspace/phase1/bert_config.j
385: son --seed=18263'
385: ++ ./bind.sh --cpu=exclusive --ib=single --cluster=circe -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=1 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
387: + eval '    ./bind.sh --cpu=exclusive --ib=single --cluster=circe --     python -u /workspace/bert/run_pretraining.py         --train_batch_size=18     --learning_rate=2.0e-3     --opt_lamb_beta_1=0.86     --opt_lamb_beta_2=0.975     --warmup_proportion=0.0     --max_steps=750     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data_phase2     --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.712     --max_samples_termination=7500000     --eval_iter_start_samples=3000000 --eval_iter_samples=500000     --eval_batch_size=16 --eval_dir=/workspace/evaldata     --cache_eval_data     --output_dir=/results     --fp16 --fused_gelu_bias --dense_seq_output --fused_mha      --allreduce_post_accumulation --allreduce_post_accumulation_fp16     --gradient_accumulation_steps=1     --log_freq=1     --local_rank=3     --bert_config_path=/workspace/phase1/bert_config.j
387: son --seed=18263'
387: ++ ./bind.sh --cpu=exclusive --ib=single --cluster=circe -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=3 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
399: + eval '    ./bind.sh --cpu=exclusive --ib=single --cluster=circe --     python -u /workspace/bert/run_pretraining.py         --train_batch_size=18     --learning_rate=2.0e-3     --opt_lamb_beta_1=0.86     --opt_lamb_beta_2=0.975     --warmup_proportion=0.0     --max_steps=750     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data_phase2     --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.712     --max_samples_termination=7500000     --eval_iter_start_samples=3000000 --eval_iter_samples=500000     --eval_batch_size=16 --eval_dir=/workspace/evaldata     --cache_eval_data     --output_dir=/results     --fp16 --fused_gelu_bias --dense_seq_output --fused_mha      --allreduce_post_accumulation --allreduce_post_accumulation_fp16     --gradient_accumulation_steps=1     --log_freq=1     --local_rank=15     --bert_config_path=/workspace/phase1/bert_config.
399: json --seed=18263'
399: ++ ./bind.sh --cpu=exclusive --ib=single --cluster=circe -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=15 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
388: + eval '    ./bind.sh --cpu=exclusive --ib=single --cluster=circe --     python -u /workspace/bert/run_pretraining.py         --train_batch_size=18     --learning_rate=2.0e-3     --opt_lamb_beta_1=0.86     --opt_lamb_beta_2=0.975     --warmup_proportion=0.0     --max_steps=750     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data_phase2     --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.712     --max_samples_termination=7500000     --eval_iter_start_samples=3000000 --eval_iter_samples=500000     --eval_batch_size=16 --eval_dir=/workspace/evaldata     --cache_eval_data     --output_dir=/results     --fp16 --fused_gelu_bias --dense_seq_output --fused_mha      --allreduce_post_accumulation --allreduce_post_accumulation_fp16     --gradient_accumulation_steps=1     --log_freq=1     --local_rank=4     --bert_config_path=/workspace/phase1/bert_config.j
388: son --seed=18263'
388: ++ ./bind.sh --cpu=exclusive --ib=single --cluster=circe -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=4 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
397: + eval '    ./bind.sh --cpu=exclusive --ib=single --cluster=circe --     python -u /workspace/bert/run_pretraining.py         --train_batch_size=18     --learning_rate=2.0e-3     --opt_lamb_beta_1=0.86     --opt_lamb_beta_2=0.975     --warmup_proportion=0.0     --max_steps=750     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data_phase2     --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.712     --max_samples_termination=7500000     --eval_iter_start_samples=3000000 --eval_iter_samples=500000     --eval_batch_size=16 --eval_dir=/workspace/evaldata     --cache_eval_data     --output_dir=/results     --fp16 --fused_gelu_bias --dense_seq_output --fused_mha      --allreduce_post_accumulation --allreduce_post_accumulation_fp16     --gradient_accumulation_steps=1     --log_freq=1     --local_rank=13     --bert_config_path=/workspace/phase1/bert_config.
397: json --seed=18263'
397: ++ ./bind.sh --cpu=exclusive --ib=single --cluster=circe -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=13 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
398: + eval '    ./bind.sh --cpu=exclusive --ib=single --cluster=circe --     python -u /workspace/bert/run_pretraining.py         --train_batch_size=18     --learning_rate=2.0e-3     --opt_lamb_beta_1=0.86     --opt_lamb_beta_2=0.975     --warmup_proportion=0.0     --max_steps=750     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data_phase2     --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.712     --max_samples_termination=7500000     --eval_iter_start_samples=3000000 --eval_iter_samples=500000     --eval_batch_size=16 --eval_dir=/workspace/evaldata     --cache_eval_data     --output_dir=/results     --fp16 --fused_gelu_bias --dense_seq_output --fused_mha      --allreduce_post_accumulation --allreduce_post_accumulation_fp16     --gradient_accumulation_steps=1     --log_freq=1     --local_rank=14     --bert_config_path=/workspace/phase1/bert_config.
398: json --seed=18263'
398: ++ ./bind.sh --cpu=exclusive --ib=single --cluster=circe -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=14 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
384: + eval '    ./bind.sh --cpu=exclusive --ib=single --cluster=circe --     python -u /workspace/bert/run_pretraining.py         --train_batch_size=18     --learning_rate=2.0e-3     --opt_lamb_beta_1=0.86     --opt_lamb_beta_2=0.975     --warmup_proportion=0.0     --max_steps=750     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data_phase2     --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.712     --max_samples_termination=7500000     --eval_iter_start_samples=3000000 --eval_iter_samples=500000     --eval_batch_size=16 --eval_dir=/workspace/evaldata     --cache_eval_data     --output_dir=/results     --fp16 --fused_gelu_bias --dense_seq_output --fused_mha      --allreduce_post_accumulation --allreduce_post_accumulation_fp16     --gradient_accumulation_steps=1     --log_freq=1     --local_rank=0     --bert_config_path=/workspace/phase1/bert_config.j
384: son --seed=18263'
395: + eval '    ./bind.sh --cpu=exclusive --ib=single --cluster=circe --     python -u /workspace/bert/run_pretraining.py         --train_batch_size=18     --learning_rate=2.0e-3     --opt_lamb_beta_1=0.86     --opt_lamb_beta_2=0.975     --warmup_proportion=0.0     --max_steps=750     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data_phase2     --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.712     --max_samples_termination=7500000     --eval_iter_start_samples=3000000 --eval_iter_samples=500000     --eval_batch_size=16 --eval_dir=/workspace/evaldata     --cache_eval_data     --output_dir=/results     --fp16 --fused_gelu_bias --dense_seq_output --fused_mha      --allreduce_post_accumulation --allreduce_post_accumulation_fp16     --gradient_accumulation_steps=1     --log_freq=1     --local_rank=11     --bert_config_path=/workspace/phase1/bert_config.
395: json --seed=18263'
384: ++ ./bind.sh --cpu=exclusive --ib=single --cluster=circe -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=0 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
386: + eval '    ./bind.sh --cpu=exclusive --ib=single --cluster=circe --     python -u /workspace/bert/run_pretraining.py         --train_batch_size=18     --learning_rate=2.0e-3     --opt_lamb_beta_1=0.86     --opt_lamb_beta_2=0.975     --warmup_proportion=0.0     --max_steps=750     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data_phase2     --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.712     --max_samples_termination=7500000     --eval_iter_start_samples=3000000 --eval_iter_samples=500000     --eval_batch_size=16 --eval_dir=/workspace/evaldata     --cache_eval_data     --output_dir=/results     --fp16 --fused_gelu_bias --dense_seq_output --fused_mha      --allreduce_post_accumulation --allreduce_post_accumulation_fp16     --gradient_accumulation_steps=1     --log_freq=1     --local_rank=2     --bert_config_path=/workspace/phase1/bert_config.j
386: son --seed=18263'
395: ++ ./bind.sh --cpu=exclusive --ib=single --cluster=circe -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=11 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
386: ++ ./bind.sh --cpu=exclusive --ib=single --cluster=circe -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=2 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
389: + eval '    ./bind.sh --cpu=exclusive --ib=single --cluster=circe --     python -u /workspace/bert/run_pretraining.py         --train_batch_size=18     --learning_rate=2.0e-3     --opt_lamb_beta_1=0.86     --opt_lamb_beta_2=0.975     --warmup_proportion=0.0     --max_steps=750     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data_phase2     --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.712     --max_samples_termination=7500000     --eval_iter_start_samples=3000000 --eval_iter_samples=500000     --eval_batch_size=16 --eval_dir=/workspace/evaldata     --cache_eval_data     --output_dir=/results     --fp16 --fused_gelu_bias --dense_seq_output --fused_mha      --allreduce_post_accumulation --allreduce_post_accumulation_fp16     --gradient_accumulation_steps=1     --log_freq=1     --local_rank=5     --bert_config_path=/workspace/phase1/bert_config.j
389: son --seed=18263'
389: ++ ./bind.sh --cpu=exclusive --ib=single --cluster=circe -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=5 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
243: slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
240: slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
242: slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
391: + eval '    ./bind.sh --cpu=exclusive --ib=single --cluster=circe --     python -u /workspace/bert/run_pretraining.py         --train_batch_size=18     --learning_rate=2.0e-3     --opt_lamb_beta_1=0.86     --opt_lamb_beta_2=0.975     --warmup_proportion=0.0     --max_steps=750     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data_phase2     --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.712     --max_samples_termination=7500000     --eval_iter_start_samples=3000000 --eval_iter_samples=500000     --eval_batch_size=16 --eval_dir=/workspace/evaldata     --cache_eval_data     --output_dir=/results     --fp16 --fused_gelu_bias --dense_seq_output --fused_mha      --allreduce_post_accumulation --allreduce_post_accumulation_fp16     --gradient_accumulation_steps=1     --log_freq=1     --local_rank=7     --bert_config_path=/workspace/phase1/bert_config.j
244: slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
391: son --seed=18263'
252: slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
392: + eval '    ./bind.sh --cpu=exclusive --ib=single --cluster=circe --     python -u /workspace/bert/run_pretraining.py         --train_batch_size=18     --learning_rate=2.0e-3     --opt_lamb_beta_1=0.86     --opt_lamb_beta_2=0.975     --warmup_proportion=0.0     --max_steps=750     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data_phase2     --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.712     --max_samples_termination=7500000     --eval_iter_start_samples=3000000 --eval_iter_samples=500000     --eval_batch_size=16 --eval_dir=/workspace/evaldata     --cache_eval_data     --output_dir=/results     --fp16 --fused_gelu_bias --dense_seq_output --fused_mha      --allreduce_post_accumulation --allreduce_post_accumulation_fp16     --gradient_accumulation_steps=1     --log_freq=1     --local_rank=8     --bert_config_path=/workspace/phase1/bert_config.j
251: slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
392: son --seed=18263'
254: slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
246: slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
247: slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
392: ++ ./bind.sh --cpu=exclusive --ib=single --cluster=circe -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=8 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
248: slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
249: slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
253: slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
245: slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
391: ++ ./bind.sh --cpu=exclusive --ib=single --cluster=circe -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=7 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
255: slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
250: slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
242: + eval '    ./bind.sh --cpu=exclusive --ib=single --cluster=circe --     python -u /workspace/bert/run_pretraining.py         --train_batch_size=18     --learning_rate=2.0e-3     --opt_lamb_beta_1=0.86     --opt_lamb_beta_2=0.975     --warmup_proportion=0.0     --max_steps=750     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data_phase2     --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.712     --max_samples_termination=7500000     --eval_iter_start_samples=3000000 --eval_iter_samples=500000     --eval_batch_size=16 --eval_dir=/workspace/evaldata     --cache_eval_data     --output_dir=/results     --fp16 --fused_gelu_bias --dense_seq_output --fused_mha      --allreduce_post_accumulation --allreduce_post_accumulation_fp16     --gradient_accumulation_steps=1     --log_freq=1     --local_rank=2     --bert_config_path=/workspace/phase1/bert_config.j
242: son --seed=18263'
242: ++ ./bind.sh --cpu=exclusive --ib=single --cluster=circe -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=2 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
241: + eval '    ./bind.sh --cpu=exclusive --ib=single --cluster=circe --     python -u /workspace/bert/run_pretraining.py         --train_batch_size=18     --learning_rate=2.0e-3     --opt_lamb_beta_1=0.86     --opt_lamb_beta_2=0.975     --warmup_proportion=0.0     --max_steps=750     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data_phase2     --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.712     --max_samples_termination=7500000     --eval_iter_start_samples=3000000 --eval_iter_samples=500000     --eval_batch_size=16 --eval_dir=/workspace/evaldata     --cache_eval_data     --output_dir=/results     --fp16 --fused_gelu_bias --dense_seq_output --fused_mha      --allreduce_post_accumulation --allreduce_post_accumulation_fp16     --gradient_accumulation_steps=1     --log_freq=1     --local_rank=1     --bert_config_path=/workspace/phase1/bert_config.j
393: + eval '    ./bind.sh --cpu=exclusive --ib=single --cluster=circe --     python -u /workspace/bert/run_pretraining.py         --train_batch_size=18     --learning_rate=2.0e-3     --opt_lamb_beta_1=0.86     --opt_lamb_beta_2=0.975     --warmup_proportion=0.0     --max_steps=750     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data_phase2     --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.712     --max_samples_termination=7500000     --eval_iter_start_samples=3000000 --eval_iter_samples=500000     --eval_batch_size=16 --eval_dir=/workspace/evaldata     --cache_eval_data     --output_dir=/results     --fp16 --fused_gelu_bias --dense_seq_output --fused_mha      --allreduce_post_accumulation --allreduce_post_accumulation_fp16     --gradient_accumulation_steps=1     --log_freq=1     --local_rank=9     --bert_config_path=/workspace/phase1/bert_config.j
393: son --seed=18263'
396: + eval '    ./bind.sh --cpu=exclusive --ib=single --cluster=circe --     python -u /workspace/bert/run_pretraining.py         --train_batch_size=18     --learning_rate=2.0e-3     --opt_lamb_beta_1=0.86     --opt_lamb_beta_2=0.975     --warmup_proportion=0.0     --max_steps=750     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data_phase2     --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.712     --max_samples_termination=7500000     --eval_iter_start_samples=3000000 --eval_iter_samples=500000     --eval_batch_size=16 --eval_dir=/workspace/evaldata     --cache_eval_data     --output_dir=/results     --fp16 --fused_gelu_bias --dense_seq_output --fused_mha      --allreduce_post_accumulation --allreduce_post_accumulation_fp16     --gradient_accumulation_steps=1     --log_freq=1     --local_rank=12     --bert_config_path=/workspace/phase1/bert_config.
396: json --seed=18263'
396: ++ ./bind.sh --cpu=exclusive --ib=single --cluster=circe -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=12 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
393: ++ ./bind.sh --cpu=exclusive --ib=single --cluster=circe -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=9 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
390: + eval '    ./bind.sh --cpu=exclusive --ib=single --cluster=circe --     python -u /workspace/bert/run_pretraining.py         --train_batch_size=18     --learning_rate=2.0e-3     --opt_lamb_beta_1=0.86     --opt_lamb_beta_2=0.975     --warmup_proportion=0.0     --max_steps=750     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data_phase2     --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.712     --max_samples_termination=7500000     --eval_iter_start_samples=3000000 --eval_iter_samples=500000     --eval_batch_size=16 --eval_dir=/workspace/evaldata     --cache_eval_data     --output_dir=/results     --fp16 --fused_gelu_bias --dense_seq_output --fused_mha      --allreduce_post_accumulation --allreduce_post_accumulation_fp16     --gradient_accumulation_steps=1     --log_freq=1     --local_rank=6     --bert_config_path=/workspace/phase1/bert_config.j
390: son --seed=18263'
390: ++ ./bind.sh --cpu=exclusive --ib=single --cluster=circe -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=6 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
238: + exec numactl --physcpubind=42-44,90-92 --membind=1 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=14 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
401: slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
400: slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
394: + eval '    ./bind.sh --cpu=exclusive --ib=single --cluster=circe --     python -u /workspace/bert/run_pretraining.py         --train_batch_size=18     --learning_rate=2.0e-3     --opt_lamb_beta_1=0.86     --opt_lamb_beta_2=0.975     --warmup_proportion=0.0     --max_steps=750     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data_phase2     --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.712     --max_samples_termination=7500000     --eval_iter_start_samples=3000000 --eval_iter_samples=500000     --eval_batch_size=16 --eval_dir=/workspace/evaldata     --cache_eval_data     --output_dir=/results     --fp16 --fused_gelu_bias --dense_seq_output --fused_mha      --allreduce_post_accumulation --allreduce_post_accumulation_fp16     --gradient_accumulation_steps=1     --log_freq=1     --local_rank=10     --bert_config_path=/workspace/phase1/bert_config.
406: slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
394: json --seed=18263'
215: slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
405: slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
394: ++ ./bind.sh --cpu=exclusive --ib=single --cluster=circe -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=10 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
210: slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
402: slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
209: slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
404: slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
217: slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
403: slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
216: slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
415: slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
220: slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
409: slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
222: slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
413: slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
219: slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
412: slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
223: slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
408: slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
218: slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
410: slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
221: slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
414: slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
214: slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
411: slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
211: slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
213: slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
212: slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
402: + eval '    ./bind.sh --cpu=exclusive --ib=single --cluster=circe --     python -u /workspace/bert/run_pretraining.py         --train_batch_size=18     --learning_rate=2.0e-3     --opt_lamb_beta_1=0.86     --opt_lamb_beta_2=0.975     --warmup_proportion=0.0     --max_steps=750     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data_phase2     --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.712     --max_samples_termination=7500000     --eval_iter_start_samples=3000000 --eval_iter_samples=500000     --eval_batch_size=16 --eval_dir=/workspace/evaldata     --cache_eval_data     --output_dir=/results     --fp16 --fused_gelu_bias --dense_seq_output --fused_mha      --allreduce_post_accumulation --allreduce_post_accumulation_fp16     --gradient_accumulation_steps=1     --log_freq=1     --local_rank=2     --bert_config_path=/workspace/phase1/bert_config.j
217: + eval '    ./bind.sh --cpu=exclusive --ib=single --cluster=circe --     python -u /workspace/bert/run_pretraining.py         --train_batch_size=18     --learning_rate=2.0e-3     --opt_lamb_beta_1=0.86     --opt_lamb_beta_2=0.975     --warmup_proportion=0.0     --max_steps=750     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data_phase2     --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.712     --max_samples_termination=7500000     --eval_iter_start_samples=3000000 --eval_iter_samples=500000     --eval_batch_size=16 --eval_dir=/workspace/evaldata     --cache_eval_data     --output_dir=/results     --fp16 --fused_gelu_bias --dense_seq_output --fused_mha      --allreduce_post_accumulation --allreduce_post_accumulation_fp16     --gradient_accumulation_steps=1     --log_freq=1     --local_rank=9     --bert_config_path=/workspace/phase1/bert_config.j
402: son --seed=18263'
217: son --seed=18263'
406: + eval '    ./bind.sh --cpu=exclusive --ib=single --cluster=circe --     python -u /workspace/bert/run_pretraining.py         --train_batch_size=18     --learning_rate=2.0e-3     --opt_lamb_beta_1=0.86     --opt_lamb_beta_2=0.975     --warmup_proportion=0.0     --max_steps=750     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data_phase2     --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.712     --max_samples_termination=7500000     --eval_iter_start_samples=3000000 --eval_iter_samples=500000     --eval_batch_size=16 --eval_dir=/workspace/evaldata     --cache_eval_data     --output_dir=/results     --fp16 --fused_gelu_bias --dense_seq_output --fused_mha      --allreduce_post_accumulation --allreduce_post_accumulation_fp16     --gradient_accumulation_steps=1     --log_freq=1     --local_rank=6     --bert_config_path=/workspace/phase1/bert_config.j
217: ++ ./bind.sh --cpu=exclusive --ib=single --cluster=circe -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=9 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
406: son --seed=18263'
406: ++ ./bind.sh --cpu=exclusive --ib=single --cluster=circe -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=6 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
403: + eval '    ./bind.sh --cpu=exclusive --ib=single --cluster=circe --     python -u /workspace/bert/run_pretraining.py         --train_batch_size=18     --learning_rate=2.0e-3     --opt_lamb_beta_1=0.86     --opt_lamb_beta_2=0.975     --warmup_proportion=0.0     --max_steps=750     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data_phase2     --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.712     --max_samples_termination=7500000     --eval_iter_start_samples=3000000 --eval_iter_samples=500000     --eval_batch_size=16 --eval_dir=/workspace/evaldata     --cache_eval_data     --output_dir=/results     --fp16 --fused_gelu_bias --dense_seq_output --fused_mha      --allreduce_post_accumulation --allreduce_post_accumulation_fp16     --gradient_accumulation_steps=1     --log_freq=1     --local_rank=3     --bert_config_path=/workspace/phase1/bert_config.j
403: son --seed=18263'
402: ++ ./bind.sh --cpu=exclusive --ib=single --cluster=circe -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=2 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
403: ++ ./bind.sh --cpu=exclusive --ib=single --cluster=circe -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=3 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
413: + eval '    ./bind.sh --cpu=exclusive --ib=single --cluster=circe --     python -u /workspace/bert/run_pretraining.py         --train_batch_size=18     --learning_rate=2.0e-3     --opt_lamb_beta_1=0.86     --opt_lamb_beta_2=0.975     --warmup_proportion=0.0     --max_steps=750     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data_phase2     --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.712     --max_samples_termination=7500000     --eval_iter_start_samples=3000000 --eval_iter_samples=500000     --eval_batch_size=16 --eval_dir=/workspace/evaldata     --cache_eval_data     --output_dir=/results     --fp16 --fused_gelu_bias --dense_seq_output --fused_mha      --allreduce_post_accumulation --allreduce_post_accumulation_fp16     --gradient_accumulation_steps=1     --log_freq=1     --local_rank=13     --bert_config_path=/workspace/phase1/bert_config.
413: json --seed=18263'
413: ++ ./bind.sh --cpu=exclusive --ib=single --cluster=circe -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=13 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
197: num_sockets = 2 num_nodes=2 cores_per_socket=24
219: + eval '    ./bind.sh --cpu=exclusive --ib=single --cluster=circe --     python -u /workspace/bert/run_pretraining.py         --train_batch_size=18     --learning_rate=2.0e-3     --opt_lamb_beta_1=0.86     --opt_lamb_beta_2=0.975     --warmup_proportion=0.0     --max_steps=750     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data_phase2     --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.712     --max_samples_termination=7500000     --eval_iter_start_samples=3000000 --eval_iter_samples=500000     --eval_batch_size=16 --eval_dir=/workspace/evaldata     --cache_eval_data     --output_dir=/results     --fp16 --fused_gelu_bias --dense_seq_output --fused_mha      --allreduce_post_accumulation --allreduce_post_accumulation_fp16     --gradient_accumulation_steps=1     --log_freq=1     --local_rank=11     --bert_config_path=/workspace/phase1/bert_config.
 19: num_sockets = 2 num_nodes=2 cores_per_socket=24
219: json --seed=18263'
219: ++ ./bind.sh --cpu=exclusive --ib=single --cluster=circe -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=11 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
241: son --seed=18263'
241: ++ ./bind.sh --cpu=exclusive --ib=single --cluster=circe -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=1 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
248: + eval '    ./bind.sh --cpu=exclusive --ib=single --cluster=circe --     python -u /workspace/bert/run_pretraining.py         --train_batch_size=18     --learning_rate=2.0e-3     --opt_lamb_beta_1=0.86     --opt_lamb_beta_2=0.975     --warmup_proportion=0.0     --max_steps=750     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data_phase2     --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.712     --max_samples_termination=7500000     --eval_iter_start_samples=3000000 --eval_iter_samples=500000     --eval_batch_size=16 --eval_dir=/workspace/evaldata     --cache_eval_data     --output_dir=/results     --fp16 --fused_gelu_bias --dense_seq_output --fused_mha      --allreduce_post_accumulation --allreduce_post_accumulation_fp16     --gradient_accumulation_steps=1     --log_freq=1     --local_rank=8     --bert_config_path=/workspace/phase1/bert_config.j
248: son --seed=18263'
248: ++ ./bind.sh --cpu=exclusive --ib=single --cluster=circe -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=8 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
214: + eval '    ./bind.sh --cpu=exclusive --ib=single --cluster=circe --     python -u /workspace/bert/run_pretraining.py         --train_batch_size=18     --learning_rate=2.0e-3     --opt_lamb_beta_1=0.86     --opt_lamb_beta_2=0.975     --warmup_proportion=0.0     --max_steps=750     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data_phase2     --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.712     --max_samples_termination=7500000     --eval_iter_start_samples=3000000 --eval_iter_samples=500000     --eval_batch_size=16 --eval_dir=/workspace/evaldata     --cache_eval_data     --output_dir=/results     --fp16 --fused_gelu_bias --dense_seq_output --fused_mha      --allreduce_post_accumulation --allreduce_post_accumulation_fp16     --gradient_accumulation_steps=1     --log_freq=1     --local_rank=6     --bert_config_path=/workspace/phase1/bert_config.j
214: son --seed=18263'
254: + eval '    ./bind.sh --cpu=exclusive --ib=single --cluster=circe --     python -u /workspace/bert/run_pretraining.py         --train_batch_size=18     --learning_rate=2.0e-3     --opt_lamb_beta_1=0.86     --opt_lamb_beta_2=0.975     --warmup_proportion=0.0     --max_steps=750     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data_phase2     --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.712     --max_samples_termination=7500000     --eval_iter_start_samples=3000000 --eval_iter_samples=500000     --eval_batch_size=16 --eval_dir=/workspace/evaldata     --cache_eval_data     --output_dir=/results     --fp16 --fused_gelu_bias --dense_seq_output --fused_mha      --allreduce_post_accumulation --allreduce_post_accumulation_fp16     --gradient_accumulation_steps=1     --log_freq=1     --local_rank=14     --bert_config_path=/workspace/phase1/bert_config.
214: ++ ./bind.sh --cpu=exclusive --ib=single --cluster=circe -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=6 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
312: num_sockets = 2 num_nodes=2 cores_per_socket=24
254: json --seed=18263'
401: + eval '    ./bind.sh --cpu=exclusive --ib=single --cluster=circe --     python -u /workspace/bert/run_pretraining.py         --train_batch_size=18     --learning_rate=2.0e-3     --opt_lamb_beta_1=0.86     --opt_lamb_beta_2=0.975     --warmup_proportion=0.0     --max_steps=750     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data_phase2     --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.712     --max_samples_termination=7500000     --eval_iter_start_samples=3000000 --eval_iter_samples=500000     --eval_batch_size=16 --eval_dir=/workspace/evaldata     --cache_eval_data     --output_dir=/results     --fp16 --fused_gelu_bias --dense_seq_output --fused_mha      --allreduce_post_accumulation --allreduce_post_accumulation_fp16     --gradient_accumulation_steps=1     --log_freq=1     --local_rank=1     --bert_config_path=/workspace/phase1/bert_config.j
401: son --seed=18263'
401: ++ ./bind.sh --cpu=exclusive --ib=single --cluster=circe -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=1 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
254: ++ ./bind.sh --cpu=exclusive --ib=single --cluster=circe -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=14 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
249: + eval '    ./bind.sh --cpu=exclusive --ib=single --cluster=circe --     python -u /workspace/bert/run_pretraining.py         --train_batch_size=18     --learning_rate=2.0e-3     --opt_lamb_beta_1=0.86     --opt_lamb_beta_2=0.975     --warmup_proportion=0.0     --max_steps=750     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data_phase2     --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.712     --max_samples_termination=7500000     --eval_iter_start_samples=3000000 --eval_iter_samples=500000     --eval_batch_size=16 --eval_dir=/workspace/evaldata     --cache_eval_data     --output_dir=/results     --fp16 --fused_gelu_bias --dense_seq_output --fused_mha      --allreduce_post_accumulation --allreduce_post_accumulation_fp16     --gradient_accumulation_steps=1     --log_freq=1     --local_rank=9     --bert_config_path=/workspace/phase1/bert_config.j
249: son --seed=18263'
409: + eval '    ./bind.sh --cpu=exclusive --ib=single --cluster=circe --     python -u /workspace/bert/run_pretraining.py         --train_batch_size=18     --learning_rate=2.0e-3     --opt_lamb_beta_1=0.86     --opt_lamb_beta_2=0.975     --warmup_proportion=0.0     --max_steps=750     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data_phase2     --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.712     --max_samples_termination=7500000     --eval_iter_start_samples=3000000 --eval_iter_samples=500000     --eval_batch_size=16 --eval_dir=/workspace/evaldata     --cache_eval_data     --output_dir=/results     --fp16 --fused_gelu_bias --dense_seq_output --fused_mha      --allreduce_post_accumulation --allreduce_post_accumulation_fp16     --gradient_accumulation_steps=1     --log_freq=1     --local_rank=9     --bert_config_path=/workspace/phase1/bert_config.j
249: ++ ./bind.sh --cpu=exclusive --ib=single --cluster=circe -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=9 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
409: son --seed=18263'
252: + eval '    ./bind.sh --cpu=exclusive --ib=single --cluster=circe --     python -u /workspace/bert/run_pretraining.py         --train_batch_size=18     --learning_rate=2.0e-3     --opt_lamb_beta_1=0.86     --opt_lamb_beta_2=0.975     --warmup_proportion=0.0     --max_steps=750     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data_phase2     --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.712     --max_samples_termination=7500000     --eval_iter_start_samples=3000000 --eval_iter_samples=500000     --eval_batch_size=16 --eval_dir=/workspace/evaldata     --cache_eval_data     --output_dir=/results     --fp16 --fused_gelu_bias --dense_seq_output --fused_mha      --allreduce_post_accumulation --allreduce_post_accumulation_fp16     --gradient_accumulation_steps=1     --log_freq=1     --local_rank=12     --bert_config_path=/workspace/phase1/bert_config.
409: ++ ./bind.sh --cpu=exclusive --ib=single --cluster=circe -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=9 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
 61: num_sockets = 2 num_nodes=2 cores_per_socket=24
252: json --seed=18263'
252: ++ ./bind.sh --cpu=exclusive --ib=single --cluster=circe -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=12 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
246: + eval '    ./bind.sh --cpu=exclusive --ib=single --cluster=circe --     python -u /workspace/bert/run_pretraining.py         --train_batch_size=18     --learning_rate=2.0e-3     --opt_lamb_beta_1=0.86     --opt_lamb_beta_2=0.975     --warmup_proportion=0.0     --max_steps=750     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data_phase2     --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.712     --max_samples_termination=7500000     --eval_iter_start_samples=3000000 --eval_iter_samples=500000     --eval_batch_size=16 --eval_dir=/workspace/evaldata     --cache_eval_data     --output_dir=/results     --fp16 --fused_gelu_bias --dense_seq_output --fused_mha      --allreduce_post_accumulation --allreduce_post_accumulation_fp16     --gradient_accumulation_steps=1     --log_freq=1     --local_rank=6     --bert_config_path=/workspace/phase1/bert_config.j
246: son --seed=18263'
246: ++ ./bind.sh --cpu=exclusive --ib=single --cluster=circe -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=6 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
253: + eval '    ./bind.sh --cpu=exclusive --ib=single --cluster=circe --     python -u /workspace/bert/run_pretraining.py         --train_batch_size=18     --learning_rate=2.0e-3     --opt_lamb_beta_1=0.86     --opt_lamb_beta_2=0.975     --warmup_proportion=0.0     --max_steps=750     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data_phase2     --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.712     --max_samples_termination=7500000     --eval_iter_start_samples=3000000 --eval_iter_samples=500000     --eval_batch_size=16 --eval_dir=/workspace/evaldata     --cache_eval_data     --output_dir=/results     --fp16 --fused_gelu_bias --dense_seq_output --fused_mha      --allreduce_post_accumulation --allreduce_post_accumulation_fp16     --gradient_accumulation_steps=1     --log_freq=1     --local_rank=13     --bert_config_path=/workspace/phase1/bert_config.
253: json --seed=18263'
209: + eval '    ./bind.sh --cpu=exclusive --ib=single --cluster=circe --     python -u /workspace/bert/run_pretraining.py         --train_batch_size=18     --learning_rate=2.0e-3     --opt_lamb_beta_1=0.86     --opt_lamb_beta_2=0.975     --warmup_proportion=0.0     --max_steps=750     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data_phase2     --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.712     --max_samples_termination=7500000     --eval_iter_start_samples=3000000 --eval_iter_samples=500000     --eval_batch_size=16 --eval_dir=/workspace/evaldata     --cache_eval_data     --output_dir=/results     --fp16 --fused_gelu_bias --dense_seq_output --fused_mha      --allreduce_post_accumulation --allreduce_post_accumulation_fp16     --gradient_accumulation_steps=1     --log_freq=1     --local_rank=1     --bert_config_path=/workspace/phase1/bert_config.j
209: son --seed=18263'
209: ++ ./bind.sh --cpu=exclusive --ib=single --cluster=circe -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=1 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
253: ++ ./bind.sh --cpu=exclusive --ib=single --cluster=circe -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=13 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
245: + eval '    ./bind.sh --cpu=exclusive --ib=single --cluster=circe --     python -u /workspace/bert/run_pretraining.py         --train_batch_size=18     --learning_rate=2.0e-3     --opt_lamb_beta_1=0.86     --opt_lamb_beta_2=0.975     --warmup_proportion=0.0     --max_steps=750     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data_phase2     --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.712     --max_samples_termination=7500000     --eval_iter_start_samples=3000000 --eval_iter_samples=500000     --eval_batch_size=16 --eval_dir=/workspace/evaldata     --cache_eval_data     --output_dir=/results     --fp16 --fused_gelu_bias --dense_seq_output --fused_mha      --allreduce_post_accumulation --allreduce_post_accumulation_fp16     --gradient_accumulation_steps=1     --log_freq=1     --local_rank=5     --bert_config_path=/workspace/phase1/bert_config.j
102: num_sockets = 2 num_nodes=2 cores_per_socket=24
245: son --seed=18263'
240: + eval '    ./bind.sh --cpu=exclusive --ib=single --cluster=circe --     python -u /workspace/bert/run_pretraining.py         --train_batch_size=18     --learning_rate=2.0e-3     --opt_lamb_beta_1=0.86     --opt_lamb_beta_2=0.975     --warmup_proportion=0.0     --max_steps=750     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data_phase2     --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.712     --max_samples_termination=7500000     --eval_iter_start_samples=3000000 --eval_iter_samples=500000     --eval_batch_size=16 --eval_dir=/workspace/evaldata     --cache_eval_data     --output_dir=/results     --fp16 --fused_gelu_bias --dense_seq_output --fused_mha      --allreduce_post_accumulation --allreduce_post_accumulation_fp16     --gradient_accumulation_steps=1     --log_freq=1     --local_rank=0     --bert_config_path=/workspace/phase1/bert_config.j
240: son --seed=18263'
243: + eval '    ./bind.sh --cpu=exclusive --ib=single --cluster=circe --     python -u /workspace/bert/run_pretraining.py         --train_batch_size=18     --learning_rate=2.0e-3     --opt_lamb_beta_1=0.86     --opt_lamb_beta_2=0.975     --warmup_proportion=0.0     --max_steps=750     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data_phase2     --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.712     --max_samples_termination=7500000     --eval_iter_start_samples=3000000 --eval_iter_samples=500000     --eval_batch_size=16 --eval_dir=/workspace/evaldata     --cache_eval_data     --output_dir=/results     --fp16 --fused_gelu_bias --dense_seq_output --fused_mha      --allreduce_post_accumulation --allreduce_post_accumulation_fp16     --gradient_accumulation_steps=1     --log_freq=1     --local_rank=3     --bert_config_path=/workspace/phase1/bert_config.j
243: son --seed=18263'
245: ++ ./bind.sh --cpu=exclusive --ib=single --cluster=circe -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=5 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
240: ++ ./bind.sh --cpu=exclusive --ib=single --cluster=circe -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=0 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
407: + eval '    ./bind.sh --cpu=exclusive --ib=single --cluster=circe --     python -u /workspace/bert/run_pretraining.py         --train_batch_size=18     --learning_rate=2.0e-3     --opt_lamb_beta_1=0.86     --opt_lamb_beta_2=0.975     --warmup_proportion=0.0     --max_steps=750     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data_phase2     --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.712     --max_samples_termination=7500000     --eval_iter_start_samples=3000000 --eval_iter_samples=500000     --eval_batch_size=16 --eval_dir=/workspace/evaldata     --cache_eval_data     --output_dir=/results     --fp16 --fused_gelu_bias --dense_seq_output --fused_mha      --allreduce_post_accumulation --allreduce_post_accumulation_fp16     --gradient_accumulation_steps=1     --log_freq=1     --local_rank=7     --bert_config_path=/workspace/phase1/bert_config.j
407: son --seed=18263'
400: + eval '    ./bind.sh --cpu=exclusive --ib=single --cluster=circe --     python -u /workspace/bert/run_pretraining.py         --train_batch_size=18     --learning_rate=2.0e-3     --opt_lamb_beta_1=0.86     --opt_lamb_beta_2=0.975     --warmup_proportion=0.0     --max_steps=750     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data_phase2     --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.712     --max_samples_termination=7500000     --eval_iter_start_samples=3000000 --eval_iter_samples=500000     --eval_batch_size=16 --eval_dir=/workspace/evaldata     --cache_eval_data     --output_dir=/results     --fp16 --fused_gelu_bias --dense_seq_output --fused_mha      --allreduce_post_accumulation --allreduce_post_accumulation_fp16     --gradient_accumulation_steps=1     --log_freq=1     --local_rank=0     --bert_config_path=/workspace/phase1/bert_config.j
400: son --seed=18263'
243: ++ ./bind.sh --cpu=exclusive --ib=single --cluster=circe -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=3 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
346: num_sockets = 2 num_nodes=2 cores_per_socket=24
255: + eval '    ./bind.sh --cpu=exclusive --ib=single --cluster=circe --     python -u /workspace/bert/run_pretraining.py         --train_batch_size=18     --learning_rate=2.0e-3     --opt_lamb_beta_1=0.86     --opt_lamb_beta_2=0.975     --warmup_proportion=0.0     --max_steps=750     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data_phase2     --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.712     --max_samples_termination=7500000     --eval_iter_start_samples=3000000 --eval_iter_samples=500000     --eval_batch_size=16 --eval_dir=/workspace/evaldata     --cache_eval_data     --output_dir=/results     --fp16 --fused_gelu_bias --dense_seq_output --fused_mha      --allreduce_post_accumulation --allreduce_post_accumulation_fp16     --gradient_accumulation_steps=1     --log_freq=1     --local_rank=15     --bert_config_path=/workspace/phase1/bert_config.
125: num_sockets = 2 num_nodes=2 cores_per_socket=24
255: json --seed=18263'
270: num_sockets = 2 num_nodes=2 cores_per_socket=24
251: + eval '    ./bind.sh --cpu=exclusive --ib=single --cluster=circe --     python -u /workspace/bert/run_pretraining.py         --train_batch_size=18     --learning_rate=2.0e-3     --opt_lamb_beta_1=0.86     --opt_lamb_beta_2=0.975     --warmup_proportion=0.0     --max_steps=750     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data_phase2     --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.712     --max_samples_termination=7500000     --eval_iter_start_samples=3000000 --eval_iter_samples=500000     --eval_batch_size=16 --eval_dir=/workspace/evaldata     --cache_eval_data     --output_dir=/results     --fp16 --fused_gelu_bias --dense_seq_output --fused_mha      --allreduce_post_accumulation --allreduce_post_accumulation_fp16     --gradient_accumulation_steps=1     --log_freq=1     --local_rank=11     --bert_config_path=/workspace/phase1/bert_config.
374: num_sockets = 2 num_nodes=2 cores_per_socket=24
251: json --seed=18263'
255: ++ ./bind.sh --cpu=exclusive --ib=single --cluster=circe -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=15 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
407: ++ ./bind.sh --cpu=exclusive --ib=single --cluster=circe -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=7 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
400: ++ ./bind.sh --cpu=exclusive --ib=single --cluster=circe -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=0 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
251: ++ ./bind.sh --cpu=exclusive --ib=single --cluster=circe -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=11 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
250: + eval '    ./bind.sh --cpu=exclusive --ib=single --cluster=circe --     python -u /workspace/bert/run_pretraining.py         --train_batch_size=18     --learning_rate=2.0e-3     --opt_lamb_beta_1=0.86     --opt_lamb_beta_2=0.975     --warmup_proportion=0.0     --max_steps=750     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data_phase2     --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.712     --max_samples_termination=7500000     --eval_iter_start_samples=3000000 --eval_iter_samples=500000     --eval_batch_size=16 --eval_dir=/workspace/evaldata     --cache_eval_data     --output_dir=/results     --fp16 --fused_gelu_bias --dense_seq_output --fused_mha      --allreduce_post_accumulation --allreduce_post_accumulation_fp16     --gradient_accumulation_steps=1     --log_freq=1     --local_rank=10     --bert_config_path=/workspace/phase1/bert_config.
 39: num_sockets = 2 num_nodes=2 cores_per_socket=24
250: json --seed=18263'
430: num_sockets = 2 num_nodes=2 cores_per_socket=24
404: + eval '    ./bind.sh --cpu=exclusive --ib=single --cluster=circe --     python -u /workspace/bert/run_pretraining.py         --train_batch_size=18     --learning_rate=2.0e-3     --opt_lamb_beta_1=0.86     --opt_lamb_beta_2=0.975     --warmup_proportion=0.0     --max_steps=750     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data_phase2     --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.712     --max_samples_termination=7500000     --eval_iter_start_samples=3000000 --eval_iter_samples=500000     --eval_batch_size=16 --eval_dir=/workspace/evaldata     --cache_eval_data     --output_dir=/results     --fp16 --fused_gelu_bias --dense_seq_output --fused_mha      --allreduce_post_accumulation --allreduce_post_accumulation_fp16     --gradient_accumulation_steps=1     --log_freq=1     --local_rank=4     --bert_config_path=/workspace/phase1/bert_config.j
120: num_sockets = 2 num_nodes=2 cores_per_socket=24
404: son --seed=18263'
410: + eval '    ./bind.sh --cpu=exclusive --ib=single --cluster=circe --     python -u /workspace/bert/run_pretraining.py         --train_batch_size=18     --learning_rate=2.0e-3     --opt_lamb_beta_1=0.86     --opt_lamb_beta_2=0.975     --warmup_proportion=0.0     --max_steps=750     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data_phase2     --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.712     --max_samples_termination=7500000     --eval_iter_start_samples=3000000 --eval_iter_samples=500000     --eval_batch_size=16 --eval_dir=/workspace/evaldata     --cache_eval_data     --output_dir=/results     --fp16 --fused_gelu_bias --dense_seq_output --fused_mha      --allreduce_post_accumulation --allreduce_post_accumulation_fp16     --gradient_accumulation_steps=1     --log_freq=1     --local_rank=10     --bert_config_path=/workspace/phase1/bert_config.
410: json --seed=18263'
408: + eval '    ./bind.sh --cpu=exclusive --ib=single --cluster=circe --     python -u /workspace/bert/run_pretraining.py         --train_batch_size=18     --learning_rate=2.0e-3     --opt_lamb_beta_1=0.86     --opt_lamb_beta_2=0.975     --warmup_proportion=0.0     --max_steps=750     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data_phase2     --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.712     --max_samples_termination=7500000     --eval_iter_start_samples=3000000 --eval_iter_samples=500000     --eval_batch_size=16 --eval_dir=/workspace/evaldata     --cache_eval_data     --output_dir=/results     --fp16 --fused_gelu_bias --dense_seq_output --fused_mha      --allreduce_post_accumulation --allreduce_post_accumulation_fp16     --gradient_accumulation_steps=1     --log_freq=1     --local_rank=8     --bert_config_path=/workspace/phase1/bert_config.j
408: son --seed=18263'
404: ++ ./bind.sh --cpu=exclusive --ib=single --cluster=circe -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=4 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
221: + eval '    ./bind.sh --cpu=exclusive --ib=single --cluster=circe --     python -u /workspace/bert/run_pretraining.py         --train_batch_size=18     --learning_rate=2.0e-3     --opt_lamb_beta_1=0.86     --opt_lamb_beta_2=0.975     --warmup_proportion=0.0     --max_steps=750     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data_phase2     --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.712     --max_samples_termination=7500000     --eval_iter_start_samples=3000000 --eval_iter_samples=500000     --eval_batch_size=16 --eval_dir=/workspace/evaldata     --cache_eval_data     --output_dir=/results     --fp16 --fused_gelu_bias --dense_seq_output --fused_mha      --allreduce_post_accumulation --allreduce_post_accumulation_fp16     --gradient_accumulation_steps=1     --log_freq=1     --local_rank=13     --bert_config_path=/workspace/phase1/bert_config.
221: json --seed=18263'
221: ++ ./bind.sh --cpu=exclusive --ib=single --cluster=circe -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=13 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
250: ++ ./bind.sh --cpu=exclusive --ib=single --cluster=circe -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=10 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
478: num_sockets = 2 num_nodes=2 cores_per_socket=24
244: + eval '    ./bind.sh --cpu=exclusive --ib=single --cluster=circe --     python -u /workspace/bert/run_pretraining.py         --train_batch_size=18     --learning_rate=2.0e-3     --opt_lamb_beta_1=0.86     --opt_lamb_beta_2=0.975     --warmup_proportion=0.0     --max_steps=750     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data_phase2     --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.712     --max_samples_termination=7500000     --eval_iter_start_samples=3000000 --eval_iter_samples=500000     --eval_batch_size=16 --eval_dir=/workspace/evaldata     --cache_eval_data     --output_dir=/results     --fp16 --fused_gelu_bias --dense_seq_output --fused_mha      --allreduce_post_accumulation --allreduce_post_accumulation_fp16     --gradient_accumulation_steps=1     --log_freq=1     --local_rank=4     --bert_config_path=/workspace/phase1/bert_config.j
233: num_sockets = 2 num_nodes=2 cores_per_socket=24
244: son --seed=18263'
236: num_sockets = 2 num_nodes=2 cores_per_socket=24
410: ++ ./bind.sh --cpu=exclusive --ib=single --cluster=circe -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=10 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
231: num_sockets = 2 num_nodes=2 cores_per_socket=24
411: + eval '    ./bind.sh --cpu=exclusive --ib=single --cluster=circe --     python -u /workspace/bert/run_pretraining.py         --train_batch_size=18     --learning_rate=2.0e-3     --opt_lamb_beta_1=0.86     --opt_lamb_beta_2=0.975     --warmup_proportion=0.0     --max_steps=750     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data_phase2     --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.712     --max_samples_termination=7500000     --eval_iter_start_samples=3000000 --eval_iter_samples=500000     --eval_batch_size=16 --eval_dir=/workspace/evaldata     --cache_eval_data     --output_dir=/results     --fp16 --fused_gelu_bias --dense_seq_output --fused_mha      --allreduce_post_accumulation --allreduce_post_accumulation_fp16     --gradient_accumulation_steps=1     --log_freq=1     --local_rank=11     --bert_config_path=/workspace/phase1/bert_config.
235: num_sockets = 2 num_nodes=2 cores_per_socket=24
411: json --seed=18263'
177: num_sockets = 2 num_nodes=2 cores_per_socket=24
408: ++ ./bind.sh --cpu=exclusive --ib=single --cluster=circe -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=8 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
226: num_sockets = 2 num_nodes=2 cores_per_socket=24
411: ++ ./bind.sh --cpu=exclusive --ib=single --cluster=circe -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=11 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
225: num_sockets = 2 num_nodes=2 cores_per_socket=24
405: + eval '    ./bind.sh --cpu=exclusive --ib=single --cluster=circe --     python -u /workspace/bert/run_pretraining.py         --train_batch_size=18     --learning_rate=2.0e-3     --opt_lamb_beta_1=0.86     --opt_lamb_beta_2=0.975     --warmup_proportion=0.0     --max_steps=750     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data_phase2     --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.712     --max_samples_termination=7500000     --eval_iter_start_samples=3000000 --eval_iter_samples=500000     --eval_batch_size=16 --eval_dir=/workspace/evaldata     --cache_eval_data     --output_dir=/results     --fp16 --fused_gelu_bias --dense_seq_output --fused_mha      --allreduce_post_accumulation --allreduce_post_accumulation_fp16     --gradient_accumulation_steps=1     --log_freq=1     --local_rank=5     --bert_config_path=/workspace/phase1/bert_config.j
 79: num_sockets = 2 num_nodes=2 cores_per_socket=24
405: son --seed=18263'
228: num_sockets = 2 num_nodes=2 cores_per_socket=24
218: + eval '    ./bind.sh --cpu=exclusive --ib=single --cluster=circe --     python -u /workspace/bert/run_pretraining.py         --train_batch_size=18     --learning_rate=2.0e-3     --opt_lamb_beta_1=0.86     --opt_lamb_beta_2=0.975     --warmup_proportion=0.0     --max_steps=750     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data_phase2     --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.712     --max_samples_termination=7500000     --eval_iter_start_samples=3000000 --eval_iter_samples=500000     --eval_batch_size=16 --eval_dir=/workspace/evaldata     --cache_eval_data     --output_dir=/results     --fp16 --fused_gelu_bias --dense_seq_output --fused_mha      --allreduce_post_accumulation --allreduce_post_accumulation_fp16     --gradient_accumulation_steps=1     --log_freq=1     --local_rank=10     --bert_config_path=/workspace/phase1/bert_config.
227: num_sockets = 2 num_nodes=2 cores_per_socket=24
218: json --seed=18263'
230: num_sockets = 2 num_nodes=2 cores_per_socket=24
218: ++ ./bind.sh --cpu=exclusive --ib=single --cluster=circe -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=10 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
234: num_sockets = 2 num_nodes=2 cores_per_socket=24
405: ++ ./bind.sh --cpu=exclusive --ib=single --cluster=circe -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=5 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
237: num_sockets = 2 num_nodes=2 cores_per_socket=24
415: + eval '    ./bind.sh --cpu=exclusive --ib=single --cluster=circe --     python -u /workspace/bert/run_pretraining.py         --train_batch_size=18     --learning_rate=2.0e-3     --opt_lamb_beta_1=0.86     --opt_lamb_beta_2=0.975     --warmup_proportion=0.0     --max_steps=750     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data_phase2     --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.712     --max_samples_termination=7500000     --eval_iter_start_samples=3000000 --eval_iter_samples=500000     --eval_batch_size=16 --eval_dir=/workspace/evaldata     --cache_eval_data     --output_dir=/results     --fp16 --fused_gelu_bias --dense_seq_output --fused_mha      --allreduce_post_accumulation --allreduce_post_accumulation_fp16     --gradient_accumulation_steps=1     --log_freq=1     --local_rank=15     --bert_config_path=/workspace/phase1/bert_config.
327: num_sockets = 2 num_nodes=2 cores_per_socket=24
415: json --seed=18263'
213: + eval '    ./bind.sh --cpu=exclusive --ib=single --cluster=circe --     python -u /workspace/bert/run_pretraining.py         --train_batch_size=18     --learning_rate=2.0e-3     --opt_lamb_beta_1=0.86     --opt_lamb_beta_2=0.975     --warmup_proportion=0.0     --max_steps=750     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data_phase2     --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.712     --max_samples_termination=7500000     --eval_iter_start_samples=3000000 --eval_iter_samples=500000     --eval_batch_size=16 --eval_dir=/workspace/evaldata     --cache_eval_data     --output_dir=/results     --fp16 --fused_gelu_bias --dense_seq_output --fused_mha      --allreduce_post_accumulation --allreduce_post_accumulation_fp16     --gradient_accumulation_steps=1     --log_freq=1     --local_rank=5     --bert_config_path=/workspace/phase1/bert_config.j
213: son --seed=18263'
213: ++ ./bind.sh --cpu=exclusive --ib=single --cluster=circe -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=5 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
244: ++ ./bind.sh --cpu=exclusive --ib=single --cluster=circe -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=4 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
159: num_sockets = 2 num_nodes=2 cores_per_socket=24
415: ++ ./bind.sh --cpu=exclusive --ib=single --cluster=circe -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=15 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
247: + eval '    ./bind.sh --cpu=exclusive --ib=single --cluster=circe --     python -u /workspace/bert/run_pretraining.py         --train_batch_size=18     --learning_rate=2.0e-3     --opt_lamb_beta_1=0.86     --opt_lamb_beta_2=0.975     --warmup_proportion=0.0     --max_steps=750     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data_phase2     --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.712     --max_samples_termination=7500000     --eval_iter_start_samples=3000000 --eval_iter_samples=500000     --eval_batch_size=16 --eval_dir=/workspace/evaldata     --cache_eval_data     --output_dir=/results     --fp16 --fused_gelu_bias --dense_seq_output --fused_mha      --allreduce_post_accumulation --allreduce_post_accumulation_fp16     --gradient_accumulation_steps=1     --log_freq=1     --local_rank=7     --bert_config_path=/workspace/phase1/bert_config.j
247: son --seed=18263'
412: + eval '    ./bind.sh --cpu=exclusive --ib=single --cluster=circe --     python -u /workspace/bert/run_pretraining.py         --train_batch_size=18     --learning_rate=2.0e-3     --opt_lamb_beta_1=0.86     --opt_lamb_beta_2=0.975     --warmup_proportion=0.0     --max_steps=750     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data_phase2     --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.712     --max_samples_termination=7500000     --eval_iter_start_samples=3000000 --eval_iter_samples=500000     --eval_batch_size=16 --eval_dir=/workspace/evaldata     --cache_eval_data     --output_dir=/results     --fp16 --fused_gelu_bias --dense_seq_output --fused_mha      --allreduce_post_accumulation --allreduce_post_accumulation_fp16     --gradient_accumulation_steps=1     --log_freq=1     --local_rank=12     --bert_config_path=/workspace/phase1/bert_config.
412: json --seed=18263'
212: + eval '    ./bind.sh --cpu=exclusive --ib=single --cluster=circe --     python -u /workspace/bert/run_pretraining.py         --train_batch_size=18     --learning_rate=2.0e-3     --opt_lamb_beta_1=0.86     --opt_lamb_beta_2=0.975     --warmup_proportion=0.0     --max_steps=750     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data_phase2     --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.712     --max_samples_termination=7500000     --eval_iter_start_samples=3000000 --eval_iter_samples=500000     --eval_batch_size=16 --eval_dir=/workspace/evaldata     --cache_eval_data     --output_dir=/results     --fp16 --fused_gelu_bias --dense_seq_output --fused_mha      --allreduce_post_accumulation --allreduce_post_accumulation_fp16     --gradient_accumulation_steps=1     --log_freq=1     --local_rank=4     --bert_config_path=/workspace/phase1/bert_config.j
212: son --seed=18263'
212: ++ ./bind.sh --cpu=exclusive --ib=single --cluster=circe -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=4 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
412: ++ ./bind.sh --cpu=exclusive --ib=single --cluster=circe -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=12 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
414: + eval '    ./bind.sh --cpu=exclusive --ib=single --cluster=circe --     python -u /workspace/bert/run_pretraining.py         --train_batch_size=18     --learning_rate=2.0e-3     --opt_lamb_beta_1=0.86     --opt_lamb_beta_2=0.975     --warmup_proportion=0.0     --max_steps=750     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data_phase2     --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.712     --max_samples_termination=7500000     --eval_iter_start_samples=3000000 --eval_iter_samples=500000     --eval_batch_size=16 --eval_dir=/workspace/evaldata     --cache_eval_data     --output_dir=/results     --fp16 --fused_gelu_bias --dense_seq_output --fused_mha      --allreduce_post_accumulation --allreduce_post_accumulation_fp16     --gradient_accumulation_steps=1     --log_freq=1     --local_rank=14     --bert_config_path=/workspace/phase1/bert_config.
414: json --seed=18263'
210: + eval '    ./bind.sh --cpu=exclusive --ib=single --cluster=circe --     python -u /workspace/bert/run_pretraining.py         --train_batch_size=18     --learning_rate=2.0e-3     --opt_lamb_beta_1=0.86     --opt_lamb_beta_2=0.975     --warmup_proportion=0.0     --max_steps=750     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data_phase2     --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.712     --max_samples_termination=7500000     --eval_iter_start_samples=3000000 --eval_iter_samples=500000     --eval_batch_size=16 --eval_dir=/workspace/evaldata     --cache_eval_data     --output_dir=/results     --fp16 --fused_gelu_bias --dense_seq_output --fused_mha      --allreduce_post_accumulation --allreduce_post_accumulation_fp16     --gradient_accumulation_steps=1     --log_freq=1     --local_rank=2     --bert_config_path=/workspace/phase1/bert_config.j
 38: num_sockets = 2 num_nodes=2 cores_per_socket=24
210: son --seed=18263'
435: num_sockets = 2 num_nodes=2 cores_per_socket=24
210: ++ ./bind.sh --cpu=exclusive --ib=single --cluster=circe -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=2 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
  5: num_sockets = 2 num_nodes=2 cores_per_socket=24
216: + eval '    ./bind.sh --cpu=exclusive --ib=single --cluster=circe --     python -u /workspace/bert/run_pretraining.py         --train_batch_size=18     --learning_rate=2.0e-3     --opt_lamb_beta_1=0.86     --opt_lamb_beta_2=0.975     --warmup_proportion=0.0     --max_steps=750     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data_phase2     --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.712     --max_samples_termination=7500000     --eval_iter_start_samples=3000000 --eval_iter_samples=500000     --eval_batch_size=16 --eval_dir=/workspace/evaldata     --cache_eval_data     --output_dir=/results     --fp16 --fused_gelu_bias --dense_seq_output --fused_mha      --allreduce_post_accumulation --allreduce_post_accumulation_fp16     --gradient_accumulation_steps=1     --log_freq=1     --local_rank=8     --bert_config_path=/workspace/phase1/bert_config.j
112: num_sockets = 2 num_nodes=2 cores_per_socket=24
216: son --seed=18263'
433: num_sockets = 2 num_nodes=2 cores_per_socket=24
216: ++ ./bind.sh --cpu=exclusive --ib=single --cluster=circe -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=8 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
190: num_sockets = 2 num_nodes=2 cores_per_socket=24
215: + eval '    ./bind.sh --cpu=exclusive --ib=single --cluster=circe --     python -u /workspace/bert/run_pretraining.py         --train_batch_size=18     --learning_rate=2.0e-3     --opt_lamb_beta_1=0.86     --opt_lamb_beta_2=0.975     --warmup_proportion=0.0     --max_steps=750     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data_phase2     --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.712     --max_samples_termination=7500000     --eval_iter_start_samples=3000000 --eval_iter_samples=500000     --eval_batch_size=16 --eval_dir=/workspace/evaldata     --cache_eval_data     --output_dir=/results     --fp16 --fused_gelu_bias --dense_seq_output --fused_mha      --allreduce_post_accumulation --allreduce_post_accumulation_fp16     --gradient_accumulation_steps=1     --log_freq=1     --local_rank=7     --bert_config_path=/workspace/phase1/bert_config.j
215: son --seed=18263'
215: ++ ./bind.sh --cpu=exclusive --ib=single --cluster=circe -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=7 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
220: + eval '    ./bind.sh --cpu=exclusive --ib=single --cluster=circe --     python -u /workspace/bert/run_pretraining.py         --train_batch_size=18     --learning_rate=2.0e-3     --opt_lamb_beta_1=0.86     --opt_lamb_beta_2=0.975     --warmup_proportion=0.0     --max_steps=750     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data_phase2     --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.712     --max_samples_termination=7500000     --eval_iter_start_samples=3000000 --eval_iter_samples=500000     --eval_batch_size=16 --eval_dir=/workspace/evaldata     --cache_eval_data     --output_dir=/results     --fp16 --fused_gelu_bias --dense_seq_output --fused_mha      --allreduce_post_accumulation --allreduce_post_accumulation_fp16     --gradient_accumulation_steps=1     --log_freq=1     --local_rank=12     --bert_config_path=/workspace/phase1/bert_config.
220: json --seed=18263'
220: ++ ./bind.sh --cpu=exclusive --ib=single --cluster=circe -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=12 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
119: num_sockets = 2 num_nodes=2 cores_per_socket=24
360: slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
118: num_sockets = 2 num_nodes=2 cores_per_socket=24
358: slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
127: num_sockets = 2 num_nodes=2 cores_per_socket=24
353: slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
 71: num_sockets = 2 num_nodes=2 cores_per_socket=24
352: slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
186: num_sockets = 2 num_nodes=2 cores_per_socket=24
361: slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
123: num_sockets = 2 num_nodes=2 cores_per_socket=24
356: slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
189: num_sockets = 2 num_nodes=2 cores_per_socket=24
355: slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
124: num_sockets = 2 num_nodes=2 cores_per_socket=24
362: slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
325: num_sockets = 2 num_nodes=2 cores_per_socket=24
359: slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
117: num_sockets = 2 num_nodes=2 cores_per_socket=24
223: + eval '    ./bind.sh --cpu=exclusive --ib=single --cluster=circe --     python -u /workspace/bert/run_pretraining.py         --train_batch_size=18     --learning_rate=2.0e-3     --opt_lamb_beta_1=0.86     --opt_lamb_beta_2=0.975     --warmup_proportion=0.0     --max_steps=750     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data_phase2     --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.712     --max_samples_termination=7500000     --eval_iter_start_samples=3000000 --eval_iter_samples=500000     --eval_batch_size=16 --eval_dir=/workspace/evaldata     --cache_eval_data     --output_dir=/results     --fp16 --fused_gelu_bias --dense_seq_output --fused_mha      --allreduce_post_accumulation --allreduce_post_accumulation_fp16     --gradient_accumulation_steps=1     --log_freq=1     --local_rank=15     --bert_config_path=/workspace/phase1/bert_config.
114: num_sockets = 2 num_nodes=2 cores_per_socket=24
354: slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
 32: num_sockets = 2 num_nodes=2 cores_per_socket=24
223: json --seed=18263'
126: num_sockets = 2 num_nodes=2 cores_per_socket=24
363: slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
116: num_sockets = 2 num_nodes=2 cores_per_socket=24
223: ++ ./bind.sh --cpu=exclusive --ib=single --cluster=circe -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=15 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
183: num_sockets = 2 num_nodes=2 cores_per_socket=24
367: slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
453: num_sockets = 2 num_nodes=2 cores_per_socket=24
365: slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
335: num_sockets = 2 num_nodes=2 cores_per_socket=24
222: + eval '    ./bind.sh --cpu=exclusive --ib=single --cluster=circe --     python -u /workspace/bert/run_pretraining.py         --train_batch_size=18     --learning_rate=2.0e-3     --opt_lamb_beta_1=0.86     --opt_lamb_beta_2=0.975     --warmup_proportion=0.0     --max_steps=750     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data_phase2     --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.712     --max_samples_termination=7500000     --eval_iter_start_samples=3000000 --eval_iter_samples=500000     --eval_batch_size=16 --eval_dir=/workspace/evaldata     --cache_eval_data     --output_dir=/results     --fp16 --fused_gelu_bias --dense_seq_output --fused_mha      --allreduce_post_accumulation --allreduce_post_accumulation_fp16     --gradient_accumulation_steps=1     --log_freq=1     --local_rank=14     --bert_config_path=/workspace/phase1/bert_config.
 45: num_sockets = 2 num_nodes=2 cores_per_socket=24
366: slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
320: num_sockets = 2 num_nodes=2 cores_per_socket=24
222: json --seed=18263'
461: num_sockets = 2 num_nodes=2 cores_per_socket=24
364: slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
 42: num_sockets = 2 num_nodes=2 cores_per_socket=24
355: + eval '    ./bind.sh --cpu=exclusive --ib=single --cluster=circe --     python -u /workspace/bert/run_pretraining.py         --train_batch_size=18     --learning_rate=2.0e-3     --opt_lamb_beta_1=0.86     --opt_lamb_beta_2=0.975     --warmup_proportion=0.0     --max_steps=750     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data_phase2     --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.712     --max_samples_termination=7500000     --eval_iter_start_samples=3000000 --eval_iter_samples=500000     --eval_batch_size=16 --eval_dir=/workspace/evaldata     --cache_eval_data     --output_dir=/results     --fp16 --fused_gelu_bias --dense_seq_output --fused_mha      --allreduce_post_accumulation --allreduce_post_accumulation_fp16     --gradient_accumulation_steps=1     --log_freq=1     --local_rank=3     --bert_config_path=/workspace/phase1/bert_config.j
133: num_sockets = 2 num_nodes=2 cores_per_socket=24
355: son --seed=18263'
176: num_sockets = 2 num_nodes=2 cores_per_socket=24
222: ++ ./bind.sh --cpu=exclusive --ib=single --cluster=circe -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=14 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
188: num_sockets = 2 num_nodes=2 cores_per_socket=24
355: ++ ./bind.sh --cpu=exclusive --ib=single --cluster=circe -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=3 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
187: num_sockets = 2 num_nodes=2 cores_per_socket=24
359: + eval '    ./bind.sh --cpu=exclusive --ib=single --cluster=circe --     python -u /workspace/bert/run_pretraining.py         --train_batch_size=18     --learning_rate=2.0e-3     --opt_lamb_beta_1=0.86     --opt_lamb_beta_2=0.975     --warmup_proportion=0.0     --max_steps=750     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data_phase2     --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.712     --max_samples_termination=7500000     --eval_iter_start_samples=3000000 --eval_iter_samples=500000     --eval_batch_size=16 --eval_dir=/workspace/evaldata     --cache_eval_data     --output_dir=/results     --fp16 --fused_gelu_bias --dense_seq_output --fused_mha      --allreduce_post_accumulation --allreduce_post_accumulation_fp16     --gradient_accumulation_steps=1     --log_freq=1     --local_rank=7     --bert_config_path=/workspace/phase1/bert_config.j
179: num_sockets = 2 num_nodes=2 cores_per_socket=24
359: son --seed=18263'
180: num_sockets = 2 num_nodes=2 cores_per_socket=24
363: + eval '    ./bind.sh --cpu=exclusive --ib=single --cluster=circe --     python -u /workspace/bert/run_pretraining.py         --train_batch_size=18     --learning_rate=2.0e-3     --opt_lamb_beta_1=0.86     --opt_lamb_beta_2=0.975     --warmup_proportion=0.0     --max_steps=750     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data_phase2     --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.712     --max_samples_termination=7500000     --eval_iter_start_samples=3000000 --eval_iter_samples=500000     --eval_batch_size=16 --eval_dir=/workspace/evaldata     --cache_eval_data     --output_dir=/results     --fp16 --fused_gelu_bias --dense_seq_output --fused_mha      --allreduce_post_accumulation --allreduce_post_accumulation_fp16     --gradient_accumulation_steps=1     --log_freq=1     --local_rank=11     --bert_config_path=/workspace/phase1/bert_config.
329: num_sockets = 2 num_nodes=2 cores_per_socket=24
363: json --seed=18263'
184: num_sockets = 2 num_nodes=2 cores_per_socket=24
363: ++ ./bind.sh --cpu=exclusive --ib=single --cluster=circe -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=11 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
130: num_sockets = 2 num_nodes=2 cores_per_socket=24
359: ++ ./bind.sh --cpu=exclusive --ib=single --cluster=circe -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=7 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
333: num_sockets = 2 num_nodes=2 cores_per_socket=24
208: + eval '    ./bind.sh --cpu=exclusive --ib=single --cluster=circe --     python -u /workspace/bert/run_pretraining.py         --train_batch_size=18     --learning_rate=2.0e-3     --opt_lamb_beta_1=0.86     --opt_lamb_beta_2=0.975     --warmup_proportion=0.0     --max_steps=750     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data_phase2     --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.712     --max_samples_termination=7500000     --eval_iter_start_samples=3000000 --eval_iter_samples=500000     --eval_batch_size=16 --eval_dir=/workspace/evaldata     --cache_eval_data     --output_dir=/results     --fp16 --fused_gelu_bias --dense_seq_output --fused_mha      --allreduce_post_accumulation --allreduce_post_accumulation_fp16     --gradient_accumulation_steps=1     --log_freq=1     --local_rank=0     --bert_config_path=/workspace/phase1/bert_config.j
323: num_sockets = 2 num_nodes=2 cores_per_socket=24
208: son --seed=18263'
 34: num_sockets = 2 num_nodes=2 cores_per_socket=24
208: ++ ./bind.sh --cpu=exclusive --ib=single --cluster=circe -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=0 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
334: num_sockets = 2 num_nodes=2 cores_per_socket=24
360: + eval '    ./bind.sh --cpu=exclusive --ib=single --cluster=circe --     python -u /workspace/bert/run_pretraining.py         --train_batch_size=18     --learning_rate=2.0e-3     --opt_lamb_beta_1=0.86     --opt_lamb_beta_2=0.975     --warmup_proportion=0.0     --max_steps=750     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data_phase2     --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.712     --max_samples_termination=7500000     --eval_iter_start_samples=3000000 --eval_iter_samples=500000     --eval_batch_size=16 --eval_dir=/workspace/evaldata     --cache_eval_data     --output_dir=/results     --fp16 --fused_gelu_bias --dense_seq_output --fused_mha      --allreduce_post_accumulation --allreduce_post_accumulation_fp16     --gradient_accumulation_steps=1     --log_freq=1     --local_rank=8     --bert_config_path=/workspace/phase1/bert_config.j
 33: num_sockets = 2 num_nodes=2 cores_per_socket=24
360: son --seed=18263'
 35: num_sockets = 2 num_nodes=2 cores_per_socket=24
360: ++ ./bind.sh --cpu=exclusive --ib=single --cluster=circe -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=8 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
 43: num_sockets = 2 num_nodes=2 cores_per_socket=24
211: + eval '    ./bind.sh --cpu=exclusive --ib=single --cluster=circe --     python -u /workspace/bert/run_pretraining.py         --train_batch_size=18     --learning_rate=2.0e-3     --opt_lamb_beta_1=0.86     --opt_lamb_beta_2=0.975     --warmup_proportion=0.0     --max_steps=750     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data_phase2     --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.712     --max_samples_termination=7500000     --eval_iter_start_samples=3000000 --eval_iter_samples=500000     --eval_batch_size=16 --eval_dir=/workspace/evaldata     --cache_eval_data     --output_dir=/results     --fp16 --fused_gelu_bias --dense_seq_output --fused_mha      --allreduce_post_accumulation --allreduce_post_accumulation_fp16     --gradient_accumulation_steps=1     --log_freq=1     --local_rank=3     --bert_config_path=/workspace/phase1/bert_config.j
326: num_sockets = 2 num_nodes=2 cores_per_socket=24
211: son --seed=18263'
 36: num_sockets = 2 num_nodes=2 cores_per_socket=24
211: ++ ./bind.sh --cpu=exclusive --ib=single --cluster=circe -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=3 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
 37: num_sockets = 2 num_nodes=2 cores_per_socket=24
358: + eval '    ./bind.sh --cpu=exclusive --ib=single --cluster=circe --     python -u /workspace/bert/run_pretraining.py         --train_batch_size=18     --learning_rate=2.0e-3     --opt_lamb_beta_1=0.86     --opt_lamb_beta_2=0.975     --warmup_proportion=0.0     --max_steps=750     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data_phase2     --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.712     --max_samples_termination=7500000     --eval_iter_start_samples=3000000 --eval_iter_samples=500000     --eval_batch_size=16 --eval_dir=/workspace/evaldata     --cache_eval_data     --output_dir=/results     --fp16 --fused_gelu_bias --dense_seq_output --fused_mha      --allreduce_post_accumulation --allreduce_post_accumulation_fp16     --gradient_accumulation_steps=1     --log_freq=1     --local_rank=6     --bert_config_path=/workspace/phase1/bert_config.j
321: num_sockets = 2 num_nodes=2 cores_per_socket=24
358: son --seed=18263'
332: num_sockets = 2 num_nodes=2 cores_per_socket=24
358: ++ ./bind.sh --cpu=exclusive --ib=single --cluster=circe -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=6 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
 44: num_sockets = 2 num_nodes=2 cores_per_socket=24
356: + eval '    ./bind.sh --cpu=exclusive --ib=single --cluster=circe --     python -u /workspace/bert/run_pretraining.py         --train_batch_size=18     --learning_rate=2.0e-3     --opt_lamb_beta_1=0.86     --opt_lamb_beta_2=0.975     --warmup_proportion=0.0     --max_steps=750     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data_phase2     --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.712     --max_samples_termination=7500000     --eval_iter_start_samples=3000000 --eval_iter_samples=500000     --eval_batch_size=16 --eval_dir=/workspace/evaldata     --cache_eval_data     --output_dir=/results     --fp16 --fused_gelu_bias --dense_seq_output --fused_mha      --allreduce_post_accumulation --allreduce_post_accumulation_fp16     --gradient_accumulation_steps=1     --log_freq=1     --local_rank=4     --bert_config_path=/workspace/phase1/bert_config.j
140: num_sockets = 2 num_nodes=2 cores_per_socket=24
356: son --seed=18263'
458: num_sockets = 2 num_nodes=2 cores_per_socket=24
356: ++ ./bind.sh --cpu=exclusive --ib=single --cluster=circe -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=4 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
135: num_sockets = 2 num_nodes=2 cores_per_socket=24
365: + eval '    ./bind.sh --cpu=exclusive --ib=single --cluster=circe --     python -u /workspace/bert/run_pretraining.py         --train_batch_size=18     --learning_rate=2.0e-3     --opt_lamb_beta_1=0.86     --opt_lamb_beta_2=0.975     --warmup_proportion=0.0     --max_steps=750     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data_phase2     --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.712     --max_samples_termination=7500000     --eval_iter_start_samples=3000000 --eval_iter_samples=500000     --eval_batch_size=16 --eval_dir=/workspace/evaldata     --cache_eval_data     --output_dir=/results     --fp16 --fused_gelu_bias --dense_seq_output --fused_mha      --allreduce_post_accumulation --allreduce_post_accumulation_fp16     --gradient_accumulation_steps=1     --log_freq=1     --local_rank=13     --bert_config_path=/workspace/phase1/bert_config.
167: num_sockets = 2 num_nodes=2 cores_per_socket=24
365: json --seed=18263'
 51: num_sockets = 2 num_nodes=2 cores_per_socket=24
365: ++ ./bind.sh --cpu=exclusive --ib=single --cluster=circe -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=13 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
 59: num_sockets = 2 num_nodes=2 cores_per_socket=24
362: + eval '    ./bind.sh --cpu=exclusive --ib=single --cluster=circe --     python -u /workspace/bert/run_pretraining.py         --train_batch_size=18     --learning_rate=2.0e-3     --opt_lamb_beta_1=0.86     --opt_lamb_beta_2=0.975     --warmup_proportion=0.0     --max_steps=750     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data_phase2     --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.712     --max_samples_termination=7500000     --eval_iter_start_samples=3000000 --eval_iter_samples=500000     --eval_batch_size=16 --eval_dir=/workspace/evaldata     --cache_eval_data     --output_dir=/results     --fp16 --fused_gelu_bias --dense_seq_output --fused_mha      --allreduce_post_accumulation --allreduce_post_accumulation_fp16     --gradient_accumulation_steps=1     --log_freq=1     --local_rank=10     --bert_config_path=/workspace/phase1/bert_config.
509: num_sockets = 2 num_nodes=2 cores_per_socket=24
362: json --seed=18263'
 58: num_sockets = 2 num_nodes=2 cores_per_socket=24
362: ++ ./bind.sh --cpu=exclusive --ib=single --cluster=circe -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=10 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
134: num_sockets = 2 num_nodes=2 cores_per_socket=24
354: + eval '    ./bind.sh --cpu=exclusive --ib=single --cluster=circe --     python -u /workspace/bert/run_pretraining.py         --train_batch_size=18     --learning_rate=2.0e-3     --opt_lamb_beta_1=0.86     --opt_lamb_beta_2=0.975     --warmup_proportion=0.0     --max_steps=750     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data_phase2     --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.712     --max_samples_termination=7500000     --eval_iter_start_samples=3000000 --eval_iter_samples=500000     --eval_batch_size=16 --eval_dir=/workspace/evaldata     --cache_eval_data     --output_dir=/results     --fp16 --fused_gelu_bias --dense_seq_output --fused_mha      --allreduce_post_accumulation --allreduce_post_accumulation_fp16     --gradient_accumulation_steps=1     --log_freq=1     --local_rank=2     --bert_config_path=/workspace/phase1/bert_config.j
142: num_sockets = 2 num_nodes=2 cores_per_socket=24
354: son --seed=18263'
 56: num_sockets = 2 num_nodes=2 cores_per_socket=24
352: + eval '    ./bind.sh --cpu=exclusive --ib=single --cluster=circe --     python -u /workspace/bert/run_pretraining.py         --train_batch_size=18     --learning_rate=2.0e-3     --opt_lamb_beta_1=0.86     --opt_lamb_beta_2=0.975     --warmup_proportion=0.0     --max_steps=750     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data_phase2     --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.712     --max_samples_termination=7500000     --eval_iter_start_samples=3000000 --eval_iter_samples=500000     --eval_batch_size=16 --eval_dir=/workspace/evaldata     --cache_eval_data     --output_dir=/results     --fp16 --fused_gelu_bias --dense_seq_output --fused_mha      --allreduce_post_accumulation --allreduce_post_accumulation_fp16     --gradient_accumulation_steps=1     --log_freq=1     --local_rank=0     --bert_config_path=/workspace/phase1/bert_config.j
 54: num_sockets = 2 num_nodes=2 cores_per_socket=24
352: son --seed=18263'
129: num_sockets = 2 num_nodes=2 cores_per_socket=24
354: ++ ./bind.sh --cpu=exclusive --ib=single --cluster=circe -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=2 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
143: num_sockets = 2 num_nodes=2 cores_per_socket=24
352: ++ ./bind.sh --cpu=exclusive --ib=single --cluster=circe -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=0 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
440: num_sockets = 2 num_nodes=2 cores_per_socket=24
272: slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
462: num_sockets = 2 num_nodes=2 cores_per_socket=24
367: + eval '    ./bind.sh --cpu=exclusive --ib=single --cluster=circe --     python -u /workspace/bert/run_pretraining.py         --train_batch_size=18     --learning_rate=2.0e-3     --opt_lamb_beta_1=0.86     --opt_lamb_beta_2=0.975     --warmup_proportion=0.0     --max_steps=750     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data_phase2     --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.712     --max_samples_termination=7500000     --eval_iter_start_samples=3000000 --eval_iter_samples=500000     --eval_batch_size=16 --eval_dir=/workspace/evaldata     --cache_eval_data     --output_dir=/results     --fp16 --fused_gelu_bias --dense_seq_output --fused_mha      --allreduce_post_accumulation --allreduce_post_accumulation_fp16     --gradient_accumulation_steps=1     --log_freq=1     --local_rank=15     --bert_config_path=/workspace/phase1/bert_config.
 57: num_sockets = 2 num_nodes=2 cores_per_socket=24
367: json --seed=18263'
 60: num_sockets = 2 num_nodes=2 cores_per_socket=24
367: ++ ./bind.sh --cpu=exclusive --ib=single --cluster=circe -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=15 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
455: num_sockets = 2 num_nodes=2 cores_per_socket=24
353: + eval '    ./bind.sh --cpu=exclusive --ib=single --cluster=circe --     python -u /workspace/bert/run_pretraining.py         --train_batch_size=18     --learning_rate=2.0e-3     --opt_lamb_beta_1=0.86     --opt_lamb_beta_2=0.975     --warmup_proportion=0.0     --max_steps=750     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data_phase2     --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.712     --max_samples_termination=7500000     --eval_iter_start_samples=3000000 --eval_iter_samples=500000     --eval_batch_size=16 --eval_dir=/workspace/evaldata     --cache_eval_data     --output_dir=/results     --fp16 --fused_gelu_bias --dense_seq_output --fused_mha      --allreduce_post_accumulation --allreduce_post_accumulation_fp16     --gradient_accumulation_steps=1     --log_freq=1     --local_rank=1     --bert_config_path=/workspace/phase1/bert_config.j
255: num_sockets = 2 num_nodes=2 cores_per_socket=24
353: son --seed=18263'
 52: num_sockets = 2 num_nodes=2 cores_per_socket=24
353: ++ ./bind.sh --cpu=exclusive --ib=single --cluster=circe -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=1 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
 53: num_sockets = 2 num_nodes=2 cores_per_socket=24
366: + eval '    ./bind.sh --cpu=exclusive --ib=single --cluster=circe --     python -u /workspace/bert/run_pretraining.py         --train_batch_size=18     --learning_rate=2.0e-3     --opt_lamb_beta_1=0.86     --opt_lamb_beta_2=0.975     --warmup_proportion=0.0     --max_steps=750     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data_phase2     --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.712     --max_samples_termination=7500000     --eval_iter_start_samples=3000000 --eval_iter_samples=500000     --eval_batch_size=16 --eval_dir=/workspace/evaldata     --cache_eval_data     --output_dir=/results     --fp16 --fused_gelu_bias --dense_seq_output --fused_mha      --allreduce_post_accumulation --allreduce_post_accumulation_fp16     --gradient_accumulation_steps=1     --log_freq=1     --local_rank=14     --bert_config_path=/workspace/phase1/bert_config.
 62: num_sockets = 2 num_nodes=2 cores_per_socket=24
366: json --seed=18263'
131: num_sockets = 2 num_nodes=2 cores_per_socket=24
357: + eval '    ./bind.sh --cpu=exclusive --ib=single --cluster=circe --     python -u /workspace/bert/run_pretraining.py         --train_batch_size=18     --learning_rate=2.0e-3     --opt_lamb_beta_1=0.86     --opt_lamb_beta_2=0.975     --warmup_proportion=0.0     --max_steps=750     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data_phase2     --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.712     --max_samples_termination=7500000     --eval_iter_start_samples=3000000 --eval_iter_samples=500000     --eval_batch_size=16 --eval_dir=/workspace/evaldata     --cache_eval_data     --output_dir=/results     --fp16 --fused_gelu_bias --dense_seq_output --fused_mha      --allreduce_post_accumulation --allreduce_post_accumulation_fp16     --gradient_accumulation_steps=1     --log_freq=1     --local_rank=5     --bert_config_path=/workspace/phase1/bert_config.j
 55: num_sockets = 2 num_nodes=2 cores_per_socket=24
357: son --seed=18263'
132: num_sockets = 2 num_nodes=2 cores_per_socket=24
366: ++ ./bind.sh --cpu=exclusive --ib=single --cluster=circe -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=14 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
138: num_sockets = 2 num_nodes=2 cores_per_socket=24
357: ++ ./bind.sh --cpu=exclusive --ib=single --cluster=circe -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=5 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
139: num_sockets = 2 num_nodes=2 cores_per_socket=24
364: + eval '    ./bind.sh --cpu=exclusive --ib=single --cluster=circe --     python -u /workspace/bert/run_pretraining.py         --train_batch_size=18     --learning_rate=2.0e-3     --opt_lamb_beta_1=0.86     --opt_lamb_beta_2=0.975     --warmup_proportion=0.0     --max_steps=750     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data_phase2     --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.712     --max_samples_termination=7500000     --eval_iter_start_samples=3000000 --eval_iter_samples=500000     --eval_batch_size=16 --eval_dir=/workspace/evaldata     --cache_eval_data     --output_dir=/results     --fp16 --fused_gelu_bias --dense_seq_output --fused_mha      --allreduce_post_accumulation --allreduce_post_accumulation_fp16     --gradient_accumulation_steps=1     --log_freq=1     --local_rank=12     --bert_config_path=/workspace/phase1/bert_config.
436: num_sockets = 2 num_nodes=2 cores_per_socket=24
364: json --seed=18263'
454: num_sockets = 2 num_nodes=2 cores_per_socket=24
361: + eval '    ./bind.sh --cpu=exclusive --ib=single --cluster=circe --     python -u /workspace/bert/run_pretraining.py         --train_batch_size=18     --learning_rate=2.0e-3     --opt_lamb_beta_1=0.86     --opt_lamb_beta_2=0.975     --warmup_proportion=0.0     --max_steps=750     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data_phase2     --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.712     --max_samples_termination=7500000     --eval_iter_start_samples=3000000 --eval_iter_samples=500000     --eval_batch_size=16 --eval_dir=/workspace/evaldata     --cache_eval_data     --output_dir=/results     --fp16 --fused_gelu_bias --dense_seq_output --fused_mha      --allreduce_post_accumulation --allreduce_post_accumulation_fp16     --gradient_accumulation_steps=1     --log_freq=1     --local_rank=9     --bert_config_path=/workspace/phase1/bert_config.j
457: num_sockets = 2 num_nodes=2 cores_per_socket=24
361: son --seed=18263'
460: num_sockets = 2 num_nodes=2 cores_per_socket=24
361: ++ ./bind.sh --cpu=exclusive --ib=single --cluster=circe -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=9 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
451: num_sockets = 2 num_nodes=2 cores_per_socket=24
364: ++ ./bind.sh --cpu=exclusive --ib=single --cluster=circe -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=12 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
459: num_sockets = 2 num_nodes=2 cores_per_socket=24
485: slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
452: num_sockets = 2 num_nodes=2 cores_per_socket=24
484: slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
463: num_sockets = 2 num_nodes=2 cores_per_socket=24
488: slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
339: num_sockets = 2 num_nodes=2 cores_per_socket=24
480: slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
212: num_sockets = 2 num_nodes=2 cores_per_socket=24
487: slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
446: num_sockets = 2 num_nodes=2 cores_per_socket=24
481: slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
497: num_sockets = 2 num_nodes=2 cores_per_socket=24
486: slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
222: num_sockets = 2 num_nodes=2 cores_per_socket=24
482: slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
470: num_sockets = 2 num_nodes=2 cores_per_socket=24
490: slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
444: num_sockets = 2 num_nodes=2 cores_per_socket=24
494: slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
447: num_sockets = 2 num_nodes=2 cores_per_socket=24
495: slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
442: num_sockets = 2 num_nodes=2 cores_per_socket=24
492: slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
438: num_sockets = 2 num_nodes=2 cores_per_socket=24
489: slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
443: num_sockets = 2 num_nodes=2 cores_per_socket=24
493: slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
445: num_sockets = 2 num_nodes=2 cores_per_socket=24
491: slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
341: num_sockets = 2 num_nodes=2 cores_per_socket=24
482: + eval '    ./bind.sh --cpu=exclusive --ib=single --cluster=circe --     python -u /workspace/bert/run_pretraining.py         --train_batch_size=18     --learning_rate=2.0e-3     --opt_lamb_beta_1=0.86     --opt_lamb_beta_2=0.975     --warmup_proportion=0.0     --max_steps=750     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data_phase2     --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.712     --max_samples_termination=7500000     --eval_iter_start_samples=3000000 --eval_iter_samples=500000     --eval_batch_size=16 --eval_dir=/workspace/evaldata     --cache_eval_data     --output_dir=/results     --fp16 --fused_gelu_bias --dense_seq_output --fused_mha      --allreduce_post_accumulation --allreduce_post_accumulation_fp16     --gradient_accumulation_steps=1     --log_freq=1     --local_rank=2     --bert_config_path=/workspace/phase1/bert_config.j
439: num_sockets = 2 num_nodes=2 cores_per_socket=24
482: son --seed=18263'
343: num_sockets = 2 num_nodes=2 cores_per_socket=24
482: ++ ./bind.sh --cpu=exclusive --ib=single --cluster=circe -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=2 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
437: num_sockets = 2 num_nodes=2 cores_per_socket=24
486: + eval '    ./bind.sh --cpu=exclusive --ib=single --cluster=circe --     python -u /workspace/bert/run_pretraining.py         --train_batch_size=18     --learning_rate=2.0e-3     --opt_lamb_beta_1=0.86     --opt_lamb_beta_2=0.975     --warmup_proportion=0.0     --max_steps=750     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data_phase2     --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.712     --max_samples_termination=7500000     --eval_iter_start_samples=3000000 --eval_iter_samples=500000     --eval_batch_size=16 --eval_dir=/workspace/evaldata     --cache_eval_data     --output_dir=/results     --fp16 --fused_gelu_bias --dense_seq_output --fused_mha      --allreduce_post_accumulation --allreduce_post_accumulation_fp16     --gradient_accumulation_steps=1     --log_freq=1     --local_rank=6     --bert_config_path=/workspace/phase1/bert_config.j
441: num_sockets = 2 num_nodes=2 cores_per_socket=24
486: son --seed=18263'
344: num_sockets = 2 num_nodes=2 cores_per_socket=24
486: ++ ./bind.sh --cpu=exclusive --ib=single --cluster=circe -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=6 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
164: num_sockets = 2 num_nodes=2 cores_per_socket=24
494: + eval '    ./bind.sh --cpu=exclusive --ib=single --cluster=circe --     python -u /workspace/bert/run_pretraining.py         --train_batch_size=18     --learning_rate=2.0e-3     --opt_lamb_beta_1=0.86     --opt_lamb_beta_2=0.975     --warmup_proportion=0.0     --max_steps=750     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data_phase2     --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.712     --max_samples_termination=7500000     --eval_iter_start_samples=3000000 --eval_iter_samples=500000     --eval_batch_size=16 --eval_dir=/workspace/evaldata     --cache_eval_data     --output_dir=/results     --fp16 --fused_gelu_bias --dense_seq_output --fused_mha      --allreduce_post_accumulation --allreduce_post_accumulation_fp16     --gradient_accumulation_steps=1     --log_freq=1     --local_rank=14     --bert_config_path=/workspace/phase1/bert_config.
151: num_sockets = 2 num_nodes=2 cores_per_socket=24
494: json --seed=18263'
342: num_sockets = 2 num_nodes=2 cores_per_socket=24
494: ++ ./bind.sh --cpu=exclusive --ib=single --cluster=circe -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=14 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
345: num_sockets = 2 num_nodes=2 cores_per_socket=24
488: + eval '    ./bind.sh --cpu=exclusive --ib=single --cluster=circe --     python -u /workspace/bert/run_pretraining.py         --train_batch_size=18     --learning_rate=2.0e-3     --opt_lamb_beta_1=0.86     --opt_lamb_beta_2=0.975     --warmup_proportion=0.0     --max_steps=750     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data_phase2     --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.712     --max_samples_termination=7500000     --eval_iter_start_samples=3000000 --eval_iter_samples=500000     --eval_batch_size=16 --eval_dir=/workspace/evaldata     --cache_eval_data     --output_dir=/results     --fp16 --fused_gelu_bias --dense_seq_output --fused_mha      --allreduce_post_accumulation --allreduce_post_accumulation_fp16     --gradient_accumulation_steps=1     --log_freq=1     --local_rank=8     --bert_config_path=/workspace/phase1/bert_config.j
347: num_sockets = 2 num_nodes=2 cores_per_socket=24
488: son --seed=18263'
349: num_sockets = 2 num_nodes=2 cores_per_socket=24
488: ++ ./bind.sh --cpu=exclusive --ib=single --cluster=circe -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=8 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
350: num_sockets = 2 num_nodes=2 cores_per_socket=24
484: + eval '    ./bind.sh --cpu=exclusive --ib=single --cluster=circe --     python -u /workspace/bert/run_pretraining.py         --train_batch_size=18     --learning_rate=2.0e-3     --opt_lamb_beta_1=0.86     --opt_lamb_beta_2=0.975     --warmup_proportion=0.0     --max_steps=750     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data_phase2     --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.712     --max_samples_termination=7500000     --eval_iter_start_samples=3000000 --eval_iter_samples=500000     --eval_batch_size=16 --eval_dir=/workspace/evaldata     --cache_eval_data     --output_dir=/results     --fp16 --fused_gelu_bias --dense_seq_output --fused_mha      --allreduce_post_accumulation --allreduce_post_accumulation_fp16     --gradient_accumulation_steps=1     --log_freq=1     --local_rank=4     --bert_config_path=/workspace/phase1/bert_config.j
201: num_sockets = 2 num_nodes=2 cores_per_socket=24
484: son --seed=18263'
 69: num_sockets = 2 num_nodes=2 cores_per_socket=24
484: ++ ./bind.sh --cpu=exclusive --ib=single --cluster=circe -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=4 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
 64: num_sockets = 2 num_nodes=2 cores_per_socket=24
495: + eval '    ./bind.sh --cpu=exclusive --ib=single --cluster=circe --     python -u /workspace/bert/run_pretraining.py         --train_batch_size=18     --learning_rate=2.0e-3     --opt_lamb_beta_1=0.86     --opt_lamb_beta_2=0.975     --warmup_proportion=0.0     --max_steps=750     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data_phase2     --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.712     --max_samples_termination=7500000     --eval_iter_start_samples=3000000 --eval_iter_samples=500000     --eval_batch_size=16 --eval_dir=/workspace/evaldata     --cache_eval_data     --output_dir=/results     --fp16 --fused_gelu_bias --dense_seq_output --fused_mha      --allreduce_post_accumulation --allreduce_post_accumulation_fp16     --gradient_accumulation_steps=1     --log_freq=1     --local_rank=15     --bert_config_path=/workspace/phase1/bert_config.
 70: num_sockets = 2 num_nodes=2 cores_per_socket=24
495: json --seed=18263'
 75: num_sockets = 2 num_nodes=2 cores_per_socket=24
495: ++ ./bind.sh --cpu=exclusive --ib=single --cluster=circe -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=15 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
398: num_sockets = 2 num_nodes=2 cores_per_socket=24
490: + eval '    ./bind.sh --cpu=exclusive --ib=single --cluster=circe --     python -u /workspace/bert/run_pretraining.py         --train_batch_size=18     --learning_rate=2.0e-3     --opt_lamb_beta_1=0.86     --opt_lamb_beta_2=0.975     --warmup_proportion=0.0     --max_steps=750     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data_phase2     --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.712     --max_samples_termination=7500000     --eval_iter_start_samples=3000000 --eval_iter_samples=500000     --eval_batch_size=16 --eval_dir=/workspace/evaldata     --cache_eval_data     --output_dir=/results     --fp16 --fused_gelu_bias --dense_seq_output --fused_mha      --allreduce_post_accumulation --allreduce_post_accumulation_fp16     --gradient_accumulation_steps=1     --log_freq=1     --local_rank=10     --bert_config_path=/workspace/phase1/bert_config.
504: num_sockets = 2 num_nodes=2 cores_per_socket=24
490: json --seed=18263'
383: num_sockets = 2 num_nodes=2 cores_per_socket=24
490: ++ ./bind.sh --cpu=exclusive --ib=single --cluster=circe -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=10 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
 65: num_sockets = 2 num_nodes=2 cores_per_socket=24
485: + eval '    ./bind.sh --cpu=exclusive --ib=single --cluster=circe --     python -u /workspace/bert/run_pretraining.py         --train_batch_size=18     --learning_rate=2.0e-3     --opt_lamb_beta_1=0.86     --opt_lamb_beta_2=0.975     --warmup_proportion=0.0     --max_steps=750     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data_phase2     --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.712     --max_samples_termination=7500000     --eval_iter_start_samples=3000000 --eval_iter_samples=500000     --eval_batch_size=16 --eval_dir=/workspace/evaldata     --cache_eval_data     --output_dir=/results     --fp16 --fused_gelu_bias --dense_seq_output --fused_mha      --allreduce_post_accumulation --allreduce_post_accumulation_fp16     --gradient_accumulation_steps=1     --log_freq=1     --local_rank=5     --bert_config_path=/workspace/phase1/bert_config.j
 72: num_sockets = 2 num_nodes=2 cores_per_socket=24
485: son --seed=18263'
505: num_sockets = 2 num_nodes=2 cores_per_socket=24
485: ++ ./bind.sh --cpu=exclusive --ib=single --cluster=circe -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=5 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
 76: num_sockets = 2 num_nodes=2 cores_per_socket=24
480: + eval '    ./bind.sh --cpu=exclusive --ib=single --cluster=circe --     python -u /workspace/bert/run_pretraining.py         --train_batch_size=18     --learning_rate=2.0e-3     --opt_lamb_beta_1=0.86     --opt_lamb_beta_2=0.975     --warmup_proportion=0.0     --max_steps=750     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data_phase2     --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.712     --max_samples_termination=7500000     --eval_iter_start_samples=3000000 --eval_iter_samples=500000     --eval_batch_size=16 --eval_dir=/workspace/evaldata     --cache_eval_data     --output_dir=/results     --fp16 --fused_gelu_bias --dense_seq_output --fused_mha      --allreduce_post_accumulation --allreduce_post_accumulation_fp16     --gradient_accumulation_steps=1     --log_freq=1     --local_rank=0     --bert_config_path=/workspace/phase1/bert_config.j
507: num_sockets = 2 num_nodes=2 cores_per_socket=24
480: son --seed=18263'
 67: num_sockets = 2 num_nodes=2 cores_per_socket=24
480: ++ ./bind.sh --cpu=exclusive --ib=single --cluster=circe -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=0 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
 74: num_sockets = 2 num_nodes=2 cores_per_socket=24
483: + eval '    ./bind.sh --cpu=exclusive --ib=single --cluster=circe --     python -u /workspace/bert/run_pretraining.py         --train_batch_size=18     --learning_rate=2.0e-3     --opt_lamb_beta_1=0.86     --opt_lamb_beta_2=0.975     --warmup_proportion=0.0     --max_steps=750     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data_phase2     --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.712     --max_samples_termination=7500000     --eval_iter_start_samples=3000000 --eval_iter_samples=500000     --eval_batch_size=16 --eval_dir=/workspace/evaldata     --cache_eval_data     --output_dir=/results     --fp16 --fused_gelu_bias --dense_seq_output --fused_mha      --allreduce_post_accumulation --allreduce_post_accumulation_fp16     --gradient_accumulation_steps=1     --log_freq=1     --local_rank=3     --bert_config_path=/workspace/phase1/bert_config.j
 78: num_sockets = 2 num_nodes=2 cores_per_socket=24
483: son --seed=18263'
 73: num_sockets = 2 num_nodes=2 cores_per_socket=24
483: ++ ./bind.sh --cpu=exclusive --ib=single --cluster=circe -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=3 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
238: :::MLLOG {"namespace": "", "time_ms": 1593107941880, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 657}}
491: + eval '    ./bind.sh --cpu=exclusive --ib=single --cluster=circe --     python -u /workspace/bert/run_pretraining.py         --train_batch_size=18     --learning_rate=2.0e-3     --opt_lamb_beta_1=0.86     --opt_lamb_beta_2=0.975     --warmup_proportion=0.0     --max_steps=750     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data_phase2     --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.712     --max_samples_termination=7500000     --eval_iter_start_samples=3000000 --eval_iter_samples=500000     --eval_batch_size=16 --eval_dir=/workspace/evaldata     --cache_eval_data     --output_dir=/results     --fp16 --fused_gelu_bias --dense_seq_output --fused_mha      --allreduce_post_accumulation --allreduce_post_accumulation_fp16     --gradient_accumulation_steps=1     --log_freq=1     --local_rank=11     --bert_config_path=/workspace/phase1/bert_config.
153: num_sockets = 2 num_nodes=2 cores_per_socket=24
491: json --seed=18263'
 11: num_sockets = 2 num_nodes=2 cores_per_socket=24
491: ++ ./bind.sh --cpu=exclusive --ib=single --cluster=circe -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=11 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
 20: num_sockets = 2 num_nodes=2 cores_per_socket=24
492: + eval '    ./bind.sh --cpu=exclusive --ib=single --cluster=circe --     python -u /workspace/bert/run_pretraining.py         --train_batch_size=18     --learning_rate=2.0e-3     --opt_lamb_beta_1=0.86     --opt_lamb_beta_2=0.975     --warmup_proportion=0.0     --max_steps=750     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data_phase2     --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.712     --max_samples_termination=7500000     --eval_iter_start_samples=3000000 --eval_iter_samples=500000     --eval_batch_size=16 --eval_dir=/workspace/evaldata     --cache_eval_data     --output_dir=/results     --fp16 --fused_gelu_bias --dense_seq_output --fused_mha      --allreduce_post_accumulation --allreduce_post_accumulation_fp16     --gradient_accumulation_steps=1     --log_freq=1     --local_rank=12     --bert_config_path=/workspace/phase1/bert_config.
502: num_sockets = 2 num_nodes=2 cores_per_socket=24
492: json --seed=18263'
503: num_sockets = 2 num_nodes=2 cores_per_socket=24
492: ++ ./bind.sh --cpu=exclusive --ib=single --cluster=circe -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=12 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
198: num_sockets = 2 num_nodes=2 cores_per_socket=24
481: + eval '    ./bind.sh --cpu=exclusive --ib=single --cluster=circe --     python -u /workspace/bert/run_pretraining.py         --train_batch_size=18     --learning_rate=2.0e-3     --opt_lamb_beta_1=0.86     --opt_lamb_beta_2=0.975     --warmup_proportion=0.0     --max_steps=750     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data_phase2     --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.712     --max_samples_termination=7500000     --eval_iter_start_samples=3000000 --eval_iter_samples=500000     --eval_batch_size=16 --eval_dir=/workspace/evaldata     --cache_eval_data     --output_dir=/results     --fp16 --fused_gelu_bias --dense_seq_output --fused_mha      --allreduce_post_accumulation --allreduce_post_accumulation_fp16     --gradient_accumulation_steps=1     --log_freq=1     --local_rank=1     --bert_config_path=/workspace/phase1/bert_config.j
511: num_sockets = 2 num_nodes=2 cores_per_socket=24
481: son --seed=18263'
498: num_sockets = 2 num_nodes=2 cores_per_socket=24
481: ++ ./bind.sh --cpu=exclusive --ib=single --cluster=circe -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=1 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
244: num_sockets = 2 num_nodes=2 cores_per_socket=24
493: + eval '    ./bind.sh --cpu=exclusive --ib=single --cluster=circe --     python -u /workspace/bert/run_pretraining.py         --train_batch_size=18     --learning_rate=2.0e-3     --opt_lamb_beta_1=0.86     --opt_lamb_beta_2=0.975     --warmup_proportion=0.0     --max_steps=750     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data_phase2     --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.712     --max_samples_termination=7500000     --eval_iter_start_samples=3000000 --eval_iter_samples=500000     --eval_batch_size=16 --eval_dir=/workspace/evaldata     --cache_eval_data     --output_dir=/results     --fp16 --fused_gelu_bias --dense_seq_output --fused_mha      --allreduce_post_accumulation --allreduce_post_accumulation_fp16     --gradient_accumulation_steps=1     --log_freq=1     --local_rank=13     --bert_config_path=/workspace/phase1/bert_config.
506: num_sockets = 2 num_nodes=2 cores_per_socket=24
493: json --seed=18263'
246: num_sockets = 2 num_nodes=2 cores_per_socket=24
493: ++ ./bind.sh --cpu=exclusive --ib=single --cluster=circe -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=13 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
500: num_sockets = 2 num_nodes=2 cores_per_socket=24
489: + eval '    ./bind.sh --cpu=exclusive --ib=single --cluster=circe --     python -u /workspace/bert/run_pretraining.py         --train_batch_size=18     --learning_rate=2.0e-3     --opt_lamb_beta_1=0.86     --opt_lamb_beta_2=0.975     --warmup_proportion=0.0     --max_steps=750     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data_phase2     --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.712     --max_samples_termination=7500000     --eval_iter_start_samples=3000000 --eval_iter_samples=500000     --eval_batch_size=16 --eval_dir=/workspace/evaldata     --cache_eval_data     --output_dir=/results     --fp16 --fused_gelu_bias --dense_seq_output --fused_mha      --allreduce_post_accumulation --allreduce_post_accumulation_fp16     --gradient_accumulation_steps=1     --log_freq=1     --local_rank=9     --bert_config_path=/workspace/phase1/bert_config.j
510: num_sockets = 2 num_nodes=2 cores_per_socket=24
489: son --seed=18263'
250: num_sockets = 2 num_nodes=2 cores_per_socket=24
489: ++ ./bind.sh --cpu=exclusive --ib=single --cluster=circe -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=9 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
245: num_sockets = 2 num_nodes=2 cores_per_socket=24
487: + eval '    ./bind.sh --cpu=exclusive --ib=single --cluster=circe --     python -u /workspace/bert/run_pretraining.py         --train_batch_size=18     --learning_rate=2.0e-3     --opt_lamb_beta_1=0.86     --opt_lamb_beta_2=0.975     --warmup_proportion=0.0     --max_steps=750     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data_phase2     --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.712     --max_samples_termination=7500000     --eval_iter_start_samples=3000000 --eval_iter_samples=500000     --eval_batch_size=16 --eval_dir=/workspace/evaldata     --cache_eval_data     --output_dir=/results     --fp16 --fused_gelu_bias --dense_seq_output --fused_mha      --allreduce_post_accumulation --allreduce_post_accumulation_fp16     --gradient_accumulation_steps=1     --log_freq=1     --local_rank=7     --bert_config_path=/workspace/phase1/bert_config.j
211: num_sockets = 2 num_nodes=2 cores_per_socket=24
487: son --seed=18263'
247: num_sockets = 2 num_nodes=2 cores_per_socket=24
  0: slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
303: num_sockets = 2 num_nodes=2 cores_per_socket=24
304: slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
379: num_sockets = 2 num_nodes=2 cores_per_socket=24
307: slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
206: num_sockets = 2 num_nodes=2 cores_per_socket=24
309: slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
360: num_sockets = 2 num_nodes=2 cores_per_socket=24
305: slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
370: num_sockets = 2 num_nodes=2 cores_per_socket=24
313: slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
375: num_sockets = 2 num_nodes=2 cores_per_socket=24
310: slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
369: num_sockets = 2 num_nodes=2 cores_per_socket=24
308: slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
378: num_sockets = 2 num_nodes=2 cores_per_socket=24
306: slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
464: num_sockets = 2 num_nodes=2 cores_per_socket=24
312: slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
381: num_sockets = 2 num_nodes=2 cores_per_socket=24
311: slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
220: num_sockets = 2 num_nodes=2 cores_per_socket=24
314: slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
 30: num_sockets = 2 num_nodes=2 cores_per_socket=24
315: slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
368: num_sockets = 2 num_nodes=2 cores_per_socket=24
319: slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
372: num_sockets = 2 num_nodes=2 cores_per_socket=24
318: slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
380: num_sockets = 2 num_nodes=2 cores_per_socket=24
316: slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
204: num_sockets = 2 num_nodes=2 cores_per_socket=24
317: slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
155: num_sockets = 2 num_nodes=2 cores_per_socket=24
161: + exec numactl --physcpubind=3-5,51-53 --membind=0 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=1 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
192: num_sockets = 2 num_nodes=2 cores_per_socket=24
115: + exec numactl --physcpubind=9-11,57-59 --membind=0 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=3 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
205: num_sockets = 2 num_nodes=2 cores_per_socket=24
247: ++ ./bind.sh --cpu=exclusive --ib=single --cluster=circe -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=7 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
474: num_sockets = 2 num_nodes=2 cores_per_socket=24
273: slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
240: num_sockets = 2 num_nodes=2 cores_per_socket=24
277: slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
144: num_sockets = 2 num_nodes=2 cores_per_socket=24
275: slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
207: num_sockets = 2 num_nodes=2 cores_per_socket=24
276: slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
195: num_sockets = 2 num_nodes=2 cores_per_socket=24
274: slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
203: num_sockets = 2 num_nodes=2 cores_per_socket=24
286: slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
251: num_sockets = 2 num_nodes=2 cores_per_socket=24
287: slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
243: num_sockets = 2 num_nodes=2 cores_per_socket=24
285: slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
253: num_sockets = 2 num_nodes=2 cores_per_socket=24
282: slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
249: num_sockets = 2 num_nodes=2 cores_per_socket=24
283: slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
200: num_sockets = 2 num_nodes=2 cores_per_socket=24
284: slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
 31: num_sockets = 2 num_nodes=2 cores_per_socket=24
279: slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
 21: num_sockets = 2 num_nodes=2 cores_per_socket=24
278: slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
193: num_sockets = 2 num_nodes=2 cores_per_socket=24
281: slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
199: num_sockets = 2 num_nodes=2 cores_per_socket=24
280: slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
221: num_sockets = 2 num_nodes=2 cores_per_socket=24
287: + eval '    ./bind.sh --cpu=exclusive --ib=single --cluster=circe --     python -u /workspace/bert/run_pretraining.py         --train_batch_size=18     --learning_rate=2.0e-3     --opt_lamb_beta_1=0.86     --opt_lamb_beta_2=0.975     --warmup_proportion=0.0     --max_steps=750     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data_phase2     --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.712     --max_samples_termination=7500000     --eval_iter_start_samples=3000000 --eval_iter_samples=500000     --eval_batch_size=16 --eval_dir=/workspace/evaldata     --cache_eval_data     --output_dir=/results     --fp16 --fused_gelu_bias --dense_seq_output --fused_mha      --allreduce_post_accumulation --allreduce_post_accumulation_fp16     --gradient_accumulation_steps=1     --log_freq=1     --local_rank=15     --bert_config_path=/workspace/phase1/bert_config.
174: num_sockets = 2 num_nodes=2 cores_per_socket=24
287: json --seed=18263'
 29: num_sockets = 2 num_nodes=2 cores_per_socket=24
277: + eval '    ./bind.sh --cpu=exclusive --ib=single --cluster=circe --     python -u /workspace/bert/run_pretraining.py         --train_batch_size=18     --learning_rate=2.0e-3     --opt_lamb_beta_1=0.86     --opt_lamb_beta_2=0.975     --warmup_proportion=0.0     --max_steps=750     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data_phase2     --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.712     --max_samples_termination=7500000     --eval_iter_start_samples=3000000 --eval_iter_samples=500000     --eval_batch_size=16 --eval_dir=/workspace/evaldata     --cache_eval_data     --output_dir=/results     --fp16 --fused_gelu_bias --dense_seq_output --fused_mha      --allreduce_post_accumulation --allreduce_post_accumulation_fp16     --gradient_accumulation_steps=1     --log_freq=1     --local_rank=5     --bert_config_path=/workspace/phase1/bert_config.j
223: num_sockets = 2 num_nodes=2 cores_per_socket=24
277: son --seed=18263'
210: num_sockets = 2 num_nodes=2 cores_per_socket=24
414: ++ ./bind.sh --cpu=exclusive --ib=single --cluster=circe -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=14 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
208: num_sockets = 2 num_nodes=2 cores_per_socket=24
287: ++ ./bind.sh --cpu=exclusive --ib=single --cluster=circe -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=15 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
213: num_sockets = 2 num_nodes=2 cores_per_socket=24
279: + eval '    ./bind.sh --cpu=exclusive --ib=single --cluster=circe --     python -u /workspace/bert/run_pretraining.py         --train_batch_size=18     --learning_rate=2.0e-3     --opt_lamb_beta_1=0.86     --opt_lamb_beta_2=0.975     --warmup_proportion=0.0     --max_steps=750     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data_phase2     --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.712     --max_samples_termination=7500000     --eval_iter_start_samples=3000000 --eval_iter_samples=500000     --eval_batch_size=16 --eval_dir=/workspace/evaldata     --cache_eval_data     --output_dir=/results     --fp16 --fused_gelu_bias --dense_seq_output --fused_mha      --allreduce_post_accumulation --allreduce_post_accumulation_fp16     --gradient_accumulation_steps=1     --log_freq=1     --local_rank=7     --bert_config_path=/workspace/phase1/bert_config.j
 25: num_sockets = 2 num_nodes=2 cores_per_socket=24
279: son --seed=18263'
 22: num_sockets = 2 num_nodes=2 cores_per_socket=24
277: ++ ./bind.sh --cpu=exclusive --ib=single --cluster=circe -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=5 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
216: num_sockets = 2 num_nodes=2 cores_per_socket=24
279: ++ ./bind.sh --cpu=exclusive --ib=single --cluster=circe -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=7 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
 23: num_sockets = 2 num_nodes=2 cores_per_socket=24
276: + eval '    ./bind.sh --cpu=exclusive --ib=single --cluster=circe --     python -u /workspace/bert/run_pretraining.py         --train_batch_size=18     --learning_rate=2.0e-3     --opt_lamb_beta_1=0.86     --opt_lamb_beta_2=0.975     --warmup_proportion=0.0     --max_steps=750     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data_phase2     --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.712     --max_samples_termination=7500000     --eval_iter_start_samples=3000000 --eval_iter_samples=500000     --eval_batch_size=16 --eval_dir=/workspace/evaldata     --cache_eval_data     --output_dir=/results     --fp16 --fused_gelu_bias --dense_seq_output --fused_mha      --allreduce_post_accumulation --allreduce_post_accumulation_fp16     --gradient_accumulation_steps=1     --log_freq=1     --local_rank=4     --bert_config_path=/workspace/phase1/bert_config.j
291: num_sockets = 2 num_nodes=2 cores_per_socket=24
276: son --seed=18263'
146: num_sockets = 2 num_nodes=2 cores_per_socket=24
276: ++ ./bind.sh --cpu=exclusive --ib=single --cluster=circe -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=4 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
154: num_sockets = 2 num_nodes=2 cores_per_socket=24
272: + eval '    ./bind.sh --cpu=exclusive --ib=single --cluster=circe --     python -u /workspace/bert/run_pretraining.py         --train_batch_size=18     --learning_rate=2.0e-3     --opt_lamb_beta_1=0.86     --opt_lamb_beta_2=0.975     --warmup_proportion=0.0     --max_steps=750     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data_phase2     --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.712     --max_samples_termination=7500000     --eval_iter_start_samples=3000000 --eval_iter_samples=500000     --eval_batch_size=16 --eval_dir=/workspace/evaldata     --cache_eval_data     --output_dir=/results     --fp16 --fused_gelu_bias --dense_seq_output --fused_mha      --allreduce_post_accumulation --allreduce_post_accumulation_fp16     --gradient_accumulation_steps=1     --log_freq=1     --local_rank=0     --bert_config_path=/workspace/phase1/bert_config.j
 28: num_sockets = 2 num_nodes=2 cores_per_socket=24
272: son --seed=18263'
215: num_sockets = 2 num_nodes=2 cores_per_socket=24
275: + eval '    ./bind.sh --cpu=exclusive --ib=single --cluster=circe --     python -u /workspace/bert/run_pretraining.py         --train_batch_size=18     --learning_rate=2.0e-3     --opt_lamb_beta_1=0.86     --opt_lamb_beta_2=0.975     --warmup_proportion=0.0     --max_steps=750     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data_phase2     --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.712     --max_samples_termination=7500000     --eval_iter_start_samples=3000000 --eval_iter_samples=500000     --eval_batch_size=16 --eval_dir=/workspace/evaldata     --cache_eval_data     --output_dir=/results     --fp16 --fused_gelu_bias --dense_seq_output --fused_mha      --allreduce_post_accumulation --allreduce_post_accumulation_fp16     --gradient_accumulation_steps=1     --log_freq=1     --local_rank=3     --bert_config_path=/workspace/phase1/bert_config.j
152: num_sockets = 2 num_nodes=2 cores_per_socket=24
275: son --seed=18263'
160: num_sockets = 2 num_nodes=2 cores_per_socket=24
272: ++ ./bind.sh --cpu=exclusive --ib=single --cluster=circe -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=0 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
149: num_sockets = 2 num_nodes=2 cores_per_socket=24
275: ++ ./bind.sh --cpu=exclusive --ib=single --cluster=circe -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=3 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
148: num_sockets = 2 num_nodes=2 cores_per_socket=24
286: + eval '    ./bind.sh --cpu=exclusive --ib=single --cluster=circe --     python -u /workspace/bert/run_pretraining.py         --train_batch_size=18     --learning_rate=2.0e-3     --opt_lamb_beta_1=0.86     --opt_lamb_beta_2=0.975     --warmup_proportion=0.0     --max_steps=750     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data_phase2     --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.712     --max_samples_termination=7500000     --eval_iter_start_samples=3000000 --eval_iter_samples=500000     --eval_batch_size=16 --eval_dir=/workspace/evaldata     --cache_eval_data     --output_dir=/results     --fp16 --fused_gelu_bias --dense_seq_output --fused_mha      --allreduce_post_accumulation --allreduce_post_accumulation_fp16     --gradient_accumulation_steps=1     --log_freq=1     --local_rank=14     --bert_config_path=/workspace/phase1/bert_config.
 16: num_sockets = 2 num_nodes=2 cores_per_socket=24
286: json --seed=18263'
145: num_sockets = 2 num_nodes=2 cores_per_socket=24
278: + eval '    ./bind.sh --cpu=exclusive --ib=single --cluster=circe --     python -u /workspace/bert/run_pretraining.py         --train_batch_size=18     --learning_rate=2.0e-3     --opt_lamb_beta_1=0.86     --opt_lamb_beta_2=0.975     --warmup_proportion=0.0     --max_steps=750     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data_phase2     --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.712     --max_samples_termination=7500000     --eval_iter_start_samples=3000000 --eval_iter_samples=500000     --eval_batch_size=16 --eval_dir=/workspace/evaldata     --cache_eval_data     --output_dir=/results     --fp16 --fused_gelu_bias --dense_seq_output --fused_mha      --allreduce_post_accumulation --allreduce_post_accumulation_fp16     --gradient_accumulation_steps=1     --log_freq=1     --local_rank=6     --bert_config_path=/workspace/phase1/bert_config.j
 27: num_sockets = 2 num_nodes=2 cores_per_socket=24
278: son --seed=18263'
156: num_sockets = 2 num_nodes=2 cores_per_socket=24
286: ++ ./bind.sh --cpu=exclusive --ib=single --cluster=circe -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=14 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
467: num_sockets = 2 num_nodes=2 cores_per_socket=24
281: + eval '    ./bind.sh --cpu=exclusive --ib=single --cluster=circe --     python -u /workspace/bert/run_pretraining.py         --train_batch_size=18     --learning_rate=2.0e-3     --opt_lamb_beta_1=0.86     --opt_lamb_beta_2=0.975     --warmup_proportion=0.0     --max_steps=750     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data_phase2     --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.712     --max_samples_termination=7500000     --eval_iter_start_samples=3000000 --eval_iter_samples=500000     --eval_batch_size=16 --eval_dir=/workspace/evaldata     --cache_eval_data     --output_dir=/results     --fp16 --fused_gelu_bias --dense_seq_output --fused_mha      --allreduce_post_accumulation --allreduce_post_accumulation_fp16     --gradient_accumulation_steps=1     --log_freq=1     --local_rank=9     --bert_config_path=/workspace/phase1/bert_config.j
173: num_sockets = 2 num_nodes=2 cores_per_socket=24
281: son --seed=18263'
389: num_sockets = 2 num_nodes=2 cores_per_socket=24
278: ++ ./bind.sh --cpu=exclusive --ib=single --cluster=circe -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=6 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
471: num_sockets = 2 num_nodes=2 cores_per_socket=24
281: ++ ./bind.sh --cpu=exclusive --ib=single --cluster=circe -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=9 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
393: num_sockets = 2 num_nodes=2 cores_per_socket=24
282: + eval '    ./bind.sh --cpu=exclusive --ib=single --cluster=circe --     python -u /workspace/bert/run_pretraining.py         --train_batch_size=18     --learning_rate=2.0e-3     --opt_lamb_beta_1=0.86     --opt_lamb_beta_2=0.975     --warmup_proportion=0.0     --max_steps=750     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data_phase2     --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.712     --max_samples_termination=7500000     --eval_iter_start_samples=3000000 --eval_iter_samples=500000     --eval_batch_size=16 --eval_dir=/workspace/evaldata     --cache_eval_data     --output_dir=/results     --fp16 --fused_gelu_bias --dense_seq_output --fused_mha      --allreduce_post_accumulation --allreduce_post_accumulation_fp16     --gradient_accumulation_steps=1     --log_freq=1     --local_rank=10     --bert_config_path=/workspace/phase1/bert_config.
396: num_sockets = 2 num_nodes=2 cores_per_socket=24
282: json --seed=18263'
166: num_sockets = 2 num_nodes=2 cores_per_socket=24
284: + eval '    ./bind.sh --cpu=exclusive --ib=single --cluster=circe --     python -u /workspace/bert/run_pretraining.py         --train_batch_size=18     --learning_rate=2.0e-3     --opt_lamb_beta_1=0.86     --opt_lamb_beta_2=0.975     --warmup_proportion=0.0     --max_steps=750     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data_phase2     --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.712     --max_samples_termination=7500000     --eval_iter_start_samples=3000000 --eval_iter_samples=500000     --eval_batch_size=16 --eval_dir=/workspace/evaldata     --cache_eval_data     --output_dir=/results     --fp16 --fused_gelu_bias --dense_seq_output --fused_mha      --allreduce_post_accumulation --allreduce_post_accumulation_fp16     --gradient_accumulation_steps=1     --log_freq=1     --local_rank=12     --bert_config_path=/workspace/phase1/bert_config.
162: num_sockets = 2 num_nodes=2 cores_per_socket=24
284: json --seed=18263'
473: num_sockets = 2 num_nodes=2 cores_per_socket=24
282: ++ ./bind.sh --cpu=exclusive --ib=single --cluster=circe -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=10 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
390: num_sockets = 2 num_nodes=2 cores_per_socket=24
284: ++ ./bind.sh --cpu=exclusive --ib=single --cluster=circe -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=12 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
168: num_sockets = 2 num_nodes=2 cores_per_socket=24
273: + eval '    ./bind.sh --cpu=exclusive --ib=single --cluster=circe --     python -u /workspace/bert/run_pretraining.py         --train_batch_size=18     --learning_rate=2.0e-3     --opt_lamb_beta_1=0.86     --opt_lamb_beta_2=0.975     --warmup_proportion=0.0     --max_steps=750     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data_phase2     --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.712     --max_samples_termination=7500000     --eval_iter_start_samples=3000000 --eval_iter_samples=500000     --eval_batch_size=16 --eval_dir=/workspace/evaldata     --cache_eval_data     --output_dir=/results     --fp16 --fused_gelu_bias --dense_seq_output --fused_mha      --allreduce_post_accumulation --allreduce_post_accumulation_fp16     --gradient_accumulation_steps=1     --log_freq=1     --local_rank=1     --bert_config_path=/workspace/phase1/bert_config.j
465: num_sockets = 2 num_nodes=2 cores_per_socket=24
273: son --seed=18263'
394: num_sockets = 2 num_nodes=2 cores_per_socket=24
273: ++ ./bind.sh --cpu=exclusive --ib=single --cluster=circe -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=1 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
171: num_sockets = 2 num_nodes=2 cores_per_socket=24
274: + eval '    ./bind.sh --cpu=exclusive --ib=single --cluster=circe --     python -u /workspace/bert/run_pretraining.py         --train_batch_size=18     --learning_rate=2.0e-3     --opt_lamb_beta_1=0.86     --opt_lamb_beta_2=0.975     --warmup_proportion=0.0     --max_steps=750     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data_phase2     --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.712     --max_samples_termination=7500000     --eval_iter_start_samples=3000000 --eval_iter_samples=500000     --eval_batch_size=16 --eval_dir=/workspace/evaldata     --cache_eval_data     --output_dir=/results     --fp16 --fused_gelu_bias --dense_seq_output --fused_mha      --allreduce_post_accumulation --allreduce_post_accumulation_fp16     --gradient_accumulation_steps=1     --log_freq=1     --local_rank=2     --bert_config_path=/workspace/phase1/bert_config.j
172: num_sockets = 2 num_nodes=2 cores_per_socket=24
274: son --seed=18263'
170: num_sockets = 2 num_nodes=2 cores_per_socket=24
274: ++ ./bind.sh --cpu=exclusive --ib=single --cluster=circe -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=2 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
297: num_sockets = 2 num_nodes=2 cores_per_socket=24
280: + eval '    ./bind.sh --cpu=exclusive --ib=single --cluster=circe --     python -u /workspace/bert/run_pretraining.py         --train_batch_size=18     --learning_rate=2.0e-3     --opt_lamb_beta_1=0.86     --opt_lamb_beta_2=0.975     --warmup_proportion=0.0     --max_steps=750     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data_phase2     --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.712     --max_samples_termination=7500000     --eval_iter_start_samples=3000000 --eval_iter_samples=500000     --eval_batch_size=16 --eval_dir=/workspace/evaldata     --cache_eval_data     --output_dir=/results     --fp16 --fused_gelu_bias --dense_seq_output --fused_mha      --allreduce_post_accumulation --allreduce_post_accumulation_fp16     --gradient_accumulation_steps=1     --log_freq=1     --local_rank=8     --bert_config_path=/workspace/phase1/bert_config.j
163: num_sockets = 2 num_nodes=2 cores_per_socket=24
280: son --seed=18263'
169: num_sockets = 2 num_nodes=2 cores_per_socket=24
280: ++ ./bind.sh --cpu=exclusive --ib=single --cluster=circe -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=8 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
469: num_sockets = 2 num_nodes=2 cores_per_socket=24
285: + eval '    ./bind.sh --cpu=exclusive --ib=single --cluster=circe --     python -u /workspace/bert/run_pretraining.py         --train_batch_size=18     --learning_rate=2.0e-3     --opt_lamb_beta_1=0.86     --opt_lamb_beta_2=0.975     --warmup_proportion=0.0     --max_steps=750     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data_phase2     --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.712     --max_samples_termination=7500000     --eval_iter_start_samples=3000000 --eval_iter_samples=500000     --eval_batch_size=16 --eval_dir=/workspace/evaldata     --cache_eval_data     --output_dir=/results     --fp16 --fused_gelu_bias --dense_seq_output --fused_mha      --allreduce_post_accumulation --allreduce_post_accumulation_fp16     --gradient_accumulation_steps=1     --log_freq=1     --local_rank=13     --bert_config_path=/workspace/phase1/bert_config.
468: num_sockets = 2 num_nodes=2 cores_per_socket=24
285: json --seed=18263'
472: num_sockets = 2 num_nodes=2 cores_per_socket=24
285: ++ ./bind.sh --cpu=exclusive --ib=single --cluster=circe -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=13 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
395: num_sockets = 2 num_nodes=2 cores_per_socket=24
283: + eval '    ./bind.sh --cpu=exclusive --ib=single --cluster=circe --     python -u /workspace/bert/run_pretraining.py         --train_batch_size=18     --learning_rate=2.0e-3     --opt_lamb_beta_1=0.86     --opt_lamb_beta_2=0.975     --warmup_proportion=0.0     --max_steps=750     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data_phase2     --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.712     --max_samples_termination=7500000     --eval_iter_start_samples=3000000 --eval_iter_samples=500000     --eval_batch_size=16 --eval_dir=/workspace/evaldata     --cache_eval_data     --output_dir=/results     --fp16 --fused_gelu_bias --dense_seq_output --fused_mha      --allreduce_post_accumulation --allreduce_post_accumulation_fp16     --gradient_accumulation_steps=1     --log_freq=1     --local_rank=11     --bert_config_path=/workspace/phase1/bert_config.
466: num_sockets = 2 num_nodes=2 cores_per_socket=24
175: + exec numactl --physcpubind=45-47,93-95 --membind=1 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=15 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
386: num_sockets = 2 num_nodes=2 cores_per_socket=24
305: + eval '    ./bind.sh --cpu=exclusive --ib=single --cluster=circe --     python -u /workspace/bert/run_pretraining.py         --train_batch_size=18     --learning_rate=2.0e-3     --opt_lamb_beta_1=0.86     --opt_lamb_beta_2=0.975     --warmup_proportion=0.0     --max_steps=750     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data_phase2     --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.712     --max_samples_termination=7500000     --eval_iter_start_samples=3000000 --eval_iter_samples=500000     --eval_batch_size=16 --eval_dir=/workspace/evaldata     --cache_eval_data     --output_dir=/results     --fp16 --fused_gelu_bias --dense_seq_output --fused_mha      --allreduce_post_accumulation --allreduce_post_accumulation_fp16     --gradient_accumulation_steps=1     --log_freq=1     --local_rank=1     --bert_config_path=/workspace/phase1/bert_config.j
384: num_sockets = 2 num_nodes=2 cores_per_socket=24
305: son --seed=18263'
391: num_sockets = 2 num_nodes=2 cores_per_socket=24
305: ++ ./bind.sh --cpu=exclusive --ib=single --cluster=circe -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=1 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
392: num_sockets = 2 num_nodes=2 cores_per_socket=24
319: + eval '    ./bind.sh --cpu=exclusive --ib=single --cluster=circe --     python -u /workspace/bert/run_pretraining.py         --train_batch_size=18     --learning_rate=2.0e-3     --opt_lamb_beta_1=0.86     --opt_lamb_beta_2=0.975     --warmup_proportion=0.0     --max_steps=750     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data_phase2     --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.712     --max_samples_termination=7500000     --eval_iter_start_samples=3000000 --eval_iter_samples=500000     --eval_batch_size=16 --eval_dir=/workspace/evaldata     --cache_eval_data     --output_dir=/results     --fp16 --fused_gelu_bias --dense_seq_output --fused_mha      --allreduce_post_accumulation --allreduce_post_accumulation_fp16     --gradient_accumulation_steps=1     --log_freq=1     --local_rank=15     --bert_config_path=/workspace/phase1/bert_config.
266: num_sockets = 2 num_nodes=2 cores_per_socket=24
319: json --seed=18263'
271: num_sockets = 2 num_nodes=2 cores_per_socket=24
319: ++ ./bind.sh --cpu=exclusive --ib=single --cluster=circe -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=15 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
298: num_sockets = 2 num_nodes=2 cores_per_socket=24
487: ++ ./bind.sh --cpu=exclusive --ib=single --cluster=circe -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=7 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
268: num_sockets = 2 num_nodes=2 cores_per_socket=24
  2: slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
258: num_sockets = 2 num_nodes=2 cores_per_socket=24
318: + eval '    ./bind.sh --cpu=exclusive --ib=single --cluster=circe --     python -u /workspace/bert/run_pretraining.py         --train_batch_size=18     --learning_rate=2.0e-3     --opt_lamb_beta_1=0.86     --opt_lamb_beta_2=0.975     --warmup_proportion=0.0     --max_steps=750     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data_phase2     --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.712     --max_samples_termination=7500000     --eval_iter_start_samples=3000000 --eval_iter_samples=500000     --eval_batch_size=16 --eval_dir=/workspace/evaldata     --cache_eval_data     --output_dir=/results     --fp16 --fused_gelu_bias --dense_seq_output --fused_mha      --allreduce_post_accumulation --allreduce_post_accumulation_fp16     --gradient_accumulation_steps=1     --log_freq=1     --local_rank=14     --bert_config_path=/workspace/phase1/bert_config.
293: num_sockets = 2 num_nodes=2 cores_per_socket=24
 40: + exec numactl --physcpubind=24-26,72-74 --membind=1 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=8 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
301: num_sockets = 2 num_nodes=2 cores_per_socket=24
  6: slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
264: num_sockets = 2 num_nodes=2 cores_per_socket=24
  9: slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
300: num_sockets = 2 num_nodes=2 cores_per_socket=24
  5: slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
292: num_sockets = 2 num_nodes=2 cores_per_socket=24
  4: slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
265: num_sockets = 2 num_nodes=2 cores_per_socket=24
  8: slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
299: num_sockets = 2 num_nodes=2 cores_per_socket=24
318: json --seed=18263'
289: num_sockets = 2 num_nodes=2 cores_per_socket=24
  1: slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
296: num_sockets = 2 num_nodes=2 cores_per_socket=24
  3: slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
295: num_sockets = 2 num_nodes=2 cores_per_socket=24
  7: slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
259: num_sockets = 2 num_nodes=2 cores_per_socket=24
 11: slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
260: num_sockets = 2 num_nodes=2 cores_per_socket=24
 12: slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
261: num_sockets = 2 num_nodes=2 cores_per_socket=24
 10: slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
267: num_sockets = 2 num_nodes=2 cores_per_socket=24
 13: slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
262: num_sockets = 2 num_nodes=2 cores_per_socket=24
 14: slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
269: num_sockets = 2 num_nodes=2 cores_per_socket=24
 15: slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
115: :::MLLOG {"namespace": "", "time_ms": 1593107941931, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 657}}
318: ++ ./bind.sh --cpu=exclusive --ib=single --cluster=circe -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=14 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
175: :::MLLOG {"namespace": "", "time_ms": 1593107941947, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 657}}
  1: + eval '    ./bind.sh --cpu=exclusive --ib=single --cluster=circe --     python -u /workspace/bert/run_pretraining.py         --train_batch_size=18     --learning_rate=2.0e-3     --opt_lamb_beta_1=0.86     --opt_lamb_beta_2=0.975     --warmup_proportion=0.0     --max_steps=750     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data_phase2     --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.712     --max_samples_termination=7500000     --eval_iter_start_samples=3000000 --eval_iter_samples=500000     --eval_batch_size=16 --eval_dir=/workspace/evaldata     --cache_eval_data     --output_dir=/results     --fp16 --fused_gelu_bias --dense_seq_output --fused_mha      --allreduce_post_accumulation --allreduce_post_accumulation_fp16     --gradient_accumulation_steps=1     --log_freq=1     --local_rank=1     --bert_config_path=/workspace/phase1/bert_config.j
161: :::MLLOG {"namespace": "", "time_ms": 1593107941947, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 657}}
  1: son --seed=18263'
 40: :::MLLOG {"namespace": "", "time_ms": 1593107941949, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 657}}
  1: ++ ./bind.sh --cpu=exclusive --ib=single --cluster=circe -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=1 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
424: num_sockets = 2 num_nodes=2 cores_per_socket=24
  9: + eval '    ./bind.sh --cpu=exclusive --ib=single --cluster=circe --     python -u /workspace/bert/run_pretraining.py         --train_batch_size=18     --learning_rate=2.0e-3     --opt_lamb_beta_1=0.86     --opt_lamb_beta_2=0.975     --warmup_proportion=0.0     --max_steps=750     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data_phase2     --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.712     --max_samples_termination=7500000     --eval_iter_start_samples=3000000 --eval_iter_samples=500000     --eval_batch_size=16 --eval_dir=/workspace/evaldata     --cache_eval_data     --output_dir=/results     --fp16 --fused_gelu_bias --dense_seq_output --fused_mha      --allreduce_post_accumulation --allreduce_post_accumulation_fp16     --gradient_accumulation_steps=1     --log_freq=1     --local_rank=9     --bert_config_path=/workspace/phase1/bert_config.j
418: num_sockets = 2 num_nodes=2 cores_per_socket=24
  9: son --seed=18263'
 91: num_sockets = 2 num_nodes=2 cores_per_socket=24
  9: ++ ./bind.sh --cpu=exclusive --ib=single --cluster=circe -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=9 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
113: :::MLLOG {"namespace": "", "time_ms": 1593107941970, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 657}}
  5: + eval '    ./bind.sh --cpu=exclusive --ib=single --cluster=circe --     python -u /workspace/bert/run_pretraining.py         --train_batch_size=18     --learning_rate=2.0e-3     --opt_lamb_beta_1=0.86     --opt_lamb_beta_2=0.975     --warmup_proportion=0.0     --max_steps=750     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data_phase2     --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.712     --max_samples_termination=7500000     --eval_iter_start_samples=3000000 --eval_iter_samples=500000     --eval_batch_size=16 --eval_dir=/workspace/evaldata     --cache_eval_data     --output_dir=/results     --fp16 --fused_gelu_bias --dense_seq_output --fused_mha      --allreduce_post_accumulation --allreduce_post_accumulation_fp16     --gradient_accumulation_steps=1     --log_freq=1     --local_rank=5     --bert_config_path=/workspace/phase1/bert_config.j
 87: num_sockets = 2 num_nodes=2 cores_per_socket=24
  5: son --seed=18263'
307: num_sockets = 2 num_nodes=2 cores_per_socket=24
  5: ++ ./bind.sh --cpu=exclusive --ib=single --cluster=circe -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=5 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
  6: + eval '    ./bind.sh --cpu=exclusive --ib=single --cluster=circe --     python -u /workspace/bert/run_pretraining.py         --train_batch_size=18     --learning_rate=2.0e-3     --opt_lamb_beta_1=0.86     --opt_lamb_beta_2=0.975     --warmup_proportion=0.0     --max_steps=750     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data_phase2     --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.712     --max_samples_termination=7500000     --eval_iter_start_samples=3000000 --eval_iter_samples=500000     --eval_batch_size=16 --eval_dir=/workspace/evaldata     --cache_eval_data     --output_dir=/results     --fp16 --fused_gelu_bias --dense_seq_output --fused_mha      --allreduce_post_accumulation --allreduce_post_accumulation_fp16     --gradient_accumulation_steps=1     --log_freq=1     --local_rank=6     --bert_config_path=/workspace/phase1/bert_config.j
  6: son --seed=18263'
  6: ++ ./bind.sh --cpu=exclusive --ib=single --cluster=circe -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=6 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
 12: + eval '    ./bind.sh --cpu=exclusive --ib=single --cluster=circe --     python -u /workspace/bert/run_pretraining.py         --train_batch_size=18     --learning_rate=2.0e-3     --opt_lamb_beta_1=0.86     --opt_lamb_beta_2=0.975     --warmup_proportion=0.0     --max_steps=750     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data_phase2     --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.712     --max_samples_termination=7500000     --eval_iter_start_samples=3000000 --eval_iter_samples=500000     --eval_batch_size=16 --eval_dir=/workspace/evaldata     --cache_eval_data     --output_dir=/results     --fp16 --fused_gelu_bias --dense_seq_output --fused_mha      --allreduce_post_accumulation --allreduce_post_accumulation_fp16     --gradient_accumulation_steps=1     --log_freq=1     --local_rank=12     --bert_config_path=/workspace/phase1/bert_config.
 12: json --seed=18263'
 12: ++ ./bind.sh --cpu=exclusive --ib=single --cluster=circe -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=12 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
 11: + eval '    ./bind.sh --cpu=exclusive --ib=single --cluster=circe --     python -u /workspace/bert/run_pretraining.py         --train_batch_size=18     --learning_rate=2.0e-3     --opt_lamb_beta_1=0.86     --opt_lamb_beta_2=0.975     --warmup_proportion=0.0     --max_steps=750     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data_phase2     --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.712     --max_samples_termination=7500000     --eval_iter_start_samples=3000000 --eval_iter_samples=500000     --eval_batch_size=16 --eval_dir=/workspace/evaldata     --cache_eval_data     --output_dir=/results     --fp16 --fused_gelu_bias --dense_seq_output --fused_mha      --allreduce_post_accumulation --allreduce_post_accumulation_fp16     --gradient_accumulation_steps=1     --log_freq=1     --local_rank=11     --bert_config_path=/workspace/phase1/bert_config.
 11: json --seed=18263'
 11: ++ ./bind.sh --cpu=exclusive --ib=single --cluster=circe -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=11 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
  3: + eval '    ./bind.sh --cpu=exclusive --ib=single --cluster=circe --     python -u /workspace/bert/run_pretraining.py         --train_batch_size=18     --learning_rate=2.0e-3     --opt_lamb_beta_1=0.86     --opt_lamb_beta_2=0.975     --warmup_proportion=0.0     --max_steps=750     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data_phase2     --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.712     --max_samples_termination=7500000     --eval_iter_start_samples=3000000 --eval_iter_samples=500000     --eval_batch_size=16 --eval_dir=/workspace/evaldata     --cache_eval_data     --output_dir=/results     --fp16 --fused_gelu_bias --dense_seq_output --fused_mha      --allreduce_post_accumulation --allreduce_post_accumulation_fp16     --gradient_accumulation_steps=1     --log_freq=1     --local_rank=3     --bert_config_path=/workspace/phase1/bert_config.j
  3: son --seed=18263'
  3: ++ ./bind.sh --cpu=exclusive --ib=single --cluster=circe -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=3 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
 14: + eval '    ./bind.sh --cpu=exclusive --ib=single --cluster=circe --     python -u /workspace/bert/run_pretraining.py         --train_batch_size=18     --learning_rate=2.0e-3     --opt_lamb_beta_1=0.86     --opt_lamb_beta_2=0.975     --warmup_proportion=0.0     --max_steps=750     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data_phase2     --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.712     --max_samples_termination=7500000     --eval_iter_start_samples=3000000 --eval_iter_samples=500000     --eval_batch_size=16 --eval_dir=/workspace/evaldata     --cache_eval_data     --output_dir=/results     --fp16 --fused_gelu_bias --dense_seq_output --fused_mha      --allreduce_post_accumulation --allreduce_post_accumulation_fp16     --gradient_accumulation_steps=1     --log_freq=1     --local_rank=14     --bert_config_path=/workspace/phase1/bert_config.
 14: json --seed=18263'
 14: ++ ./bind.sh --cpu=exclusive --ib=single --cluster=circe -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=14 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
309: + eval '    ./bind.sh --cpu=exclusive --ib=single --cluster=circe --     python -u /workspace/bert/run_pretraining.py         --train_batch_size=18     --learning_rate=2.0e-3     --opt_lamb_beta_1=0.86     --opt_lamb_beta_2=0.975     --warmup_proportion=0.0     --max_steps=750     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data_phase2     --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.712     --max_samples_termination=7500000     --eval_iter_start_samples=3000000 --eval_iter_samples=500000     --eval_batch_size=16 --eval_dir=/workspace/evaldata     --cache_eval_data     --output_dir=/results     --fp16 --fused_gelu_bias --dense_seq_output --fused_mha      --allreduce_post_accumulation --allreduce_post_accumulation_fp16     --gradient_accumulation_steps=1     --log_freq=1     --local_rank=5     --bert_config_path=/workspace/phase1/bert_config.j
309: son --seed=18263'
309: ++ ./bind.sh --cpu=exclusive --ib=single --cluster=circe -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=5 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
449: + exec numactl --physcpubind=3-5,51-53 --membind=0 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=1 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
  0: + eval '    ./bind.sh --cpu=exclusive --ib=single --cluster=circe --     python -u /workspace/bert/run_pretraining.py         --train_batch_size=18     --learning_rate=2.0e-3     --opt_lamb_beta_1=0.86     --opt_lamb_beta_2=0.975     --warmup_proportion=0.0     --max_steps=750     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data_phase2     --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.712     --max_samples_termination=7500000     --eval_iter_start_samples=3000000 --eval_iter_samples=500000     --eval_batch_size=16 --eval_dir=/workspace/evaldata     --cache_eval_data     --output_dir=/results     --fp16 --fused_gelu_bias --dense_seq_output --fused_mha      --allreduce_post_accumulation --allreduce_post_accumulation_fp16     --gradient_accumulation_steps=1     --log_freq=1     --local_rank=0     --bert_config_path=/workspace/phase1/bert_config.j
308: + eval '    ./bind.sh --cpu=exclusive --ib=single --cluster=circe --     python -u /workspace/bert/run_pretraining.py         --train_batch_size=18     --learning_rate=2.0e-3     --opt_lamb_beta_1=0.86     --opt_lamb_beta_2=0.975     --warmup_proportion=0.0     --max_steps=750     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data_phase2     --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.712     --max_samples_termination=7500000     --eval_iter_start_samples=3000000 --eval_iter_samples=500000     --eval_batch_size=16 --eval_dir=/workspace/evaldata     --cache_eval_data     --output_dir=/results     --fp16 --fused_gelu_bias --dense_seq_output --fused_mha      --allreduce_post_accumulation --allreduce_post_accumulation_fp16     --gradient_accumulation_steps=1     --log_freq=1     --local_rank=4     --bert_config_path=/workspace/phase1/bert_config.j
  0: son --seed=18263'
308: son --seed=18263'
  0: ++ ./bind.sh --cpu=exclusive --ib=single --cluster=circe -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=0 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
310: + eval '    ./bind.sh --cpu=exclusive --ib=single --cluster=circe --     python -u /workspace/bert/run_pretraining.py         --train_batch_size=18     --learning_rate=2.0e-3     --opt_lamb_beta_1=0.86     --opt_lamb_beta_2=0.975     --warmup_proportion=0.0     --max_steps=750     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data_phase2     --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.712     --max_samples_termination=7500000     --eval_iter_start_samples=3000000 --eval_iter_samples=500000     --eval_batch_size=16 --eval_dir=/workspace/evaldata     --cache_eval_data     --output_dir=/results     --fp16 --fused_gelu_bias --dense_seq_output --fused_mha      --allreduce_post_accumulation --allreduce_post_accumulation_fp16     --gradient_accumulation_steps=1     --log_freq=1     --local_rank=6     --bert_config_path=/workspace/phase1/bert_config.j
310: son --seed=18263'
  2: + eval '    ./bind.sh --cpu=exclusive --ib=single --cluster=circe --     python -u /workspace/bert/run_pretraining.py         --train_batch_size=18     --learning_rate=2.0e-3     --opt_lamb_beta_1=0.86     --opt_lamb_beta_2=0.975     --warmup_proportion=0.0     --max_steps=750     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data_phase2     --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.712     --max_samples_termination=7500000     --eval_iter_start_samples=3000000 --eval_iter_samples=500000     --eval_batch_size=16 --eval_dir=/workspace/evaldata     --cache_eval_data     --output_dir=/results     --fp16 --fused_gelu_bias --dense_seq_output --fused_mha      --allreduce_post_accumulation --allreduce_post_accumulation_fp16     --gradient_accumulation_steps=1     --log_freq=1     --local_rank=2     --bert_config_path=/workspace/phase1/bert_config.j
308: ++ ./bind.sh --cpu=exclusive --ib=single --cluster=circe -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=4 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
  2: son --seed=18263'
310: ++ ./bind.sh --cpu=exclusive --ib=single --cluster=circe -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=6 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
  2: ++ ./bind.sh --cpu=exclusive --ib=single --cluster=circe -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=2 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
312: + eval '    ./bind.sh --cpu=exclusive --ib=single --cluster=circe --     python -u /workspace/bert/run_pretraining.py         --train_batch_size=18     --learning_rate=2.0e-3     --opt_lamb_beta_1=0.86     --opt_lamb_beta_2=0.975     --warmup_proportion=0.0     --max_steps=750     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data_phase2     --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.712     --max_samples_termination=7500000     --eval_iter_start_samples=3000000 --eval_iter_samples=500000     --eval_batch_size=16 --eval_dir=/workspace/evaldata     --cache_eval_data     --output_dir=/results     --fp16 --fused_gelu_bias --dense_seq_output --fused_mha      --allreduce_post_accumulation --allreduce_post_accumulation_fp16     --gradient_accumulation_steps=1     --log_freq=1     --local_rank=8     --bert_config_path=/workspace/phase1/bert_config.j
312: son --seed=18263'
312: ++ ./bind.sh --cpu=exclusive --ib=single --cluster=circe -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=8 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
 10: + eval '    ./bind.sh --cpu=exclusive --ib=single --cluster=circe --     python -u /workspace/bert/run_pretraining.py         --train_batch_size=18     --learning_rate=2.0e-3     --opt_lamb_beta_1=0.86     --opt_lamb_beta_2=0.975     --warmup_proportion=0.0     --max_steps=750     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data_phase2     --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.712     --max_samples_termination=7500000     --eval_iter_start_samples=3000000 --eval_iter_samples=500000     --eval_batch_size=16 --eval_dir=/workspace/evaldata     --cache_eval_data     --output_dir=/results     --fp16 --fused_gelu_bias --dense_seq_output --fused_mha      --allreduce_post_accumulation --allreduce_post_accumulation_fp16     --gradient_accumulation_steps=1     --log_freq=1     --local_rank=10     --bert_config_path=/workspace/phase1/bert_config.
 10: json --seed=18263'
 10: ++ ./bind.sh --cpu=exclusive --ib=single --cluster=circe -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=10 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
313: + eval '    ./bind.sh --cpu=exclusive --ib=single --cluster=circe --     python -u /workspace/bert/run_pretraining.py         --train_batch_size=18     --learning_rate=2.0e-3     --opt_lamb_beta_1=0.86     --opt_lamb_beta_2=0.975     --warmup_proportion=0.0     --max_steps=750     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data_phase2     --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.712     --max_samples_termination=7500000     --eval_iter_start_samples=3000000 --eval_iter_samples=500000     --eval_batch_size=16 --eval_dir=/workspace/evaldata     --cache_eval_data     --output_dir=/results     --fp16 --fused_gelu_bias --dense_seq_output --fused_mha      --allreduce_post_accumulation --allreduce_post_accumulation_fp16     --gradient_accumulation_steps=1     --log_freq=1     --local_rank=9     --bert_config_path=/workspace/phase1/bert_config.j
313: son --seed=18263'
307: + eval '    ./bind.sh --cpu=exclusive --ib=single --cluster=circe --     python -u /workspace/bert/run_pretraining.py         --train_batch_size=18     --learning_rate=2.0e-3     --opt_lamb_beta_1=0.86     --opt_lamb_beta_2=0.975     --warmup_proportion=0.0     --max_steps=750     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data_phase2     --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.712     --max_samples_termination=7500000     --eval_iter_start_samples=3000000 --eval_iter_samples=500000     --eval_batch_size=16 --eval_dir=/workspace/evaldata     --cache_eval_data     --output_dir=/results     --fp16 --fused_gelu_bias --dense_seq_output --fused_mha      --allreduce_post_accumulation --allreduce_post_accumulation_fp16     --gradient_accumulation_steps=1     --log_freq=1     --local_rank=3     --bert_config_path=/workspace/phase1/bert_config.j
 13: + eval '    ./bind.sh --cpu=exclusive --ib=single --cluster=circe --     python -u /workspace/bert/run_pretraining.py         --train_batch_size=18     --learning_rate=2.0e-3     --opt_lamb_beta_1=0.86     --opt_lamb_beta_2=0.975     --warmup_proportion=0.0     --max_steps=750     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data_phase2     --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.712     --max_samples_termination=7500000     --eval_iter_start_samples=3000000 --eval_iter_samples=500000     --eval_batch_size=16 --eval_dir=/workspace/evaldata     --cache_eval_data     --output_dir=/results     --fp16 --fused_gelu_bias --dense_seq_output --fused_mha      --allreduce_post_accumulation --allreduce_post_accumulation_fp16     --gradient_accumulation_steps=1     --log_freq=1     --local_rank=13     --bert_config_path=/workspace/phase1/bert_config.
307: son --seed=18263'
 13: json --seed=18263'
313: ++ ./bind.sh --cpu=exclusive --ib=single --cluster=circe -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=9 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
307: ++ ./bind.sh --cpu=exclusive --ib=single --cluster=circe -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=3 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
 13: ++ ./bind.sh --cpu=exclusive --ib=single --cluster=circe -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=13 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
  7: + eval '    ./bind.sh --cpu=exclusive --ib=single --cluster=circe --     python -u /workspace/bert/run_pretraining.py         --train_batch_size=18     --learning_rate=2.0e-3     --opt_lamb_beta_1=0.86     --opt_lamb_beta_2=0.975     --warmup_proportion=0.0     --max_steps=750     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data_phase2     --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.712     --max_samples_termination=7500000     --eval_iter_start_samples=3000000 --eval_iter_samples=500000     --eval_batch_size=16 --eval_dir=/workspace/evaldata     --cache_eval_data     --output_dir=/results     --fp16 --fused_gelu_bias --dense_seq_output --fused_mha      --allreduce_post_accumulation --allreduce_post_accumulation_fp16     --gradient_accumulation_steps=1     --log_freq=1     --local_rank=7     --bert_config_path=/workspace/phase1/bert_config.j
  7: son --seed=18263'
315: + eval '    ./bind.sh --cpu=exclusive --ib=single --cluster=circe --     python -u /workspace/bert/run_pretraining.py         --train_batch_size=18     --learning_rate=2.0e-3     --opt_lamb_beta_1=0.86     --opt_lamb_beta_2=0.975     --warmup_proportion=0.0     --max_steps=750     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data_phase2     --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.712     --max_samples_termination=7500000     --eval_iter_start_samples=3000000 --eval_iter_samples=500000     --eval_batch_size=16 --eval_dir=/workspace/evaldata     --cache_eval_data     --output_dir=/results     --fp16 --fused_gelu_bias --dense_seq_output --fused_mha      --allreduce_post_accumulation --allreduce_post_accumulation_fp16     --gradient_accumulation_steps=1     --log_freq=1     --local_rank=11     --bert_config_path=/workspace/phase1/bert_config.
315: json --seed=18263'
304: + eval '    ./bind.sh --cpu=exclusive --ib=single --cluster=circe --     python -u /workspace/bert/run_pretraining.py         --train_batch_size=18     --learning_rate=2.0e-3     --opt_lamb_beta_1=0.86     --opt_lamb_beta_2=0.975     --warmup_proportion=0.0     --max_steps=750     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data_phase2     --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.712     --max_samples_termination=7500000     --eval_iter_start_samples=3000000 --eval_iter_samples=500000     --eval_batch_size=16 --eval_dir=/workspace/evaldata     --cache_eval_data     --output_dir=/results     --fp16 --fused_gelu_bias --dense_seq_output --fused_mha      --allreduce_post_accumulation --allreduce_post_accumulation_fp16     --gradient_accumulation_steps=1     --log_freq=1     --local_rank=0     --bert_config_path=/workspace/phase1/bert_config.j
304: son --seed=18263'
306: + eval '    ./bind.sh --cpu=exclusive --ib=single --cluster=circe --     python -u /workspace/bert/run_pretraining.py         --train_batch_size=18     --learning_rate=2.0e-3     --opt_lamb_beta_1=0.86     --opt_lamb_beta_2=0.975     --warmup_proportion=0.0     --max_steps=750     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data_phase2     --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.712     --max_samples_termination=7500000     --eval_iter_start_samples=3000000 --eval_iter_samples=500000     --eval_batch_size=16 --eval_dir=/workspace/evaldata     --cache_eval_data     --output_dir=/results     --fp16 --fused_gelu_bias --dense_seq_output --fused_mha      --allreduce_post_accumulation --allreduce_post_accumulation_fp16     --gradient_accumulation_steps=1     --log_freq=1     --local_rank=2     --bert_config_path=/workspace/phase1/bert_config.j
  7: ++ ./bind.sh --cpu=exclusive --ib=single --cluster=circe -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=7 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
306: son --seed=18263'
315: ++ ./bind.sh --cpu=exclusive --ib=single --cluster=circe -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=11 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
316: + eval '    ./bind.sh --cpu=exclusive --ib=single --cluster=circe --     python -u /workspace/bert/run_pretraining.py         --train_batch_size=18     --learning_rate=2.0e-3     --opt_lamb_beta_1=0.86     --opt_lamb_beta_2=0.975     --warmup_proportion=0.0     --max_steps=750     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data_phase2     --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.712     --max_samples_termination=7500000     --eval_iter_start_samples=3000000 --eval_iter_samples=500000     --eval_batch_size=16 --eval_dir=/workspace/evaldata     --cache_eval_data     --output_dir=/results     --fp16 --fused_gelu_bias --dense_seq_output --fused_mha      --allreduce_post_accumulation --allreduce_post_accumulation_fp16     --gradient_accumulation_steps=1     --log_freq=1     --local_rank=12     --bert_config_path=/workspace/phase1/bert_config.
316: json --seed=18263'
304: ++ ./bind.sh --cpu=exclusive --ib=single --cluster=circe -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=0 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
306: ++ ./bind.sh --cpu=exclusive --ib=single --cluster=circe -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=2 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
316: ++ ./bind.sh --cpu=exclusive --ib=single --cluster=circe -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=12 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
 15: + eval '    ./bind.sh --cpu=exclusive --ib=single --cluster=circe --     python -u /workspace/bert/run_pretraining.py         --train_batch_size=18     --learning_rate=2.0e-3     --opt_lamb_beta_1=0.86     --opt_lamb_beta_2=0.975     --warmup_proportion=0.0     --max_steps=750     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data_phase2     --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.712     --max_samples_termination=7500000     --eval_iter_start_samples=3000000 --eval_iter_samples=500000     --eval_batch_size=16 --eval_dir=/workspace/evaldata     --cache_eval_data     --output_dir=/results     --fp16 --fused_gelu_bias --dense_seq_output --fused_mha      --allreduce_post_accumulation --allreduce_post_accumulation_fp16     --gradient_accumulation_steps=1     --log_freq=1     --local_rank=15     --bert_config_path=/workspace/phase1/bert_config.
 15: json --seed=18263'
 15: ++ ./bind.sh --cpu=exclusive --ib=single --cluster=circe -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=15 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
  4: + eval '    ./bind.sh --cpu=exclusive --ib=single --cluster=circe --     python -u /workspace/bert/run_pretraining.py         --train_batch_size=18     --learning_rate=2.0e-3     --opt_lamb_beta_1=0.86     --opt_lamb_beta_2=0.975     --warmup_proportion=0.0     --max_steps=750     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data_phase2     --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.712     --max_samples_termination=7500000     --eval_iter_start_samples=3000000 --eval_iter_samples=500000     --eval_batch_size=16 --eval_dir=/workspace/evaldata     --cache_eval_data     --output_dir=/results     --fp16 --fused_gelu_bias --dense_seq_output --fused_mha      --allreduce_post_accumulation --allreduce_post_accumulation_fp16     --gradient_accumulation_steps=1     --log_freq=1     --local_rank=4     --bert_config_path=/workspace/phase1/bert_config.j
  4: son --seed=18263'
  4: ++ ./bind.sh --cpu=exclusive --ib=single --cluster=circe -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=4 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
317: + eval '    ./bind.sh --cpu=exclusive --ib=single --cluster=circe --     python -u /workspace/bert/run_pretraining.py         --train_batch_size=18     --learning_rate=2.0e-3     --opt_lamb_beta_1=0.86     --opt_lamb_beta_2=0.975     --warmup_proportion=0.0     --max_steps=750     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data_phase2     --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.712     --max_samples_termination=7500000     --eval_iter_start_samples=3000000 --eval_iter_samples=500000     --eval_batch_size=16 --eval_dir=/workspace/evaldata     --cache_eval_data     --output_dir=/results     --fp16 --fused_gelu_bias --dense_seq_output --fused_mha      --allreduce_post_accumulation --allreduce_post_accumulation_fp16     --gradient_accumulation_steps=1     --log_freq=1     --local_rank=13     --bert_config_path=/workspace/phase1/bert_config.
317: json --seed=18263'
317: ++ ./bind.sh --cpu=exclusive --ib=single --cluster=circe -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=13 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
314: + eval '    ./bind.sh --cpu=exclusive --ib=single --cluster=circe --     python -u /workspace/bert/run_pretraining.py         --train_batch_size=18     --learning_rate=2.0e-3     --opt_lamb_beta_1=0.86     --opt_lamb_beta_2=0.975     --warmup_proportion=0.0     --max_steps=750     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data_phase2     --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.712     --max_samples_termination=7500000     --eval_iter_start_samples=3000000 --eval_iter_samples=500000     --eval_batch_size=16 --eval_dir=/workspace/evaldata     --cache_eval_data     --output_dir=/results     --fp16 --fused_gelu_bias --dense_seq_output --fused_mha      --allreduce_post_accumulation --allreduce_post_accumulation_fp16     --gradient_accumulation_steps=1     --log_freq=1     --local_rank=10     --bert_config_path=/workspace/phase1/bert_config.
314: json --seed=18263'
314: ++ ./bind.sh --cpu=exclusive --ib=single --cluster=circe -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=10 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
449: :::MLLOG {"namespace": "", "time_ms": 1593107941993, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 657}}
  8: + eval '    ./bind.sh --cpu=exclusive --ib=single --cluster=circe --     python -u /workspace/bert/run_pretraining.py         --train_batch_size=18     --learning_rate=2.0e-3     --opt_lamb_beta_1=0.86     --opt_lamb_beta_2=0.975     --warmup_proportion=0.0     --max_steps=750     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data_phase2     --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.712     --max_samples_termination=7500000     --eval_iter_start_samples=3000000 --eval_iter_samples=500000     --eval_batch_size=16 --eval_dir=/workspace/evaldata     --cache_eval_data     --output_dir=/results     --fp16 --fused_gelu_bias --dense_seq_output --fused_mha      --allreduce_post_accumulation --allreduce_post_accumulation_fp16     --gradient_accumulation_steps=1     --log_freq=1     --local_rank=8     --bert_config_path=/workspace/phase1/bert_config.j
450: :::MLLOG {"namespace": "", "time_ms": 1593107941993, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 657}}
  8: son --seed=18263'
105: num_sockets = 2 num_nodes=2 cores_per_socket=24
  8: ++ ./bind.sh --cpu=exclusive --ib=single --cluster=circe -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=8 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
371: :::MLLOG {"namespace": "", "time_ms": 1593107941994, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 657}}
311: + eval '    ./bind.sh --cpu=exclusive --ib=single --cluster=circe --     python -u /workspace/bert/run_pretraining.py         --train_batch_size=18     --learning_rate=2.0e-3     --opt_lamb_beta_1=0.86     --opt_lamb_beta_2=0.975     --warmup_proportion=0.0     --max_steps=750     --phase2     --max_seq_length=512     --max_predictions_per_seq=76     --input_dir=/workspace/data_phase2     --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt          --do_train     --skip_checkpoint     --train_mlm_accuracy_window_size=0     --target_mlm_accuracy=0.712     --max_samples_termination=7500000     --eval_iter_start_samples=3000000 --eval_iter_samples=500000     --eval_batch_size=16 --eval_dir=/workspace/evaldata     --cache_eval_data     --output_dir=/results     --fp16 --fused_gelu_bias --dense_seq_output --fused_mha      --allreduce_post_accumulation --allreduce_post_accumulation_fp16     --gradient_accumulation_steps=1     --log_freq=1     --local_rank=7     --bert_config_path=/workspace/phase1/bert_config.j
324: :::MLLOG {"namespace": "", "time_ms": 1593107941999, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 657}}
311: son --seed=18263'
311: ++ ./bind.sh --cpu=exclusive --ib=single --cluster=circe -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=7 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
450: + exec numactl --physcpubind=6-8,54-56 --membind=0 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=2 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
283: json --seed=18263'
283: ++ ./bind.sh --cpu=exclusive --ib=single --cluster=circe -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=11 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
324: + exec numactl --physcpubind=12-14,60-62 --membind=0 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=4 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
 47: :::MLLOG {"namespace": "", "time_ms": 1593107942006, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 657}}
113: + exec numactl --physcpubind=3-5,51-53 --membind=0 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=1 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
150: :::MLLOG {"namespace": "", "time_ms": 1593107942005, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 657}}
121: + exec numactl --physcpubind=27-29,75-77 --membind=1 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=9 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
456: :::MLLOG {"namespace": "", "time_ms": 1593107942003, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 657}}
456: + exec numactl --physcpubind=24-26,72-74 --membind=1 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=8 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
137: :::MLLOG {"namespace": "", "time_ms": 1593107942009, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 657}}
371: + exec numactl --physcpubind=9-11,57-59 --membind=0 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=3 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
400: num_sockets = 2 num_nodes=2 cores_per_socket=24
136: + exec numactl --physcpubind=24-26,72-74 --membind=1 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=8 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
257: :::MLLOG {"namespace": "", "time_ms": 1593107942011, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 657}}
257: + exec numactl --physcpubind=3-5,51-53 --membind=0 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=1 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
263: :::MLLOG {"namespace": "", "time_ms": 1593107942011, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 657}}
263: + exec numactl --physcpubind=21-23,69-71 --membind=0 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=7 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
317: num_sockets = 2 num_nodes=2 cores_per_socket=24
150: + exec numactl --physcpubind=18-20,66-68 --membind=0 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=6 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
136: :::MLLOG {"namespace": "", "time_ms": 1593107942017, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 657}}
137: + exec numactl --physcpubind=27-29,75-77 --membind=1 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=9 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
411: num_sockets = 2 num_nodes=2 cores_per_socket=24
 47: + exec numactl --physcpubind=45-47,93-95 --membind=1 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=15 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
409: num_sockets = 2 num_nodes=2 cores_per_socket=24
239: + exec numactl --physcpubind=45-47,93-95 --membind=1 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=15 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
106: num_sockets = 2 num_nodes=2 cores_per_socket=24
232: + exec numactl --physcpubind=24-26,72-74 --membind=1 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=8 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
239: :::MLLOG {"namespace": "", "time_ms": 1593107942031, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 657}}
338: + exec numactl --physcpubind=6-8,54-56 --membind=0 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=2 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
 88: num_sockets = 2 num_nodes=2 cores_per_socket=24
337: + exec numactl --physcpubind=3-5,51-53 --membind=0 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=1 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
141: :::MLLOG {"namespace": "", "time_ms": 1593107942033, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 657}}
229: + exec numactl --physcpubind=15-17,63-65 --membind=0 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=5 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
408: num_sockets = 2 num_nodes=2 cores_per_socket=24
496: + exec numactl --physcpubind=0-2,48-50 --membind=0 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=0 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
354: num_sockets = 2 num_nodes=2 cores_per_socket=24
385: + exec numactl --physcpubind=3-5,51-53 --membind=0 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=1 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
229: :::MLLOG {"namespace": "", "time_ms": 1593107942036, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 657}}
479: + exec numactl --physcpubind=45-47,93-95 --membind=1 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=15 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
415: num_sockets = 2 num_nodes=2 cores_per_socket=24
 24: + exec numactl --physcpubind=24-26,72-74 --membind=1 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=8 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
 81: num_sockets = 2 num_nodes=2 cores_per_socket=24
294: + exec numactl --physcpubind=18-20,66-68 --membind=0 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=6 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
495: num_sockets = 2 num_nodes=2 cores_per_socket=24
178: + exec numactl --physcpubind=6-8,54-56 --membind=0 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=2 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
490: num_sockets = 2 num_nodes=2 cores_per_socket=24
185: + exec numactl --physcpubind=27-29,75-77 --membind=1 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=9 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
306: num_sockets = 2 num_nodes=2 cores_per_socket=24
241: + exec numactl --physcpubind=3-5,51-53 --membind=0 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=1 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
311: num_sockets = 2 num_nodes=2 cores_per_socket=24
302: + exec numactl --physcpubind=42-44,90-92 --membind=1 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=14 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
121: :::MLLOG {"namespace": "", "time_ms": 1593107942042, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 657}}
182: + exec numactl --physcpubind=18-20,66-68 --membind=0 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=6 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
421: num_sockets = 2 num_nodes=2 cores_per_socket=24
224: + exec numactl --physcpubind=0-2,48-50 --membind=0 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=0 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
 83: num_sockets = 2 num_nodes=2 cores_per_socket=24
141: + exec numactl --physcpubind=39-41,87-89 --membind=1 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=13 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
 84: num_sockets = 2 num_nodes=2 cores_per_socket=24
202: + exec numactl --physcpubind=30-32,78-80 --membind=1 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=10 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
485: num_sockets = 2 num_nodes=2 cores_per_socket=24
242: + exec numactl --physcpubind=6-8,54-56 --membind=0 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=2 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
111: num_sockets = 2 num_nodes=2 cores_per_socket=24
158: + exec numactl --physcpubind=42-44,90-92 --membind=1 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=14 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
 97: num_sockets = 2 num_nodes=2 cores_per_socket=24
477: + exec numactl --physcpubind=39-41,87-89 --membind=1 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=13 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
104: num_sockets = 2 num_nodes=2 cores_per_socket=24
434: + exec numactl --physcpubind=6-8,54-56 --membind=0 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=2 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
100: num_sockets = 2 num_nodes=2 cores_per_socket=24
 41: + exec numactl --physcpubind=27-29,75-77 --membind=1 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=9 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
401: num_sockets = 2 num_nodes=2 cores_per_socket=24
194: + exec numactl --physcpubind=6-8,54-56 --membind=0 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=2 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
 89: num_sockets = 2 num_nodes=2 cores_per_socket=24
196: + exec numactl --physcpubind=12-14,60-62 --membind=0 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=4 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
310: num_sockets = 2 num_nodes=2 cores_per_socket=24
432: + exec numactl --physcpubind=0-2,48-50 --membind=0 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=0 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
414: num_sockets = 2 num_nodes=2 cores_per_socket=24
387: + exec numactl --physcpubind=9-11,57-59 --membind=0 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=3 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
 90: num_sockets = 2 num_nodes=2 cores_per_socket=24
340: + exec numactl --physcpubind=12-14,60-62 --membind=0 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=4 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
404: num_sockets = 2 num_nodes=2 cores_per_socket=24
336: + exec numactl --physcpubind=0-2,48-50 --membind=0 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=0 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
313: num_sockets = 2 num_nodes=2 cores_per_socket=24
122: + exec numactl --physcpubind=30-32,78-80 --membind=1 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=10 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
410: num_sockets = 2 num_nodes=2 cores_per_socket=24
128: + exec numactl --physcpubind=0-2,48-50 --membind=0 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=0 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
405: num_sockets = 2 num_nodes=2 cores_per_socket=24
 48: + exec numactl --physcpubind=0-2,48-50 --membind=0 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=0 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
314: num_sockets = 2 num_nodes=2 cores_per_socket=24
403: + exec numactl --physcpubind=9-11,57-59 --membind=0 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=3 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
 85: num_sockets = 2 num_nodes=2 cores_per_socket=24
448: + exec numactl --physcpubind=0-2,48-50 --membind=0 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=0 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
412: num_sockets = 2 num_nodes=2 cores_per_socket=24
 50: + exec numactl --physcpubind=6-8,54-56 --membind=0 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=2 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
407: num_sockets = 2 num_nodes=2 cores_per_socket=24
 49: + exec numactl --physcpubind=3-5,51-53 --membind=0 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=1 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
316: num_sockets = 2 num_nodes=2 cores_per_socket=24
388: + exec numactl --physcpubind=12-14,60-62 --membind=0 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=4 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
315: num_sockets = 2 num_nodes=2 cores_per_socket=24
322: + exec numactl --physcpubind=6-8,54-56 --membind=0 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=2 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
308: num_sockets = 2 num_nodes=2 cores_per_socket=24
482: + exec numactl --physcpubind=6-8,54-56 --membind=0 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=2 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
494: num_sockets = 2 num_nodes=2 cores_per_socket=24
486: + exec numactl --physcpubind=18-20,66-68 --membind=0 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=6 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
304: num_sockets = 2 num_nodes=2 cores_per_socket=24
305: + exec numactl --physcpubind=3-5,51-53 --membind=0 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=1 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
108: num_sockets = 2 num_nodes=2 cores_per_socket=24
 77: + exec numactl --physcpubind=39-41,87-89 --membind=1 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=13 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
232: :::MLLOG {"namespace": "", "time_ms": 1593107942046, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 657}}
319: + exec numactl --physcpubind=45-47,93-95 --membind=1 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=15 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
487: num_sockets = 2 num_nodes=2 cores_per_socket=24
475: + exec numactl --physcpubind=33-35,81-83 --membind=1 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=11 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
 82: num_sockets = 2 num_nodes=2 cores_per_socket=24
499: + exec numactl --physcpubind=9-11,57-59 --membind=0 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=3 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
491: num_sockets = 2 num_nodes=2 cores_per_socket=24
330: + exec numactl --physcpubind=30-32,78-80 --membind=1 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=10 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
358: num_sockets = 2 num_nodes=2 cores_per_socket=24
 68: + exec numactl --physcpubind=12-14,60-62 --membind=0 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=4 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
483: num_sockets = 2 num_nodes=2 cores_per_socket=24
 17: + exec numactl --physcpubind=3-5,51-53 --membind=0 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=1 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
110: num_sockets = 2 num_nodes=2 cores_per_socket=24
157: + exec numactl --physcpubind=39-41,87-89 --membind=1 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=13 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
488: num_sockets = 2 num_nodes=2 cores_per_socket=24
191: + exec numactl --physcpubind=45-47,93-95 --membind=1 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=15 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
101: num_sockets = 2 num_nodes=2 cores_per_socket=24
248: + exec numactl --physcpubind=24-26,72-74 --membind=1 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=8 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
 98: num_sockets = 2 num_nodes=2 cores_per_socket=24
181: + exec numactl --physcpubind=15-17,63-65 --membind=0 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=5 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
493: num_sockets = 2 num_nodes=2 cores_per_socket=24
 26: + exec numactl --physcpubind=30-32,78-80 --membind=1 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=10 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
 96: num_sockets = 2 num_nodes=2 cores_per_socket=24
331: + exec numactl --physcpubind=33-35,81-83 --membind=1 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=11 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
 80: num_sockets = 2 num_nodes=2 cores_per_socket=24
290: + exec numactl --physcpubind=6-8,54-56 --membind=0 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=2 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
 94: num_sockets = 2 num_nodes=2 cores_per_socket=24
 66: + exec numactl --physcpubind=6-8,54-56 --membind=0 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=2 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
 95: num_sockets = 2 num_nodes=2 cores_per_socket=24
476: + exec numactl --physcpubind=36-38,84-86 --membind=1 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=12 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
109: num_sockets = 2 num_nodes=2 cores_per_socket=24
288: + exec numactl --physcpubind=0-2,48-50 --membind=0 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=0 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
489: num_sockets = 2 num_nodes=2 cores_per_socket=24
 18: + exec numactl --physcpubind=6-8,54-56 --membind=0 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=2 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
107: num_sockets = 2 num_nodes=2 cores_per_socket=24
399: + exec numactl --physcpubind=45-47,93-95 --membind=1 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=15 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
 86: num_sockets = 2 num_nodes=2 cores_per_socket=24
501: + exec numactl --physcpubind=15-17,63-65 --membind=0 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=5 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
 92: num_sockets = 2 num_nodes=2 cores_per_socket=24
217: + exec numactl --physcpubind=27-29,75-77 --membind=1 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=9 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
 93: num_sockets = 2 num_nodes=2 cores_per_socket=24
219: + exec numactl --physcpubind=33-35,81-83 --membind=1 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=11 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
481: num_sockets = 2 num_nodes=2 cores_per_socket=24
214: + exec numactl --physcpubind=18-20,66-68 --membind=0 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=6 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
492: num_sockets = 2 num_nodes=2 cores_per_socket=24
 99: + exec numactl --physcpubind=9-11,57-59 --membind=0 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=3 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
484: num_sockets = 2 num_nodes=2 cores_per_socket=24
348: + exec numactl --physcpubind=36-38,84-86 --membind=1 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=12 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
480: num_sockets = 2 num_nodes=2 cores_per_socket=24
376: + exec numactl --physcpubind=24-26,72-74 --membind=1 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=8 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
356: num_sockets = 2 num_nodes=2 cores_per_socket=24
373: + exec numactl --physcpubind=15-17,63-65 --membind=0 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=5 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
357: num_sockets = 2 num_nodes=2 cores_per_socket=24
254: + exec numactl --physcpubind=42-44,90-92 --membind=1 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=14 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
419: num_sockets = 2 num_nodes=2 cores_per_socket=24
209: + exec numactl --physcpubind=3-5,51-53 --membind=0 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=1 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
224: :::MLLOG {"namespace": "", "time_ms": 1593107942056, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 657}}
103: + exec numactl --physcpubind=21-23,69-71 --membind=0 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=7 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
294: :::MLLOG {"namespace": "", "time_ms": 1593107942058, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 657}}
355: + exec numactl --physcpubind=9-11,57-59 --membind=0 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=3 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
422: num_sockets = 2 num_nodes=2 cores_per_socket=24
359: + exec numactl --physcpubind=21-23,69-71 --membind=0 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=7 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
302: :::MLLOG {"namespace": "", "time_ms": 1593107942059, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 657}}
363: + exec numactl --physcpubind=33-35,81-83 --membind=1 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=11 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
427: num_sockets = 2 num_nodes=2 cores_per_socket=24
413: + exec numactl --physcpubind=39-41,87-89 --membind=1 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=13 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
361: num_sockets = 2 num_nodes=2 cores_per_socket=24
309: + exec numactl --physcpubind=15-17,63-65 --membind=0 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=5 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
417: num_sockets = 2 num_nodes=2 cores_per_socket=24
377: + exec numactl --physcpubind=27-29,75-77 --membind=1 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=9 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
353: num_sockets = 2 num_nodes=2 cores_per_socket=24
402: + exec numactl --physcpubind=6-8,54-56 --membind=0 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=2 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
426: num_sockets = 2 num_nodes=2 cores_per_socket=24
218: + exec numactl --physcpubind=30-32,78-80 --membind=1 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=10 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
428: num_sockets = 2 num_nodes=2 cores_per_socket=24
406: + exec numactl --physcpubind=18-20,66-68 --membind=0 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=6 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
425: num_sockets = 2 num_nodes=2 cores_per_socket=24
382: + exec numactl --physcpubind=42-44,90-92 --membind=1 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=14 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
 41: :::MLLOG {"namespace": "", "time_ms": 1593107942067, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 657}}
351: + exec numactl --physcpubind=45-47,93-95 --membind=1 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=15 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
365: num_sockets = 2 num_nodes=2 cores_per_socket=24
 46: + exec numactl --physcpubind=42-44,90-92 --membind=1 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=14 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
420: num_sockets = 2 num_nodes=2 cores_per_socket=24
318: + exec numactl --physcpubind=42-44,90-92 --membind=1 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=14 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
431: num_sockets = 2 num_nodes=2 cores_per_socket=24
165: + exec numactl --physcpubind=15-17,63-65 --membind=0 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=5 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
352: num_sockets = 2 num_nodes=2 cores_per_socket=24
147: + exec numactl --physcpubind=9-11,57-59 --membind=0 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=3 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
423: num_sockets = 2 num_nodes=2 cores_per_socket=24
  1: + exec numactl --physcpubind=3-5,51-53 --membind=0 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=1 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
429: num_sockets = 2 num_nodes=2 cores_per_socket=24
397: + exec numactl --physcpubind=39-41,87-89 --membind=1 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=13 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
416: num_sockets = 2 num_nodes=2 cores_per_socket=24
 12: + exec numactl --physcpubind=36-38,84-86 --membind=1 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=12 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
366: num_sockets = 2 num_nodes=2 cores_per_socket=24
328: + exec numactl --physcpubind=24-26,72-74 --membind=1 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=8 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
364: num_sockets = 2 num_nodes=2 cores_per_socket=24
508: + exec numactl --physcpubind=36-38,84-86 --membind=1 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=12 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
367: num_sockets = 2 num_nodes=2 cores_per_socket=24
252: + exec numactl --physcpubind=36-38,84-86 --membind=1 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=12 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
362: num_sockets = 2 num_nodes=2 cores_per_socket=24
 63: + exec numactl --physcpubind=45-47,93-95 --membind=1 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=15 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
 24: :::MLLOG {"namespace": "", "time_ms": 1593107942065, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 657}}
256: + exec numactl --physcpubind=0-2,48-50 --membind=0 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=0 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
385: :::MLLOG {"namespace": "", "time_ms": 1593107942070, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 657}}
197: + exec numactl --physcpubind=15-17,63-65 --membind=0 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=5 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
337: :::MLLOG {"namespace": "", "time_ms": 1593107942070, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 657}}
 19: + exec numactl --physcpubind=9-11,57-59 --membind=0 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=3 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
338: :::MLLOG {"namespace": "", "time_ms": 1593107942070, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 657}}
312: + exec numactl --physcpubind=24-26,72-74 --membind=1 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=8 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
202: :::MLLOG {"namespace": "", "time_ms": 1593107942070, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 657}}
 61: + exec numactl --physcpubind=39-41,87-89 --membind=1 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=13 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
275: num_sockets = 2 num_nodes=2 cores_per_socket=24
102: + exec numactl --physcpubind=18-20,66-68 --membind=0 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=6 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
477: :::MLLOG {"namespace": "", "time_ms": 1593107942073, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 657}}
346: + exec numactl --physcpubind=30-32,78-80 --membind=1 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=10 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
479: :::MLLOG {"namespace": "", "time_ms": 1593107942073, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 657}}
125: + exec numactl --physcpubind=39-41,87-89 --membind=1 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=13 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
241: :::MLLOG {"namespace": "", "time_ms": 1593107942076, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 657}}
270: + exec numactl --physcpubind=42-44,90-92 --membind=1 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=14 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
242: :::MLLOG {"namespace": "", "time_ms": 1593107942076, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 657}}
374: + exec numactl --physcpubind=18-20,66-68 --membind=0 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=6 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
196: :::MLLOG {"namespace": "", "time_ms": 1593107942081, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 657}}
 39: + exec numactl --physcpubind=21-23,69-71 --membind=0 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=7 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
178: :::MLLOG {"namespace": "", "time_ms": 1593107942083, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 657}}
430: + exec numactl --physcpubind=42-44,90-92 --membind=1 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=14 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
496: :::MLLOG {"namespace": "", "time_ms": 1593107942082, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 657}}
120: + exec numactl --physcpubind=24-26,72-74 --membind=1 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=8 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
182: :::MLLOG {"namespace": "", "time_ms": 1593107942083, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 657}}
478: + exec numactl --physcpubind=42-44,90-92 --membind=1 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=14 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
185: :::MLLOG {"namespace": "", "time_ms": 1593107942083, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 657}}
233: + exec numactl --physcpubind=27-29,75-77 --membind=1 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=9 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
158: :::MLLOG {"namespace": "", "time_ms": 1593107942084, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 657}}
236: + exec numactl --physcpubind=36-38,84-86 --membind=1 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=12 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
387: :::MLLOG {"namespace": "", "time_ms": 1593107942091, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 657}}
231: + exec numactl --physcpubind=21-23,69-71 --membind=0 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=7 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
194: :::MLLOG {"namespace": "", "time_ms": 1593107942092, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 657}}
235: + exec numactl --physcpubind=33-35,81-83 --membind=1 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=11 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
 10: num_sockets = 2 num_nodes=2 cores_per_socket=24
177: + exec numactl --physcpubind=3-5,51-53 --membind=0 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=1 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
 15: num_sockets = 2 num_nodes=2 cores_per_socket=24
 79: + exec numactl --physcpubind=45-47,93-95 --membind=1 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=15 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
  8: num_sockets = 2 num_nodes=2 cores_per_socket=24
226: + exec numactl --physcpubind=6-8,54-56 --membind=0 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=2 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
434: :::MLLOG {"namespace": "", "time_ms": 1593107942102, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 657}}
225: + exec numactl --physcpubind=3-5,51-53 --membind=0 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=1 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
  4: num_sockets = 2 num_nodes=2 cores_per_socket=24
228: + exec numactl --physcpubind=12-14,60-62 --membind=0 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=4 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
432: :::MLLOG {"namespace": "", "time_ms": 1593107942107, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 657}}
227: + exec numactl --physcpubind=9-11,57-59 --membind=0 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=3 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
388: :::MLLOG {"namespace": "", "time_ms": 1593107942110, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 657}}
230: + exec numactl --physcpubind=18-20,66-68 --membind=0 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=6 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
122: :::MLLOG {"namespace": "", "time_ms": 1593107942113, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 657}}
234: + exec numactl --physcpubind=30-32,78-80 --membind=1 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=10 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
340: :::MLLOG {"namespace": "", "time_ms": 1593107942127, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 657}}
237: + exec numactl --physcpubind=39-41,87-89 --membind=1 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=13 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
403: :::MLLOG {"namespace": "", "time_ms": 1593107942133, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 657}}
327: + exec numactl --physcpubind=21-23,69-71 --membind=0 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=7 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
448: :::MLLOG {"namespace": "", "time_ms": 1593107942138, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 657}}
159: + exec numactl --physcpubind=45-47,93-95 --membind=1 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=15 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
157: :::MLLOG {"namespace": "", "time_ms": 1593107942140, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 657}}
 38: + exec numactl --physcpubind=18-20,66-68 --membind=0 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=6 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
 48: :::MLLOG {"namespace": "", "time_ms": 1593107942142, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 657}}
435: + exec numactl --physcpubind=9-11,57-59 --membind=0 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=3 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
 50: :::MLLOG {"namespace": "", "time_ms": 1593107942142, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 657}}
  5: + exec numactl --physcpubind=15-17,63-65 --membind=0 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=5 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
 49: :::MLLOG {"namespace": "", "time_ms": 1593107942142, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 657}}
112: + exec numactl --physcpubind=0-2,48-50 --membind=0 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=0 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
336: :::MLLOG {"namespace": "", "time_ms": 1593107942143, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 657}}
433: + exec numactl --physcpubind=3-5,51-53 --membind=0 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=1 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
 13: num_sockets = 2 num_nodes=2 cores_per_socket=24
190: + exec numactl --physcpubind=42-44,90-92 --membind=1 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=14 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
  6: num_sockets = 2 num_nodes=2 cores_per_socket=24
119: + exec numactl --physcpubind=21-23,69-71 --membind=0 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=7 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
  9: num_sockets = 2 num_nodes=2 cores_per_socket=24
118: + exec numactl --physcpubind=18-20,66-68 --membind=0 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=6 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
  3: num_sockets = 2 num_nodes=2 cores_per_socket=24
127: + exec numactl --physcpubind=45-47,93-95 --membind=1 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=15 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
  2: num_sockets = 2 num_nodes=2 cores_per_socket=24
 71: + exec numactl --physcpubind=21-23,69-71 --membind=0 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=7 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
  0: num_sockets = 2 num_nodes=2 cores_per_socket=24
186: + exec numactl --physcpubind=30-32,78-80 --membind=1 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=10 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
  7: num_sockets = 2 num_nodes=2 cores_per_socket=24
123: + exec numactl --physcpubind=33-35,81-83 --membind=1 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=11 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
 14: num_sockets = 2 num_nodes=2 cores_per_socket=24
124: + exec numactl --physcpubind=36-38,84-86 --membind=1 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=12 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
277: num_sockets = 2 num_nodes=2 cores_per_socket=24
189: + exec numactl --physcpubind=39-41,87-89 --membind=1 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=13 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
319: :::MLLOG {"namespace": "", "time_ms": 1593107942151, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 657}}
117: + exec numactl --physcpubind=15-17,63-65 --membind=0 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=5 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
305: :::MLLOG {"namespace": "", "time_ms": 1593107942152, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 657}}
325: + exec numactl --physcpubind=15-17,63-65 --membind=0 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=5 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
322: :::MLLOG {"namespace": "", "time_ms": 1593107942157, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 657}}
114: + exec numactl --physcpubind=6-8,54-56 --membind=0 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=2 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
 77: :::MLLOG {"namespace": "", "time_ms": 1593107942158, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 657}}
126: + exec numactl --physcpubind=42-44,90-92 --membind=1 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=14 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
287: num_sockets = 2 num_nodes=2 cores_per_socket=24
 32: + exec numactl --physcpubind=0-2,48-50 --membind=0 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=0 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
482: :::MLLOG {"namespace": "", "time_ms": 1593107942163, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 657}}
116: + exec numactl --physcpubind=12-14,60-62 --membind=0 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=4 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
486: :::MLLOG {"namespace": "", "time_ms": 1593107942163, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 657}}
183: + exec numactl --physcpubind=21-23,69-71 --membind=0 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=7 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
191: :::MLLOG {"namespace": "", "time_ms": 1593107942173, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 657}}
453: + exec numactl --physcpubind=15-17,63-65 --membind=0 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=5 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
 26: :::MLLOG {"namespace": "", "time_ms": 1593107942173, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 657}}
335: + exec numactl --physcpubind=45-47,93-95 --membind=1 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=15 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
181: :::MLLOG {"namespace": "", "time_ms": 1593107942178, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 657}}
 45: + exec numactl --physcpubind=39-41,87-89 --membind=1 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=13 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
 66: :::MLLOG {"namespace": "", "time_ms": 1593107942175, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 657}}
320: + exec numactl --physcpubind=0-2,48-50 --membind=0 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=0 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
 68: :::MLLOG {"namespace": "", "time_ms": 1593107942176, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 657}}
461: + exec numactl --physcpubind=39-41,87-89 --membind=1 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=13 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
499: :::MLLOG {"namespace": "", "time_ms": 1593107942182, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 657}}
133: + exec numactl --physcpubind=15-17,63-65 --membind=0 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=5 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
 18: :::MLLOG {"namespace": "", "time_ms": 1593107942183, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 657}}
 42: + exec numactl --physcpubind=30-32,78-80 --membind=1 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=10 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
 17: :::MLLOG {"namespace": "", "time_ms": 1593107942183, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 657}}
188: + exec numactl --physcpubind=36-38,84-86 --membind=1 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=12 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
248: :::MLLOG {"namespace": "", "time_ms": 1593107942185, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 657}}
176: + exec numactl --physcpubind=0-2,48-50 --membind=0 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=0 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
331: :::MLLOG {"namespace": "", "time_ms": 1593107942194, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 657}}
187: + exec numactl --physcpubind=33-35,81-83 --membind=1 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=11 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
330: :::MLLOG {"namespace": "", "time_ms": 1593107942194, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 657}}
329: + exec numactl --physcpubind=27-29,75-77 --membind=1 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=9 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
476: :::MLLOG {"namespace": "", "time_ms": 1593107942199, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 657}}
180: + exec numactl --physcpubind=12-14,60-62 --membind=0 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=4 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
501: :::MLLOG {"namespace": "", "time_ms": 1593107942203, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 657}}
179: + exec numactl --physcpubind=9-11,57-59 --membind=0 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=3 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
399: :::MLLOG {"namespace": "", "time_ms": 1593107942204, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 657}}
184: + exec numactl --physcpubind=24-26,72-74 --membind=1 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=8 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
475: :::MLLOG {"namespace": "", "time_ms": 1593107942209, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 657}}
130: + exec numactl --physcpubind=6-8,54-56 --membind=0 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=2 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
290: :::MLLOG {"namespace": "", "time_ms": 1593107942209, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 657}}
333: + exec numactl --physcpubind=39-41,87-89 --membind=1 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=13 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
288: :::MLLOG {"namespace": "", "time_ms": 1593107942209, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 657}}
323: + exec numactl --physcpubind=9-11,57-59 --membind=0 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=3 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
128: :::MLLOG {"namespace": "", "time_ms": 1593107942225, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 657}}
 34: + exec numactl --physcpubind=6-8,54-56 --membind=0 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=2 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
219: :::MLLOG {"namespace": "", "time_ms": 1593107942229, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 657}}
334: + exec numactl --physcpubind=42-44,90-92 --membind=1 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=14 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
217: :::MLLOG {"namespace": "", "time_ms": 1593107942229, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 657}}
 33: + exec numactl --physcpubind=3-5,51-53 --membind=0 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=1 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
254: :::MLLOG {"namespace": "", "time_ms": 1593107942232, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 657}}
 35: + exec numactl --physcpubind=9-11,57-59 --membind=0 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=3 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
214: :::MLLOG {"namespace": "", "time_ms": 1593107942232, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 657}}
326: + exec numactl --physcpubind=18-20,66-68 --membind=0 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=6 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
309: :::MLLOG {"namespace": "", "time_ms": 1593107942236, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 657}}
321: + exec numactl --physcpubind=3-5,51-53 --membind=0 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=1 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
373: :::MLLOG {"namespace": "", "time_ms": 1593107942238, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 657}}
 43: + exec numactl --physcpubind=33-35,81-83 --membind=1 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=11 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
276: num_sockets = 2 num_nodes=2 cores_per_socket=24
 36: + exec numactl --physcpubind=12-14,60-62 --membind=0 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=4 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
280: num_sockets = 2 num_nodes=2 cores_per_socket=24
332: + exec numactl --physcpubind=36-38,84-86 --membind=1 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=12 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
377: :::MLLOG {"namespace": "", "time_ms": 1593107942246, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 657}}
 37: + exec numactl --physcpubind=15-17,63-65 --membind=0 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=5 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
285: num_sockets = 2 num_nodes=2 cores_per_socket=24
 44: + exec numactl --physcpubind=36-38,84-86 --membind=1 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=12 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
355: :::MLLOG {"namespace": "", "time_ms": 1593107942247, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 657}}
140: + exec numactl --physcpubind=36-38,84-86 --membind=1 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=12 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
359: :::MLLOG {"namespace": "", "time_ms": 1593107942247, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 657}}
458: + exec numactl --physcpubind=30-32,78-80 --membind=1 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=10 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
363: :::MLLOG {"namespace": "", "time_ms": 1593107942247, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 657}}
135: + exec numactl --physcpubind=21-23,69-71 --membind=0 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=7 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
274: num_sockets = 2 num_nodes=2 cores_per_socket=24
167: + exec numactl --physcpubind=21-23,69-71 --membind=0 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=7 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
279: num_sockets = 2 num_nodes=2 cores_per_socket=24
 51: + exec numactl --physcpubind=9-11,57-59 --membind=0 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=3 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
278: num_sockets = 2 num_nodes=2 cores_per_socket=24
 59: + exec numactl --physcpubind=33-35,81-83 --membind=1 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=11 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
382: :::MLLOG {"namespace": "", "time_ms": 1593107942254, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 657}}
509: + exec numactl --physcpubind=39-41,87-89 --membind=1 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=13 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
272: num_sockets = 2 num_nodes=2 cores_per_socket=24
 58: + exec numactl --physcpubind=30-32,78-80 --membind=1 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=10 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
273: num_sockets = 2 num_nodes=2 cores_per_socket=24
134: + exec numactl --physcpubind=18-20,66-68 --membind=0 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=6 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
286: num_sockets = 2 num_nodes=2 cores_per_socket=24
142: + exec numactl --physcpubind=42-44,90-92 --membind=1 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=14 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
283: num_sockets = 2 num_nodes=2 cores_per_socket=24
 56: + exec numactl --physcpubind=24-26,72-74 --membind=1 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=8 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
376: :::MLLOG {"namespace": "", "time_ms": 1593107942253, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 657}}
 54: + exec numactl --physcpubind=18-20,66-68 --membind=0 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=6 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
281: num_sockets = 2 num_nodes=2 cores_per_socket=24
129: + exec numactl --physcpubind=3-5,51-53 --membind=0 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=1 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
282: num_sockets = 2 num_nodes=2 cores_per_socket=24
440: + exec numactl --physcpubind=24-26,72-74 --membind=1 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=8 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
284: num_sockets = 2 num_nodes=2 cores_per_socket=24
143: + exec numactl --physcpubind=45-47,93-95 --membind=1 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=15 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
209: :::MLLOG {"namespace": "", "time_ms": 1593107942257, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 657}}
462: + exec numactl --physcpubind=42-44,90-92 --membind=1 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=14 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
 99: :::MLLOG {"namespace": "", "time_ms": 1593107942261, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 657}}
 57: + exec numactl --physcpubind=27-29,75-77 --membind=1 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=9 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
103: :::MLLOG {"namespace": "", "time_ms": 1593107942261, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 657}}
 60: + exec numactl --physcpubind=36-38,84-86 --membind=1 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=12 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
413: :::MLLOG {"namespace": "", "time_ms": 1593107942263, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 657}}
131: + exec numactl --physcpubind=9-11,57-59 --membind=0 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=3 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
348: :::MLLOG {"namespace": "", "time_ms": 1593107942267, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 657}}
255: + exec numactl --physcpubind=45-47,93-95 --membind=1 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=15 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
218: :::MLLOG {"namespace": "", "time_ms": 1593107942275, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 657}}
455: + exec numactl --physcpubind=21-23,69-71 --membind=0 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=7 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
351: :::MLLOG {"namespace": "", "time_ms": 1593107942283, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 657}}
 53: + exec numactl --physcpubind=15-17,63-65 --membind=0 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=5 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
318: :::MLLOG {"namespace": "", "time_ms": 1593107942294, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 657}}
 62: + exec numactl --physcpubind=42-44,90-92 --membind=1 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=14 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
402: :::MLLOG {"namespace": "", "time_ms": 1593107942300, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 657}}
 52: + exec numactl --physcpubind=12-14,60-62 --membind=0 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=4 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
406: :::MLLOG {"namespace": "", "time_ms": 1593107942300, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 657}}
 55: + exec numactl --physcpubind=21-23,69-71 --membind=0 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=7 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
 46: :::MLLOG {"namespace": "", "time_ms": 1593107942307, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 657}}
132: + exec numactl --physcpubind=12-14,60-62 --membind=0 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=4 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
165: :::MLLOG {"namespace": "", "time_ms": 1593107942368, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 657}}
138: + exec numactl --physcpubind=30-32,78-80 --membind=1 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=10 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
147: :::MLLOG {"namespace": "", "time_ms": 1593107942368, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 657}}
139: + exec numactl --physcpubind=33-35,81-83 --membind=1 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=11 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
 12: :::MLLOG {"namespace": "", "time_ms": 1593107942408, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 657}}
436: + exec numactl --physcpubind=12-14,60-62 --membind=0 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=4 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
  1: :::MLLOG {"namespace": "", "time_ms": 1593107942425, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 657}}
454: + exec numactl --physcpubind=18-20,66-68 --membind=0 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=6 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
457: + exec numactl --physcpubind=27-29,75-77 --membind=1 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=9 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
460: + exec numactl --physcpubind=36-38,84-86 --membind=1 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=12 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
451: + exec numactl --physcpubind=9-11,57-59 --membind=0 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=3 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
452: + exec numactl --physcpubind=12-14,60-62 --membind=0 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=4 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
459: + exec numactl --physcpubind=33-35,81-83 --membind=1 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=11 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
463: + exec numactl --physcpubind=45-47,93-95 --membind=1 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=15 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
339: + exec numactl --physcpubind=9-11,57-59 --membind=0 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=3 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
328: :::MLLOG {"namespace": "", "time_ms": 1593107942444, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 657}}
212: + exec numactl --physcpubind=12-14,60-62 --membind=0 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=4 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
446: + exec numactl --physcpubind=42-44,90-92 --membind=1 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=14 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
497: + exec numactl --physcpubind=3-5,51-53 --membind=0 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=1 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
222: + exec numactl --physcpubind=42-44,90-92 --membind=1 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=14 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
470: + exec numactl --physcpubind=18-20,66-68 --membind=0 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=6 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
444: + exec numactl --physcpubind=36-38,84-86 --membind=1 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=12 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
447: + exec numactl --physcpubind=45-47,93-95 --membind=1 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=15 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
442: + exec numactl --physcpubind=30-32,78-80 --membind=1 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=10 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
443: + exec numactl --physcpubind=33-35,81-83 --membind=1 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=11 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
438: + exec numactl --physcpubind=18-20,66-68 --membind=0 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=6 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
445: + exec numactl --physcpubind=39-41,87-89 --membind=1 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=13 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
341: + exec numactl --physcpubind=15-17,63-65 --membind=0 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=5 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
439: + exec numactl --physcpubind=21-23,69-71 --membind=0 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=7 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
343: + exec numactl --physcpubind=21-23,69-71 --membind=0 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=7 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
437: + exec numactl --physcpubind=15-17,63-65 --membind=0 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=5 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
441: + exec numactl --physcpubind=27-29,75-77 --membind=1 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=9 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
344: + exec numactl --physcpubind=24-26,72-74 --membind=1 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=8 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
164: + exec numactl --physcpubind=12-14,60-62 --membind=0 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=4 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
151: + exec numactl --physcpubind=21-23,69-71 --membind=0 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=7 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
342: + exec numactl --physcpubind=18-20,66-68 --membind=0 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=6 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
345: + exec numactl --physcpubind=27-29,75-77 --membind=1 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=9 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
347: + exec numactl --physcpubind=33-35,81-83 --membind=1 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=11 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
349: + exec numactl --physcpubind=39-41,87-89 --membind=1 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=13 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
350: + exec numactl --physcpubind=42-44,90-92 --membind=1 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=14 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
201: + exec numactl --physcpubind=27-29,75-77 --membind=1 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=9 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
 69: + exec numactl --physcpubind=15-17,63-65 --membind=0 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=5 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
 64: + exec numactl --physcpubind=0-2,48-50 --membind=0 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=0 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
 75: + exec numactl --physcpubind=33-35,81-83 --membind=1 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=11 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
 70: + exec numactl --physcpubind=18-20,66-68 --membind=0 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=6 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
398: + exec numactl --physcpubind=42-44,90-92 --membind=1 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=14 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
504: + exec numactl --physcpubind=24-26,72-74 --membind=1 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=8 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
383: + exec numactl --physcpubind=45-47,93-95 --membind=1 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=15 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
 65: + exec numactl --physcpubind=3-5,51-53 --membind=0 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=1 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
 72: + exec numactl --physcpubind=24-26,72-74 --membind=1 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=8 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
 76: + exec numactl --physcpubind=36-38,84-86 --membind=1 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=12 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
505: + exec numactl --physcpubind=27-29,75-77 --membind=1 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=9 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
507: + exec numactl --physcpubind=33-35,81-83 --membind=1 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=11 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
 67: + exec numactl --physcpubind=9-11,57-59 --membind=0 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=3 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
 74: + exec numactl --physcpubind=30-32,78-80 --membind=1 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=10 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
 78: + exec numactl --physcpubind=42-44,90-92 --membind=1 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=14 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
 73: + exec numactl --physcpubind=27-29,75-77 --membind=1 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=9 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
153: + exec numactl --physcpubind=27-29,75-77 --membind=1 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=9 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
 11: + exec numactl --physcpubind=33-35,81-83 --membind=1 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=11 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
 20: + exec numactl --physcpubind=12-14,60-62 --membind=0 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=4 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
502: + exec numactl --physcpubind=18-20,66-68 --membind=0 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=6 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
503: + exec numactl --physcpubind=21-23,69-71 --membind=0 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=7 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
198: + exec numactl --physcpubind=18-20,66-68 --membind=0 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=6 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
511: + exec numactl --physcpubind=45-47,93-95 --membind=1 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=15 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
244: + exec numactl --physcpubind=12-14,60-62 --membind=0 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=4 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
498: + exec numactl --physcpubind=6-8,54-56 --membind=0 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=2 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
506: + exec numactl --physcpubind=30-32,78-80 --membind=1 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=10 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
246: + exec numactl --physcpubind=18-20,66-68 --membind=0 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=6 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
500: + exec numactl --physcpubind=12-14,60-62 --membind=0 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=4 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
510: + exec numactl --physcpubind=42-44,90-92 --membind=1 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=14 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
250: + exec numactl --physcpubind=30-32,78-80 --membind=1 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=10 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
245: + exec numactl --physcpubind=15-17,63-65 --membind=0 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=5 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
303: + exec numactl --physcpubind=45-47,93-95 --membind=1 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=15 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
211: + exec numactl --physcpubind=9-11,57-59 --membind=0 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=3 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
247: + exec numactl --physcpubind=21-23,69-71 --membind=0 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=7 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
379: + exec numactl --physcpubind=33-35,81-83 --membind=1 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=11 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
206: + exec numactl --physcpubind=42-44,90-92 --membind=1 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=14 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
360: + exec numactl --physcpubind=24-26,72-74 --membind=1 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=8 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
370: + exec numactl --physcpubind=6-8,54-56 --membind=0 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=2 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
369: + exec numactl --physcpubind=3-5,51-53 --membind=0 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=1 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
375: + exec numactl --physcpubind=21-23,69-71 --membind=0 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=7 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
220: + exec numactl --physcpubind=36-38,84-86 --membind=1 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=12 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
378: + exec numactl --physcpubind=30-32,78-80 --membind=1 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=10 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
464: + exec numactl --physcpubind=0-2,48-50 --membind=0 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=0 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
381: + exec numactl --physcpubind=39-41,87-89 --membind=1 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=13 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
368: + exec numactl --physcpubind=0-2,48-50 --membind=0 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=0 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
372: + exec numactl --physcpubind=12-14,60-62 --membind=0 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=4 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
380: + exec numactl --physcpubind=36-38,84-86 --membind=1 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=12 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
397: :::MLLOG {"namespace": "", "time_ms": 1593107942465, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 657}}
 30: + exec numactl --physcpubind=42-44,90-92 --membind=1 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=14 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
508: :::MLLOG {"namespace": "", "time_ms": 1593107942465, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 657}}
204: + exec numactl --physcpubind=36-38,84-86 --membind=1 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=12 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
155: + exec numactl --physcpubind=33-35,81-83 --membind=1 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=11 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
192: + exec numactl --physcpubind=0-2,48-50 --membind=0 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=0 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
205: + exec numactl --physcpubind=39-41,87-89 --membind=1 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=13 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
240: + exec numactl --physcpubind=0-2,48-50 --membind=0 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=0 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
474: + exec numactl --physcpubind=30-32,78-80 --membind=1 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=10 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
144: + exec numactl --physcpubind=0-2,48-50 --membind=0 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=0 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
207: + exec numactl --physcpubind=45-47,93-95 --membind=1 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=15 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
195: + exec numactl --physcpubind=9-11,57-59 --membind=0 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=3 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
203: + exec numactl --physcpubind=33-35,81-83 --membind=1 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=11 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
243: + exec numactl --physcpubind=9-11,57-59 --membind=0 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=3 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
251: + exec numactl --physcpubind=33-35,81-83 --membind=1 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=11 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
253: + exec numactl --physcpubind=39-41,87-89 --membind=1 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=13 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
249: + exec numactl --physcpubind=27-29,75-77 --membind=1 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=9 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
200: + exec numactl --physcpubind=24-26,72-74 --membind=1 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=8 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
 63: :::MLLOG {"namespace": "", "time_ms": 1593107942490, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 657}}
 31: + exec numactl --physcpubind=45-47,93-95 --membind=1 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=15 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
252: :::MLLOG {"namespace": "", "time_ms": 1593107942490, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 657}}
 21: + exec numactl --physcpubind=15-17,63-65 --membind=0 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=5 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
197: :::MLLOG {"namespace": "", "time_ms": 1593107942498, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 657}}
193: + exec numactl --physcpubind=3-5,51-53 --membind=0 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=1 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
174: + exec numactl --physcpubind=42-44,90-92 --membind=1 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=14 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
221: + exec numactl --physcpubind=39-41,87-89 --membind=1 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=13 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
199: + exec numactl --physcpubind=21-23,69-71 --membind=0 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=7 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
 29: + exec numactl --physcpubind=39-41,87-89 --membind=1 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=13 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
208: + exec numactl --physcpubind=0-2,48-50 --membind=0 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=0 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
210: + exec numactl --physcpubind=6-8,54-56 --membind=0 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=2 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
223: + exec numactl --physcpubind=45-47,93-95 --membind=1 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=15 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
213: + exec numactl --physcpubind=15-17,63-65 --membind=0 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=5 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
256: :::MLLOG {"namespace": "", "time_ms": 1593107942509, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 657}}
 25: + exec numactl --physcpubind=27-29,75-77 --membind=1 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=9 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
 19: :::MLLOG {"namespace": "", "time_ms": 1593107942511, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 657}}
 22: + exec numactl --physcpubind=18-20,66-68 --membind=0 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=6 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
216: + exec numactl --physcpubind=24-26,72-74 --membind=1 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=8 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
291: + exec numactl --physcpubind=9-11,57-59 --membind=0 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=3 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
 23: + exec numactl --physcpubind=21-23,69-71 --membind=0 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=7 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
146: + exec numactl --physcpubind=6-8,54-56 --membind=0 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=2 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
154: + exec numactl --physcpubind=30-32,78-80 --membind=1 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=10 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
 28: + exec numactl --physcpubind=36-38,84-86 --membind=1 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=12 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
215: + exec numactl --physcpubind=21-23,69-71 --membind=0 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=7 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
152: + exec numactl --physcpubind=24-26,72-74 --membind=1 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=8 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
149: + exec numactl --physcpubind=15-17,63-65 --membind=0 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=5 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
148: + exec numactl --physcpubind=12-14,60-62 --membind=0 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=4 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
 16: + exec numactl --physcpubind=0-2,48-50 --membind=0 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=0 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
 27: + exec numactl --physcpubind=33-35,81-83 --membind=1 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=11 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
467: + exec numactl --physcpubind=9-11,57-59 --membind=0 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=3 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
145: + exec numactl --physcpubind=3-5,51-53 --membind=0 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=1 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
312: :::MLLOG {"namespace": "", "time_ms": 1593107942542, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 657}}
156: + exec numactl --physcpubind=36-38,84-86 --membind=1 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=12 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
173: + exec numactl --physcpubind=39-41,87-89 --membind=1 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=13 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
160: + exec numactl --physcpubind=0-2,48-50 --membind=0 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=0 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
389: + exec numactl --physcpubind=15-17,63-65 --membind=0 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=5 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
471: + exec numactl --physcpubind=21-23,69-71 --membind=0 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=7 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
396: + exec numactl --physcpubind=36-38,84-86 --membind=1 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=12 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
393: + exec numactl --physcpubind=27-29,75-77 --membind=1 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=9 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
473: + exec numactl --physcpubind=27-29,75-77 --membind=1 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=9 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
166: + exec numactl --physcpubind=18-20,66-68 --membind=0 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=6 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
162: + exec numactl --physcpubind=6-8,54-56 --membind=0 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=2 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
168: + exec numactl --physcpubind=24-26,72-74 --membind=1 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=8 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
465: + exec numactl --physcpubind=3-5,51-53 --membind=0 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=1 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
394: + exec numactl --physcpubind=30-32,78-80 --membind=1 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=10 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
171: + exec numactl --physcpubind=33-35,81-83 --membind=1 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=11 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
297: + exec numactl --physcpubind=27-29,75-77 --membind=1 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=9 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
170: + exec numactl --physcpubind=30-32,78-80 --membind=1 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=10 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
 61: :::MLLOG {"namespace": "", "time_ms": 1593107942565, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 657}}
172: + exec numactl --physcpubind=36-38,84-86 --membind=1 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=12 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
102: :::MLLOG {"namespace": "", "time_ms": 1593107942570, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 657}}
169: + exec numactl --physcpubind=27-29,75-77 --membind=1 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=9 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
163: + exec numactl --physcpubind=9-11,57-59 --membind=0 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=3 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
390: + exec numactl --physcpubind=18-20,66-68 --membind=0 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=6 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
395: + exec numactl --physcpubind=33-35,81-83 --membind=1 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=11 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
468: + exec numactl --physcpubind=12-14,60-62 --membind=0 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=4 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
472: + exec numactl --physcpubind=24-26,72-74 --membind=1 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=8 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
466: + exec numactl --physcpubind=6-8,54-56 --membind=0 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=2 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
469: + exec numactl --physcpubind=15-17,63-65 --membind=0 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=5 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
384: + exec numactl --physcpubind=0-2,48-50 --membind=0 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=0 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
391: + exec numactl --physcpubind=21-23,69-71 --membind=0 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=7 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
386: + exec numactl --physcpubind=6-8,54-56 --membind=0 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=2 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
478: :::MLLOG {"namespace": "", "time_ms": 1593107942586, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 657}}
392: + exec numactl --physcpubind=24-26,72-74 --membind=1 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=8 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
346: :::MLLOG {"namespace": "", "time_ms": 1593107942586, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 657}}
266: + exec numactl --physcpubind=30-32,78-80 --membind=1 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=10 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
125: :::MLLOG {"namespace": "", "time_ms": 1593107942590, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 657}}
271: + exec numactl --physcpubind=45-47,93-95 --membind=1 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=15 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
298: + exec numactl --physcpubind=30-32,78-80 --membind=1 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=10 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
268: + exec numactl --physcpubind=36-38,84-86 --membind=1 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=12 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
258: + exec numactl --physcpubind=6-8,54-56 --membind=0 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=2 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
301: + exec numactl --physcpubind=39-41,87-89 --membind=1 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=13 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
233: :::MLLOG {"namespace": "", "time_ms": 1593107942603, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 657}}
293: + exec numactl --physcpubind=15-17,63-65 --membind=0 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=5 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
234: :::MLLOG {"namespace": "", "time_ms": 1593107942603, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 657}}
300: + exec numactl --physcpubind=36-38,84-86 --membind=1 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=12 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
235: :::MLLOG {"namespace": "", "time_ms": 1593107942603, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 657}}
264: + exec numactl --physcpubind=24-26,72-74 --membind=1 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=8 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
237: :::MLLOG {"namespace": "", "time_ms": 1593107942603, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 657}}
292: + exec numactl --physcpubind=12-14,60-62 --membind=0 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=4 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
265: + exec numactl --physcpubind=27-29,75-77 --membind=1 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=9 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
299: + exec numactl --physcpubind=33-35,81-83 --membind=1 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=11 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
289: + exec numactl --physcpubind=3-5,51-53 --membind=0 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=1 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
228: :::MLLOG {"namespace": "", "time_ms": 1593107942606, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 657}}
295: + exec numactl --physcpubind=21-23,69-71 --membind=0 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=7 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
227: :::MLLOG {"namespace": "", "time_ms": 1593107942606, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 657}}
296: + exec numactl --physcpubind=24-26,72-74 --membind=1 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=8 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
231: :::MLLOG {"namespace": "", "time_ms": 1593107942606, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 657}}
259: + exec numactl --physcpubind=9-11,57-59 --membind=0 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=3 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
226: :::MLLOG {"namespace": "", "time_ms": 1593107942606, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 657}}
260: + exec numactl --physcpubind=12-14,60-62 --membind=0 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=4 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
 39: :::MLLOG {"namespace": "", "time_ms": 1593107942608, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 657}}
261: + exec numactl --physcpubind=15-17,63-65 --membind=0 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=5 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
120: :::MLLOG {"namespace": "", "time_ms": 1593107942610, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 657}}
267: + exec numactl --physcpubind=33-35,81-83 --membind=1 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=11 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
236: :::MLLOG {"namespace": "", "time_ms": 1593107942611, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 657}}
262: + exec numactl --physcpubind=18-20,66-68 --membind=0 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=6 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
177: :::MLLOG {"namespace": "", "time_ms": 1593107942615, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 657}}
269: + exec numactl --physcpubind=39-41,87-89 --membind=1 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=13 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
225: :::MLLOG {"namespace": "", "time_ms": 1593107942616, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 657}}
424: + exec numactl --physcpubind=24-26,72-74 --membind=1 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=8 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
230: :::MLLOG {"namespace": "", "time_ms": 1593107942616, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 657}}
418: + exec numactl --physcpubind=6-8,54-56 --membind=0 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=2 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
374: :::MLLOG {"namespace": "", "time_ms": 1593107942622, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 657}}
 91: + exec numactl --physcpubind=33-35,81-83 --membind=1 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=11 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
327: :::MLLOG {"namespace": "", "time_ms": 1593107942635, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 657}}
 87: + exec numactl --physcpubind=21-23,69-71 --membind=0 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=7 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
 79: :::MLLOG {"namespace": "", "time_ms": 1593107942634, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 657}}
307: + exec numactl --physcpubind=9-11,57-59 --membind=0 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=3 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
 71: :::MLLOG {"namespace": "", "time_ms": 1593107942636, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 657}}
105: + exec numactl --physcpubind=27-29,75-77 --membind=1 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=9 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
  5: :::MLLOG {"namespace": "", "time_ms": 1593107942639, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 657}}
400: + exec numactl --physcpubind=0-2,48-50 --membind=0 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=0 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
430: :::MLLOG {"namespace": "", "time_ms": 1593107942636, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 657}}
317: + exec numactl --physcpubind=39-41,87-89 --membind=1 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=13 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
435: :::MLLOG {"namespace": "", "time_ms": 1593107942640, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 657}}
411: + exec numactl --physcpubind=33-35,81-83 --membind=1 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=11 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
433: :::MLLOG {"namespace": "", "time_ms": 1593107942640, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 657}}
409: + exec numactl --physcpubind=27-29,75-77 --membind=1 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=9 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
124: :::MLLOG {"namespace": "", "time_ms": 1593107942641, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 657}}
106: + exec numactl --physcpubind=30-32,78-80 --membind=1 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=10 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
159: :::MLLOG {"namespace": "", "time_ms": 1593107942646, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 657}}
 88: + exec numactl --physcpubind=24-26,72-74 --membind=1 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=8 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
119: :::MLLOG {"namespace": "", "time_ms": 1593107942648, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 657}}
408: + exec numactl --physcpubind=24-26,72-74 --membind=1 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=8 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
112: :::MLLOG {"namespace": "", "time_ms": 1593107942648, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 657}}
354: + exec numactl --physcpubind=6-8,54-56 --membind=0 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=2 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
117: :::MLLOG {"namespace": "", "time_ms": 1593107942648, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 657}}
415: + exec numactl --physcpubind=45-47,93-95 --membind=1 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=15 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
118: :::MLLOG {"namespace": "", "time_ms": 1593107942648, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 657}}
 81: + exec numactl --physcpubind=3-5,51-53 --membind=0 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=1 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
116: :::MLLOG {"namespace": "", "time_ms": 1593107942648, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 657}}
495: + exec numactl --physcpubind=45-47,93-95 --membind=1 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=15 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
270: :::MLLOG {"namespace": "", "time_ms": 1593107942653, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 657}}
306: + exec numactl --physcpubind=6-8,54-56 --membind=0 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=2 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
123: :::MLLOG {"namespace": "", "time_ms": 1593107942652, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 657}}
490: + exec numactl --physcpubind=30-32,78-80 --membind=1 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=10 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
127: :::MLLOG {"namespace": "", "time_ms": 1593107942652, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 657}}
311: + exec numactl --physcpubind=21-23,69-71 --membind=0 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=7 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
126: :::MLLOG {"namespace": "", "time_ms": 1593107942653, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 657}}
421: + exec numactl --physcpubind=15-17,63-65 --membind=0 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=5 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
 43: :::MLLOG {"namespace": "", "time_ms": 1593107942656, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 657}}
 83: + exec numactl --physcpubind=9-11,57-59 --membind=0 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=3 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
 45: :::MLLOG {"namespace": "", "time_ms": 1593107942656, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 657}}
 84: + exec numactl --physcpubind=12-14,60-62 --membind=0 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=4 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
 42: :::MLLOG {"namespace": "", "time_ms": 1593107942656, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 657}}
485: + exec numactl --physcpubind=15-17,63-65 --membind=0 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=5 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
114: :::MLLOG {"namespace": "", "time_ms": 1593107942656, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 657}}
104: + exec numactl --physcpubind=24-26,72-74 --membind=1 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=8 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
 44: :::MLLOG {"namespace": "", "time_ms": 1593107942656, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 657}}
111: + exec numactl --physcpubind=45-47,93-95 --membind=1 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=15 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
332: :::MLLOG {"namespace": "", "time_ms": 1593107942662, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 657}}
 97: + exec numactl --physcpubind=3-5,51-53 --membind=0 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=1 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
334: :::MLLOG {"namespace": "", "time_ms": 1593107942662, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 657}}
100: + exec numactl --physcpubind=12-14,60-62 --membind=0 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=4 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
183: :::MLLOG {"namespace": "", "time_ms": 1593107942665, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 657}}
401: + exec numactl --physcpubind=3-5,51-53 --membind=0 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=1 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
179: :::MLLOG {"namespace": "", "time_ms": 1593107942665, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 657}}
310: + exec numactl --physcpubind=18-20,66-68 --membind=0 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=6 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
180: :::MLLOG {"namespace": "", "time_ms": 1593107942665, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 657}}
 89: + exec numactl --physcpubind=27-29,75-77 --membind=1 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=9 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
189: :::MLLOG {"namespace": "", "time_ms": 1593107942665, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 657}}
414: + exec numactl --physcpubind=42-44,90-92 --membind=1 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=14 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
187: :::MLLOG {"namespace": "", "time_ms": 1593107942665, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 657}}
404: + exec numactl --physcpubind=12-14,60-62 --membind=0 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=4 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
188: :::MLLOG {"namespace": "", "time_ms": 1593107942665, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 657}}
313: + exec numactl --physcpubind=27-29,75-77 --membind=1 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=9 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
190: :::MLLOG {"namespace": "", "time_ms": 1593107942665, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 657}}
 90: + exec numactl --physcpubind=30-32,78-80 --membind=1 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=10 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
176: :::MLLOG {"namespace": "", "time_ms": 1593107942665, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 657}}
410: + exec numactl --physcpubind=30-32,78-80 --membind=1 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=10 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
186: :::MLLOG {"namespace": "", "time_ms": 1593107942665, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 657}}
405: + exec numactl --physcpubind=15-17,63-65 --membind=0 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=5 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
184: :::MLLOG {"namespace": "", "time_ms": 1593107942669, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 657}}
314: + exec numactl --physcpubind=30-32,78-80 --membind=1 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=10 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
329: :::MLLOG {"namespace": "", "time_ms": 1593107942674, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 657}}
412: + exec numactl --physcpubind=36-38,84-86 --membind=1 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=12 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
333: :::MLLOG {"namespace": "", "time_ms": 1593107942674, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 657}}
 85: + exec numactl --physcpubind=15-17,63-65 --membind=0 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=5 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
335: :::MLLOG {"namespace": "", "time_ms": 1593107942674, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 657}}
407: + exec numactl --physcpubind=21-23,69-71 --membind=0 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=7 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
255: :::MLLOG {"namespace": "", "time_ms": 1593107942675, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 657}}
494: + exec numactl --physcpubind=42-44,90-92 --membind=1 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=14 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
323: :::MLLOG {"namespace": "", "time_ms": 1593107942679, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 657}}
315: + exec numactl --physcpubind=33-35,81-83 --membind=1 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=11 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
325: :::MLLOG {"namespace": "", "time_ms": 1593107942679, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 657}}
308: + exec numactl --physcpubind=12-14,60-62 --membind=0 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=4 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
326: :::MLLOG {"namespace": "", "time_ms": 1593107942679, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 657}}
316: + exec numactl --physcpubind=36-38,84-86 --membind=1 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=12 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
320: :::MLLOG {"namespace": "", "time_ms": 1593107942680, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 657}}
304: + exec numactl --physcpubind=0-2,48-50 --membind=0 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=0 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
321: :::MLLOG {"namespace": "", "time_ms": 1593107942679, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 657}}
108: + exec numactl --physcpubind=36-38,84-86 --membind=1 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=12 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
 37: :::MLLOG {"namespace": "", "time_ms": 1593107942679, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 657}}
358: + exec numactl --physcpubind=18-20,66-68 --membind=0 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=6 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
 33: :::MLLOG {"namespace": "", "time_ms": 1593107942680, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 657}}
 82: + exec numactl --physcpubind=6-8,54-56 --membind=0 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=2 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
 38: :::MLLOG {"namespace": "", "time_ms": 1593107942679, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 657}}
491: + exec numactl --physcpubind=33-35,81-83 --membind=1 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=11 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
 35: :::MLLOG {"namespace": "", "time_ms": 1593107942679, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 657}}
487: + exec numactl --physcpubind=21-23,69-71 --membind=0 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=7 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
 34: :::MLLOG {"namespace": "", "time_ms": 1593107942680, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 657}}
483: + exec numactl --physcpubind=9-11,57-59 --membind=0 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=3 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
 32: :::MLLOG {"namespace": "", "time_ms": 1593107942680, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 657}}
110: + exec numactl --physcpubind=42-44,90-92 --membind=1 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=14 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
 36: :::MLLOG {"namespace": "", "time_ms": 1593107942679, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 657}}
101: + exec numactl --physcpubind=15-17,63-65 --membind=0 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=5 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
135: :::MLLOG {"namespace": "", "time_ms": 1593107942684, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 657}}
488: + exec numactl --physcpubind=24-26,72-74 --membind=1 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=8 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
130: :::MLLOG {"namespace": "", "time_ms": 1593107942684, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 657}}
493: + exec numactl --physcpubind=39-41,87-89 --membind=1 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=13 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
132: :::MLLOG {"namespace": "", "time_ms": 1593107942684, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 657}}
 96: + exec numactl --physcpubind=0-2,48-50 --membind=0 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=0 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
131: :::MLLOG {"namespace": "", "time_ms": 1593107942684, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 657}}
107: + exec numactl --physcpubind=33-35,81-83 --membind=1 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=11 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
129: :::MLLOG {"namespace": "", "time_ms": 1593107942684, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 657}}
489: + exec numactl --physcpubind=27-29,75-77 --membind=1 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=9 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
509: :::MLLOG {"namespace": "", "time_ms": 1593107942684, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 657}}
 98: + exec numactl --physcpubind=6-8,54-56 --membind=0 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=2 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
133: :::MLLOG {"namespace": "", "time_ms": 1593107942684, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 657}}
 86: + exec numactl --physcpubind=18-20,66-68 --membind=0 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=6 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
167: :::MLLOG {"namespace": "", "time_ms": 1593107942687, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 657}}
109: + exec numactl --physcpubind=39-41,87-89 --membind=1 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=13 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
507: :::MLLOG {"namespace": "", "time_ms": 1593107942691, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 657}}
 94: + exec numactl --physcpubind=42-44,90-92 --membind=1 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=14 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
452: :::MLLOG {"namespace": "", "time_ms": 1593107942692, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 657}}
 80: + exec numactl --physcpubind=0-2,48-50 --membind=0 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=0 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
453: :::MLLOG {"namespace": "", "time_ms": 1593107942692, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 657}}
 93: + exec numactl --physcpubind=39-41,87-89 --membind=1 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=13 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
455: :::MLLOG {"namespace": "", "time_ms": 1593107942692, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 657}}
 95: + exec numactl --physcpubind=45-47,93-95 --membind=1 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=15 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
451: :::MLLOG {"namespace": "", "time_ms": 1593107942692, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 657}}
 92: + exec numactl --physcpubind=36-38,84-86 --membind=1 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=12 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
454: :::MLLOG {"namespace": "", "time_ms": 1593107942692, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 657}}
481: + exec numactl --physcpubind=3-5,51-53 --membind=0 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=1 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
511: :::MLLOG {"namespace": "", "time_ms": 1593107942694, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 657}}
492: + exec numactl --physcpubind=36-38,84-86 --membind=1 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=12 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
461: :::MLLOG {"namespace": "", "time_ms": 1593107942697, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 657}}
484: + exec numactl --physcpubind=12-14,60-62 --membind=0 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=4 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
457: :::MLLOG {"namespace": "", "time_ms": 1593107942697, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 657}}
480: + exec numactl --physcpubind=0-2,48-50 --membind=0 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=0 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
460: :::MLLOG {"namespace": "", "time_ms": 1593107942697, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 657}}
356: + exec numactl --physcpubind=12-14,60-62 --membind=0 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=4 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
462: :::MLLOG {"namespace": "", "time_ms": 1593107942697, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 657}}
357: + exec numactl --physcpubind=15-17,63-65 --membind=0 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=5 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
459: :::MLLOG {"namespace": "", "time_ms": 1593107942697, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 657}}
419: + exec numactl --physcpubind=9-11,57-59 --membind=0 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=3 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
458: :::MLLOG {"namespace": "", "time_ms": 1593107942697, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 657}}
422: + exec numactl --physcpubind=18-20,66-68 --membind=0 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=6 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
463: :::MLLOG {"namespace": "", "time_ms": 1593107942697, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 657}}
427: + exec numactl --physcpubind=33-35,81-83 --membind=1 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=11 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
 53: :::MLLOG {"namespace": "", "time_ms": 1593107942701, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 657}}
361: + exec numactl --physcpubind=27-29,75-77 --membind=1 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=9 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
 51: :::MLLOG {"namespace": "", "time_ms": 1593107942701, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 657}}
417: + exec numactl --physcpubind=3-5,51-53 --membind=0 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=1 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
 55: :::MLLOG {"namespace": "", "time_ms": 1593107942701, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 657}}
353: + exec numactl --physcpubind=3-5,51-53 --membind=0 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=1 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
 54: :::MLLOG {"namespace": "", "time_ms": 1593107942701, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 657}}
426: + exec numactl --physcpubind=30-32,78-80 --membind=1 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=10 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
 52: :::MLLOG {"namespace": "", "time_ms": 1593107942701, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 657}}
428: + exec numactl --physcpubind=36-38,84-86 --membind=1 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=12 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
 59: :::MLLOG {"namespace": "", "time_ms": 1593107942703, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 657}}
365: + exec numactl --physcpubind=39-41,87-89 --membind=1 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=13 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
 58: :::MLLOG {"namespace": "", "time_ms": 1593107942703, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 657}}
425: + exec numactl --physcpubind=27-29,75-77 --membind=1 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=9 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
222: :::MLLOG {"namespace": "", "time_ms": 1593107942703, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 657}}
420: + exec numactl --physcpubind=12-14,60-62 --membind=0 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=4 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
 60: :::MLLOG {"namespace": "", "time_ms": 1593107942703, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 657}}
431: + exec numactl --physcpubind=45-47,93-95 --membind=1 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=15 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
 57: :::MLLOG {"namespace": "", "time_ms": 1593107942703, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 657}}
364: + exec numactl --physcpubind=36-38,84-86 --membind=1 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=12 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
 62: :::MLLOG {"namespace": "", "time_ms": 1593107942703, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 657}}
366: + exec numactl --physcpubind=42-44,90-92 --membind=1 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=14 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
134: :::MLLOG {"namespace": "", "time_ms": 1593107942705, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 657}}
352: + exec numactl --physcpubind=0-2,48-50 --membind=0 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=0 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
138: :::MLLOG {"namespace": "", "time_ms": 1593107942706, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 657}}
367: + exec numactl --physcpubind=45-47,93-95 --membind=1 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=15 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
143: :::MLLOG {"namespace": "", "time_ms": 1593107942706, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 657}}
362: + exec numactl --physcpubind=30-32,78-80 --membind=1 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=10 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
142: :::MLLOG {"namespace": "", "time_ms": 1593107942706, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 657}}
423: + exec numactl --physcpubind=21-23,69-71 --membind=0 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=7 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
140: :::MLLOG {"namespace": "", "time_ms": 1593107942706, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 657}}
416: + exec numactl --physcpubind=0-2,48-50 --membind=0 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=0 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
139: :::MLLOG {"namespace": "", "time_ms": 1593107942706, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 657}}
429: + exec numactl --physcpubind=39-41,87-89 --membind=1 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=13 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
212: :::MLLOG {"namespace": "", "time_ms": 1593107942707, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 657}}
275: + exec numactl --physcpubind=9-11,57-59 --membind=0 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=3 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
 56: :::MLLOG {"namespace": "", "time_ms": 1593107942707, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 657}}
 10: + exec numactl --physcpubind=30-32,78-80 --membind=1 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=10 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
164: :::MLLOG {"namespace": "", "time_ms": 1593107942711, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 657}}
 15: + exec numactl --physcpubind=45-47,93-95 --membind=1 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=15 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
470: :::MLLOG {"namespace": "", "time_ms": 1593107942713, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 657}}
  8: + exec numactl --physcpubind=24-26,72-74 --membind=1 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=8 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
343: :::MLLOG {"namespace": "", "time_ms": 1593107942713, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 657}}
  4: + exec numactl --physcpubind=12-14,60-62 --membind=0 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=4 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
342: :::MLLOG {"namespace": "", "time_ms": 1593107942713, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 657}}
 13: + exec numactl --physcpubind=39-41,87-89 --membind=1 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=13 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
339: :::MLLOG {"namespace": "", "time_ms": 1593107942713, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 657}}
  6: + exec numactl --physcpubind=18-20,66-68 --membind=0 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=6 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
341: :::MLLOG {"namespace": "", "time_ms": 1593107942713, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 657}}
  9: + exec numactl --physcpubind=27-29,75-77 --membind=1 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=9 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
153: :::MLLOG {"namespace": "", "time_ms": 1593107942715, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 657}}
  3: + exec numactl --physcpubind=9-11,57-59 --membind=0 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=3 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
156: :::MLLOG {"namespace": "", "time_ms": 1593107942715, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 657}}
  2: + exec numactl --physcpubind=6-8,54-56 --membind=0 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=2 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
155: :::MLLOG {"namespace": "", "time_ms": 1593107942715, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 657}}
  0: + exec numactl --physcpubind=0-2,48-50 --membind=0 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=0 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
 69: :::MLLOG {"namespace": "", "time_ms": 1593107942715, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 657}}
  7: + exec numactl --physcpubind=21-23,69-71 --membind=0 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=7 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
347: :::MLLOG {"namespace": "", "time_ms": 1593107942717, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 657}}
 14: + exec numactl --physcpubind=42-44,90-92 --membind=1 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=14 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
349: :::MLLOG {"namespace": "", "time_ms": 1593107942717, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 657}}
277: + exec numactl --physcpubind=15-17,63-65 --membind=0 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=5 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
345: :::MLLOG {"namespace": "", "time_ms": 1593107942717, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 657}}
287: + exec numactl --physcpubind=45-47,93-95 --membind=1 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=15 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
350: :::MLLOG {"namespace": "", "time_ms": 1593107942719, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 657}}
276: + exec numactl --physcpubind=12-14,60-62 --membind=0 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=4 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
 67: :::MLLOG {"namespace": "", "time_ms": 1593107942719, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 657}}
280: + exec numactl --physcpubind=24-26,72-74 --membind=1 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=8 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
443: :::MLLOG {"namespace": "", "time_ms": 1593107942717, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 657}}
285: + exec numactl --physcpubind=39-41,87-89 --membind=1 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=13 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
445: :::MLLOG {"namespace": "", "time_ms": 1593107942717, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 657}}
274: + exec numactl --physcpubind=6-8,54-56 --membind=0 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=2 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
381: :::MLLOG {"namespace": "", "time_ms": 1593107942722, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 657}}
279: + exec numactl --physcpubind=21-23,69-71 --membind=0 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=7 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
378: :::MLLOG {"namespace": "", "time_ms": 1593107942722, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 657}}
278: + exec numactl --physcpubind=18-20,66-68 --membind=0 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=6 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
379: :::MLLOG {"namespace": "", "time_ms": 1593107942721, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 657}}
272: + exec numactl --physcpubind=0-2,48-50 --membind=0 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=0 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
380: :::MLLOG {"namespace": "", "time_ms": 1593107942722, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 657}}
273: + exec numactl --physcpubind=3-5,51-53 --membind=0 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=1 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
 65: :::MLLOG {"namespace": "", "time_ms": 1593107942720, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 657}}
283: + exec numactl --physcpubind=33-35,81-83 --membind=1 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=11 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
344: :::MLLOG {"namespace": "", "time_ms": 1593107942720, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 657}}
286: + exec numactl --physcpubind=42-44,90-92 --membind=1 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=14 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
 70: :::MLLOG {"namespace": "", "time_ms": 1593107942720, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 657}}
282: + exec numactl --physcpubind=30-32,78-80 --membind=1 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=10 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
 64: :::MLLOG {"namespace": "", "time_ms": 1593107942721, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 657}}
281: + exec numactl --physcpubind=27-29,75-77 --membind=1 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=9 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
383: :::MLLOG {"namespace": "", "time_ms": 1593107942722, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 657}}
284: + exec numactl --physcpubind=36-38,84-86 --membind=1 -- python -u /workspace/bert/run_pretraining.py --train_batch_size=18 --learning_rate=2.0e-3 --opt_lamb_beta_1=0.86 --opt_lamb_beta_2=0.975 --warmup_proportion=0.0 --max_steps=750 --phase2 --max_seq_length=512 --max_predictions_per_seq=76 --input_dir=/workspace/data_phase2 --init_checkpoint=/workspace/phase1/model.ckpt-28252.pt --do_train --skip_checkpoint --train_mlm_accuracy_window_size=0 --target_mlm_accuracy=0.712 --max_samples_termination=7500000 --eval_iter_start_samples=3000000 --eval_iter_samples=500000 --eval_batch_size=16 --eval_dir=/workspace/evaldata --cache_eval_data --output_dir=/results --fp16 --fused_gelu_bias --dense_seq_output --fused_mha --allreduce_post_accumulation --allreduce_post_accumulation_fp16 --gradient_accumulation_steps=1 --log_freq=1 --local_rank=12 --bert_config_path=/workspace/phase1/bert_config.json --seed=18263
 78: :::MLLOG {"namespace": "", "time_ms": 1593107942721, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 657}}
 75: :::MLLOG {"namespace": "", "time_ms": 1593107942721, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 657}}
 76: :::MLLOG {"namespace": "", "time_ms": 1593107942721, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 657}}
 73: :::MLLOG {"namespace": "", "time_ms": 1593107942721, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 657}}
210: :::MLLOG {"namespace": "", "time_ms": 1593107942724, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 657}}
211: :::MLLOG {"namespace": "", "time_ms": 1593107942723, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 657}}
372: :::MLLOG {"namespace": "", "time_ms": 1593107942723, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 657}}
497: :::MLLOG {"namespace": "", "time_ms": 1593107942722, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 657}}
440: :::MLLOG {"namespace": "", "time_ms": 1593107942725, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 657}}
447: :::MLLOG {"namespace": "", "time_ms": 1593107942730, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 657}}
251: :::MLLOG {"namespace": "", "time_ms": 1593107942729, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 657}}
253: :::MLLOG {"namespace": "", "time_ms": 1593107942729, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 657}}
446: :::MLLOG {"namespace": "", "time_ms": 1593107942730, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 657}}
249: :::MLLOG {"namespace": "", "time_ms": 1593107942729, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 657}}
442: :::MLLOG {"namespace": "", "time_ms": 1593107942730, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 657}}
 11: :::MLLOG {"namespace": "", "time_ms": 1593107942732, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 657}}
441: :::MLLOG {"namespace": "", "time_ms": 1593107942730, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 657}}
 72: :::MLLOG {"namespace": "", "time_ms": 1593107942730, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 657}}
 20: :::MLLOG {"namespace": "", "time_ms": 1593107942732, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 657}}
250: :::MLLOG {"namespace": "", "time_ms": 1593107942731, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 657}}
444: :::MLLOG {"namespace": "", "time_ms": 1593107942731, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 657}}
500: :::MLLOG {"namespace": "", "time_ms": 1593107942732, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 657}}
498: :::MLLOG {"namespace": "", "time_ms": 1593107942732, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 657}}
502: :::MLLOG {"namespace": "", "time_ms": 1593107942732, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 657}}
503: :::MLLOG {"namespace": "", "time_ms": 1593107942732, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 657}}
162: :::MLLOG {"namespace": "", "time_ms": 1593107942735, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 657}}
201: :::MLLOG {"namespace": "", "time_ms": 1593107942734, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 657}}
221: :::MLLOG {"namespace": "", "time_ms": 1593107942737, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 657}}
223: :::MLLOG {"namespace": "", "time_ms": 1593107942737, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 657}}
240: :::MLLOG {"namespace": "", "time_ms": 1593107942736, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 657}}
244: :::MLLOG {"namespace": "", "time_ms": 1593107942736, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 657}}
247: :::MLLOG {"namespace": "", "time_ms": 1593107942736, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 657}}
243: :::MLLOG {"namespace": "", "time_ms": 1593107942736, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 657}}
246: :::MLLOG {"namespace": "", "time_ms": 1593107942736, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 657}}
220: :::MLLOG {"namespace": "", "time_ms": 1593107942737, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 657}}
245: :::MLLOG {"namespace": "", "time_ms": 1593107942736, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 657}}
439: :::MLLOG {"namespace": "", "time_ms": 1593107942736, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 657}}
398: :::MLLOG {"namespace": "", "time_ms": 1593107942739, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 657}}
436: :::MLLOG {"namespace": "", "time_ms": 1593107942736, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 657}}
437: :::MLLOG {"namespace": "", "time_ms": 1593107942736, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 657}}
438: :::MLLOG {"namespace": "", "time_ms": 1593107942736, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 657}}
370: :::MLLOG {"namespace": "", "time_ms": 1593107942742, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 657}}
368: :::MLLOG {"namespace": "", "time_ms": 1593107942742, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 657}}
369: :::MLLOG {"namespace": "", "time_ms": 1593107942742, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 657}}
375: :::MLLOG {"namespace": "", "time_ms": 1593107942742, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 657}}
506: :::MLLOG {"namespace": "", "time_ms": 1593107942741, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 657}}
203: :::MLLOG {"namespace": "", "time_ms": 1593107942742, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 657}}
510: :::MLLOG {"namespace": "", "time_ms": 1593107942741, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 657}}
195: :::MLLOG {"namespace": "", "time_ms": 1593107942742, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 657}}
199: :::MLLOG {"namespace": "", "time_ms": 1593107942742, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 657}}
193: :::MLLOG {"namespace": "", "time_ms": 1593107942742, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 657}}
198: :::MLLOG {"namespace": "", "time_ms": 1593107942742, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 657}}
204: :::MLLOG {"namespace": "", "time_ms": 1593107942743, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 657}}
205: :::MLLOG {"namespace": "", "time_ms": 1593107942742, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 657}}
192: :::MLLOG {"namespace": "", "time_ms": 1593107942742, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 657}}
504: :::MLLOG {"namespace": "", "time_ms": 1593107942742, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 657}}
391: :::MLLOG {"namespace": "", "time_ms": 1593107942744, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 657}}
206: :::MLLOG {"namespace": "", "time_ms": 1593107942744, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 657}}
303: :::MLLOG {"namespace": "", "time_ms": 1593107942743, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 657}}
291: :::MLLOG {"namespace": "", "time_ms": 1593107942747, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 657}}
386: :::MLLOG {"namespace": "", "time_ms": 1593107942750, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 657}}
389: :::MLLOG {"namespace": "", "time_ms": 1593107942750, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 657}}
200: :::MLLOG {"namespace": "", "time_ms": 1593107942749, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 657}}
216: :::MLLOG {"namespace": "", "time_ms": 1593107942749, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 657}}
390: :::MLLOG {"namespace": "", "time_ms": 1593107942750, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 657}}
384: :::MLLOG {"namespace": "", "time_ms": 1593107942751, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 657}}
208: :::MLLOG {"namespace": "", "time_ms": 1593107942752, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 657}}
505: :::MLLOG {"namespace": "", "time_ms": 1593107942752, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 657}}
215: :::MLLOG {"namespace": "", "time_ms": 1593107942752, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 657}}
213: :::MLLOG {"namespace": "", "time_ms": 1593107942751, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 657}}
 74: :::MLLOG {"namespace": "", "time_ms": 1593107942757, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 657}}
144: :::MLLOG {"namespace": "", "time_ms": 1593107942755, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 657}}
146: :::MLLOG {"namespace": "", "time_ms": 1593107942755, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 657}}
148: :::MLLOG {"namespace": "", "time_ms": 1593107942755, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 657}}
149: :::MLLOG {"namespace": "", "time_ms": 1593107942755, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 657}}
151: :::MLLOG {"namespace": "", "time_ms": 1593107942755, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 657}}
145: :::MLLOG {"namespace": "", "time_ms": 1593107942755, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 657}}
207: :::MLLOG {"namespace": "", "time_ms": 1593107942757, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 657}}
473: :::MLLOG {"namespace": "", "time_ms": 1593107942756, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 657}}
474: :::MLLOG {"namespace": "", "time_ms": 1593107942756, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 657}}
293: :::MLLOG {"namespace": "", "time_ms": 1593107942759, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 657}}
289: :::MLLOG {"namespace": "", "time_ms": 1593107942759, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 657}}
295: :::MLLOG {"namespace": "", "time_ms": 1593107942759, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 657}}
292: :::MLLOG {"namespace": "", "time_ms": 1593107942759, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 657}}
154: :::MLLOG {"namespace": "", "time_ms": 1593107942760, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 657}}
469: :::MLLOG {"namespace": "", "time_ms": 1593107942760, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 657}}
467: :::MLLOG {"namespace": "", "time_ms": 1593107942760, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 657}}
466: :::MLLOG {"namespace": "", "time_ms": 1593107942760, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 657}}
465: :::MLLOG {"namespace": "", "time_ms": 1593107942760, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 657}}
464: :::MLLOG {"namespace": "", "time_ms": 1593107942760, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 657}}
468: :::MLLOG {"namespace": "", "time_ms": 1593107942761, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 657}}
471: :::MLLOG {"namespace": "", "time_ms": 1593107942760, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 657}}
163: :::MLLOG {"namespace": "", "time_ms": 1593107942761, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 657}}
166: :::MLLOG {"namespace": "", "time_ms": 1593107942761, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 657}}
160: :::MLLOG {"namespace": "", "time_ms": 1593107942762, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 657}}
472: :::MLLOG {"namespace": "", "time_ms": 1593107942760, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 657}}
152: :::MLLOG {"namespace": "", "time_ms": 1593107942760, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 657}}
395: :::MLLOG {"namespace": "", "time_ms": 1593107942763, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 657}}
394: :::MLLOG {"namespace": "", "time_ms": 1593107942763, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 657}}
396: :::MLLOG {"namespace": "", "time_ms": 1593107942764, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 657}}
393: :::MLLOG {"namespace": "", "time_ms": 1593107942763, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 657}}
260: :::MLLOG {"namespace": "", "time_ms": 1593107942763, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 657}}
 23: :::MLLOG {"namespace": "", "time_ms": 1593107942763, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 657}}
 22: :::MLLOG {"namespace": "", "time_ms": 1593107942764, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 657}}
 21: :::MLLOG {"namespace": "", "time_ms": 1593107942763, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 657}}
360: :::MLLOG {"namespace": "", "time_ms": 1593107942765, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 657}}
299: :::MLLOG {"namespace": "", "time_ms": 1593107942765, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 657}}
301: :::MLLOG {"namespace": "", "time_ms": 1593107942765, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 657}}
297: :::MLLOG {"namespace": "", "time_ms": 1593107942765, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 657}}
 16: :::MLLOG {"namespace": "", "time_ms": 1593107942764, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 657}}
298: :::MLLOG {"namespace": "", "time_ms": 1593107942765, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 657}}
 27: :::MLLOG {"namespace": "", "time_ms": 1593107942764, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 657}}
 28: :::MLLOG {"namespace": "", "time_ms": 1593107942764, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 657}}
 29: :::MLLOG {"namespace": "", "time_ms": 1593107942764, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 657}}
 30: :::MLLOG {"namespace": "", "time_ms": 1593107942764, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 657}}
 31: :::MLLOG {"namespace": "", "time_ms": 1593107942764, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 657}}
 25: :::MLLOG {"namespace": "", "time_ms": 1593107942764, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 657}}
300: :::MLLOG {"namespace": "", "time_ms": 1593107942765, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 657}}
169: :::MLLOG {"namespace": "", "time_ms": 1593107942766, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 657}}
173: :::MLLOG {"namespace": "", "time_ms": 1593107942766, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 657}}
170: :::MLLOG {"namespace": "", "time_ms": 1593107942766, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 657}}
171: :::MLLOG {"namespace": "", "time_ms": 1593107942766, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 657}}
174: :::MLLOG {"namespace": "", "time_ms": 1593107942766, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 657}}
172: :::MLLOG {"namespace": "", "time_ms": 1593107942766, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 657}}
271: :::MLLOG {"namespace": "", "time_ms": 1593107942768, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 657}}
392: :::MLLOG {"namespace": "", "time_ms": 1593107942766, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 657}}
296: :::MLLOG {"namespace": "", "time_ms": 1593107942769, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 657}}
168: :::MLLOG {"namespace": "", "time_ms": 1593107942771, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 657}}
264: :::MLLOG {"namespace": "", "time_ms": 1593107942774, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 657}}
261: :::MLLOG {"namespace": "", "time_ms": 1593107942778, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 657}}
258: :::MLLOG {"namespace": "", "time_ms": 1593107942778, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 657}}
259: :::MLLOG {"namespace": "", "time_ms": 1593107942778, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 657}}
262: :::MLLOG {"namespace": "", "time_ms": 1593107942778, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 657}}
267: :::MLLOG {"namespace": "", "time_ms": 1593107942780, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 657}}
269: :::MLLOG {"namespace": "", "time_ms": 1593107942780, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 657}}
268: :::MLLOG {"namespace": "", "time_ms": 1593107942781, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 657}}
265: :::MLLOG {"namespace": "", "time_ms": 1593107942780, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 657}}
266: :::MLLOG {"namespace": "", "time_ms": 1593107942781, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 657}}
418: :::MLLOG {"namespace": "", "time_ms": 1593107942833, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 657}}
307: :::MLLOG {"namespace": "", "time_ms": 1593107942846, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 657}}
 87: :::MLLOG {"namespace": "", "time_ms": 1593107942861, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 657}}
 91: :::MLLOG {"namespace": "", "time_ms": 1593107942861, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 657}}
404: :::MLLOG {"namespace": "", "time_ms": 1593107942868, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 657}}
105: :::MLLOG {"namespace": "", "time_ms": 1593107942870, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 657}}
316: :::MLLOG {"namespace": "", "time_ms": 1593107942868, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 657}}
317: :::MLLOG {"namespace": "", "time_ms": 1593107942868, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 657}}
400: :::MLLOG {"namespace": "", "time_ms": 1593107942869, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 657}}
310: :::MLLOG {"namespace": "", "time_ms": 1593107942872, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 657}}
411: :::MLLOG {"namespace": "", "time_ms": 1593107942882, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 657}}
409: :::MLLOG {"namespace": "", "time_ms": 1593107942882, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 657}}
415: :::MLLOG {"namespace": "", "time_ms": 1593107942882, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 657}}
106: :::MLLOG {"namespace": "", "time_ms": 1593107942885, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 657}}
111: :::MLLOG {"namespace": "", "time_ms": 1593107942885, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 657}}
315: :::MLLOG {"namespace": "", "time_ms": 1593107942887, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 657}}
408: :::MLLOG {"namespace": "", "time_ms": 1593107942884, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 657}}
314: :::MLLOG {"namespace": "", "time_ms": 1593107942887, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 657}}
313: :::MLLOG {"namespace": "", "time_ms": 1593107942887, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 657}}
424: :::MLLOG {"namespace": "", "time_ms": 1593107942882, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 657}}
407: :::MLLOG {"namespace": "", "time_ms": 1593107942891, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 657}}
306: :::MLLOG {"namespace": "", "time_ms": 1593107942893, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 657}}
412: :::MLLOG {"namespace": "", "time_ms": 1593107942893, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 657}}
410: :::MLLOG {"namespace": "", "time_ms": 1593107942892, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 657}}
401: :::MLLOG {"namespace": "", "time_ms": 1593107942892, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 657}}
311: :::MLLOG {"namespace": "", "time_ms": 1593107942893, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 657}}
485: :::MLLOG {"namespace": "", "time_ms": 1593107942894, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 657}}
483: :::MLLOG {"namespace": "", "time_ms": 1593107942894, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 657}}
487: :::MLLOG {"namespace": "", "time_ms": 1593107942894, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 657}}
480: :::MLLOG {"namespace": "", "time_ms": 1593107942895, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 657}}
484: :::MLLOG {"namespace": "", "time_ms": 1593107942895, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 657}}
 96: :::MLLOG {"namespace": "", "time_ms": 1593107942896, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 657}}
 98: :::MLLOG {"namespace": "", "time_ms": 1593107942896, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 657}}
100: :::MLLOG {"namespace": "", "time_ms": 1593107942896, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 657}}
101: :::MLLOG {"namespace": "", "time_ms": 1593107942895, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 657}}
 97: :::MLLOG {"namespace": "", "time_ms": 1593107942896, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 657}}
481: :::MLLOG {"namespace": "", "time_ms": 1593107942897, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 657}}
 82: :::MLLOG {"namespace": "", "time_ms": 1593107942897, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 657}}
490: :::MLLOG {"namespace": "", "time_ms": 1593107942898, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 657}}
491: :::MLLOG {"namespace": "", "time_ms": 1593107942898, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 657}}
492: :::MLLOG {"namespace": "", "time_ms": 1593107942898, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 657}}
494: :::MLLOG {"namespace": "", "time_ms": 1593107942898, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 657}}
489: :::MLLOG {"namespace": "", "time_ms": 1593107942898, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 657}}
495: :::MLLOG {"namespace": "", "time_ms": 1593107942898, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 657}}
304: :::MLLOG {"namespace": "", "time_ms": 1593107942901, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 657}}
308: :::MLLOG {"namespace": "", "time_ms": 1593107942900, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 657}}
364: :::MLLOG {"namespace": "", "time_ms": 1593107942901, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 657}}
362: :::MLLOG {"namespace": "", "time_ms": 1593107942901, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 657}}
367: :::MLLOG {"namespace": "", "time_ms": 1593107942901, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 657}}
108: :::MLLOG {"namespace": "", "time_ms": 1593107942903, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 657}}
109: :::MLLOG {"namespace": "", "time_ms": 1593107942903, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 657}}
107: :::MLLOG {"namespace": "", "time_ms": 1593107942903, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 657}}
354: :::MLLOG {"namespace": "", "time_ms": 1593107942903, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 657}}
361: :::MLLOG {"namespace": "", "time_ms": 1593107942904, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 657}}
365: :::MLLOG {"namespace": "", "time_ms": 1593107942904, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 657}}
356: :::MLLOG {"namespace": "", "time_ms": 1593107942903, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 657}}
366: :::MLLOG {"namespace": "", "time_ms": 1593107942906, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 657}}
488: :::MLLOG {"namespace": "", "time_ms": 1593107942905, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 657}}
104: :::MLLOG {"namespace": "", "time_ms": 1593107942905, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 657}}
357: :::MLLOG {"namespace": "", "time_ms": 1593107942907, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 657}}
352: :::MLLOG {"namespace": "", "time_ms": 1593107942908, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 657}}
358: :::MLLOG {"namespace": "", "time_ms": 1593107942908, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 657}}
353: :::MLLOG {"namespace": "", "time_ms": 1593107942909, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 657}}
 84: :::MLLOG {"namespace": "", "time_ms": 1593107942908, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 657}}
 80: :::MLLOG {"namespace": "", "time_ms": 1593107942908, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 657}}
 81: :::MLLOG {"namespace": "", "time_ms": 1593107942908, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 657}}
 85: :::MLLOG {"namespace": "", "time_ms": 1593107942908, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 657}}
 83: :::MLLOG {"namespace": "", "time_ms": 1593107942908, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 657}}
 92: :::MLLOG {"namespace": "", "time_ms": 1593107942910, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 657}}
 94: :::MLLOG {"namespace": "", "time_ms": 1593107942910, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 657}}
 95: :::MLLOG {"namespace": "", "time_ms": 1593107942910, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 657}}
 89: :::MLLOG {"namespace": "", "time_ms": 1593107942910, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 657}}
 90: :::MLLOG {"namespace": "", "time_ms": 1593107942910, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 657}}
 93: :::MLLOG {"namespace": "", "time_ms": 1593107942910, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 657}}
405: :::MLLOG {"namespace": "", "time_ms": 1593107942912, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 657}}
493: :::MLLOG {"namespace": "", "time_ms": 1593107942913, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 657}}
 88: :::MLLOG {"namespace": "", "time_ms": 1593107942911, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 657}}
429: :::MLLOG {"namespace": "", "time_ms": 1593107942915, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 657}}
414: :::MLLOG {"namespace": "", "time_ms": 1593107942919, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 657}}
421: :::MLLOG {"namespace": "", "time_ms": 1593107942922, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 657}}
423: :::MLLOG {"namespace": "", "time_ms": 1593107942922, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 657}}
110: :::MLLOG {"namespace": "", "time_ms": 1593107942929, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 657}}
419: :::MLLOG {"namespace": "", "time_ms": 1593107942928, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 657}}
420: :::MLLOG {"namespace": "", "time_ms": 1593107942930, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 657}}
422: :::MLLOG {"namespace": "", "time_ms": 1593107942931, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 657}}
417: :::MLLOG {"namespace": "", "time_ms": 1593107942931, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 657}}
416: :::MLLOG {"namespace": "", "time_ms": 1593107942933, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 657}}
 86: :::MLLOG {"namespace": "", "time_ms": 1593107942949, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 657}}
427: :::MLLOG {"namespace": "", "time_ms": 1593107942949, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 657}}
431: :::MLLOG {"namespace": "", "time_ms": 1593107942949, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 657}}
428: :::MLLOG {"namespace": "", "time_ms": 1593107942949, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 657}}
426: :::MLLOG {"namespace": "", "time_ms": 1593107942949, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 657}}
425: :::MLLOG {"namespace": "", "time_ms": 1593107942959, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 657}}
  3: :::MLLOG {"namespace": "", "time_ms": 1593107942970, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 657}}
  9: :::MLLOG {"namespace": "", "time_ms": 1593107942969, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 657}}
 10: :::MLLOG {"namespace": "", "time_ms": 1593107942969, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 657}}
 13: :::MLLOG {"namespace": "", "time_ms": 1593107942969, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 657}}
  0: :::MLLOG {"namespace": "", "time_ms": 1593107942970, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 657}}
  4: :::MLLOG {"namespace": "", "time_ms": 1593107942970, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 657}}
  7: :::MLLOG {"namespace": "", "time_ms": 1593107942970, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 657}}
 14: :::MLLOG {"namespace": "", "time_ms": 1593107942970, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 657}}
 15: :::MLLOG {"namespace": "", "time_ms": 1593107942969, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 657}}
  6: :::MLLOG {"namespace": "", "time_ms": 1593107942970, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 657}}
  2: :::MLLOG {"namespace": "", "time_ms": 1593107942970, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 657}}
  8: :::MLLOG {"namespace": "", "time_ms": 1593107942970, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 657}}
275: :::MLLOG {"namespace": "", "time_ms": 1593107943013, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 657}}
277: :::MLLOG {"namespace": "", "time_ms": 1593107943013, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 657}}
287: :::MLLOG {"namespace": "", "time_ms": 1593107943043, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 657}}
283: :::MLLOG {"namespace": "", "time_ms": 1593107943095, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 657}}
281: :::MLLOG {"namespace": "", "time_ms": 1593107943096, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 657}}
285: :::MLLOG {"namespace": "", "time_ms": 1593107943096, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 657}}
286: :::MLLOG {"namespace": "", "time_ms": 1593107943097, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 657}}
284: :::MLLOG {"namespace": "", "time_ms": 1593107943098, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 657}}
276: :::MLLOG {"namespace": "", "time_ms": 1593107943106, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 657}}
272: :::MLLOG {"namespace": "", "time_ms": 1593107943106, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 657}}
282: :::MLLOG {"namespace": "", "time_ms": 1593107943111, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 657}}
279: :::MLLOG {"namespace": "", "time_ms": 1593107943112, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 657}}
273: :::MLLOG {"namespace": "", "time_ms": 1593107943113, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 657}}
274: :::MLLOG {"namespace": "", "time_ms": 1593107943113, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 657}}
278: :::MLLOG {"namespace": "", "time_ms": 1593107943114, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 657}}
280: :::MLLOG {"namespace": "", "time_ms": 1593107943114, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 657}}
 44: device: cuda:12 n_gpu: 512, distributed training: True, 16-bits training: True
 44: Torch distributed is available.
 44: Torch distributed is initialized.
 41: device: cuda:9 n_gpu: 512, distributed training: True, 16-bits training: True
 41: Torch distributed is available.
 41: Torch distributed is initialized.
 72: device: cuda:8 n_gpu: 512, distributed training: True, 16-bits training: True
 72: Torch distributed is available.
 72: Torch distributed is initialized.
495: device: cuda:15 n_gpu: 512, distributed training: True, 16-bits training: True
293: device: cuda:5 n_gpu: 512, distributed training: True, 16-bits training: True
239: device: cuda:15 n_gpu: 512, distributed training: True, 16-bits training: True
239: Torch distributed is available.
239: Torch distributed is initialized.
495: Torch distributed is available.
495: Torch distributed is initialized.
293: Torch distributed is available.
293: Torch distributed is initialized.
449: device: cuda:1 n_gpu: 512, distributed training: True, 16-bits training: True
456: device: cuda:8 n_gpu: 512, distributed training: True, 16-bits training: True
449: Torch distributed is available.
449: Torch distributed is initialized.
456: Torch distributed is available.
456: Torch distributed is initialized.
486: device: cuda:6 n_gpu: 512, distributed training: True, 16-bits training: True
486: Torch distributed is available.
486: Torch distributed is initialized.
352: device: cuda:0 n_gpu: 512, distributed training: True, 16-bits training: True
352: Torch distributed is available.
352: Torch distributed is initialized.
101: device: cuda:5 n_gpu: 512, distributed training: True, 16-bits training: True
101: Torch distributed is available.
101: Torch distributed is initialized.
290: device: cuda:2 n_gpu: 512, distributed training: True, 16-bits training: True
290: Torch distributed is available.
290: Torch distributed is initialized.
213: device: cuda:5 n_gpu: 512, distributed training: True, 16-bits training: True
213: Torch distributed is available.
213: Torch distributed is initialized.
  3: device: cuda:3 n_gpu: 512, distributed training: True, 16-bits training: True
  3: Torch distributed is available.
  3: Torch distributed is initialized.
343: device: cuda:7 n_gpu: 512, distributed training: True, 16-bits training: True
343: Torch distributed is available.
343: Torch distributed is initialized.
 76: device: cuda:12 n_gpu: 512, distributed training: True, 16-bits training: True
 76: Torch distributed is available.
 76: Torch distributed is initialized.
115: device: cuda:3 n_gpu: 512, distributed training: True, 16-bits training: True
115: Torch distributed is available.
115: Torch distributed is initialized.
210: device: cuda:2 n_gpu: 512, distributed training: True, 16-bits training: True
210: Torch distributed is available.
210: Torch distributed is initialized.
137: device: cuda:9 n_gpu: 512, distributed training: True, 16-bits training: True
137: Torch distributed is available.
137: Torch distributed is initialized.
494: device: cuda:14 n_gpu: 512, distributed training: True, 16-bits training: True
494: Torch distributed is available.
494: Torch distributed is initialized.
 96: device: cuda:0 n_gpu: 512, distributed training: True, 16-bits training: True
212: device: cuda:4 n_gpu: 512, distributed training: True, 16-bits training: True
 96: Torch distributed is available.
 96: Torch distributed is initialized.
212: Torch distributed is available.
212: Torch distributed is initialized.
 73: device: cuda:9 n_gpu: 512, distributed training: True, 16-bits training: True
 73: Torch distributed is available.
 73: Torch distributed is initialized.
413: device: cuda:13 n_gpu: 512, distributed training: True, 16-bits training: True
413: Torch distributed is available.
413: Torch distributed is initialized.
113: device: cuda:1 n_gpu: 512, distributed training: True, 16-bits training: True
113: Torch distributed is available.
113: Torch distributed is initialized.
451: device: cuda:3 n_gpu: 512, distributed training: True, 16-bits training: True
451: Torch distributed is available.
451: Torch distributed is initialized.
492: device: cuda:12 n_gpu: 512, distributed training: True, 16-bits training: True
492: Torch distributed is available.
492: Torch distributed is initialized.
270: device: cuda:14 n_gpu: 512, distributed training: True, 16-bits training: True
270: Torch distributed is available.
270: Torch distributed is initialized.
309: device: cuda:5 n_gpu: 512, distributed training: True, 16-bits training: True
309: Torch distributed is available.
309: Torch distributed is initialized.
221: device: cuda:13 n_gpu: 512, distributed training: True, 16-bits training: True
221: Torch distributed is available.
221: Torch distributed is initialized.
 86: device: cuda:6 n_gpu: 512, distributed training: True, 16-bits training: True
 86: Torch distributed is available.
 86: Torch distributed is initialized.
 67: device: cuda:3 n_gpu: 512, distributed training: True, 16-bits training: True
 67: Torch distributed is available.
 67: Torch distributed is initialized.
432: device: cuda:0 n_gpu: 512, distributed training: True, 16-bits training: True
432: Torch distributed is available.
432: Torch distributed is initialized.
 99: device: cuda:3 n_gpu: 512, distributed training: True, 16-bits training: True
 99: Torch distributed is available.
 99: Torch distributed is initialized.
475: device: cuda:11 n_gpu: 512, distributed training: True, 16-bits training: True
475: Torch distributed is available.
475: Torch distributed is initialized.
 71: device: cuda:7 n_gpu: 512, distributed training: True, 16-bits training: True
 71: Torch distributed is available.
 71: Torch distributed is initialized.
165: device: cuda:5 n_gpu: 512, distributed training: True, 16-bits training: True
165: Torch distributed is available.
165: Torch distributed is initialized.
499: device: cuda:3 n_gpu: 512, distributed training: True, 16-bits training: True
499: Torch distributed is available.
499: Torch distributed is initialized.
177: device: cuda:1 n_gpu: 512, distributed training: True, 16-bits training: True
177: Torch distributed is available.
177: Torch distributed is initialized.
215: device: cuda:7 n_gpu: 512, distributed training: True, 16-bits training: True
215: Torch distributed is available.
215: Torch distributed is initialized.
388: device: cuda:4 n_gpu: 512, distributed training: True, 16-bits training: True
388: Torch distributed is available.
388: Torch distributed is initialized.
354: device: cuda:2 n_gpu: 512, distributed training: True, 16-bits training: True
354: Torch distributed is available.
354: Torch distributed is initialized.
482: device: cuda:2 n_gpu: 512, distributed training: True, 16-bits training: True
491: device: cuda:11 n_gpu: 512, distributed training: True, 16-bits training: True
491: Torch distributed is available.
491: Torch distributed is initialized.
482: Torch distributed is available.
482: Torch distributed is initialized.
353: device: cuda:1 n_gpu: 512, distributed training: True, 16-bits training: True
242: device: cuda:2 n_gpu: 512, distributed training: True, 16-bits training: True
353: Torch distributed is available.
353: Torch distributed is initialized.
242: Torch distributed is available.
242: Torch distributed is initialized.
222: device: cuda:14 n_gpu: 512, distributed training: True, 16-bits training: True
222: Torch distributed is available.
222: Torch distributed is initialized.
281: device: cuda:9 n_gpu: 512, distributed training: True, 16-bits training: True
281: Torch distributed is available.
281: Torch distributed is initialized.
 48: device: cuda:0 n_gpu: 512, distributed training: True, 16-bits training: True
 48: Torch distributed is available.
 48: Torch distributed is initialized.
478: device: cuda:14 n_gpu: 512, distributed training: True, 16-bits training: True
478: Torch distributed is available.
478: Torch distributed is initialized.
271: device: cuda:15 n_gpu: 512, distributed training: True, 16-bits training: True
271: Torch distributed is available.
271: Torch distributed is initialized.
373: device: cuda:5 n_gpu: 512, distributed training: True, 16-bits training: True
373: Torch distributed is available.
373: Torch distributed is initialized.
387: device: cuda:3 n_gpu: 512, distributed training: True, 16-bits training: True
387: Torch distributed is available.
387: Torch distributed is initialized.
488: device: cuda:8 n_gpu: 512, distributed training: True, 16-bits training: True
488: Torch distributed is available.
488: Torch distributed is initialized.
 14: device: cuda:14 n_gpu: 512, distributed training: True, 16-bits training: True
 14: Torch distributed is available.
 14: Torch distributed is initialized.
 75: device: cuda:11 n_gpu: 512, distributed training: True, 16-bits training: True
 75: Torch distributed is available.
 75: Torch distributed is initialized.
 66: device: cuda:2 n_gpu: 512, distributed training: True, 16-bits training: True
 66: Torch distributed is available.
 66: Torch distributed is initialized.
 53: device: cuda:5 n_gpu: 512, distributed training: True, 16-bits training: True
 53: Torch distributed is available.
 53: Torch distributed is initialized.
196: device: cuda:4 n_gpu: 512, distributed training: True, 16-bits training: True
196: Torch distributed is available.
196: Torch distributed is initialized.
120: device: cuda:8 n_gpu: 512, distributed training: True, 16-bits training: True
120: Torch distributed is available.
120: Torch distributed is initialized.
109: device: cuda:13 n_gpu: 512, distributed training: True, 16-bits training: True
109: Torch distributed is available.
109: Torch distributed is initialized.
417: device: cuda:1 n_gpu: 512, distributed training: True, 16-bits training: True
417: Torch distributed is available.
417: Torch distributed is initialized.
363: device: cuda:11 n_gpu: 512, distributed training: True, 16-bits training: True
363: Torch distributed is available.
363: Torch distributed is initialized.
 55: device: cuda:7 n_gpu: 512, distributed training: True, 16-bits training: True
 55: Torch distributed is available.
 55: Torch distributed is initialized.
433: device: cuda:1 n_gpu: 512, distributed training: True, 16-bits training: True
433: Torch distributed is available.
433: Torch distributed is initialized.
202: device: cuda:10 n_gpu: 512, distributed training: True, 16-bits training: True
202: Torch distributed is available.
202: Torch distributed is initialized.
374: device: cuda:6 n_gpu: 512, distributed training: True, 16-bits training: True
374: Torch distributed is available.
374: Torch distributed is initialized.
 80: device: cuda:0 n_gpu: 512, distributed training: True, 16-bits training: True
 80: Torch distributed is available.
 80: Torch distributed is initialized.
302: device: cuda:14 n_gpu: 512, distributed training: True, 16-bits training: True
302: Torch distributed is available.
302: Torch distributed is initialized.
138: device: cuda:10 n_gpu: 512, distributed training: True, 16-bits training: True
122: device: cuda:10 n_gpu: 512, distributed training: True, 16-bits training: True
138: Torch distributed is available.
138: Torch distributed is initialized.
122: Torch distributed is available.
122: Torch distributed is initialized.
476: device: cuda:12 n_gpu: 512, distributed training: True, 16-bits training: True
476: Torch distributed is available.
476: Torch distributed is initialized.
356: device: cuda:4 n_gpu: 512, distributed training: True, 16-bits training: True
356: Torch distributed is available.
356: Torch distributed is initialized.
108: device: cuda:12 n_gpu: 512, distributed training: True, 16-bits training: True
108: Torch distributed is available.
108: Torch distributed is initialized.
232: device: cuda:8 n_gpu: 512, distributed training: True, 16-bits training: True
232: Torch distributed is available.
232: Torch distributed is initialized.
 77: device: cuda:13 n_gpu: 512, distributed training: True, 16-bits training: True
 77: Torch distributed is available.
 77: Torch distributed is initialized.
396: device: cuda:12 n_gpu: 512, distributed training: True, 16-bits training: True
396: Torch distributed is available.
396: Torch distributed is initialized.
 18: device: cuda:2 n_gpu: 512, distributed training: True, 16-bits training: True
 18: Torch distributed is available.
 18: Torch distributed is initialized.
490: device: cuda:10 n_gpu: 512, distributed training: True, 16-bits training: True
490: Torch distributed is available.
490: Torch distributed is initialized.
220: device: cuda:12 n_gpu: 512, distributed training: True, 16-bits training: True
220: Torch distributed is available.
220: Torch distributed is initialized.
136: device: cuda:8 n_gpu: 512, distributed training: True, 16-bits training: True
241: device: cuda:1 n_gpu: 512, distributed training: True, 16-bits training: True
241: Torch distributed is available.
241: Torch distributed is initialized.
136: Torch distributed is available.
136: Torch distributed is initialized.
110: device: cuda:14 n_gpu: 512, distributed training: True, 16-bits training: True
110: Torch distributed is available.
110: Torch distributed is initialized.
357: device: cuda:5 n_gpu: 512, distributed training: True, 16-bits training: True
103: device: cuda:7 n_gpu: 512, distributed training: True, 16-bits training: True
357: Torch distributed is available.
357: Torch distributed is initialized.
103: Torch distributed is available.
103: Torch distributed is initialized.
305: device: cuda:1 n_gpu: 512, distributed training: True, 16-bits training: True
322: device: cuda:2 n_gpu: 512, distributed training: True, 16-bits training: True
305: Torch distributed is available.
305: Torch distributed is initialized.
322: Torch distributed is available.
322: Torch distributed is initialized.
224: device: cuda:0 n_gpu: 512, distributed training: True, 16-bits training: True
224: Torch distributed is available.
224: Torch distributed is initialized.
450: device: cuda:2 n_gpu: 512, distributed training: True, 16-bits training: True
450: Torch distributed is available.
450: Torch distributed is initialized.
399: device: cuda:15 n_gpu: 512, distributed training: True, 16-bits training: True
399: Torch distributed is available.
399: Torch distributed is initialized.
398: device: cuda:14 n_gpu: 512, distributed training: True, 16-bits training: True
398: Torch distributed is available.
398: Torch distributed is initialized.
194: device: cuda:2 n_gpu: 512, distributed training: True, 16-bits training: True
194: Torch distributed is available.
194: Torch distributed is initialized.
382: device: cuda:14 n_gpu: 512, distributed training: True, 16-bits training: True
382: Torch distributed is available.
382: Torch distributed is initialized.
435: device: cuda:3 n_gpu: 512, distributed training: True, 16-bits training: True
435: Torch distributed is available.
435: Torch distributed is initialized.
294: device: cuda:6 n_gpu: 512, distributed training: True, 16-bits training: True
294: Torch distributed is available.
294: Torch distributed is initialized.
107: device: cuda:11 n_gpu: 512, distributed training: True, 16-bits training: True
107: Torch distributed is available.
107: Torch distributed is initialized.
508: device: cuda:12 n_gpu: 512, distributed training: True, 16-bits training: True
508: Torch distributed is available.
508: Torch distributed is initialized.
337: device: cuda:1 n_gpu: 512, distributed training: True, 16-bits training: True
337: Torch distributed is available.
337: Torch distributed is initialized.
497: device: cuda:1 n_gpu: 512, distributed training: True, 16-bits training: True
497: Torch distributed is available.
497: Torch distributed is initialized.
  6: device: cuda:6 n_gpu: 512, distributed training: True, 16-bits training: True
  6: Torch distributed is available.
  6: Torch distributed is initialized.
452: device: cuda:4 n_gpu: 512, distributed training: True, 16-bits training: True
452: Torch distributed is available.
452: Torch distributed is initialized.
419: device: cuda:3 n_gpu: 512, distributed training: True, 16-bits training: True
419: Torch distributed is available.
419: Torch distributed is initialized.
358: device: cuda:6 n_gpu: 512, distributed training: True, 16-bits training: True
358: Torch distributed is available.
358: Torch distributed is initialized.
 26: device: cuda:10 n_gpu: 512, distributed training: True, 16-bits training: True
 26: Torch distributed is available.
 26: Torch distributed is initialized.
355: device: cuda:3 n_gpu: 512, distributed training: True, 16-bits training: True
355: Torch distributed is available.
355: Torch distributed is initialized.
164: device: cuda:4 n_gpu: 512, distributed training: True, 16-bits training: True
164: Torch distributed is available.
164: Torch distributed is initialized.
383: device: cuda:15 n_gpu: 512, distributed training: True, 16-bits training: True
383: Torch distributed is available.
383: Torch distributed is initialized.
377: device: cuda:9 n_gpu: 512, distributed training: True, 16-bits training: True
377: Torch distributed is available.
377: Torch distributed is initialized.
129: device: cuda:1 n_gpu: 512, distributed training: True, 16-bits training: True
129: Torch distributed is available.
129: Torch distributed is initialized.
288: device: cuda:0 n_gpu: 512, distributed training: True, 16-bits training: True
288: Torch distributed is available.
288: Torch distributed is initialized.
402: device: cuda:2 n_gpu: 512, distributed training: True, 16-bits training: True
402: Torch distributed is available.
402: Torch distributed is initialized.
 19: device: cuda:3 n_gpu: 512, distributed training: True, 16-bits training: True
 19: Torch distributed is available.
 19: Torch distributed is initialized.
 52: device: cuda:4 n_gpu: 512, distributed training: True, 16-bits training: True
 52: Torch distributed is available.
 52: Torch distributed is initialized.
 98: device: cuda:2 n_gpu: 512, distributed training: True, 16-bits training: True
 98: Torch distributed is available.
 98: Torch distributed is initialized.
487: device: cuda:7 n_gpu: 512, distributed training: True, 16-bits training: True
487: Torch distributed is available.
487: Torch distributed is initialized.
327: device: cuda:7 n_gpu: 512, distributed training: True, 16-bits training: True
327: Torch distributed is available.
327: Torch distributed is initialized.
509: device: cuda:13 n_gpu: 512, distributed training: True, 16-bits training: True
509: Torch distributed is available.
509: Torch distributed is initialized.
330: device: cuda:10 n_gpu: 512, distributed training: True, 16-bits training: True
199: device: cuda:7 n_gpu: 512, distributed training: True, 16-bits training: True
330: Torch distributed is available.
330: Torch distributed is initialized.
199: Torch distributed is available.
199: Torch distributed is initialized.
346: device: cuda:10 n_gpu: 512, distributed training: True, 16-bits training: True
346: Torch distributed is available.
346: Torch distributed is initialized.
313: device: cuda:9 n_gpu: 512, distributed training: True, 16-bits training: True
313: Torch distributed is available.
313: Torch distributed is initialized.
453: device: cuda:5 n_gpu: 512, distributed training: True, 16-bits training: True
453: Torch distributed is available.
453: Torch distributed is initialized.
 10: device: cuda:10 n_gpu: 512, distributed training: True, 16-bits training: True
 10: Torch distributed is available.
 10: Torch distributed is initialized.
181: device: cuda:5 n_gpu: 512, distributed training: True, 16-bits training: True
181: Torch distributed is available.
181: Torch distributed is initialized.
171: device: cuda:11 n_gpu: 512, distributed training: True, 16-bits training: True
336: device: cuda:0 n_gpu: 512, distributed training: True, 16-bits training: True
171: Torch distributed is available.
171: Torch distributed is initialized.
336: Torch distributed is available.
336: Torch distributed is initialized.
  1: device: cuda:1 n_gpu: 512, distributed training: True, 16-bits training: True
  1: Torch distributed is available.
  1: Torch distributed is initialized.
226: device: cuda:2 n_gpu: 512, distributed training: True, 16-bits training: True
226: Torch distributed is available.
226: Torch distributed is initialized.
439: device: cuda:7 n_gpu: 512, distributed training: True, 16-bits training: True
439: Torch distributed is available.
439: Torch distributed is initialized.
431: device: cuda:15 n_gpu: 512, distributed training: True, 16-bits training: True
431: Torch distributed is available.
431: Torch distributed is initialized.
406: device: cuda:6 n_gpu: 512, distributed training: True, 16-bits training: True
406: Torch distributed is available.
406: Torch distributed is initialized.
197: device: cuda:5 n_gpu: 512, distributed training: True, 16-bits training: True
197: Torch distributed is available.
197: Torch distributed is initialized.
278: device: cuda:6 n_gpu: 512, distributed training: True, 16-bits training: True
278: Torch distributed is available.
278: Torch distributed is initialized.
284: device: cuda:12 n_gpu: 512, distributed training: True, 16-bits training: True
284: Torch distributed is available.
284: Torch distributed is initialized.
 32: device: cuda:0 n_gpu: 512, distributed training: True, 16-bits training: True
 32: Torch distributed is available.
 32: Torch distributed is initialized.
319: device: cuda:15 n_gpu: 512, distributed training: True, 16-bits training: True
319: Torch distributed is available.
319: Torch distributed is initialized.
185: device: cuda:9 n_gpu: 512, distributed training: True, 16-bits training: True
185: Torch distributed is available.
185: Torch distributed is initialized.
125: device: cuda:13 n_gpu: 512, distributed training: True, 16-bits training: True
125: Torch distributed is available.
125: Torch distributed is initialized.
259: device: cuda:3 n_gpu: 512, distributed training: True, 16-bits training: True
259: Torch distributed is available.
259: Torch distributed is initialized.
141: device: cuda:13 n_gpu: 512, distributed training: True, 16-bits training: True
141: Torch distributed is available.
141: Torch distributed is initialized.
347: device: cuda:11 n_gpu: 512, distributed training: True, 16-bits training: True
347: Torch distributed is available.
347: Torch distributed is initialized.
485: device: cuda:5 n_gpu: 512, distributed training: True, 16-bits training: True
485: Torch distributed is available.
485: Torch distributed is initialized.
157: device: cuda:13 n_gpu: 512, distributed training: True, 16-bits training: True
157: Torch distributed is available.
157: Torch distributed is initialized.
233: device: cuda:9 n_gpu: 512, distributed training: True, 16-bits training: True
233: Torch distributed is available.
233: Torch distributed is initialized.
328: device: cuda:8 n_gpu: 512, distributed training: True, 16-bits training: True
328: Torch distributed is available.
328: Torch distributed is initialized.
 30: device: cuda:14 n_gpu: 512, distributed training: True, 16-bits training: True
 30: Torch distributed is available.
 30: Torch distributed is initialized.
 34: device: cuda:2 n_gpu: 512, distributed training: True, 16-bits training: True
 34: Torch distributed is available.
 34: Torch distributed is initialized.
130: device: cuda:2 n_gpu: 512, distributed training: True, 16-bits training: True
130: Torch distributed is available.
130: Torch distributed is initialized.
303: device: cuda:15 n_gpu: 512, distributed training: True, 16-bits training: True
303: Torch distributed is available.
303: Torch distributed is initialized.
446: device: cuda:14 n_gpu: 512, distributed training: True, 16-bits training: True
446: Torch distributed is available.
446: Torch distributed is initialized.
408: device: cuda:8 n_gpu: 512, distributed training: True, 16-bits training: True
408: Torch distributed is available.
408: Torch distributed is initialized.
 12: device: cuda:12 n_gpu: 512, distributed training: True, 16-bits training: True
 12: Torch distributed is available.
 12: Torch distributed is initialized.
351: device: cuda:15 n_gpu: 512, distributed training: True, 16-bits training: True
351: Torch distributed is available.
351: Torch distributed is initialized.
254: device: cuda:14 n_gpu: 512, distributed training: True, 16-bits training: True
254: Torch distributed is available.
254: Torch distributed is initialized.
361: device: cuda:9 n_gpu: 512, distributed training: True, 16-bits training: True
361: Torch distributed is available.
361: Torch distributed is initialized.
295: device: cuda:7 n_gpu: 512, distributed training: True, 16-bits training: True
295: Torch distributed is available.
295: Torch distributed is initialized.
442: device: cuda:10 n_gpu: 512, distributed training: True, 16-bits training: True
442: Torch distributed is available.
442: Torch distributed is initialized.
231: device: cuda:7 n_gpu: 512, distributed training: True, 16-bits training: True
231: Torch distributed is available.
231: Torch distributed is initialized.
 15: device: cuda:15 n_gpu: 512, distributed training: True, 16-bits training: True
 15: Torch distributed is available.
 15: Torch distributed is initialized.
 95: device: cuda:15 n_gpu: 512, distributed training: True, 16-bits training: True
 95: Torch distributed is available.
 95: Torch distributed is initialized.
 17: device: cuda:1 n_gpu: 512, distributed training: True, 16-bits training: True
 17: Torch distributed is available.
 17: Torch distributed is initialized.
454: device: cuda:6 n_gpu: 512, distributed training: True, 16-bits training: True
207: device: cuda:15 n_gpu: 512, distributed training: True, 16-bits training: True
348: device: cuda:12 n_gpu: 512, distributed training: True, 16-bits training: True
454: Torch distributed is available.
454: Torch distributed is initialized.
379: device: cuda:11 n_gpu: 512, distributed training: True, 16-bits training: True
207: Torch distributed is available.
207: Torch distributed is initialized.
348: Torch distributed is available.
348: Torch distributed is initialized.
 84: device: cuda:4 n_gpu: 512, distributed training: True, 16-bits training: True
 84: Torch distributed is available.
 84: Torch distributed is initialized.
379: Torch distributed is available.
379: Torch distributed is initialized.
250: device: cuda:10 n_gpu: 512, distributed training: True, 16-bits training: True
250: Torch distributed is available.
250: Torch distributed is initialized.
236: device: cuda:12 n_gpu: 512, distributed training: True, 16-bits training: True
236: Torch distributed is available.
236: Torch distributed is initialized.
466: device: cuda:2 n_gpu: 512, distributed training: True, 16-bits training: True
466: Torch distributed is available.
466: Torch distributed is initialized.
128: device: cuda:0 n_gpu: 512, distributed training: True, 16-bits training: True
128: Torch distributed is available.
128: Torch distributed is initialized.
143: device: cuda:15 n_gpu: 512, distributed training: True, 16-bits training: True
143: Torch distributed is available.
143: Torch distributed is initialized.
312: device: cuda:8 n_gpu: 512, distributed training: True, 16-bits training: True
312: Torch distributed is available.
312: Torch distributed is initialized.
 39: device: cuda:7 n_gpu: 512, distributed training: True, 16-bits training: True
179: device: cuda:3 n_gpu: 512, distributed training: True, 16-bits training: True
163: device: cuda:3 n_gpu: 512, distributed training: True, 16-bits training: True
 39: Torch distributed is available.
 39: Torch distributed is initialized.
179: Torch distributed is available.
179: Torch distributed is initialized.
163: Torch distributed is available.
163: Torch distributed is initialized.
 79: device: cuda:15 n_gpu: 512, distributed training: True, 16-bits training: True
 79: Torch distributed is available.
 79: Torch distributed is initialized.
256: device: cuda:0 n_gpu: 512, distributed training: True, 16-bits training: True
256: Torch distributed is available.
256: Torch distributed is initialized.
191: device: cuda:15 n_gpu: 512, distributed training: True, 16-bits training: True
191: Torch distributed is available.
191: Torch distributed is initialized.
311: device: cuda:7 n_gpu: 512, distributed training: True, 16-bits training: True
311: Torch distributed is available.
311: Torch distributed is initialized.
260: device: cuda:4 n_gpu: 512, distributed training: True, 16-bits training: True
260: Torch distributed is available.
260: Torch distributed is initialized.
247: device: cuda:7 n_gpu: 512, distributed training: True, 16-bits training: True
247: Torch distributed is available.
247: Torch distributed is initialized.
 78: device: cuda:14 n_gpu: 512, distributed training: True, 16-bits training: True
133: device: cuda:5 n_gpu: 512, distributed training: True, 16-bits training: True
 78: Torch distributed is available.
 78: Torch distributed is initialized.
133: Torch distributed is available.
133: Torch distributed is initialized.
159: device: cuda:15 n_gpu: 512, distributed training: True, 16-bits training: True
 37: device: cuda:5 n_gpu: 512, distributed training: True, 16-bits training: True
159: Torch distributed is available.
159: Torch distributed is initialized.
 37: Torch distributed is available.
 37: Torch distributed is initialized.
380: device: cuda:12 n_gpu: 512, distributed training: True, 16-bits training: True
119: device: cuda:7 n_gpu: 512, distributed training: True, 16-bits training: True
119: Torch distributed is available.
119: Torch distributed is initialized.
380: Torch distributed is available.
380: Torch distributed is initialized.
227: device: cuda:3 n_gpu: 512, distributed training: True, 16-bits training: True
227: Torch distributed is available.
227: Torch distributed is initialized.
 11: device: cuda:11 n_gpu: 512, distributed training: True, 16-bits training: True
 11: Torch distributed is available.
 11: Torch distributed is initialized.
  4: device: cuda:4 n_gpu: 512, distributed training: True, 16-bits training: True
  4: Torch distributed is available.
  4: Torch distributed is initialized.
286: device: cuda:14 n_gpu: 512, distributed training: True, 16-bits training: True
286: Torch distributed is available.
286: Torch distributed is initialized.
289: device: cuda:1 n_gpu: 512, distributed training: True, 16-bits training: True
289: Torch distributed is available.
289: Torch distributed is initialized.
153: device: cuda:9 n_gpu: 512, distributed training: True, 16-bits training: True
153: Torch distributed is available.
153: Torch distributed is initialized.
416: device: cuda:0 n_gpu: 512, distributed training: True, 16-bits training: True
416: Torch distributed is available.
416: Torch distributed is initialized.
338: device: cuda:2 n_gpu: 512, distributed training: True, 16-bits training: True
500: device: cuda:4 n_gpu: 512, distributed training: True, 16-bits training: True
  7: device: cuda:7 n_gpu: 512, distributed training: True, 16-bits training: True
338: Torch distributed is available.
338: Torch distributed is initialized.
390: device: cuda:6 n_gpu: 512, distributed training: True, 16-bits training: True
464: device: cuda:0 n_gpu: 512, distributed training: True, 16-bits training: True
  7: Torch distributed is available.
  7: Torch distributed is initialized.
423: device: cuda:7 n_gpu: 512, distributed training: True, 16-bits training: True
500: Torch distributed is available.
500: Torch distributed is initialized.
464: Torch distributed is available.
464: Torch distributed is initialized.
422: device: cuda:6 n_gpu: 512, distributed training: True, 16-bits training: True
193: device: cuda:1 n_gpu: 512, distributed training: True, 16-bits training: True
501: device: cuda:5 n_gpu: 512, distributed training: True, 16-bits training: True
304: device: cuda:0 n_gpu: 512, distributed training: True, 16-bits training: True
390: Torch distributed is available.
390: Torch distributed is initialized.
423: Torch distributed is available.
423: Torch distributed is initialized.
174: device: cuda:14 n_gpu: 512, distributed training: True, 16-bits training: True
219: device: cuda:11 n_gpu: 512, distributed training: True, 16-bits training: True
304: Torch distributed is available.
304: Torch distributed is initialized.
422: Torch distributed is available.
422: Torch distributed is initialized.
455: device: cuda:7 n_gpu: 512, distributed training: True, 16-bits training: True
265: device: cuda:9 n_gpu: 512, distributed training: True, 16-bits training: True
501: Torch distributed is available.
501: Torch distributed is initialized.
334: device: cuda:14 n_gpu: 512, distributed training: True, 16-bits training: True
443: device: cuda:11 n_gpu: 512, distributed training: True, 16-bits training: True
193: Torch distributed is available.
193: Torch distributed is initialized.
219: Torch distributed is available.
219: Torch distributed is initialized.
 43: device: cuda:11 n_gpu: 512, distributed training: True, 16-bits training: True
445: device: cuda:13 n_gpu: 512, distributed training: True, 16-bits training: True
455: Torch distributed is available.
455: Torch distributed is initialized.
265: Torch distributed is available.
265: Torch distributed is initialized.
334: Torch distributed is available.
334: Torch distributed is initialized.
443: Torch distributed is available.
443: Torch distributed is initialized.
249: device: cuda:9 n_gpu: 512, distributed training: True, 16-bits training: True
445: Torch distributed is available.
445: Torch distributed is initialized.
174: Torch distributed is available.
174: Torch distributed is initialized.
146: device: cuda:2 n_gpu: 512, distributed training: True, 16-bits training: True
249: Torch distributed is available.
249: Torch distributed is initialized.
 43: Torch distributed is available.
 43: Torch distributed is initialized.
146: Torch distributed is available.
146: Torch distributed is initialized.
 46: device: cuda:14 n_gpu: 512, distributed training: True, 16-bits training: True
235: device: cuda:11 n_gpu: 512, distributed training: True, 16-bits training: True
235: Torch distributed is available.
235: Torch distributed is initialized.
116: device: cuda:4 n_gpu: 512, distributed training: True, 16-bits training: True
127: device: cuda:15 n_gpu: 512, distributed training: True, 16-bits training: True
127: Torch distributed is available.
127: Torch distributed is initialized.
116: Torch distributed is available.
116: Torch distributed is initialized.
469: device: cuda:5 n_gpu: 512, distributed training: True, 16-bits training: True
469: Torch distributed is available.
469: Torch distributed is initialized.
142: device: cuda:14 n_gpu: 512, distributed training: True, 16-bits training: True
142: Torch distributed is available.
142: Torch distributed is initialized.
 46: Torch distributed is available.
 46: Torch distributed is initialized.
385: device: cuda:1 n_gpu: 512, distributed training: True, 16-bits training: True
404: device: cuda:4 n_gpu: 512, distributed training: True, 16-bits training: True
385: Torch distributed is available.
385: Torch distributed is initialized.
404: Torch distributed is available.
404: Torch distributed is initialized.
238: device: cuda:14 n_gpu: 512, distributed training: True, 16-bits training: True
238: Torch distributed is available.
238: Torch distributed is initialized.
444: device: cuda:12 n_gpu: 512, distributed training: True, 16-bits training: True
444: Torch distributed is available.
444: Torch distributed is initialized.
166: device: cuda:6 n_gpu: 512, distributed training: True, 16-bits training: True
166: Torch distributed is available.
166: Torch distributed is initialized.
360: device: cuda:8 n_gpu: 512, distributed training: True, 16-bits training: True
360: Torch distributed is available.
360: Torch distributed is initialized.
 36: device: cuda:4 n_gpu: 512, distributed training: True, 16-bits training: True
 36: Torch distributed is available.
 36: Torch distributed is initialized.
392: device: cuda:8 n_gpu: 512, distributed training: True, 16-bits training: True
392: Torch distributed is available.
392: Torch distributed is initialized.
117: device: cuda:5 n_gpu: 512, distributed training: True, 16-bits training: True
117: Torch distributed is available.
117: Torch distributed is initialized.
 38: device: cuda:6 n_gpu: 512, distributed training: True, 16-bits training: True
 38: Torch distributed is available.
 38: Torch distributed is initialized.
225: device: cuda:1 n_gpu: 512, distributed training: True, 16-bits training: True
225: Torch distributed is available.
225: Torch distributed is initialized.
381: device: cuda:13 n_gpu: 512, distributed training: True, 16-bits training: True
301: device: cuda:13 n_gpu: 512, distributed training: True, 16-bits training: True
381: Torch distributed is available.
381: Torch distributed is initialized.
438: device: cuda:6 n_gpu: 512, distributed training: True, 16-bits training: True
301: Torch distributed is available.
301: Torch distributed is initialized.
438: Torch distributed is available.
438: Torch distributed is initialized.
 33: device: cuda:1 n_gpu: 512, distributed training: True, 16-bits training: True
 33: Torch distributed is available.
 33: Torch distributed is initialized.
186: device: cuda:10 n_gpu: 512, distributed training: True, 16-bits training: True
186: Torch distributed is available.
186: Torch distributed is initialized.
340: device: cuda:4 n_gpu: 512, distributed training: True, 16-bits training: True
340: Torch distributed is available.
340: Torch distributed is initialized.
 35: device: cuda:3 n_gpu: 512, distributed training: True, 16-bits training: True
 35: Torch distributed is available.
 35: Torch distributed is initialized.
 42: device: cuda:10 n_gpu: 512, distributed training: True, 16-bits training: True
 42: Torch distributed is available.
 42: Torch distributed is initialized.
160: device: cuda:0 n_gpu: 512, distributed training: True, 16-bits training: True
160: Torch distributed is available.
160: Torch distributed is initialized.
237: device: cuda:13 n_gpu: 512, distributed training: True, 16-bits training: True
237: Torch distributed is available.
237: Torch distributed is initialized.
437: device: cuda:5 n_gpu: 512, distributed training: True, 16-bits training: True
437: Torch distributed is available.
437: Torch distributed is initialized.
504: device: cuda:8 n_gpu: 512, distributed training: True, 16-bits training: True
504: Torch distributed is available.
504: Torch distributed is initialized.
 70: device: cuda:6 n_gpu: 512, distributed training: True, 16-bits training: True
 70: Torch distributed is available.
 70: Torch distributed is initialized.
228: device: cuda:4 n_gpu: 512, distributed training: True, 16-bits training: True
228: Torch distributed is available.
228: Torch distributed is initialized.
401: device: cuda:1 n_gpu: 512, distributed training: True, 16-bits training: True
401: Torch distributed is available.
401: Torch distributed is initialized.
195: device: cuda:3 n_gpu: 512, distributed training: True, 16-bits training: True
195: Torch distributed is available.
195: Torch distributed is initialized.
  9: device: cuda:9 n_gpu: 512, distributed training: True, 16-bits training: True
  9: Torch distributed is available.
  9: Torch distributed is initialized.
230: device: cuda:6 n_gpu: 512, distributed training: True, 16-bits training: True
230: Torch distributed is available.
230: Torch distributed is initialized.
447: device: cuda:15 n_gpu: 512, distributed training: True, 16-bits training: True
447: Torch distributed is available.
447: Torch distributed is initialized.
139: device: cuda:11 n_gpu: 512, distributed training: True, 16-bits training: True
139: Torch distributed is available.
139: Torch distributed is initialized.
 56: device: cuda:8 n_gpu: 512, distributed training: True, 16-bits training: True
118: device: cuda:6 n_gpu: 512, distributed training: True, 16-bits training: True
 56: Torch distributed is available.
 56: Torch distributed is initialized.
118: Torch distributed is available.
118: Torch distributed is initialized.
234: device: cuda:10 n_gpu: 512, distributed training: True, 16-bits training: True
234: Torch distributed is available.
234: Torch distributed is initialized.
154: device: cuda:10 n_gpu: 512, distributed training: True, 16-bits training: True
154: Torch distributed is available.
154: Torch distributed is initialized.
183: device: cuda:7 n_gpu: 512, distributed training: True, 16-bits training: True
 31: device: cuda:15 n_gpu: 512, distributed training: True, 16-bits training: True
 31: Torch distributed is available.
 31: Torch distributed is initialized.
183: Torch distributed is available.
183: Torch distributed is initialized.
480: device: cuda:0 n_gpu: 512, distributed training: True, 16-bits training: True
480: Torch distributed is available.
480: Torch distributed is initialized.
457: device: cuda:9 n_gpu: 512, distributed training: True, 16-bits training: True
457: Torch distributed is available.
457: Torch distributed is initialized.
468: device: cuda:4 n_gpu: 512, distributed training: True, 16-bits training: True
468: Torch distributed is available.
468: Torch distributed is initialized.
441: device: cuda:9 n_gpu: 512, distributed training: True, 16-bits training: True
441: Torch distributed is available.
441: Torch distributed is initialized.
459: device: cuda:11 n_gpu: 512, distributed training: True, 16-bits training: True
459: Torch distributed is available.
459: Torch distributed is initialized.
 59: device: cuda:11 n_gpu: 512, distributed training: True, 16-bits training: True
 59: Torch distributed is available.
 59: Torch distributed is initialized.
418: device: cuda:2 n_gpu: 512, distributed training: True, 16-bits training: True
418: Torch distributed is available.
418: Torch distributed is initialized.
440: device: cuda:8 n_gpu: 512, distributed training: True, 16-bits training: True
251: device: cuda:11 n_gpu: 512, distributed training: True, 16-bits training: True
251: Torch distributed is available.
251: Torch distributed is initialized.
436: device: cuda:4 n_gpu: 512, distributed training: True, 16-bits training: True
440: Torch distributed is available.
440: Torch distributed is initialized.
436: Torch distributed is available.
436: Torch distributed is initialized.
368: device: cuda:0 n_gpu: 512, distributed training: True, 16-bits training: True
368: Torch distributed is available.
368: Torch distributed is initialized.
140: device: cuda:12 n_gpu: 512, distributed training: True, 16-bits training: True
140: Torch distributed is available.
140: Torch distributed is initialized.
332: device: cuda:12 n_gpu: 512, distributed training: True, 16-bits training: True
332: Torch distributed is available.
332: Torch distributed is initialized.
461: device: cuda:13 n_gpu: 512, distributed training: True, 16-bits training: True
461: Torch distributed is available.
461: Torch distributed is initialized.
268: device: cuda:12 n_gpu: 512, distributed training: True, 16-bits training: True
268: Torch distributed is available.
268: Torch distributed is initialized.
124: device: cuda:12 n_gpu: 512, distributed training: True, 16-bits training: True
124: Torch distributed is available.
124: Torch distributed is initialized.
 49: device: cuda:1 n_gpu: 512, distributed training: True, 16-bits training: True
 49: Torch distributed is available.
 49: Torch distributed is initialized.
460: device: cuda:12 n_gpu: 512, distributed training: True, 16-bits training: True
460: Torch distributed is available.
460: Torch distributed is initialized.
201: device: cuda:9 n_gpu: 512, distributed training: True, 16-bits training: True
201: Torch distributed is available.
201: Torch distributed is initialized.
126: device: cuda:14 n_gpu: 512, distributed training: True, 16-bits training: True
126: Torch distributed is available.
126: Torch distributed is initialized.
376: device: cuda:8 n_gpu: 512, distributed training: True, 16-bits training: True
123: device: cuda:11 n_gpu: 512, distributed training: True, 16-bits training: True
465: device: cuda:1 n_gpu: 512, distributed training: True, 16-bits training: True
111: device: cuda:15 n_gpu: 512, distributed training: True, 16-bits training: True
376: Torch distributed is available.
376: Torch distributed is initialized.
123: Torch distributed is available.
123: Torch distributed is initialized.
465: Torch distributed is available.
465: Torch distributed is initialized.
111: Torch distributed is available.
111: Torch distributed is initialized.
321: device: cuda:1 n_gpu: 512, distributed training: True, 16-bits training: True
173: device: cuda:13 n_gpu: 512, distributed training: True, 16-bits training: True
321: Torch distributed is available.
321: Torch distributed is initialized.
173: Torch distributed is available.
173: Torch distributed is initialized.
317: device: cuda:13 n_gpu: 512, distributed training: True, 16-bits training: True
317: Torch distributed is available.
317: Torch distributed is initialized.
463: device: cuda:15 n_gpu: 512, distributed training: True, 16-bits training: True
114: device: cuda:2 n_gpu: 512, distributed training: True, 16-bits training: True
463: Torch distributed is available.
463: Torch distributed is initialized.
114: Torch distributed is available.
114: Torch distributed is initialized.
297: device: cuda:9 n_gpu: 512, distributed training: True, 16-bits training: True
297: Torch distributed is available.
297: Torch distributed is initialized.
112: device: cuda:0 n_gpu: 512, distributed training: True, 16-bits training: True
112: Torch distributed is available.
112: Torch distributed is initialized.
 45: device: cuda:13 n_gpu: 512, distributed training: True, 16-bits training: True
 45: Torch distributed is available.
 45: Torch distributed is initialized.
462: device: cuda:14 n_gpu: 512, distributed training: True, 16-bits training: True
462: Torch distributed is available.
462: Torch distributed is initialized.
371: device: cuda:3 n_gpu: 512, distributed training: True, 16-bits training: True
106: device: cuda:10 n_gpu: 512, distributed training: True, 16-bits training: True
106: Torch distributed is available.
106: Torch distributed is initialized.
371: Torch distributed is available.
371: Torch distributed is initialized.
345: device: cuda:9 n_gpu: 512, distributed training: True, 16-bits training: True
345: Torch distributed is available.
345: Torch distributed is initialized.
458: device: cuda:10 n_gpu: 512, distributed training: True, 16-bits training: True
458: Torch distributed is available.
458: Torch distributed is initialized.
121: device: cuda:9 n_gpu: 512, distributed training: True, 16-bits training: True
121: Torch distributed is available.
121: Torch distributed is initialized.
308: device: cuda:4 n_gpu: 512, distributed training: True, 16-bits training: True
308: Torch distributed is available.
308: Torch distributed is initialized.
477: device: cuda:13 n_gpu: 512, distributed training: True, 16-bits training: True
 51: device: cuda:3 n_gpu: 512, distributed training: True, 16-bits training: True
477: Torch distributed is available.
477: Torch distributed is initialized.
 51: Torch distributed is available.
 51: Torch distributed is initialized.
149: device: cuda:5 n_gpu: 512, distributed training: True, 16-bits training: True
378: device: cuda:10 n_gpu: 512, distributed training: True, 16-bits training: True
149: Torch distributed is available.
149: Torch distributed is initialized.
378: Torch distributed is available.
378: Torch distributed is initialized.
162: device: cuda:2 n_gpu: 512, distributed training: True, 16-bits training: True
229: device: cuda:5 n_gpu: 512, distributed training: True, 16-bits training: True
162: Torch distributed is available.
162: Torch distributed is initialized.
229: Torch distributed is available.
229: Torch distributed is initialized.
498: device: cuda:2 n_gpu: 512, distributed training: True, 16-bits training: True
498: Torch distributed is available.
498: Torch distributed is initialized.
 64: device: cuda:0 n_gpu: 512, distributed training: True, 16-bits training: True
 64: Torch distributed is available.
 64: Torch distributed is initialized.
  2: device: cuda:2 n_gpu: 512, distributed training: True, 16-bits training: True
  2: Torch distributed is available.
  2: Torch distributed is initialized.
211: device: cuda:3 n_gpu: 512, distributed training: True, 16-bits training: True
211: Torch distributed is available.
211: Torch distributed is initialized.
169: device: cuda:9 n_gpu: 512, distributed training: True, 16-bits training: True
169: Torch distributed is available.
169: Torch distributed is initialized.
369: device: cuda:1 n_gpu: 512, distributed training: True, 16-bits training: True
369: Torch distributed is available.
369: Torch distributed is initialized.
205: device: cuda:13 n_gpu: 512, distributed training: True, 16-bits training: True
 25: device: cuda:9 n_gpu: 512, distributed training: True, 16-bits training: True
205: Torch distributed is available.
205: Torch distributed is initialized.
 25: Torch distributed is available.
 25: Torch distributed is initialized.
261: device: cuda:5 n_gpu: 512, distributed training: True, 16-bits training: True
261: Torch distributed is available.
261: Torch distributed is initialized.
209: device: cuda:1 n_gpu: 512, distributed training: True, 16-bits training: True
209: Torch distributed is available.
209: Torch distributed is initialized.
372: device: cuda:4 n_gpu: 512, distributed training: True, 16-bits training: True
258: device: cuda:2 n_gpu: 512, distributed training: True, 16-bits training: True
258: Torch distributed is available.
258: Torch distributed is initialized.
372: Torch distributed is available.
372: Torch distributed is initialized.
145: device: cuda:1 n_gpu: 512, distributed training: True, 16-bits training: True
145: Torch distributed is available.
145: Torch distributed is initialized.
375: device: cuda:7 n_gpu: 512, distributed training: True, 16-bits training: True
182: device: cuda:6 n_gpu: 512, distributed training: True, 16-bits training: True
375: Torch distributed is available.
375: Torch distributed is initialized.
182: Torch distributed is available.
182: Torch distributed is initialized.
 63: device: cuda:15 n_gpu: 512, distributed training: True, 16-bits training: True
 63: Torch distributed is available.
 63: Torch distributed is initialized.
370: device: cuda:2 n_gpu: 512, distributed training: True, 16-bits training: True
370: Torch distributed is available.
370: Torch distributed is initialized.
243: device: cuda:3 n_gpu: 512, distributed training: True, 16-bits training: True
243: Torch distributed is available.
243: Torch distributed is initialized.
168: device: cuda:8 n_gpu: 512, distributed training: True, 16-bits training: True
134: device: cuda:6 n_gpu: 512, distributed training: True, 16-bits training: True
168: Torch distributed is available.
168: Torch distributed is initialized.
134: Torch distributed is available.
134: Torch distributed is initialized.
339: device: cuda:3 n_gpu: 512, distributed training: True, 16-bits training: True
 90: device: cuda:10 n_gpu: 512, distributed training: True, 16-bits training: True
 90: Torch distributed is available.
 90: Torch distributed is initialized.
339: Torch distributed is available.
339: Torch distributed is initialized.
 22: device: cuda:6 n_gpu: 512, distributed training: True, 16-bits training: True
333: device: cuda:13 n_gpu: 512, distributed training: True, 16-bits training: True
 22: Torch distributed is available.
 22: Torch distributed is initialized.
333: Torch distributed is available.
333: Torch distributed is initialized.
150: device: cuda:6 n_gpu: 512, distributed training: True, 16-bits training: True
150: Torch distributed is available.
150: Torch distributed is initialized.
324: device: cuda:4 n_gpu: 512, distributed training: True, 16-bits training: True
324: Torch distributed is available.
324: Torch distributed is initialized.
359: device: cuda:7 n_gpu: 512, distributed training: True, 16-bits training: True
359: Torch distributed is available.
359: Torch distributed is initialized.
170: device: cuda:10 n_gpu: 512, distributed training: True, 16-bits training: True
170: Torch distributed is available.
170: Torch distributed is initialized.
151: device: cuda:7 n_gpu: 512, distributed training: True, 16-bits training: True
151: Torch distributed is available.
151: Torch distributed is initialized.
 20: device: cuda:4 n_gpu: 512, distributed training: True, 16-bits training: True
 20: Torch distributed is available.
 20: Torch distributed is initialized.
172: device: cuda:12 n_gpu: 512, distributed training: True, 16-bits training: True
350: device: cuda:14 n_gpu: 512, distributed training: True, 16-bits training: True
172: Torch distributed is available.
172: Torch distributed is initialized.
350: Torch distributed is available.
350: Torch distributed is initialized.
131: device: cuda:3 n_gpu: 512, distributed training: True, 16-bits training: True
131: Torch distributed is available.
131: Torch distributed is initialized.
218: device: cuda:10 n_gpu: 512, distributed training: True, 16-bits training: True
218: Torch distributed is available.
218: Torch distributed is initialized.
 47: device: cuda:15 n_gpu: 512, distributed training: True, 16-bits training: True
 47: Torch distributed is available.
 47: Torch distributed is initialized.
100: device: cuda:4 n_gpu: 512, distributed training: True, 16-bits training: True
100: Torch distributed is available.
100: Torch distributed is initialized.
161: device: cuda:1 n_gpu: 512, distributed training: True, 16-bits training: True
161: Torch distributed is available.
161: Torch distributed is initialized.
214: device: cuda:6 n_gpu: 512, distributed training: True, 16-bits training: True
214: Torch distributed is available.
214: Torch distributed is initialized.
 23: device: cuda:7 n_gpu: 512, distributed training: True, 16-bits training: True
 23: Torch distributed is available.
 23: Torch distributed is initialized.
132: device: cuda:4 n_gpu: 512, distributed training: True, 16-bits training: True
132: Torch distributed is available.
132: Torch distributed is initialized.
147: device: cuda:3 n_gpu: 512, distributed training: True, 16-bits training: True
147: Torch distributed is available.
147: Torch distributed is initialized.
264: device: cuda:8 n_gpu: 512, distributed training: True, 16-bits training: True
264: Torch distributed is available.
264: Torch distributed is initialized.
135: device: cuda:7 n_gpu: 512, distributed training: True, 16-bits training: True
135: Torch distributed is available.
135: Torch distributed is initialized.
 50: device: cuda:2 n_gpu: 512, distributed training: True, 16-bits training: True
326: device: cuda:6 n_gpu: 512, distributed training: True, 16-bits training: True
326: Torch distributed is available.
326: Torch distributed is initialized.
 50: Torch distributed is available.
 50: Torch distributed is initialized.
248: device: cuda:8 n_gpu: 512, distributed training: True, 16-bits training: True
310: device: cuda:6 n_gpu: 512, distributed training: True, 16-bits training: True
248: Torch distributed is available.
248: Torch distributed is initialized.
310: Torch distributed is available.
310: Torch distributed is initialized.
152: device: cuda:8 n_gpu: 512, distributed training: True, 16-bits training: True
152: Torch distributed is available.
152: Torch distributed is initialized.
240: device: cuda:0 n_gpu: 512, distributed training: True, 16-bits training: True
240: Torch distributed is available.
240: Torch distributed is initialized.
144: device: cuda:0 n_gpu: 512, distributed training: True, 16-bits training: True
144: Torch distributed is available.
144: Torch distributed is initialized.
 16: device: cuda:0 n_gpu: 512, distributed training: True, 16-bits training: True
 16: Torch distributed is available.
 16: Torch distributed is initialized.
148: device: cuda:4 n_gpu: 512, distributed training: True, 16-bits training: True
148: Torch distributed is available.
148: Torch distributed is initialized.
267: device: cuda:11 n_gpu: 512, distributed training: True, 16-bits training: True
267: Torch distributed is available.
267: Torch distributed is initialized.
430: device: cuda:14 n_gpu: 512, distributed training: True, 16-bits training: True
430: Torch distributed is available.
430: Torch distributed is initialized.
420: device: cuda:4 n_gpu: 512, distributed training: True, 16-bits training: True
420: Torch distributed is available.
420: Torch distributed is initialized.
190: device: cuda:14 n_gpu: 512, distributed training: True, 16-bits training: True
190: Torch distributed is available.
190: Torch distributed is initialized.
329: device: cuda:9 n_gpu: 512, distributed training: True, 16-bits training: True
329: Torch distributed is available.
329: Torch distributed is initialized.
 27: device: cuda:11 n_gpu: 512, distributed training: True, 16-bits training: True
285: device: cuda:13 n_gpu: 512, distributed training: True, 16-bits training: True
285: Torch distributed is available.
285: Torch distributed is initialized.
 27: Torch distributed is available.
 27: Torch distributed is initialized.
252: device: cuda:12 n_gpu: 512, distributed training: True, 16-bits training: True
252: Torch distributed is available.
252: Torch distributed is initialized.
204: device: cuda:12 n_gpu: 512, distributed training: True, 16-bits training: True
204: Torch distributed is available.
204: Torch distributed is initialized.
156: device: cuda:12 n_gpu: 512, distributed training: True, 16-bits training: True
156: Torch distributed is available.
156: Torch distributed is initialized.
167: device: cuda:7 n_gpu: 512, distributed training: True, 16-bits training: True
167: Torch distributed is available.
167: Torch distributed is initialized.
277: device: cuda:5 n_gpu: 512, distributed training: True, 16-bits training: True
292: device: cuda:4 n_gpu: 512, distributed training: True, 16-bits training: True
277: Torch distributed is available.
277: Torch distributed is initialized.
467: device: cuda:3 n_gpu: 512, distributed training: True, 16-bits training: True
292: Torch distributed is available.
292: Torch distributed is initialized.
467: Torch distributed is available.
467: Torch distributed is initialized.
266: device: cuda:10 n_gpu: 512, distributed training: True, 16-bits training: True
217: device: cuda:9 n_gpu: 512, distributed training: True, 16-bits training: True
217: Torch distributed is available.
217: Torch distributed is initialized.
266: Torch distributed is available.
266: Torch distributed is initialized.
269: device: cuda:13 n_gpu: 512, distributed training: True, 16-bits training: True
269: Torch distributed is available.
269: Torch distributed is initialized.
 58: device: cuda:10 n_gpu: 512, distributed training: True, 16-bits training: True
434: device: cuda:2 n_gpu: 512, distributed training: True, 16-bits training: True
434: Torch distributed is available.
434: Torch distributed is initialized.
 58: Torch distributed is available.
 58: Torch distributed is initialized.
262: device: cuda:6 n_gpu: 512, distributed training: True, 16-bits training: True
262: Torch distributed is available.
262: Torch distributed is initialized.
474: device: cuda:10 n_gpu: 512, distributed training: True, 16-bits training: True
474: Torch distributed is available.
474: Torch distributed is initialized.
298: device: cuda:10 n_gpu: 512, distributed training: True, 16-bits training: True
298: Torch distributed is available.
298: Torch distributed is initialized.
386: device: cuda:2 n_gpu: 512, distributed training: True, 16-bits training: True
386: Torch distributed is available.
386: Torch distributed is initialized.
192: device: cuda:0 n_gpu: 512, distributed training: True, 16-bits training: True
192: Torch distributed is available.
192: Torch distributed is initialized.
411: device: cuda:11 n_gpu: 512, distributed training: True, 16-bits training: True
411: Torch distributed is available.
411: Torch distributed is initialized.
473: device: cuda:9 n_gpu: 512, distributed training: True, 16-bits training: True
473: Torch distributed is available.
473: Torch distributed is initialized.
511: device: cuda:15 n_gpu: 512, distributed training: True, 16-bits training: True
511: Torch distributed is available.
511: Torch distributed is initialized.
342: device: cuda:6 n_gpu: 512, distributed training: True, 16-bits training: True
342: Torch distributed is available.
342: Torch distributed is initialized.
189: device: cuda:13 n_gpu: 512, distributed training: True, 16-bits training: True
189: Torch distributed is available.
189: Torch distributed is initialized.
244: device: cuda:4 n_gpu: 512, distributed training: True, 16-bits training: True
244: Torch distributed is available.
244: Torch distributed is initialized.
299: device: cuda:11 n_gpu: 512, distributed training: True, 16-bits training: True
299: Torch distributed is available.
299: Torch distributed is initialized.
245: device: cuda:5 n_gpu: 512, distributed training: True, 16-bits training: True
 28: device: cuda:12 n_gpu: 512, distributed training: True, 16-bits training: True
245: Torch distributed is available.
245: Torch distributed is initialized.
 28: Torch distributed is available.
 28: Torch distributed is initialized.
335: device: cuda:15 n_gpu: 512, distributed training: True, 16-bits training: True
335: Torch distributed is available.
335: Torch distributed is initialized.
349: device: cuda:13 n_gpu: 512, distributed training: True, 16-bits training: True
349: Torch distributed is available.
349: Torch distributed is initialized.
470: device: cuda:6 n_gpu: 512, distributed training: True, 16-bits training: True
470: Torch distributed is available.
470: Torch distributed is initialized.
397: device: cuda:13 n_gpu: 512, distributed training: True, 16-bits training: True
472: device: cuda:8 n_gpu: 512, distributed training: True, 16-bits training: True
397: Torch distributed is available.
397: Torch distributed is initialized.
472: Torch distributed is available.
472: Torch distributed is initialized.
426: device: cuda:10 n_gpu: 512, distributed training: True, 16-bits training: True
426: Torch distributed is available.
426: Torch distributed is initialized.
255: device: cuda:15 n_gpu: 512, distributed training: True, 16-bits training: True
255: Torch distributed is available.
255: Torch distributed is initialized.
325: device: cuda:5 n_gpu: 512, distributed training: True, 16-bits training: True
325: Torch distributed is available.
325: Torch distributed is initialized.
344: device: cuda:8 n_gpu: 512, distributed training: True, 16-bits training: True
344: Torch distributed is available.
344: Torch distributed is initialized.
246: device: cuda:6 n_gpu: 512, distributed training: True, 16-bits training: True
246: Torch distributed is available.
246: Torch distributed is initialized.
257: device: cuda:1 n_gpu: 512, distributed training: True, 16-bits training: True
448: device: cuda:0 n_gpu: 512, distributed training: True, 16-bits training: True
257: Torch distributed is available.
257: Torch distributed is initialized.
448: Torch distributed is available.
448: Torch distributed is initialized.
471: device: cuda:7 n_gpu: 512, distributed training: True, 16-bits training: True
471: Torch distributed is available.
471: Torch distributed is initialized.
  5: device: cuda:5 n_gpu: 512, distributed training: True, 16-bits training: True
  5: Torch distributed is available.
  5: Torch distributed is initialized.
 62: device: cuda:14 n_gpu: 512, distributed training: True, 16-bits training: True
 62: Torch distributed is available.
 62: Torch distributed is initialized.
 81: device: cuda:1 n_gpu: 512, distributed training: True, 16-bits training: True
 81: Torch distributed is available.
 81: Torch distributed is initialized.
400: device: cuda:0 n_gpu: 512, distributed training: True, 16-bits training: True
400: Torch distributed is available.
400: Torch distributed is initialized.
365: device: cuda:13 n_gpu: 512, distributed training: True, 16-bits training: True
 29: device: cuda:13 n_gpu: 512, distributed training: True, 16-bits training: True
323: device: cuda:3 n_gpu: 512, distributed training: True, 16-bits training: True
365: Torch distributed is available.
365: Torch distributed is initialized.
 29: Torch distributed is available.
 29: Torch distributed is initialized.
341: device: cuda:5 n_gpu: 512, distributed training: True, 16-bits training: True
323: Torch distributed is available.
323: Torch distributed is initialized.
 21: device: cuda:5 n_gpu: 512, distributed training: True, 16-bits training: True
291: device: cuda:3 n_gpu: 512, distributed training: True, 16-bits training: True
 21: Torch distributed is available.
 21: Torch distributed is initialized.
291: Torch distributed is available.
291: Torch distributed is initialized.
175: device: cuda:15 n_gpu: 512, distributed training: True, 16-bits training: True
175: Torch distributed is available.
175: Torch distributed is initialized.
320: device: cuda:0 n_gpu: 512, distributed training: True, 16-bits training: True
320: Torch distributed is available.
320: Torch distributed is initialized.
188: device: cuda:12 n_gpu: 512, distributed training: True, 16-bits training: True
188: Torch distributed is available.
188: Torch distributed is initialized.
341: Torch distributed is available.
341: Torch distributed is initialized.
105: device: cuda:9 n_gpu: 512, distributed training: True, 16-bits training: True
105: Torch distributed is available.
105: Torch distributed is initialized.
395: device: cuda:11 n_gpu: 512, distributed training: True, 16-bits training: True
395: Torch distributed is available.
395: Torch distributed is initialized.
 61: device: cuda:13 n_gpu: 512, distributed training: True, 16-bits training: True
 61: Torch distributed is available.
 61: Torch distributed is initialized.
300: device: cuda:12 n_gpu: 512, distributed training: True, 16-bits training: True
296: device: cuda:8 n_gpu: 512, distributed training: True, 16-bits training: True
203: device: cuda:11 n_gpu: 512, distributed training: True, 16-bits training: True
300: Torch distributed is available.
300: Torch distributed is initialized.
296: Torch distributed is available.
296: Torch distributed is initialized.
203: Torch distributed is available.
203: Torch distributed is initialized.
510: device: cuda:14 n_gpu: 512, distributed training: True, 16-bits training: True
510: Torch distributed is available.
510: Torch distributed is initialized.
178: device: cuda:2 n_gpu: 512, distributed training: True, 16-bits training: True
178: Torch distributed is available.
178: Torch distributed is initialized.
409: device: cuda:9 n_gpu: 512, distributed training: True, 16-bits training: True
409: Torch distributed is available.
409: Torch distributed is initialized.
389: device: cuda:5 n_gpu: 512, distributed training: True, 16-bits training: True
389: Torch distributed is available.
389: Torch distributed is initialized.
307: device: cuda:3 n_gpu: 512, distributed training: True, 16-bits training: True
307: Torch distributed is available.
307: Torch distributed is initialized.
  8: device: cuda:8 n_gpu: 512, distributed training: True, 16-bits training: True
  8: Torch distributed is available.
  8: Torch distributed is initialized.
187: device: cuda:11 n_gpu: 512, distributed training: True, 16-bits training: True
187: Torch distributed is available.
187: Torch distributed is initialized.
 57: device: cuda:9 n_gpu: 512, distributed training: True, 16-bits training: True
 57: Torch distributed is available.
 57: Torch distributed is initialized.
200: device: cuda:8 n_gpu: 512, distributed training: True, 16-bits training: True
200: Torch distributed is available.
200: Torch distributed is initialized.
176: device: cuda:0 n_gpu: 512, distributed training: True, 16-bits training: True
176: Torch distributed is available.
176: Torch distributed is initialized.
198: device: cuda:6 n_gpu: 512, distributed training: True, 16-bits training: True
198: Torch distributed is available.
198: Torch distributed is initialized.
 54: device: cuda:6 n_gpu: 512, distributed training: True, 16-bits training: True
 54: Torch distributed is available.
 54: Torch distributed is initialized.
184: device: cuda:8 n_gpu: 512, distributed training: True, 16-bits training: True
184: Torch distributed is available.
184: Torch distributed is initialized.
206: device: cuda:14 n_gpu: 512, distributed training: True, 16-bits training: True
206: Torch distributed is available.
206: Torch distributed is initialized.
 60: device: cuda:12 n_gpu: 512, distributed training: True, 16-bits training: True
 60: Torch distributed is available.
 60: Torch distributed is initialized.
489: device: cuda:9 n_gpu: 512, distributed training: True, 16-bits training: True
489: Torch distributed is available.
489: Torch distributed is initialized.
384: device: cuda:0 n_gpu: 512, distributed training: True, 16-bits training: True
 87: device: cuda:7 n_gpu: 512, distributed training: True, 16-bits training: True
384: Torch distributed is available.
384: Torch distributed is initialized.
 87: Torch distributed is available.
 87: Torch distributed is initialized.
502: device: cuda:6 n_gpu: 512, distributed training: True, 16-bits training: True
502: Torch distributed is available.
502: Torch distributed is initialized.
393: device: cuda:9 n_gpu: 512, distributed training: True, 16-bits training: True
410: device: cuda:10 n_gpu: 512, distributed training: True, 16-bits training: True
393: Torch distributed is available.
393: Torch distributed is initialized.
 68: device: cuda:4 n_gpu: 512, distributed training: True, 16-bits training: True
410: Torch distributed is available.
410: Torch distributed is initialized.
 68: Torch distributed is available.
 68: Torch distributed is initialized.
394: device: cuda:10 n_gpu: 512, distributed training: True, 16-bits training: True
394: Torch distributed is available.
394: Torch distributed is initialized.
507: device: cuda:11 n_gpu: 512, distributed training: True, 16-bits training: True
507: Torch distributed is available.
507: Torch distributed is initialized.
287: device: cuda:15 n_gpu: 512, distributed training: True, 16-bits training: True
287: Torch distributed is available.
287: Torch distributed is initialized.
407: device: cuda:7 n_gpu: 512, distributed training: True, 16-bits training: True
407: Torch distributed is available.
407: Torch distributed is initialized.
364: device: cuda:12 n_gpu: 512, distributed training: True, 16-bits training: True
364: Torch distributed is available.
364: Torch distributed is initialized.
506: device: cuda:10 n_gpu: 512, distributed training: True, 16-bits training: True
506: Torch distributed is available.
506: Torch distributed is initialized.
403: device: cuda:3 n_gpu: 512, distributed training: True, 16-bits training: True
403: Torch distributed is available.
403: Torch distributed is initialized.
505: device: cuda:9 n_gpu: 512, distributed training: True, 16-bits training: True
505: Torch distributed is available.
505: Torch distributed is initialized.
306: device: cuda:2 n_gpu: 512, distributed training: True, 16-bits training: True
306: Torch distributed is available.
306: Torch distributed is initialized.
503: device: cuda:7 n_gpu: 512, distributed training: True, 16-bits training: True
496: device: cuda:0 n_gpu: 512, distributed training: True, 16-bits training: True
412: device: cuda:12 n_gpu: 512, distributed training: True, 16-bits training: True
503: Torch distributed is available.
503: Torch distributed is initialized.
414: device: cuda:14 n_gpu: 512, distributed training: True, 16-bits training: True
104: device: cuda:8 n_gpu: 512, distributed training: True, 16-bits training: True
314: device: cuda:10 n_gpu: 512, distributed training: True, 16-bits training: True
481: device: cuda:1 n_gpu: 512, distributed training: True, 16-bits training: True
367: device: cuda:15 n_gpu: 512, distributed training: True, 16-bits training: True
496: Torch distributed is available.
496: Torch distributed is initialized.
405: device: cuda:5 n_gpu: 512, distributed training: True, 16-bits training: True
412: Torch distributed is available.
412: Torch distributed is initialized.
316: device: cuda:12 n_gpu: 512, distributed training: True, 16-bits training: True
479: device: cuda:15 n_gpu: 512, distributed training: True, 16-bits training: True
414: Torch distributed is available.
414: Torch distributed is initialized.
367: Torch distributed is available.
367: Torch distributed is initialized.
315: device: cuda:11 n_gpu: 512, distributed training: True, 16-bits training: True
104: Torch distributed is available.
104: Torch distributed is initialized.
314: Torch distributed is available.
314: Torch distributed is initialized.
315: Torch distributed is available.
315: Torch distributed is initialized.
481: Torch distributed is available.
481: Torch distributed is initialized.
316: Torch distributed is available.
316: Torch distributed is initialized.
405: Torch distributed is available.
405: Torch distributed is initialized.
263: device: cuda:7 n_gpu: 512, distributed training: True, 16-bits training: True
479: Torch distributed is available.
479: Torch distributed is initialized.
180: device: cuda:4 n_gpu: 512, distributed training: True, 16-bits training: True
155: device: cuda:11 n_gpu: 512, distributed training: True, 16-bits training: True
 69: device: cuda:5 n_gpu: 512, distributed training: True, 16-bits training: True
180: Torch distributed is available.
180: Torch distributed is initialized.
155: Torch distributed is available.
155: Torch distributed is initialized.
263: Torch distributed is available.
263: Torch distributed is initialized.
 69: Torch distributed is available.
 69: Torch distributed is initialized.
158: device: cuda:14 n_gpu: 512, distributed training: True, 16-bits training: True
158: Torch distributed is available.
158: Torch distributed is initialized.
 13: device: cuda:13 n_gpu: 512, distributed training: True, 16-bits training: True
 13: Torch distributed is available.
 13: Torch distributed is initialized.
424: device: cuda:8 n_gpu: 512, distributed training: True, 16-bits training: True
424: Torch distributed is available.
424: Torch distributed is initialized.
 40: device: cuda:8 n_gpu: 512, distributed training: True, 16-bits training: True
 40: Torch distributed is available.
 40: Torch distributed is initialized.
 65: device: cuda:1 n_gpu: 512, distributed training: True, 16-bits training: True
 65: Torch distributed is available.
 65: Torch distributed is initialized.
 97: device: cuda:1 n_gpu: 512, distributed training: True, 16-bits training: True
 97: Torch distributed is available.
 97: Torch distributed is initialized.
415: device: cuda:15 n_gpu: 512, distributed training: True, 16-bits training: True
331: device: cuda:11 n_gpu: 512, distributed training: True, 16-bits training: True
415: Torch distributed is available.
415: Torch distributed is initialized.
331: Torch distributed is available.
331: Torch distributed is initialized.
493: device: cuda:13 n_gpu: 512, distributed training: True, 16-bits training: True
493: Torch distributed is available.
493: Torch distributed is initialized.
429: device: cuda:13 n_gpu: 512, distributed training: True, 16-bits training: True
429: Torch distributed is available.
429: Torch distributed is initialized.
483: device: cuda:3 n_gpu: 512, distributed training: True, 16-bits training: True
483: Torch distributed is available.
483: Torch distributed is initialized.
208: device: cuda:0 n_gpu: 512, distributed training: True, 16-bits training: True
208: Torch distributed is available.
208: Torch distributed is initialized.
366: device: cuda:14 n_gpu: 512, distributed training: True, 16-bits training: True
366: Torch distributed is available.
366: Torch distributed is initialized.
 24: device: cuda:8 n_gpu: 512, distributed training: True, 16-bits training: True
318: device: cuda:14 n_gpu: 512, distributed training: True, 16-bits training: True
 24: Torch distributed is available.
 24: Torch distributed is initialized.
318: Torch distributed is available.
318: Torch distributed is initialized.
 74: device: cuda:10 n_gpu: 512, distributed training: True, 16-bits training: True
 74: Torch distributed is available.
 74: Torch distributed is initialized.
253: device: cuda:13 n_gpu: 512, distributed training: True, 16-bits training: True
253: Torch distributed is available.
253: Torch distributed is initialized.
484: device: cuda:4 n_gpu: 512, distributed training: True, 16-bits training: True
102: device: cuda:6 n_gpu: 512, distributed training: True, 16-bits training: True
102: Torch distributed is available.
102: Torch distributed is initialized.
484: Torch distributed is available.
484: Torch distributed is initialized.
223: device: cuda:15 n_gpu: 512, distributed training: True, 16-bits training: True
 94: device: cuda:14 n_gpu: 512, distributed training: True, 16-bits training: True
 94: Torch distributed is available.
 94: Torch distributed is initialized.
223: Torch distributed is available.
223: Torch distributed is initialized.
391: device: cuda:7 n_gpu: 512, distributed training: True, 16-bits training: True
391: Torch distributed is available.
391: Torch distributed is initialized.
362: device: cuda:10 n_gpu: 512, distributed training: True, 16-bits training: True
362: Torch distributed is available.
362: Torch distributed is initialized.
216: device: cuda:8 n_gpu: 512, distributed training: True, 16-bits training: True
216: Torch distributed is available.
216: Torch distributed is initialized.
425: device: cuda:9 n_gpu: 512, distributed training: True, 16-bits training: True
425: Torch distributed is available.
425: Torch distributed is initialized.
421: device: cuda:5 n_gpu: 512, distributed training: True, 16-bits training: True
421: Torch distributed is available.
421: Torch distributed is initialized.
276: device: cuda:4 n_gpu: 512, distributed training: True, 16-bits training: True
276: Torch distributed is available.
276: Torch distributed is initialized.
274: device: cuda:2 n_gpu: 512, distributed training: True, 16-bits training: True
274: Torch distributed is available.
274: Torch distributed is initialized.
 88: device: cuda:8 n_gpu: 512, distributed training: True, 16-bits training: True
 88: Torch distributed is available.
 88: Torch distributed is initialized.
428: device: cuda:12 n_gpu: 512, distributed training: True, 16-bits training: True
428: Torch distributed is available.
428: Torch distributed is initialized.
427: device: cuda:11 n_gpu: 512, distributed training: True, 16-bits training: True
427: Torch distributed is available.
427: Torch distributed is initialized.
 91: device: cuda:11 n_gpu: 512, distributed training: True, 16-bits training: True
 91: Torch distributed is available.
 91: Torch distributed is initialized.
283: device: cuda:11 n_gpu: 512, distributed training: True, 16-bits training: True
283: Torch distributed is available.
283: Torch distributed is initialized.
 89: device: cuda:9 n_gpu: 512, distributed training: True, 16-bits training: True
 89: Torch distributed is available.
 89: Torch distributed is initialized.
275: device: cuda:3 n_gpu: 512, distributed training: True, 16-bits training: True
275: Torch distributed is available.
275: Torch distributed is initialized.
 83: device: cuda:3 n_gpu: 512, distributed training: True, 16-bits training: True
 83: Torch distributed is available.
 83: Torch distributed is initialized.
 92: device: cuda:12 n_gpu: 512, distributed training: True, 16-bits training: True
 92: Torch distributed is available.
 92: Torch distributed is initialized.
282: device: cuda:10 n_gpu: 512, distributed training: True, 16-bits training: True
282: Torch distributed is available.
282: Torch distributed is initialized.
272: device: cuda:0 n_gpu: 512, distributed training: True, 16-bits training: True
272: Torch distributed is available.
272: Torch distributed is initialized.
279: device: cuda:7 n_gpu: 512, distributed training: True, 16-bits training: True
279: Torch distributed is available.
279: Torch distributed is initialized.
280: device: cuda:8 n_gpu: 512, distributed training: True, 16-bits training: True
280: Torch distributed is available.
280: Torch distributed is initialized.
 82: device: cuda:2 n_gpu: 512, distributed training: True, 16-bits training: True
 82: Torch distributed is available.
 82: Torch distributed is initialized.
 93: device: cuda:13 n_gpu: 512, distributed training: True, 16-bits training: True
 93: Torch distributed is available.
 93: Torch distributed is initialized.
273: device: cuda:1 n_gpu: 512, distributed training: True, 16-bits training: True
273: Torch distributed is available.
273: Torch distributed is initialized.
 85: device: cuda:5 n_gpu: 512, distributed training: True, 16-bits training: True
 85: Torch distributed is available.
 85: Torch distributed is initialized.
  0: device: cuda:0 n_gpu: 512, distributed training: True, 16-bits training: True
  0: :::MLLOG {"namespace": "", "time_ms": 1593107951202, "event_type": "POINT_IN_TIME", "key": "submission_benchmark", "value": "bert", "metadata": {"file": "/workspace/bert/mlperf_logger.py", "lineno": 68}}
  0: :::MLLOG {"namespace": "", "time_ms": 1593107951202, "event_type": "POINT_IN_TIME", "key": "submission_org", "value": "NVIDIA", "metadata": {"file": "/workspace/bert/mlperf_logger.py", "lineno": 73}}
  0: :::MLLOG {"namespace": "", "time_ms": 1593107951202, "event_type": "POINT_IN_TIME", "key": "submission_division", "value": "closed", "metadata": {"file": "/workspace/bert/mlperf_logger.py", "lineno": 77}}
  0: :::MLLOG {"namespace": "", "time_ms": 1593107951202, "event_type": "POINT_IN_TIME", "key": "submission_status", "value": "onprem", "metadata": {"file": "/workspace/bert/mlperf_logger.py", "lineno": 81}}
  0: :::MLLOG {"namespace": "", "time_ms": 1593107951203, "event_type": "POINT_IN_TIME", "key": "submission_platform", "value": "32xNVIDIA DGX-2H", "metadata": {"file": "/workspace/bert/mlperf_logger.py", "lineno": 85}}
  0: Torch distributed is available.
  0: Torch distributed is initialized.
  0: :::MLLOG {"namespace": "", "time_ms": 1593107964820, "event_type": "POINT_IN_TIME", "key": "seed", "value": 18263, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 675}}
  0: :::MLLOG {"namespace": "", "time_ms": 1593107964821, "event_type": "POINT_IN_TIME", "key": "global_batch_size", "value": 9216, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 677}}
  0: :::MLLOG {"namespace": "", "time_ms": 1593107964821, "event_type": "POINT_IN_TIME", "key": "opt_gradient_accumulation_steps", "value": 1, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 679}}
  0: :::MLLOG {"namespace": "", "time_ms": 1593107964821, "event_type": "POINT_IN_TIME", "key": "max_predictions_per_seq", "value": 76, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 681}}
  0: :::MLLOG {"namespace": "", "time_ms": 1593107964821, "event_type": "POINT_IN_TIME", "key": "opt_learning_rate_training_steps", "value": 750.0, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 683}}
  0: :::MLLOG {"namespace": "", "time_ms": 1593107964821, "event_type": "POINT_IN_TIME", "key": "num_warmup_steps", "value": 0, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 686}}
  0: parsed args:
  0: Namespace(allreduce_post_accumulation=True, allreduce_post_accumulation_fp16=True, bert_config_path='/workspace/phase1/bert_config.json', bert_model='bert-large-uncased', cache_eval_data=True, checkpoint_activations=False, dense_seq_output=True, disable_apex_softmax=False, disable_fuse_mask=False, disable_fuse_qkv=False, disable_fuse_scale=False, do_train=True, enable_fuse_dropout=False, enable_stream=False, eval_batch_size=16, eval_dir='/workspace/evaldata', eval_iter_samples=500000, eval_iter_start_samples=3000000, fp16=True, fused_gelu_bias=True, fused_mha=True, gradient_accumulation_steps=1, init_checkpoint='/workspace/phase1/model.ckpt-28252.pt', init_tf_checkpoint=None, input_dir='/workspace/data_phase2', keep_n_most_recent_checkpoints=20, learning_rate=0.002, local_rank=0, log_freq=1.0, loss_scale=0.0, max_predictions_per_seq=76, max_samples_termination=7500000.0, max_seq_length=512, max_steps=750.0, min_samples_to_start_checkpoints=3000000, n_gpu=512, num_epochs_to_generate_seeds_for=2, num_eval_examp
  0: les=10000, num_samples_per_checkpoint=500000, opt_lamb_beta_1=0.86, opt_lamb_beta_2=0.975, output_dir='/results', pad=False, phase2=True, resume_from_checkpoint=False, seed=18263, skip_checkpoint=True, target_mlm_accuracy=0.712, train_batch_size=18, train_mlm_accuracy_window_size=0, unpad=False, use_env=False, warmup_proportion=0.0)
  0: :::MLLOG {"namespace": "", "time_ms": 1593107971788, "event_type": "POINT_IN_TIME", "key": "opt_base_learning_rate", "value": 0.002, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 512}}
  0: :::MLLOG {"namespace": "", "time_ms": 1593107971789, "event_type": "POINT_IN_TIME", "key": "opt_epsilon", "value": 1e-06, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 517}}
  0: :::MLLOG {"namespace": "", "time_ms": 1593107971789, "event_type": "POINT_IN_TIME", "key": "opt_lamb_beta_1", "value": 0.86, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 519}}
  0: :::MLLOG {"namespace": "", "time_ms": 1593107971790, "event_type": "POINT_IN_TIME", "key": "opt_lamb_beta_2", "value": 0.975, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 520}}
  0: :::MLLOG {"namespace": "", "time_ms": 1593107971790, "event_type": "POINT_IN_TIME", "key": "opt_lamb_weight_decay_rate", "value": 0.01, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 523}}
  0: :::MLLOG {"namespace": "", "time_ms": 1593107971790, "event_type": "POINT_IN_TIME", "key": "opt_learning_rate_warmup_steps", "value": 0, "metadata": {"file": "/workspace/bert/schedulers.py", "lineno": 85}}
  0: :::MLLOG {"namespace": "", "time_ms": 1593107971790, "event_type": "POINT_IN_TIME", "key": "opt_lamb_learning_rate_decay_poly_power", "value": 1.0, "metadata": {"file": "/workspace/bert/schedulers.py", "lineno": 86}}
  0: :::MLLOG {"namespace": "", "time_ms": 1593107971790, "event_type": "POINT_IN_TIME", "key": "start_warmup_step", "value": 0, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 531}}
  0: Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.
  0:
  0: Defaults for this optimization level are:
  0: enabled                : True
  0: opt_level              : O2
  0: cast_model_type        : torch.float16
  0: patch_torch_functions  : False
  0: keep_batchnorm_fp32    : True
  0: master_weights         : True
  0: loss_scale             : dynamic
  0: Processing user overrides (additional kwargs that are not None)...
  0: After processing overrides, optimization options are:
  0: enabled                : True
  0: opt_level              : O2
  0: cast_model_type        : torch.float16
  0: patch_torch_functions  : False
  0: keep_batchnorm_fp32    : True
  0: master_weights         : True
  0: loss_scale             : dynamic
  0: :::MLLOG {"namespace": "", "time_ms": 1593107971840, "event_type": "INTERVAL_END", "key": "init_stop", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 735}}
  0: :::MLLOG {"namespace": "", "time_ms": 1593107972656, "event_type": "INTERVAL_START", "key": "run_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 736}}
  0: :::MLLOG {"namespace": "", "time_ms": 1593107972696, "event_type": "INTERVAL_START", "key": "epoch_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 745, "epoch_num": 1}}
  0: :::MLLOG {"namespace": "", "time_ms": 1593107972697, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 749, "first_epoch_num": 1, "epoch_count": 1}}
  0: parsed args:
  0: Namespace(allreduce_post_accumulation=True, allreduce_post_accumulation_fp16=True, bert_config_path='/workspace/phase1/bert_config.json', bert_model='bert-large-uncased', cache_eval_data=True, checkpoint_activations=False, dense_seq_output=True, disable_apex_softmax=False, disable_fuse_mask=False, disable_fuse_qkv=False, disable_fuse_scale=False, do_train=True, enable_fuse_dropout=False, enable_stream=False, eval_batch_size=16, eval_dir='/workspace/evaldata', eval_iter_samples=500000, eval_iter_start_samples=3000000, fp16=True, fused_gelu_bias=True, fused_mha=True, gradient_accumulation_steps=1, init_checkpoint='/workspace/phase1/model.ckpt-28252.pt', init_tf_checkpoint=None, input_dir='/workspace/data_phase2', keep_n_most_recent_checkpoints=20, learning_rate=0.002, local_rank=0, log_freq=1.0, loss_scale=0.0, max_predictions_per_seq=76, max_samples_termination=7500000.0, max_seq_length=512, max_steps=750.0, min_samples_to_start_checkpoints=3000000, n_gpu=512, num_epochs_to_generate_seeds_for=2, num_eval_examp
  0: les=10000, num_samples_per_checkpoint=500000, opt_lamb_beta_1=0.86, opt_lamb_beta_2=0.975, output_dir='/results', pad=False, phase2=True, resume_from_checkpoint=False, resume_step=0, seed=18263, skip_checkpoint=True, target_mlm_accuracy=0.712, train_batch_size=18, train_mlm_accuracy_window_size=0, unpad=False, use_env=False, warmup_proportion=0.0)
  0: epoch: 1
  0: {'training_steps': 1, 'average_loss': 5.17578125, 'step_loss': 5.17578125, 'learning_rate': 0.0019973333333333336, 'seq/s': 1696.0580189207012, 'global_steps': 0, 'samples_trained': 0, 'skipped_steps': 1, 'timestamp': 1593107978.1319578}
  0: {'training_steps': 2, 'average_loss': 6.0546875, 'step_loss': 6.0546875, 'learning_rate': 0.0019973333333333336, 'seq/s': 25556.779499941487, 'global_steps': 0, 'samples_trained': 0, 'skipped_steps': 2, 'timestamp': 1593107978.492568}
  0: {'training_steps': 3, 'average_loss': 5.86328125, 'step_loss': 5.86328125, 'learning_rate': 0.0019973333333333336, 'seq/s': 27892.698460712523, 'global_steps': 0, 'samples_trained': 0, 'skipped_steps': 3, 'timestamp': 1593107978.822978}
  0: {'training_steps': 4, 'average_loss': 6.3125, 'step_loss': 6.3125, 'learning_rate': 0.0019973333333333336, 'seq/s': 28154.981159945983, 'global_steps': 0, 'samples_trained': 0, 'skipped_steps': 4, 'timestamp': 1593107979.15031}
  0: {'training_steps': 5, 'average_loss': 5.71484375, 'step_loss': 5.71484375, 'learning_rate': 0.0019973333333333336, 'seq/s': 28382.94591889707, 'global_steps': 0, 'samples_trained': 0, 'skipped_steps': 5, 'timestamp': 1593107979.475013}
  0: {'training_steps': 6, 'average_loss': 5.984375, 'step_loss': 5.984375, 'learning_rate': 0.0019973333333333336, 'seq/s': 28235.8267663505, 'global_steps': 0, 'samples_trained': 0, 'skipped_steps': 6, 'timestamp': 1593107979.801408}
  0: {'training_steps': 7, 'average_loss': 6.1640625, 'step_loss': 6.1640625, 'learning_rate': 0.0019973333333333336, 'seq/s': 28235.868016856184, 'global_steps': 0, 'samples_trained': 0, 'skipped_steps': 7, 'timestamp': 1593107980.1278024}
  0: {'training_steps': 8, 'average_loss': 6.05078125, 'step_loss': 6.05078125, 'learning_rate': 0.0019973333333333336, 'seq/s': 28288.25010647967, 'global_steps': 0, 'samples_trained': 0, 'skipped_steps': 8, 'timestamp': 1593107980.453592}
  0: {'training_steps': 9, 'average_loss': 6.25, 'step_loss': 6.25, 'learning_rate': 0.0019973333333333336, 'seq/s': 28197.454195839393, 'global_steps': 0, 'samples_trained': 0, 'skipped_steps': 9, 'timestamp': 1593107980.7804313}
  0: {'training_steps': 10, 'average_loss': 5.76953125, 'step_loss': 5.76953125, 'learning_rate': 0.0019973333333333336, 'seq/s': 27066.070046724424, 'global_steps': 1, 'samples_trained': 9216, 'skipped_steps': 9, 'timestamp': 1593107981.1209326}
  0: {'training_steps': 11, 'average_loss': 5.89453125, 'step_loss': 5.89453125, 'learning_rate': 0.0019946666666666667, 'seq/s': 14600.53260489385, 'global_steps': 2, 'samples_trained': 18432, 'skipped_steps': 9, 'timestamp': 1593107981.7521431}
  0: {'training_steps': 12, 'average_loss': 6.16015625, 'step_loss': 6.16015625, 'learning_rate': 0.001992, 'seq/s': 26622.81656736268, 'global_steps': 3, 'samples_trained': 27648, 'skipped_steps': 9, 'timestamp': 1593107982.0983138}
  0: {'training_steps': 13, 'average_loss': 5.0703125, 'step_loss': 5.0703125, 'learning_rate': 0.0019893333333333334, 'seq/s': 26794.181042842596, 'global_steps': 4, 'samples_trained': 36864, 'skipped_steps': 9, 'timestamp': 1593107982.4422705}
  0: {'training_steps': 14, 'average_loss': 5.296875, 'step_loss': 5.296875, 'learning_rate': 0.0019866666666666665, 'seq/s': 26681.179505550597, 'global_steps': 5, 'samples_trained': 46080, 'skipped_steps': 9, 'timestamp': 1593107982.7876837}
  0: {'training_steps': 15, 'average_loss': 4.5859375, 'step_loss': 4.5859375, 'learning_rate': 0.001984, 'seq/s': 26625.072091780225, 'global_steps': 6, 'samples_trained': 55296, 'skipped_steps': 9, 'timestamp': 1593107983.133825}
  0: {'training_steps': 16, 'average_loss': 4.84765625, 'step_loss': 4.84765625, 'learning_rate': 0.0019813333333333336, 'seq/s': 26646.85847587495, 'global_steps': 7, 'samples_trained': 64512, 'skipped_steps': 9, 'timestamp': 1593107983.4796836}
  0: {'training_steps': 17, 'average_loss': 5.0234375, 'step_loss': 5.0234375, 'learning_rate': 0.0019786666666666668, 'seq/s': 26649.85298793629, 'global_steps': 8, 'samples_trained': 73728, 'skipped_steps': 9, 'timestamp': 1593107983.8255026}
  0: {'training_steps': 18, 'average_loss': 3.65625, 'step_loss': 3.65625, 'learning_rate': 0.001976, 'seq/s': 26623.806750534648, 'global_steps': 9, 'samples_trained': 82944, 'skipped_steps': 9, 'timestamp': 1593107984.17166}
  0: {'training_steps': 19, 'average_loss': 4.30859375, 'step_loss': 4.30859375, 'learning_rate': 0.0019733333333333334, 'seq/s': 26802.11399877689, 'global_steps': 10, 'samples_trained': 92160, 'skipped_steps': 9, 'timestamp': 1593107984.5155144}
  0: {'training_steps': 20, 'average_loss': 5.921875, 'step_loss': 5.921875, 'learning_rate': 0.0019706666666666666, 'seq/s': 26602.64388041623, 'global_steps': 11, 'samples_trained': 101376, 'skipped_steps': 9, 'timestamp': 1593107984.8619473}
  0: {'training_steps': 21, 'average_loss': 4.80859375, 'step_loss': 4.80859375, 'learning_rate': 0.001968, 'seq/s': 26653.234104815478, 'global_steps': 12, 'samples_trained': 110592, 'skipped_steps': 9, 'timestamp': 1593107985.2077227}
  0: {'training_steps': 22, 'average_loss': 4.26171875, 'step_loss': 4.26171875, 'learning_rate': 0.0019653333333333333, 'seq/s': 26828.21253131023, 'global_steps': 13, 'samples_trained': 119808, 'skipped_steps': 9, 'timestamp': 1593107985.5512428}
  0: {'training_steps': 23, 'average_loss': 3.78125, 'step_loss': 3.78125, 'learning_rate': 0.001962666666666667, 'seq/s': 26908.911831657737, 'global_steps': 14, 'samples_trained': 129024, 'skipped_steps': 9, 'timestamp': 1593107985.8937328}
  0: {'training_steps': 24, 'average_loss': 4.6875, 'step_loss': 4.6875, 'learning_rate': 0.00196, 'seq/s': 23306.5078825495, 'global_steps': 15, 'samples_trained': 138240, 'skipped_steps': 9, 'timestamp': 1593107986.2891598}
  0: {'training_steps': 25, 'average_loss': 4.58203125, 'step_loss': 4.58203125, 'learning_rate': 0.0019573333333333335, 'seq/s': 26884.245360337176, 'global_steps': 16, 'samples_trained': 147456, 'skipped_steps': 9, 'timestamp': 1593107986.631964}
  0: {'training_steps': 26, 'average_loss': 4.29296875, 'step_loss': 4.29296875, 'learning_rate': 0.0019546666666666666, 'seq/s': 27076.49749406526, 'global_steps': 17, 'samples_trained': 156672, 'skipped_steps': 9, 'timestamp': 1593107986.972334}
  0: {'training_steps': 27, 'average_loss': 3.7890625, 'step_loss': 3.7890625, 'learning_rate': 0.001952, 'seq/s': 26835.94208307646, 'global_steps': 18, 'samples_trained': 165888, 'skipped_steps': 9, 'timestamp': 1593107987.3157554}
  0: {'training_steps': 28, 'average_loss': 4.328125, 'step_loss': 4.328125, 'learning_rate': 0.0019493333333333333, 'seq/s': 26298.65213806166, 'global_steps': 19, 'samples_trained': 175104, 'skipped_steps': 9, 'timestamp': 1593107987.6661925}
  0: {'training_steps': 29, 'average_loss': 3.49609375, 'step_loss': 3.49609375, 'learning_rate': 0.0019466666666666669, 'seq/s': 13230.780963959838, 'global_steps': 20, 'samples_trained': 184320, 'skipped_steps': 9, 'timestamp': 1593107988.3627512}
  0: {'training_steps': 30, 'average_loss': 4.2734375, 'step_loss': 4.2734375, 'learning_rate': 0.001944, 'seq/s': 9928.356471164781, 'global_steps': 21, 'samples_trained': 193536, 'skipped_steps': 9, 'timestamp': 1593107989.2910023}
  0: {'training_steps': 31, 'average_loss': 4.19921875, 'step_loss': 4.19921875, 'learning_rate': 0.0019413333333333333, 'seq/s': 20123.34167356885, 'global_steps': 22, 'samples_trained': 202752, 'skipped_steps': 9, 'timestamp': 1593107989.7489786}
  0: {'training_steps': 32, 'average_loss': 4.09375, 'step_loss': 4.09375, 'learning_rate': 0.0019386666666666669, 'seq/s': 16846.77475357237, 'global_steps': 23, 'samples_trained': 211968, 'skipped_steps': 9, 'timestamp': 1593107990.296028}
  0: {'training_steps': 33, 'average_loss': 4.796875, 'step_loss': 4.796875, 'learning_rate': 0.001936, 'seq/s': 27156.639017337384, 'global_steps': 24, 'samples_trained': 221184, 'skipped_steps': 9, 'timestamp': 1593107990.6353936}
  0: {'training_steps': 34, 'average_loss': 4.37109375, 'step_loss': 4.37109375, 'learning_rate': 0.0019333333333333333, 'seq/s': 15149.321465131408, 'global_steps': 25, 'samples_trained': 230400, 'skipped_steps': 9, 'timestamp': 1593107991.243739}
  0: {'training_steps': 35, 'average_loss': 4.55859375, 'step_loss': 4.55859375, 'learning_rate': 0.0019306666666666667, 'seq/s': 14226.482940051783, 'global_steps': 26, 'samples_trained': 239616, 'skipped_steps': 9, 'timestamp': 1593107991.8915458}
  0: {'training_steps': 36, 'average_loss': 4.18359375, 'step_loss': 4.18359375, 'learning_rate': 0.001928, 'seq/s': 27092.685419671732, 'global_steps': 27, 'samples_trained': 248832, 'skipped_steps': 9, 'timestamp': 1593107992.2317128}
  0: {'training_steps': 37, 'average_loss': 4.5625, 'step_loss': 4.5625, 'learning_rate': 0.0019253333333333334, 'seq/s': 14043.331633322205, 'global_steps': 28, 'samples_trained': 258048, 'skipped_steps': 9, 'timestamp': 1593107992.8879683}
  0: {'training_steps': 38, 'average_loss': 4.18359375, 'step_loss': 4.18359375, 'learning_rate': 0.0019226666666666667, 'seq/s': 24779.928242473972, 'global_steps': 29, 'samples_trained': 267264, 'skipped_steps': 9, 'timestamp': 1593107993.2598832}
  0: {'training_steps': 39, 'average_loss': 4.34375, 'step_loss': 4.34375, 'learning_rate': 0.00192, 'seq/s': 26915.20733954153, 'global_steps': 30, 'samples_trained': 276480, 'skipped_steps': 9, 'timestamp': 1593107993.6022928}
  0: {'training_steps': 40, 'average_loss': 4.1796875, 'step_loss': 4.1796875, 'learning_rate': 0.0019173333333333334, 'seq/s': 27034.17407469056, 'global_steps': 31, 'samples_trained': 285696, 'skipped_steps': 9, 'timestamp': 1593107993.9431956}
  0: {'training_steps': 41, 'average_loss': 4.23828125, 'step_loss': 4.23828125, 'learning_rate': 0.0019146666666666667, 'seq/s': 27156.295604377076, 'global_steps': 32, 'samples_trained': 294912, 'skipped_steps': 9, 'timestamp': 1593107994.2825654}
  0: {'training_steps': 42, 'average_loss': 4.03515625, 'step_loss': 4.03515625, 'learning_rate': 0.0019119999999999999, 'seq/s': 27135.497167792084, 'global_steps': 33, 'samples_trained': 304128, 'skipped_steps': 9, 'timestamp': 1593107994.622195}
  0: {'training_steps': 43, 'average_loss': 4.02734375, 'step_loss': 4.02734375, 'learning_rate': 0.0019093333333333334, 'seq/s': 27057.80963000762, 'global_steps': 34, 'samples_trained': 313344, 'skipped_steps': 9, 'timestamp': 1593107994.9628005}
  0: {'training_steps': 44, 'average_loss': 4.265625, 'step_loss': 4.265625, 'learning_rate': 0.0019066666666666668, 'seq/s': 26988.703592586517, 'global_steps': 35, 'samples_trained': 322560, 'skipped_steps': 9, 'timestamp': 1593107995.3042774}
  0: {'training_steps': 45, 'average_loss': 3.962890625, 'step_loss': 3.962890625, 'learning_rate': 0.0019039999999999999, 'seq/s': 27131.878332024284, 'global_steps': 36, 'samples_trained': 331776, 'skipped_steps': 9, 'timestamp': 1593107995.6439524}
  0: {'training_steps': 46, 'average_loss': 4.03515625, 'step_loss': 4.03515625, 'learning_rate': 0.0019013333333333334, 'seq/s': 27056.843717976717, 'global_steps': 37, 'samples_trained': 340992, 'skipped_steps': 9, 'timestamp': 1593107995.9845698}
  0: {'training_steps': 47, 'average_loss': 4.05859375, 'step_loss': 4.05859375, 'learning_rate': 0.0018986666666666668, 'seq/s': 27089.932296729352, 'global_steps': 38, 'samples_trained': 350208, 'skipped_steps': 9, 'timestamp': 1593107996.324771}
  0: {'training_steps': 48, 'average_loss': 3.923828125, 'step_loss': 3.923828125, 'learning_rate': 0.001896, 'seq/s': 27073.330492555564, 'global_steps': 39, 'samples_trained': 359424, 'skipped_steps': 9, 'timestamp': 1593107996.6651807}
  0: {'training_steps': 49, 'average_loss': 4.0234375, 'step_loss': 4.0234375, 'learning_rate': 0.0018933333333333335, 'seq/s': 27080.993253344954, 'global_steps': 40, 'samples_trained': 368640, 'skipped_steps': 9, 'timestamp': 1593107997.0054944}
  0: {'training_steps': 50, 'average_loss': 3.884765625, 'step_loss': 3.884765625, 'learning_rate': 0.0018906666666666668, 'seq/s': 27045.52306880691, 'global_steps': 41, 'samples_trained': 377856, 'skipped_steps': 9, 'timestamp': 1593107997.346254}
  0: {'training_steps': 51, 'average_loss': 4.19921875, 'step_loss': 4.19921875, 'learning_rate': 0.001888, 'seq/s': 27010.164552583752, 'global_steps': 42, 'samples_trained': 387072, 'skipped_steps': 9, 'timestamp': 1593107997.68746}
  0: {'training_steps': 52, 'average_loss': 3.6328125, 'step_loss': 3.6328125, 'learning_rate': 0.0018853333333333333, 'seq/s': 27055.87787490577, 'global_steps': 43, 'samples_trained': 396288, 'skipped_steps': 9, 'timestamp': 1593107998.0280893}
  0: {'training_steps': 53, 'average_loss': 3.8203125, 'step_loss': 3.8203125, 'learning_rate': 0.0018826666666666668, 'seq/s': 26856.61955160116, 'global_steps': 44, 'samples_trained': 405504, 'skipped_steps': 9, 'timestamp': 1593107998.3712459}
  0: {'training_steps': 54, 'average_loss': 3.529296875, 'step_loss': 3.529296875, 'learning_rate': 0.00188, 'seq/s': 27235.31808529077, 'global_steps': 45, 'samples_trained': 414720, 'skipped_steps': 9, 'timestamp': 1593107998.7096307}
  0: {'training_steps': 55, 'average_loss': 3.828125, 'step_loss': 3.828125, 'learning_rate': 0.0018773333333333333, 'seq/s': 27139.53616631386, 'global_steps': 46, 'samples_trained': 423936, 'skipped_steps': 9, 'timestamp': 1593107999.04921}
  0: {'training_steps': 56, 'average_loss': 3.0625, 'step_loss': 3.0625, 'learning_rate': 0.0018746666666666668, 'seq/s': 27082.73884488139, 'global_steps': 47, 'samples_trained': 433152, 'skipped_steps': 9, 'timestamp': 1593107999.3895013}
  0: {'training_steps': 57, 'average_loss': 4.19921875, 'step_loss': 4.19921875, 'learning_rate': 0.001872, 'seq/s': 27186.778680845677, 'global_steps': 48, 'samples_trained': 442368, 'skipped_steps': 9, 'timestamp': 1593107999.7284908}
  0: {'training_steps': 58, 'average_loss': 3.3515625, 'step_loss': 3.3515625, 'learning_rate': 0.0018693333333333333, 'seq/s': 27020.17120524652, 'global_steps': 49, 'samples_trained': 451584, 'skipped_steps': 9, 'timestamp': 1593108000.0695705}
  0: {'training_steps': 59, 'average_loss': 4.046875, 'step_loss': 4.046875, 'learning_rate': 0.0018666666666666666, 'seq/s': 27090.938545747627, 'global_steps': 50, 'samples_trained': 460800, 'skipped_steps': 9, 'timestamp': 1593108000.4097595}
  0: {'training_steps': 60, 'average_loss': 3.431640625, 'step_loss': 3.431640625, 'learning_rate': 0.001864, 'seq/s': 27100.378142016736, 'global_steps': 51, 'samples_trained': 470016, 'skipped_steps': 9, 'timestamp': 1593108000.7498295}
  0: {'training_steps': 61, 'average_loss': 4.34765625, 'step_loss': 4.34765625, 'learning_rate': 0.0018613333333333333, 'seq/s': 27088.926122459277, 'global_steps': 52, 'samples_trained': 479232, 'skipped_steps': 9, 'timestamp': 1593108001.0900433}
  0: {'training_steps': 62, 'average_loss': 3.931640625, 'step_loss': 3.931640625, 'learning_rate': 0.0018586666666666667, 'seq/s': 27022.21120617639, 'global_steps': 53, 'samples_trained': 488448, 'skipped_steps': 9, 'timestamp': 1593108001.431097}
  0: {'training_steps': 63, 'average_loss': 3.951171875, 'step_loss': 3.951171875, 'learning_rate': 0.0018560000000000002, 'seq/s': 27172.61731764051, 'global_steps': 54, 'samples_trained': 497664, 'skipped_steps': 9, 'timestamp': 1593108001.7702627}
  0: {'training_steps': 64, 'average_loss': 3.5546875, 'step_loss': 3.5546875, 'learning_rate': 0.0018533333333333334, 'seq/s': 27091.8309423235, 'global_steps': 55, 'samples_trained': 506880, 'skipped_steps': 9, 'timestamp': 1593108002.1104405}
  0: {'training_steps': 65, 'average_loss': 4.03125, 'step_loss': 4.03125, 'learning_rate': 0.0018506666666666667, 'seq/s': 26985.952721339905, 'global_steps': 56, 'samples_trained': 516096, 'skipped_steps': 9, 'timestamp': 1593108002.4519527}
  0: {'training_steps': 66, 'average_loss': 3.80859375, 'step_loss': 3.80859375, 'learning_rate': 0.001848, 'seq/s': 27053.775984210635, 'global_steps': 57, 'samples_trained': 525312, 'skipped_steps': 9, 'timestamp': 1593108002.7926083}
  0: {'training_steps': 67, 'average_loss': 4.64453125, 'step_loss': 4.64453125, 'learning_rate': 0.0018453333333333334, 'seq/s': 27044.12284442952, 'global_steps': 58, 'samples_trained': 534528, 'skipped_steps': 9, 'timestamp': 1593108003.1333857}
  0: {'training_steps': 68, 'average_loss': 2.75, 'step_loss': 2.75, 'learning_rate': 0.0018426666666666667, 'seq/s': 27108.170287311626, 'global_steps': 59, 'samples_trained': 543744, 'skipped_steps': 9, 'timestamp': 1593108003.4733582}
  0: {'training_steps': 69, 'average_loss': 4.265625, 'step_loss': 4.265625, 'learning_rate': 0.00184, 'seq/s': 26988.17598477396, 'global_steps': 60, 'samples_trained': 552960, 'skipped_steps': 9, 'timestamp': 1593108003.8148417}
  0: {'training_steps': 70, 'average_loss': 3.70703125, 'step_loss': 3.70703125, 'learning_rate': 0.0018373333333333334, 'seq/s': 27064.98984328723, 'global_steps': 61, 'samples_trained': 562176, 'skipped_steps': 9, 'timestamp': 1593108004.1553566}
  0: {'training_steps': 71, 'average_loss': 3.91796875, 'step_loss': 3.91796875, 'learning_rate': 0.0018346666666666667, 'seq/s': 27092.30564530712, 'global_steps': 62, 'samples_trained': 571392, 'skipped_steps': 9, 'timestamp': 1593108004.495528}
  0: {'training_steps': 72, 'average_loss': 3.845703125, 'step_loss': 3.845703125, 'learning_rate': 0.001832, 'seq/s': 27064.47819806826, 'global_steps': 63, 'samples_trained': 580608, 'skipped_steps': 9, 'timestamp': 1593108004.8360488}
  0: {'training_steps': 73, 'average_loss': 3.66796875, 'step_loss': 3.66796875, 'learning_rate': 0.0018293333333333332, 'seq/s': 27035.157277202172, 'global_steps': 64, 'samples_trained': 589824, 'skipped_steps': 9, 'timestamp': 1593108005.1769404}
  0: {'training_steps': 74, 'average_loss': 3.8671875, 'step_loss': 3.8671875, 'learning_rate': 0.0018266666666666668, 'seq/s': 27110.565846809233, 'global_steps': 65, 'samples_trained': 599040, 'skipped_steps': 9, 'timestamp': 1593108005.5168827}
  0: {'training_steps': 75, 'average_loss': 3.80078125, 'step_loss': 3.80078125, 'learning_rate': 0.001824, 'seq/s': 27071.71883016788, 'global_steps': 66, 'samples_trained': 608256, 'skipped_steps': 9, 'timestamp': 1593108005.8573127}
  0: {'training_steps': 76, 'average_loss': 3.30078125, 'step_loss': 3.30078125, 'learning_rate': 0.0018213333333333332, 'seq/s': 27041.114744602753, 'global_steps': 67, 'samples_trained': 617472, 'skipped_steps': 9, 'timestamp': 1593108006.1981282}
  0: {'training_steps': 77, 'average_loss': 3.529296875, 'step_loss': 3.529296875, 'learning_rate': 0.0018186666666666668, 'seq/s': 27102.620300301143, 'global_steps': 68, 'samples_trained': 626688, 'skipped_steps': 9, 'timestamp': 1593108006.53817}
  0: {'training_steps': 78, 'average_loss': 4.05859375, 'step_loss': 4.05859375, 'learning_rate': 0.0018160000000000001, 'seq/s': 27066.922899820463, 'global_steps': 69, 'samples_trained': 635904, 'skipped_steps': 9, 'timestamp': 1593108006.8786612}
  0: {'training_steps': 79, 'average_loss': 4.078125, 'step_loss': 4.078125, 'learning_rate': 0.0018133333333333332, 'seq/s': 27079.968771804954, 'global_steps': 70, 'samples_trained': 645120, 'skipped_steps': 9, 'timestamp': 1593108007.2189875}
  0: {'training_steps': 80, 'average_loss': 4.2578125, 'step_loss': 4.2578125, 'learning_rate': 0.0018106666666666666, 'seq/s': 27132.125905814904, 'global_steps': 71, 'samples_trained': 654336, 'skipped_steps': 9, 'timestamp': 1593108007.5586596}
  0: {'training_steps': 81, 'average_loss': 4.41015625, 'step_loss': 4.41015625, 'learning_rate': 0.0018080000000000001, 'seq/s': 27182.840302497985, 'global_steps': 72, 'samples_trained': 663552, 'skipped_steps': 9, 'timestamp': 1593108007.8976982}
  0: {'training_steps': 82, 'average_loss': 4.0546875, 'step_loss': 4.0546875, 'learning_rate': 0.0018053333333333335, 'seq/s': 27048.323952591076, 'global_steps': 73, 'samples_trained': 672768, 'skipped_steps': 9, 'timestamp': 1593108008.2384233}
  0: {'training_steps': 83, 'average_loss': 4.03515625, 'step_loss': 4.03515625, 'learning_rate': 0.0018026666666666666, 'seq/s': 27098.97223201748, 'global_steps': 74, 'samples_trained': 681984, 'skipped_steps': 9, 'timestamp': 1593108008.5785117}
  0: {'training_steps': 84, 'average_loss': 3.462890625, 'step_loss': 3.462890625, 'learning_rate': 0.0018000000000000002, 'seq/s': 26984.803550809724, 'global_steps': 75, 'samples_trained': 691200, 'skipped_steps': 9, 'timestamp': 1593108008.9200385}
  0: {'training_steps': 85, 'average_loss': 3.623046875, 'step_loss': 3.623046875, 'learning_rate': 0.0017973333333333335, 'seq/s': 27048.096832294224, 'global_steps': 76, 'samples_trained': 700416, 'skipped_steps': 9, 'timestamp': 1593108009.2607665}
  0: {'training_steps': 86, 'average_loss': 4.1015625, 'step_loss': 4.1015625, 'learning_rate': 0.0017946666666666666, 'seq/s': 27070.82775805198, 'global_steps': 77, 'samples_trained': 709632, 'skipped_steps': 9, 'timestamp': 1593108009.6012075}
  0: {'training_steps': 87, 'average_loss': 4.109375, 'step_loss': 4.109375, 'learning_rate': 0.001792, 'seq/s': 26922.762088171934, 'global_steps': 78, 'samples_trained': 718848, 'skipped_steps': 9, 'timestamp': 1593108009.943521}
  0: {'training_steps': 88, 'average_loss': 3.140625, 'step_loss': 3.140625, 'learning_rate': 0.0017893333333333335, 'seq/s': 27103.741518578354, 'global_steps': 79, 'samples_trained': 728064, 'skipped_steps': 9, 'timestamp': 1593108010.2835488}
  0: {'training_steps': 89, 'average_loss': 3.57421875, 'step_loss': 3.57421875, 'learning_rate': 0.0017866666666666667, 'seq/s': 27116.023979853107, 'global_steps': 80, 'samples_trained': 737280, 'skipped_steps': 9, 'timestamp': 1593108010.6234226}
  0: {'training_steps': 90, 'average_loss': 3.494140625, 'step_loss': 3.494140625, 'learning_rate': 0.001784, 'seq/s': 27051.82587323371, 'global_steps': 81, 'samples_trained': 746496, 'skipped_steps': 9, 'timestamp': 1593108010.964103}
  0: {'training_steps': 91, 'average_loss': 4.25390625, 'step_loss': 4.25390625, 'learning_rate': 0.0017813333333333336, 'seq/s': 27143.481065776836, 'global_steps': 82, 'samples_trained': 755712, 'skipped_steps': 9, 'timestamp': 1593108011.303633}
  0: {'training_steps': 92, 'average_loss': 3.712890625, 'step_loss': 3.712890625, 'learning_rate': 0.0017786666666666667, 'seq/s': 27070.6950502025, 'global_steps': 83, 'samples_trained': 764928, 'skipped_steps': 9, 'timestamp': 1593108011.644076}
  0: {'training_steps': 93, 'average_loss': 4.09375, 'step_loss': 4.09375, 'learning_rate': 0.001776, 'seq/s': 27036.386380915545, 'global_steps': 84, 'samples_trained': 774144, 'skipped_steps': 9, 'timestamp': 1593108011.9849513}
  0: {'training_steps': 94, 'average_loss': 3.6328125, 'step_loss': 3.6328125, 'learning_rate': 0.0017733333333333334, 'seq/s': 26963.081929889177, 'global_steps': 85, 'samples_trained': 783360, 'skipped_steps': 9, 'timestamp': 1593108012.3267534}
  0: {'training_steps': 95, 'average_loss': 3.5390625, 'step_loss': 3.5390625, 'learning_rate': 0.0017706666666666667, 'seq/s': 27030.77123353002, 'global_steps': 86, 'samples_trained': 792576, 'skipped_steps': 9, 'timestamp': 1593108012.6676989}
  0: {'training_steps': 96, 'average_loss': 3.900390625, 'step_loss': 3.900390625, 'learning_rate': 0.001768, 'seq/s': 27074.032099659533, 'global_steps': 87, 'samples_trained': 801792, 'skipped_steps': 9, 'timestamp': 1593108013.0080998}
  0: {'training_steps': 97, 'average_loss': 4.45703125, 'step_loss': 4.45703125, 'learning_rate': 0.0017653333333333334, 'seq/s': 27038.33426644451, 'global_steps': 88, 'samples_trained': 811008, 'skipped_steps': 9, 'timestamp': 1593108013.3489506}
  0: {'training_steps': 98, 'average_loss': 3.685546875, 'step_loss': 3.685546875, 'learning_rate': 0.0017626666666666667, 'seq/s': 27085.946007078626, 'global_steps': 89, 'samples_trained': 820224, 'skipped_steps': 9, 'timestamp': 1593108013.689202}
  0: {'training_steps': 99, 'average_loss': 3.46484375, 'step_loss': 3.46484375, 'learning_rate': 0.00176, 'seq/s': 27039.27994314398, 'global_steps': 90, 'samples_trained': 829440, 'skipped_steps': 9, 'timestamp': 1593108014.0300405}
  0: {'training_steps': 100, 'average_loss': 3.5078125, 'step_loss': 3.5078125, 'learning_rate': 0.0017573333333333334, 'seq/s': 27028.59764456791, 'global_steps': 91, 'samples_trained': 838656, 'skipped_steps': 9, 'timestamp': 1593108014.3710136}
  0: {'training_steps': 101, 'average_loss': 3.412109375, 'step_loss': 3.412109375, 'learning_rate': 0.0017546666666666665, 'seq/s': 27043.252507755187, 'global_steps': 92, 'samples_trained': 847872, 'skipped_steps': 9, 'timestamp': 1593108014.7118018}
  0: {'training_steps': 102, 'average_loss': 3.828125, 'step_loss': 3.828125, 'learning_rate': 0.001752, 'seq/s': 26988.96740422876, 'global_steps': 93, 'samples_trained': 857088, 'skipped_steps': 9, 'timestamp': 1593108015.0532758}
  0: {'training_steps': 103, 'average_loss': 3.93359375, 'step_loss': 3.93359375, 'learning_rate': 0.0017493333333333334, 'seq/s': 26938.991945072055, 'global_steps': 94, 'samples_trained': 866304, 'skipped_steps': 9, 'timestamp': 1593108015.3953831}
  0: {'training_steps': 104, 'average_loss': 3.640625, 'step_loss': 3.640625, 'learning_rate': 0.0017466666666666665, 'seq/s': 26948.983497935675, 'global_steps': 95, 'samples_trained': 875520, 'skipped_steps': 9, 'timestamp': 1593108015.737364}
  0: {'training_steps': 105, 'average_loss': 3.474609375, 'step_loss': 3.474609375, 'learning_rate': 0.001744, 'seq/s': 26947.950193144134, 'global_steps': 96, 'samples_trained': 884736, 'skipped_steps': 9, 'timestamp': 1593108016.0793574}
  0: {'training_steps': 106, 'average_loss': 4.12890625, 'step_loss': 4.12890625, 'learning_rate': 0.0017413333333333334, 'seq/s': 26984.050049493824, 'global_steps': 97, 'samples_trained': 893952, 'skipped_steps': 9, 'timestamp': 1593108016.4208932}
  0: {'training_steps': 107, 'average_loss': 3.814453125, 'step_loss': 3.814453125, 'learning_rate': 0.0017386666666666666, 'seq/s': 27046.658492316605, 'global_steps': 98, 'samples_trained': 903168, 'skipped_steps': 9, 'timestamp': 1593108016.7616384}
  0: {'training_steps': 108, 'average_loss': 3.59765625, 'step_loss': 3.59765625, 'learning_rate': 0.0017360000000000001, 'seq/s': 27045.579837718287, 'global_steps': 99, 'samples_trained': 912384, 'skipped_steps': 9, 'timestamp': 1593108017.102398}
  0: {'training_steps': 109, 'average_loss': 3.853515625, 'step_loss': 3.853515625, 'learning_rate': 0.0017333333333333335, 'seq/s': 27021.115611829682, 'global_steps': 100, 'samples_trained': 921600, 'skipped_steps': 9, 'timestamp': 1593108017.4434652}
  0: {'training_steps': 110, 'average_loss': 3.802734375, 'step_loss': 3.802734375, 'learning_rate': 0.0017306666666666666, 'seq/s': 27071.093177654395, 'global_steps': 101, 'samples_trained': 930816, 'skipped_steps': 9, 'timestamp': 1593108017.7839031}
  0: {'training_steps': 111, 'average_loss': 4.19921875, 'step_loss': 4.19921875, 'learning_rate': 0.001728, 'seq/s': 26972.22559293295, 'global_steps': 102, 'samples_trained': 940032, 'skipped_steps': 9, 'timestamp': 1593108018.1255887}
  0: {'training_steps': 112, 'average_loss': 3.94921875, 'step_loss': 3.94921875, 'learning_rate': 0.0017253333333333335, 'seq/s': 26946.015308213737, 'global_steps': 103, 'samples_trained': 949248, 'skipped_steps': 9, 'timestamp': 1593108018.4676065}
  0: {'training_steps': 113, 'average_loss': 3.482421875, 'step_loss': 3.482421875, 'learning_rate': 0.0017226666666666666, 'seq/s': 27015.317996930484, 'global_steps': 104, 'samples_trained': 958464, 'skipped_steps': 9, 'timestamp': 1593108018.8087473}
  0: {'training_steps': 114, 'average_loss': 3.646484375, 'step_loss': 3.646484375, 'learning_rate': 0.00172, 'seq/s': 27092.45755377531, 'global_steps': 105, 'samples_trained': 967680, 'skipped_steps': 9, 'timestamp': 1593108019.148917}
  0: {'training_steps': 115, 'average_loss': 3.197265625, 'step_loss': 3.197265625, 'learning_rate': 0.0017173333333333335, 'seq/s': 27041.190411864496, 'global_steps': 106, 'samples_trained': 976896, 'skipped_steps': 9, 'timestamp': 1593108019.4897316}
  0: {'training_steps': 116, 'average_loss': 3.92578125, 'step_loss': 3.92578125, 'learning_rate': 0.0017146666666666666, 'seq/s': 27003.541583884515, 'global_steps': 107, 'samples_trained': 986112, 'skipped_steps': 9, 'timestamp': 1593108019.831021}
  0: {'training_steps': 117, 'average_loss': 3.568359375, 'step_loss': 3.568359375, 'learning_rate': 0.001712, 'seq/s': 27010.10793224887, 'global_steps': 108, 'samples_trained': 995328, 'skipped_steps': 9, 'timestamp': 1593108020.1722276}
  0: {'training_steps': 118, 'average_loss': 3.337890625, 'step_loss': 3.337890625, 'learning_rate': 0.0017093333333333333, 'seq/s': 27133.154337617485, 'global_steps': 109, 'samples_trained': 1004544, 'skipped_steps': 9, 'timestamp': 1593108020.5118868}
  0: {'training_steps': 119, 'average_loss': 3.353515625, 'step_loss': 3.353515625, 'learning_rate': 0.0017066666666666667, 'seq/s': 27197.605539896456, 'global_steps': 110, 'samples_trained': 1013760, 'skipped_steps': 9, 'timestamp': 1593108020.8507414}
  0: {'training_steps': 120, 'average_loss': 3.572265625, 'step_loss': 3.572265625, 'learning_rate': 0.001704, 'seq/s': 27127.784497568973, 'global_steps': 111, 'samples_trained': 1022976, 'skipped_steps': 9, 'timestamp': 1593108021.1904683}
  0: {'training_steps': 121, 'average_loss': 3.470703125, 'step_loss': 3.470703125, 'learning_rate': 0.0017013333333333333, 'seq/s': 27081.61936355783, 'global_steps': 112, 'samples_trained': 1032192, 'skipped_steps': 9, 'timestamp': 1593108021.530774}
  0: {'training_steps': 122, 'average_loss': 4.125, 'step_loss': 4.125, 'learning_rate': 0.0016986666666666667, 'seq/s': 27012.01428073282, 'global_steps': 113, 'samples_trained': 1041408, 'skipped_steps': 9, 'timestamp': 1593108021.8719568}
  0: {'training_steps': 123, 'average_loss': 3.9453125, 'step_loss': 3.9453125, 'learning_rate': 0.001696, 'seq/s': 27105.41401652633, 'global_steps': 114, 'samples_trained': 1050624, 'skipped_steps': 9, 'timestamp': 1593108022.2119634}
  0: {'training_steps': 124, 'average_loss': 3.501953125, 'step_loss': 3.501953125, 'learning_rate': 0.0016933333333333334, 'seq/s': 27012.995872022206, 'global_steps': 115, 'samples_trained': 1059840, 'skipped_steps': 9, 'timestamp': 1593108022.5531335}
  0: {'training_steps': 125, 'average_loss': 3.50390625, 'step_loss': 3.50390625, 'learning_rate': 0.0016906666666666665, 'seq/s': 27051.27686358159, 'global_steps': 116, 'samples_trained': 1069056, 'skipped_steps': 9, 'timestamp': 1593108022.8938208}
  0: {'training_steps': 126, 'average_loss': 3.33203125, 'step_loss': 3.33203125, 'learning_rate': 0.001688, 'seq/s': 26910.054547315205, 'global_steps': 117, 'samples_trained': 1078272, 'skipped_steps': 9, 'timestamp': 1593108023.236296}
  0: {'training_steps': 127, 'average_loss': 4.26953125, 'step_loss': 4.26953125, 'learning_rate': 0.0016853333333333334, 'seq/s': 27031.28160509203, 'global_steps': 118, 'samples_trained': 1087488, 'skipped_steps': 9, 'timestamp': 1593108023.5772352}
  0: {'training_steps': 128, 'average_loss': 3.923828125, 'step_loss': 3.923828125, 'learning_rate': 0.0016826666666666665, 'seq/s': 27039.41234316077, 'global_steps': 119, 'samples_trained': 1096704, 'skipped_steps': 9, 'timestamp': 1593108023.9180717}
  0: {'training_steps': 129, 'average_loss': 3.28515625, 'step_loss': 3.28515625, 'learning_rate': 0.00168, 'seq/s': 27017.395032200257, 'global_steps': 120, 'samples_trained': 1105920, 'skipped_steps': 9, 'timestamp': 1593108024.2591867}
  0: {'training_steps': 130, 'average_loss': 3.818359375, 'step_loss': 3.818359375, 'learning_rate': 0.0016773333333333334, 'seq/s': 27001.99480563026, 'global_steps': 121, 'samples_trained': 1115136, 'skipped_steps': 9, 'timestamp': 1593108024.6004958}
  0: {'training_steps': 131, 'average_loss': 3.689453125, 'step_loss': 3.689453125, 'learning_rate': 0.0016746666666666668, 'seq/s': 27001.391232915754, 'global_steps': 122, 'samples_trained': 1124352, 'skipped_steps': 9, 'timestamp': 1593108024.9418125}
  0: {'training_steps': 132, 'average_loss': 3.455078125, 'step_loss': 3.455078125, 'learning_rate': 0.0016719999999999999, 'seq/s': 27050.614295401534, 'global_steps': 123, 'samples_trained': 1133568, 'skipped_steps': 9, 'timestamp': 1593108025.2825086}
  0: {'training_steps': 133, 'average_loss': 3.55859375, 'step_loss': 3.55859375, 'learning_rate': 0.0016693333333333334, 'seq/s': 27088.793237105656, 'global_steps': 124, 'samples_trained': 1142784, 'skipped_steps': 9, 'timestamp': 1593108025.622724}
  0: {'training_steps': 134, 'average_loss': 3.494140625, 'step_loss': 3.494140625, 'learning_rate': 0.0016666666666666668, 'seq/s': 27012.052032924836, 'global_steps': 125, 'samples_trained': 1152000, 'skipped_steps': 9, 'timestamp': 1593108025.9639058}
  0: {'training_steps': 135, 'average_loss': 3.642578125, 'step_loss': 3.642578125, 'learning_rate': 0.001664, 'seq/s': 27022.759036666783, 'global_steps': 126, 'samples_trained': 1161216, 'skipped_steps': 9, 'timestamp': 1593108026.3049529}
  0: {'training_steps': 136, 'average_loss': 3.208984375, 'step_loss': 3.208984375, 'learning_rate': 0.0016613333333333335, 'seq/s': 25240.493794141, 'global_steps': 127, 'samples_trained': 1170432, 'skipped_steps': 9, 'timestamp': 1593108026.6700816}
  0: {'training_steps': 137, 'average_loss': 3.71484375, 'step_loss': 3.71484375, 'learning_rate': 0.0016586666666666668, 'seq/s': 24792.754382780466, 'global_steps': 128, 'samples_trained': 1179648, 'skipped_steps': 9, 'timestamp': 1593108027.041804}
  0: {'training_steps': 138, 'average_loss': 3.498046875, 'step_loss': 3.498046875, 'learning_rate': 0.0016560000000000001, 'seq/s': 26950.69332171306, 'global_steps': 129, 'samples_trained': 1188864, 'skipped_steps': 9, 'timestamp': 1593108027.3837628}
  0: {'training_steps': 139, 'average_loss': 3.3984375, 'step_loss': 3.3984375, 'learning_rate': 0.0016533333333333333, 'seq/s': 26996.507059093852, 'global_steps': 130, 'samples_trained': 1198080, 'skipped_steps': 9, 'timestamp': 1593108027.725141}
  0: {'training_steps': 140, 'average_loss': 2.8984375, 'step_loss': 2.8984375, 'learning_rate': 0.0016506666666666668, 'seq/s': 24493.574254019088, 'global_steps': 131, 'samples_trained': 1207296, 'skipped_steps': 9, 'timestamp': 1593108028.101404}
  0: {'training_steps': 141, 'average_loss': 3.44140625, 'step_loss': 3.44140625, 'learning_rate': 0.0016480000000000002, 'seq/s': 24245.261719102815, 'global_steps': 132, 'samples_trained': 1216512, 'skipped_steps': 9, 'timestamp': 1593108028.4815202}
  0: {'training_steps': 142, 'average_loss': 3.81640625, 'step_loss': 3.81640625, 'learning_rate': 0.0016453333333333333, 'seq/s': 27006.390383122678, 'global_steps': 133, 'samples_trained': 1225728, 'skipped_steps': 9, 'timestamp': 1593108028.822774}
  0: {'training_steps': 143, 'average_loss': 3.490234375, 'step_loss': 3.490234375, 'learning_rate': 0.0016426666666666668, 'seq/s': 26979.680571550616, 'global_steps': 134, 'samples_trained': 1234944, 'skipped_steps': 9, 'timestamp': 1593108029.164365}
  0: {'training_steps': 144, 'average_loss': 3.814453125, 'step_loss': 3.814453125, 'learning_rate': 0.0016400000000000002, 'seq/s': 24510.39435308679, 'global_steps': 135, 'samples_trained': 1244160, 'skipped_steps': 9, 'timestamp': 1593108029.54037}
  0: {'training_steps': 145, 'average_loss': 3.533203125, 'step_loss': 3.533203125, 'learning_rate': 0.0016373333333333333, 'seq/s': 24208.076223924687, 'global_steps': 136, 'samples_trained': 1253376, 'skipped_steps': 9, 'timestamp': 1593108029.9210706}
  0: {'training_steps': 146, 'average_loss': 2.88671875, 'step_loss': 2.88671875, 'learning_rate': 0.0016346666666666666, 'seq/s': 27166.907260025582, 'global_steps': 137, 'samples_trained': 1262592, 'skipped_steps': 9, 'timestamp': 1593108030.260308}
  0: {'training_steps': 147, 'average_loss': 3.455078125, 'step_loss': 3.455078125, 'learning_rate': 0.0016320000000000002, 'seq/s': 26974.23953697882, 'global_steps': 138, 'samples_trained': 1271808, 'skipped_steps': 9, 'timestamp': 1593108030.601968}
  0: {'training_steps': 148, 'average_loss': 3.615234375, 'step_loss': 3.615234375, 'learning_rate': 0.0016293333333333333, 'seq/s': 27044.103923471353, 'global_steps': 139, 'samples_trained': 1281024, 'skipped_steps': 9, 'timestamp': 1593108030.942746}
  0: {'training_steps': 149, 'average_loss': 3.58984375, 'step_loss': 3.58984375, 'learning_rate': 0.0016266666666666667, 'seq/s': 27008.239594359773, 'global_steps': 140, 'samples_trained': 1290240, 'skipped_steps': 9, 'timestamp': 1593108031.283976}
  0: {'training_steps': 150, 'average_loss': 2.927734375, 'step_loss': 2.927734375, 'learning_rate': 0.0016240000000000002, 'seq/s': 26973.03489878842, 'global_steps': 141, 'samples_trained': 1299456, 'skipped_steps': 9, 'timestamp': 1593108031.6256516}
  0: {'training_steps': 151, 'average_loss': 3.5625, 'step_loss': 3.5625, 'learning_rate': 0.0016213333333333334, 'seq/s': 26924.59986417444, 'global_steps': 142, 'samples_trained': 1308672, 'skipped_steps': 9, 'timestamp': 1593108031.967942}
  0: {'training_steps': 152, 'average_loss': 3.61328125, 'step_loss': 3.61328125, 'learning_rate': 0.0016186666666666667, 'seq/s': 26952.06509543286, 'global_steps': 143, 'samples_trained': 1317888, 'skipped_steps': 9, 'timestamp': 1593108032.3098838}
  0: {'training_steps': 153, 'average_loss': 3.23046875, 'step_loss': 3.23046875, 'learning_rate': 0.001616, 'seq/s': 27035.55436002778, 'global_steps': 144, 'samples_trained': 1327104, 'skipped_steps': 9, 'timestamp': 1593108032.6507697}
  0: {'training_steps': 154, 'average_loss': 3.466796875, 'step_loss': 3.466796875, 'learning_rate': 0.0016133333333333334, 'seq/s': 26971.322239107772, 'global_steps': 145, 'samples_trained': 1336320, 'skipped_steps': 9, 'timestamp': 1593108032.992467}
  0: {'training_steps': 155, 'average_loss': 3.40625, 'step_loss': 3.40625, 'learning_rate': 0.0016106666666666667, 'seq/s': 27056.59751613574, 'global_steps': 146, 'samples_trained': 1345536, 'skipped_steps': 9, 'timestamp': 1593108033.3330872}
  0: {'training_steps': 156, 'average_loss': 3.77734375, 'step_loss': 3.77734375, 'learning_rate': 0.001608, 'seq/s': 26955.617322855513, 'global_steps': 147, 'samples_trained': 1354752, 'skipped_steps': 9, 'timestamp': 1593108033.6749837}
  0: {'training_steps': 157, 'average_loss': 4.06640625, 'step_loss': 4.06640625, 'learning_rate': 0.0016053333333333334, 'seq/s': 26954.39555170639, 'global_steps': 148, 'samples_trained': 1363968, 'skipped_steps': 9, 'timestamp': 1593108034.0168955}
  0: {'training_steps': 158, 'average_loss': 3.587890625, 'step_loss': 3.587890625, 'learning_rate': 0.0016026666666666667, 'seq/s': 26975.500757873935, 'global_steps': 149, 'samples_trained': 1373184, 'skipped_steps': 9, 'timestamp': 1593108034.35854}
  0: {'training_steps': 159, 'average_loss': 3.958984375, 'step_loss': 3.958984375, 'learning_rate': 0.0016, 'seq/s': 26968.048088676056, 'global_steps': 150, 'samples_trained': 1382400, 'skipped_steps': 9, 'timestamp': 1593108034.7002788}
  0: {'training_steps': 160, 'average_loss': 3.49609375, 'step_loss': 3.49609375, 'learning_rate': 0.0015973333333333332, 'seq/s': 27083.175277927832, 'global_steps': 151, 'samples_trained': 1391616, 'skipped_steps': 9, 'timestamp': 1593108035.040565}
  0: {'training_steps': 161, 'average_loss': 3.36328125, 'step_loss': 3.36328125, 'learning_rate': 0.0015946666666666668, 'seq/s': 27090.52084840126, 'global_steps': 152, 'samples_trained': 1400832, 'skipped_steps': 9, 'timestamp': 1593108035.3807588}
  0: {'training_steps': 162, 'average_loss': 3.423828125, 'step_loss': 3.423828125, 'learning_rate': 0.001592, 'seq/s': 26945.320320293107, 'global_steps': 153, 'samples_trained': 1410048, 'skipped_steps': 9, 'timestamp': 1593108035.7227855}
  0: {'training_steps': 163, 'average_loss': 3.451171875, 'step_loss': 3.451171875, 'learning_rate': 0.0015893333333333332, 'seq/s': 26978.77671830048, 'global_steps': 154, 'samples_trained': 1419264, 'skipped_steps': 9, 'timestamp': 1593108036.0643885}
  0: {'training_steps': 164, 'average_loss': 2.57421875, 'step_loss': 2.57421875, 'learning_rate': 0.0015866666666666668, 'seq/s': 27094.85033687451, 'global_steps': 155, 'samples_trained': 1428480, 'skipped_steps': 9, 'timestamp': 1593108036.4045277}
  0: {'training_steps': 165, 'average_loss': 3.666015625, 'step_loss': 3.666015625, 'learning_rate': 0.0015840000000000001, 'seq/s': 27031.073673610674, 'global_steps': 156, 'samples_trained': 1437696, 'skipped_steps': 9, 'timestamp': 1593108036.7454698}
  0: {'training_steps': 166, 'average_loss': 3.423828125, 'step_loss': 3.423828125, 'learning_rate': 0.0015813333333333332, 'seq/s': 26967.878757689097, 'global_steps': 157, 'samples_trained': 1446912, 'skipped_steps': 9, 'timestamp': 1593108037.087211}
  0: {'training_steps': 167, 'average_loss': 3.345703125, 'step_loss': 3.345703125, 'learning_rate': 0.0015786666666666668, 'seq/s': 27060.36678889603, 'global_steps': 158, 'samples_trained': 1456128, 'skipped_steps': 9, 'timestamp': 1593108037.427784}
  0: {'training_steps': 168, 'average_loss': 3.55859375, 'step_loss': 3.55859375, 'learning_rate': 0.0015760000000000001, 'seq/s': 27048.94855307491, 'global_steps': 159, 'samples_trained': 1465344, 'skipped_steps': 9, 'timestamp': 1593108037.768501}
  0: {'training_steps': 169, 'average_loss': 3.095703125, 'step_loss': 3.095703125, 'learning_rate': 0.0015733333333333333, 'seq/s': 27023.269105712086, 'global_steps': 160, 'samples_trained': 1474560, 'skipped_steps': 9, 'timestamp': 1593108038.1095414}
  0: {'training_steps': 170, 'average_loss': 3.63671875, 'step_loss': 3.63671875, 'learning_rate': 0.0015706666666666666, 'seq/s': 26923.643439231713, 'global_steps': 161, 'samples_trained': 1483776, 'skipped_steps': 9, 'timestamp': 1593108038.451844}
  0: {'training_steps': 171, 'average_loss': 3.255859375, 'step_loss': 3.255859375, 'learning_rate': 0.0015680000000000002, 'seq/s': 26878.1325302629, 'global_steps': 162, 'samples_trained': 1492992, 'skipped_steps': 9, 'timestamp': 1593108038.794726}
  0: {'training_steps': 172, 'average_loss': 3.521484375, 'step_loss': 3.521484375, 'learning_rate': 0.0015653333333333333, 'seq/s': 27010.768517593293, 'global_steps': 163, 'samples_trained': 1502208, 'skipped_steps': 9, 'timestamp': 1593108039.1359239}
  0: {'training_steps': 173, 'average_loss': 3.44921875, 'step_loss': 3.44921875, 'learning_rate': 0.0015626666666666666, 'seq/s': 27029.18353413374, 'global_steps': 164, 'samples_trained': 1511424, 'skipped_steps': 9, 'timestamp': 1593108039.4768896}
  0: {'training_steps': 174, 'average_loss': 3.33984375, 'step_loss': 3.33984375, 'learning_rate': 0.0015600000000000002, 'seq/s': 26931.596960061088, 'global_steps': 165, 'samples_trained': 1520640, 'skipped_steps': 9, 'timestamp': 1593108039.8190908}
  0: {'training_steps': 175, 'average_loss': 3.283203125, 'step_loss': 3.283203125, 'learning_rate': 0.0015573333333333333, 'seq/s': 26987.252720737928, 'global_steps': 166, 'samples_trained': 1529856, 'skipped_steps': 9, 'timestamp': 1593108040.1605864}
  0: {'training_steps': 176, 'average_loss': 3.470703125, 'step_loss': 3.470703125, 'learning_rate': 0.0015546666666666667, 'seq/s': 26954.790265658896, 'global_steps': 167, 'samples_trained': 1539072, 'skipped_steps': 9, 'timestamp': 1593108040.5024934}
  0: {'training_steps': 177, 'average_loss': 3.10546875, 'step_loss': 3.10546875, 'learning_rate': 0.001552, 'seq/s': 26952.40336246953, 'global_steps': 168, 'samples_trained': 1548288, 'skipped_steps': 9, 'timestamp': 1593108040.8444304}
  0: {'training_steps': 178, 'average_loss': 3.18359375, 'step_loss': 3.18359375, 'learning_rate': 0.0015493333333333333, 'seq/s': 26895.936643253004, 'global_steps': 169, 'samples_trained': 1557504, 'skipped_steps': 9, 'timestamp': 1593108041.1870856}
  0: {'training_steps': 179, 'average_loss': 3.08203125, 'step_loss': 3.08203125, 'learning_rate': 0.0015466666666666667, 'seq/s': 27057.108863431782, 'global_steps': 170, 'samples_trained': 1566720, 'skipped_steps': 9, 'timestamp': 1593108041.5276997}
  0: {'training_steps': 180, 'average_loss': 3.275390625, 'step_loss': 3.275390625, 'learning_rate': 0.001544, 'seq/s': 26965.621214372564, 'global_steps': 171, 'samples_trained': 1575936, 'skipped_steps': 9, 'timestamp': 1593108041.8694692}
  0: {'training_steps': 181, 'average_loss': 1.8759765625, 'step_loss': 1.8759765625, 'learning_rate': 0.0015413333333333334, 'seq/s': 26993.434141245416, 'global_steps': 172, 'samples_trained': 1585152, 'skipped_steps': 9, 'timestamp': 1593108042.2108867}
  0: {'training_steps': 182, 'average_loss': 3.197265625, 'step_loss': 3.197265625, 'learning_rate': 0.0015386666666666667, 'seq/s': 27017.243964320613, 'global_steps': 173, 'samples_trained': 1594368, 'skipped_steps': 9, 'timestamp': 1593108042.5520036}
  0: {'training_steps': 183, 'average_loss': 2.572265625, 'step_loss': 2.572265625, 'learning_rate': 0.001536, 'seq/s': 26924.318555667, 'global_steps': 174, 'samples_trained': 1603584, 'skipped_steps': 9, 'timestamp': 1593108042.8942976}
  0: {'training_steps': 184, 'average_loss': 3.328125, 'step_loss': 3.328125, 'learning_rate': 0.0015333333333333332, 'seq/s': 26977.421051970403, 'global_steps': 175, 'samples_trained': 1612800, 'skipped_steps': 9, 'timestamp': 1593108043.2359176}
  0: {'training_steps': 185, 'average_loss': 3.119140625, 'step_loss': 3.119140625, 'learning_rate': 0.0015306666666666667, 'seq/s': 27128.012958102325, 'global_steps': 176, 'samples_trained': 1622016, 'skipped_steps': 9, 'timestamp': 1593108043.5756414}
  0: {'training_steps': 186, 'average_loss': 2.9921875, 'step_loss': 2.9921875, 'learning_rate': 0.001528, 'seq/s': 27014.37399555802, 'global_steps': 177, 'samples_trained': 1631232, 'skipped_steps': 9, 'timestamp': 1593108043.9167943}
  0: {'training_steps': 187, 'average_loss': 3.162109375, 'step_loss': 3.162109375, 'learning_rate': 0.0015253333333333332, 'seq/s': 26871.74226844974, 'global_steps': 178, 'samples_trained': 1640448, 'skipped_steps': 9, 'timestamp': 1593108044.2597578}
  0: {'training_steps': 188, 'average_loss': 3.0625, 'step_loss': 3.0625, 'learning_rate': 0.0015226666666666667, 'seq/s': 26943.21679156156, 'global_steps': 179, 'samples_trained': 1649664, 'skipped_steps': 9, 'timestamp': 1593108044.6018116}
  0: {'training_steps': 189, 'average_loss': 2.4765625, 'step_loss': 2.4765625, 'learning_rate': 0.00152, 'seq/s': 26981.676795371863, 'global_steps': 180, 'samples_trained': 1658880, 'skipped_steps': 9, 'timestamp': 1593108044.9433777}
  0: {'training_steps': 190, 'average_loss': 3.1953125, 'step_loss': 3.1953125, 'learning_rate': 0.0015173333333333332, 'seq/s': 27085.737233765252, 'global_steps': 181, 'samples_trained': 1668096, 'skipped_steps': 9, 'timestamp': 1593108045.2836316}
  0: {'training_steps': 191, 'average_loss': 2.998046875, 'step_loss': 2.998046875, 'learning_rate': 0.0015146666666666665, 'seq/s': 27017.923783083315, 'global_steps': 182, 'samples_trained': 1677312, 'skipped_steps': 9, 'timestamp': 1593108045.6247396}
  0: {'training_steps': 192, 'average_loss': 3.15625, 'step_loss': 3.15625, 'learning_rate': 0.001512, 'seq/s': 26961.389339236914, 'global_steps': 183, 'samples_trained': 1686528, 'skipped_steps': 9, 'timestamp': 1593108045.9665627}
  0: {'training_steps': 193, 'average_loss': 2.66015625, 'step_loss': 2.66015625, 'learning_rate': 0.0015093333333333334, 'seq/s': 26950.07325065467, 'global_steps': 184, 'samples_trained': 1695744, 'skipped_steps': 9, 'timestamp': 1593108046.3085296}
  0: {'training_steps': 194, 'average_loss': 3.283203125, 'step_loss': 3.283203125, 'learning_rate': 0.0015066666666666666, 'seq/s': 27028.91893570068, 'global_steps': 185, 'samples_trained': 1704960, 'skipped_steps': 9, 'timestamp': 1593108046.6494987}
  0: {'training_steps': 195, 'average_loss': 1.5859375, 'step_loss': 1.5859375, 'learning_rate': 0.0015040000000000001, 'seq/s': 26901.889071924736, 'global_steps': 186, 'samples_trained': 1714176, 'skipped_steps': 9, 'timestamp': 1593108046.992078}
  0: {'training_steps': 196, 'average_loss': 2.955078125, 'step_loss': 2.955078125, 'learning_rate': 0.0015013333333333335, 'seq/s': 27073.083990584055, 'global_steps': 187, 'samples_trained': 1723392, 'skipped_steps': 9, 'timestamp': 1593108047.332491}
  0: {'training_steps': 197, 'average_loss': 3.25390625, 'step_loss': 3.25390625, 'learning_rate': 0.0014986666666666668, 'seq/s': 26898.87509168164, 'global_steps': 188, 'samples_trained': 1732608, 'skipped_steps': 9, 'timestamp': 1593108047.6751087}
  0: {'training_steps': 198, 'average_loss': 3.2109375, 'step_loss': 3.2109375, 'learning_rate': 0.001496, 'seq/s': 26909.623675915303, 'global_steps': 189, 'samples_trained': 1741824, 'skipped_steps': 9, 'timestamp': 1593108048.0175893}
  0: {'training_steps': 199, 'average_loss': 2.791015625, 'step_loss': 2.791015625, 'learning_rate': 0.0014933333333333333, 'seq/s': 27126.413815172662, 'global_steps': 190, 'samples_trained': 1751040, 'skipped_steps': 9, 'timestamp': 1593108048.357333}
  0: {'training_steps': 200, 'average_loss': 2.806640625, 'step_loss': 2.806640625, 'learning_rate': 0.0014906666666666668, 'seq/s': 26910.373026348767, 'global_steps': 191, 'samples_trained': 1760256, 'skipped_steps': 9, 'timestamp': 1593108048.6998043}
  0: {'training_steps': 201, 'average_loss': 2.93359375, 'step_loss': 2.93359375, 'learning_rate': 0.001488, 'seq/s': 27019.000232761497, 'global_steps': 192, 'samples_trained': 1769472, 'skipped_steps': 9, 'timestamp': 1593108049.0408983}
  0: {'training_steps': 202, 'average_loss': 3.0703125, 'step_loss': 3.0703125, 'learning_rate': 0.0014853333333333333, 'seq/s': 26964.28567920801, 'global_steps': 193, 'samples_trained': 1778688, 'skipped_steps': 9, 'timestamp': 1593108049.3826847}
  0: {'training_steps': 203, 'average_loss': 2.767578125, 'step_loss': 2.767578125, 'learning_rate': 0.0014826666666666669, 'seq/s': 26992.88749910093, 'global_steps': 194, 'samples_trained': 1787904, 'skipped_steps': 9, 'timestamp': 1593108049.7241094}
  0: {'training_steps': 204, 'average_loss': 2.333984375, 'step_loss': 2.333984375, 'learning_rate': 0.00148, 'seq/s': 27024.553809369, 'global_steps': 195, 'samples_trained': 1797120, 'skipped_steps': 9, 'timestamp': 1593108050.0651333}
  0: {'training_steps': 205, 'average_loss': 3.08984375, 'step_loss': 3.08984375, 'learning_rate': 0.0014773333333333333, 'seq/s': 27075.530247355466, 'global_steps': 196, 'samples_trained': 1806336, 'skipped_steps': 9, 'timestamp': 1593108050.4055154}
  0: {'training_steps': 206, 'average_loss': 2.611328125, 'step_loss': 2.611328125, 'learning_rate': 0.0014746666666666669, 'seq/s': 27012.599451567727, 'global_steps': 197, 'samples_trained': 1815552, 'skipped_steps': 9, 'timestamp': 1593108050.7466908}
  0: {'training_steps': 207, 'average_loss': 2.95703125, 'step_loss': 2.95703125, 'learning_rate': 0.001472, 'seq/s': 26975.105437142396, 'global_steps': 198, 'samples_trained': 1824768, 'skipped_steps': 9, 'timestamp': 1593108051.08834}
  0: {'training_steps': 208, 'average_loss': 2.2890625, 'step_loss': 2.2890625, 'learning_rate': 0.0014693333333333333, 'seq/s': 27056.82477921449, 'global_steps': 199, 'samples_trained': 1833984, 'skipped_steps': 9, 'timestamp': 1593108051.4289572}
  0: {'training_steps': 209, 'average_loss': 2.974609375, 'step_loss': 2.974609375, 'learning_rate': 0.001466666666666667, 'seq/s': 25154.065784353406, 'global_steps': 200, 'samples_trained': 1843200, 'skipped_steps': 9, 'timestamp': 1593108051.79534}
  0: {'training_steps': 210, 'average_loss': 2.87109375, 'step_loss': 2.87109375, 'learning_rate': 0.001464, 'seq/s': 27078.31838124293, 'global_steps': 201, 'samples_trained': 1852416, 'skipped_steps': 9, 'timestamp': 1593108052.135687}
  0: {'training_steps': 211, 'average_loss': 3.41015625, 'step_loss': 3.41015625, 'learning_rate': 0.0014613333333333334, 'seq/s': 27100.739142444898, 'global_steps': 202, 'samples_trained': 1861632, 'skipped_steps': 9, 'timestamp': 1593108052.4757524}
  0: {'training_steps': 212, 'average_loss': 3.080078125, 'step_loss': 3.080078125, 'learning_rate': 0.0014586666666666667, 'seq/s': 26993.302191114453, 'global_steps': 203, 'samples_trained': 1870848, 'skipped_steps': 9, 'timestamp': 1593108052.8171718}
  0: {'training_steps': 213, 'average_loss': 2.634765625, 'step_loss': 2.634765625, 'learning_rate': 0.001456, 'seq/s': 24202.210469108843, 'global_steps': 204, 'samples_trained': 1880064, 'skipped_steps': 9, 'timestamp': 1593108053.1979644}
  0: {'training_steps': 214, 'average_loss': 2.650390625, 'step_loss': 2.650390625, 'learning_rate': 0.0014533333333333334, 'seq/s': 27027.955085216843, 'global_steps': 205, 'samples_trained': 1889280, 'skipped_steps': 9, 'timestamp': 1593108053.5389457}
  0: {'training_steps': 215, 'average_loss': 2.716796875, 'step_loss': 2.716796875, 'learning_rate': 0.0014506666666666667, 'seq/s': 27027.123583171528, 'global_steps': 206, 'samples_trained': 1898496, 'skipped_steps': 9, 'timestamp': 1593108053.8799372}
  0: {'training_steps': 216, 'average_loss': 2.904296875, 'step_loss': 2.904296875, 'learning_rate': 0.001448, 'seq/s': 26882.413088924404, 'global_steps': 207, 'samples_trained': 1907712, 'skipped_steps': 9, 'timestamp': 1593108054.2227645}
  0: {'training_steps': 217, 'average_loss': 2.65234375, 'step_loss': 2.65234375, 'learning_rate': 0.0014453333333333334, 'seq/s': 24146.680041403546, 'global_steps': 208, 'samples_trained': 1916928, 'skipped_steps': 9, 'timestamp': 1593108054.6044328}
  0: {'training_steps': 218, 'average_loss': 2.576171875, 'step_loss': 2.576171875, 'learning_rate': 0.0014426666666666667, 'seq/s': 27153.777508029234, 'global_steps': 209, 'samples_trained': 1926144, 'skipped_steps': 9, 'timestamp': 1593108054.943834}
  0: {'training_steps': 219, 'average_loss': 2.603515625, 'step_loss': 2.603515625, 'learning_rate': 0.0014399999999999999, 'seq/s': 27006.01302418648, 'global_steps': 210, 'samples_trained': 1935360, 'skipped_steps': 9, 'timestamp': 1593108055.2850926}
  0: {'training_steps': 220, 'average_loss': 2.9296875, 'step_loss': 2.9296875, 'learning_rate': 0.0014373333333333334, 'seq/s': 26999.241224394005, 'global_steps': 211, 'samples_trained': 1944576, 'skipped_steps': 9, 'timestamp': 1593108055.626437}
  0: {'training_steps': 221, 'average_loss': 2.279296875, 'step_loss': 2.279296875, 'learning_rate': 0.0014346666666666668, 'seq/s': 26993.980805530824, 'global_steps': 212, 'samples_trained': 1953792, 'skipped_steps': 9, 'timestamp': 1593108055.9678476}
  0: {'training_steps': 222, 'average_loss': 2.46484375, 'step_loss': 2.46484375, 'learning_rate': 0.001432, 'seq/s': 27065.61521371531, 'global_steps': 213, 'samples_trained': 1963008, 'skipped_steps': 9, 'timestamp': 1593108056.3083546}
  0: {'training_steps': 223, 'average_loss': 2.634765625, 'step_loss': 2.634765625, 'learning_rate': 0.0014293333333333335, 'seq/s': 26978.13652555527, 'global_steps': 214, 'samples_trained': 1972224, 'skipped_steps': 9, 'timestamp': 1593108056.6499655}
  0: {'training_steps': 224, 'average_loss': 2.46875, 'step_loss': 2.46875, 'learning_rate': 0.0014266666666666668, 'seq/s': 27016.71524004557, 'global_steps': 215, 'samples_trained': 1981440, 'skipped_steps': 9, 'timestamp': 1593108056.9910884}
  0: {'training_steps': 225, 'average_loss': 2.65625, 'step_loss': 2.65625, 'learning_rate': 0.001424, 'seq/s': 27042.533576231774, 'global_steps': 216, 'samples_trained': 1990656, 'skipped_steps': 9, 'timestamp': 1593108057.3318858}
  0: {'training_steps': 226, 'average_loss': 2.55859375, 'step_loss': 2.55859375, 'learning_rate': 0.0014213333333333333, 'seq/s': 26955.523336527156, 'global_steps': 217, 'samples_trained': 1999872, 'skipped_steps': 9, 'timestamp': 1593108057.6737835}
  0: {'training_steps': 227, 'average_loss': 2.90234375, 'step_loss': 2.90234375, 'learning_rate': 0.0014186666666666668, 'seq/s': 26920.474595110332, 'global_steps': 218, 'samples_trained': 2009088, 'skipped_steps': 9, 'timestamp': 1593108058.0161264}
  0: {'training_steps': 228, 'average_loss': 3.0390625, 'step_loss': 3.0390625, 'learning_rate': 0.001416, 'seq/s': 27004.97534148577, 'global_steps': 219, 'samples_trained': 2018304, 'skipped_steps': 9, 'timestamp': 1593108058.3573978}
  0: {'training_steps': 229, 'average_loss': 2.724609375, 'step_loss': 2.724609375, 'learning_rate': 0.0014133333333333333, 'seq/s': 26876.0394782859, 'global_steps': 220, 'samples_trained': 2027520, 'skipped_steps': 9, 'timestamp': 1593108058.7003064}
  0: {'training_steps': 230, 'average_loss': 2.638671875, 'step_loss': 2.638671875, 'learning_rate': 0.0014106666666666668, 'seq/s': 26952.06509543286, 'global_steps': 221, 'samples_trained': 2036736, 'skipped_steps': 9, 'timestamp': 1593108059.042248}
  0: {'training_steps': 231, 'average_loss': 2.302734375, 'step_loss': 2.302734375, 'learning_rate': 0.001408, 'seq/s': 26934.881033393096, 'global_steps': 222, 'samples_trained': 2045952, 'skipped_steps': 9, 'timestamp': 1593108059.3844075}
  0: {'training_steps': 232, 'average_loss': 2.26171875, 'step_loss': 2.26171875, 'learning_rate': 0.0014053333333333333, 'seq/s': 27019.132434249797, 'global_steps': 223, 'samples_trained': 2055168, 'skipped_steps': 9, 'timestamp': 1593108059.7255003}
  0: {'training_steps': 233, 'average_loss': 3.01953125, 'step_loss': 3.01953125, 'learning_rate': 0.0014026666666666669, 'seq/s': 26905.78390986372, 'global_steps': 224, 'samples_trained': 2064384, 'skipped_steps': 9, 'timestamp': 1593108060.0680299}
  0: {'training_steps': 234, 'average_loss': 2.330078125, 'step_loss': 2.330078125, 'learning_rate': 0.0014, 'seq/s': 27014.317357573607, 'global_steps': 225, 'samples_trained': 2073600, 'skipped_steps': 9, 'timestamp': 1593108060.4091833}
  0: {'training_steps': 235, 'average_loss': 2.671875, 'step_loss': 2.671875, 'learning_rate': 0.0013973333333333333, 'seq/s': 26871.536784150157, 'global_steps': 226, 'samples_trained': 2082816, 'skipped_steps': 9, 'timestamp': 1593108060.7521496}
  0: {'training_steps': 236, 'average_loss': 2.578125, 'step_loss': 2.578125, 'learning_rate': 0.0013946666666666667, 'seq/s': 26960.787580436594, 'global_steps': 227, 'samples_trained': 2092032, 'skipped_steps': 9, 'timestamp': 1593108061.0939808}
  0: {'training_steps': 237, 'average_loss': 3.06640625, 'step_loss': 3.06640625, 'learning_rate': 0.001392, 'seq/s': 27020.530071971414, 'global_steps': 228, 'samples_trained': 2101248, 'skipped_steps': 9, 'timestamp': 1593108061.4350562}
  0: {'training_steps': 238, 'average_loss': 2.658203125, 'step_loss': 2.658203125, 'learning_rate': 0.0013893333333333333, 'seq/s': 26925.068724749937, 'global_steps': 229, 'samples_trained': 2110464, 'skipped_steps': 9, 'timestamp': 1593108061.7773404}
  0: {'training_steps': 239, 'average_loss': 2.833984375, 'step_loss': 2.833984375, 'learning_rate': 0.0013866666666666667, 'seq/s': 26862.815939489927, 'global_steps': 230, 'samples_trained': 2119680, 'skipped_steps': 9, 'timestamp': 1593108062.1204178}
  0: {'training_steps': 240, 'average_loss': 2.248046875, 'step_loss': 2.248046875, 'learning_rate': 0.001384, 'seq/s': 27015.752259716624, 'global_steps': 231, 'samples_trained': 2128896, 'skipped_steps': 9, 'timestamp': 1593108062.461553}
  0: {'training_steps': 241, 'average_loss': 2.45703125, 'step_loss': 2.45703125, 'learning_rate': 0.0013813333333333334, 'seq/s': 26897.957927390617, 'global_steps': 232, 'samples_trained': 2138112, 'skipped_steps': 9, 'timestamp': 1593108062.8041823}
  0: {'training_steps': 242, 'average_loss': 2.431640625, 'step_loss': 2.431640625, 'learning_rate': 0.0013786666666666667, 'seq/s': 26918.97481066739, 'global_steps': 233, 'samples_trained': 2147328, 'skipped_steps': 9, 'timestamp': 1593108063.1465442}
  0: {'training_steps': 243, 'average_loss': 2.7421875, 'step_loss': 2.7421875, 'learning_rate': 0.0013759999999999998, 'seq/s': 27005.03194030414, 'global_steps': 234, 'samples_trained': 2156544, 'skipped_steps': 9, 'timestamp': 1593108063.487815}
  0: {'training_steps': 244, 'average_loss': 2.091796875, 'step_loss': 2.091796875, 'learning_rate': 0.0013733333333333334, 'seq/s': 26977.28925834397, 'global_steps': 235, 'samples_trained': 2165760, 'skipped_steps': 9, 'timestamp': 1593108063.8294368}
  0: {'training_steps': 245, 'average_loss': 2.078125, 'step_loss': 2.078125, 'learning_rate': 0.0013706666666666667, 'seq/s': 26958.869653517828, 'global_steps': 236, 'samples_trained': 2174976, 'skipped_steps': 9, 'timestamp': 1593108064.171292}
  0: {'training_steps': 246, 'average_loss': 2.2109375, 'step_loss': 2.2109375, 'learning_rate': 0.0013679999999999999, 'seq/s': 26937.90308831769, 'global_steps': 237, 'samples_trained': 2184192, 'skipped_steps': 9, 'timestamp': 1593108064.5134134}
  0: {'training_steps': 247, 'average_loss': 2.82421875, 'step_loss': 2.82421875, 'learning_rate': 0.0013653333333333334, 'seq/s': 27007.428174576, 'global_steps': 238, 'samples_trained': 2193408, 'skipped_steps': 9, 'timestamp': 1593108064.8546546}
  0: {'training_steps': 248, 'average_loss': 2.38671875, 'step_loss': 2.38671875, 'learning_rate': 0.0013626666666666668, 'seq/s': 26968.085718073045, 'global_steps': 239, 'samples_trained': 2202624, 'skipped_steps': 9, 'timestamp': 1593108065.1963928}
  0: {'training_steps': 249, 'average_loss': 2.87890625, 'step_loss': 2.87890625, 'learning_rate': 0.0013599999999999999, 'seq/s': 26897.995361443478, 'global_steps': 240, 'samples_trained': 2211840, 'skipped_steps': 9, 'timestamp': 1593108065.5390215}
  0: {'training_steps': 250, 'average_loss': 2.462890625, 'step_loss': 2.462890625, 'learning_rate': 0.0013573333333333332, 'seq/s': 27021.701177066017, 'global_steps': 241, 'samples_trained': 2221056, 'skipped_steps': 9, 'timestamp': 1593108065.8800821}
  0: {'training_steps': 251, 'average_loss': 2.689453125, 'step_loss': 2.689453125, 'learning_rate': 0.0013546666666666668, 'seq/s': 27040.622917723267, 'global_steps': 242, 'samples_trained': 2230272, 'skipped_steps': 9, 'timestamp': 1593108066.2209036}
  0: {'training_steps': 252, 'average_loss': 2.4296875, 'step_loss': 2.4296875, 'learning_rate': 0.001352, 'seq/s': 27017.98043619116, 'global_steps': 243, 'samples_trained': 2239488, 'skipped_steps': 9, 'timestamp': 1593108066.5620108}
  0: {'training_steps': 253, 'average_loss': 2.525390625, 'step_loss': 2.525390625, 'learning_rate': 0.0013493333333333335, 'seq/s': 26969.87323539775, 'global_steps': 244, 'samples_trained': 2248704, 'skipped_steps': 9, 'timestamp': 1593108066.903726}
  0: {'training_steps': 254, 'average_loss': 2.384765625, 'step_loss': 2.384765625, 'learning_rate': 0.0013466666666666668, 'seq/s': 26915.95700098738, 'global_steps': 245, 'samples_trained': 2257920, 'skipped_steps': 9, 'timestamp': 1593108067.2461264}
  0: {'training_steps': 255, 'average_loss': 2.408203125, 'step_loss': 2.408203125, 'learning_rate': 0.001344, 'seq/s': 26941.470361020543, 'global_steps': 246, 'samples_trained': 2267136, 'skipped_steps': 9, 'timestamp': 1593108067.5882022}
  0: {'training_steps': 256, 'average_loss': 1.9853515625, 'step_loss': 1.9853515625, 'learning_rate': 0.0013413333333333335, 'seq/s': 27006.12623076004, 'global_steps': 247, 'samples_trained': 2276352, 'skipped_steps': 9, 'timestamp': 1593108067.929459}
  0: {'training_steps': 257, 'average_loss': 2.572265625, 'step_loss': 2.572265625, 'learning_rate': 0.0013386666666666666, 'seq/s': 25278.4249005008, 'global_steps': 248, 'samples_trained': 2285568, 'skipped_steps': 9, 'timestamp': 1593108068.2940397}
  0: {'training_steps': 258, 'average_loss': 2.130859375, 'step_loss': 2.130859375, 'learning_rate': 0.001336, 'seq/s': 26938.278546220878, 'global_steps': 249, 'samples_trained': 2294784, 'skipped_steps': 9, 'timestamp': 1593108068.636156}
  0: {'training_steps': 259, 'average_loss': 2.234375, 'step_loss': 2.234375, 'learning_rate': 0.0013333333333333335, 'seq/s': 26973.599559542992, 'global_steps': 250, 'samples_trained': 2304000, 'skipped_steps': 9, 'timestamp': 1593108068.9778242}
  0: {'training_steps': 260, 'average_loss': 1.8388671875, 'step_loss': 1.8388671875, 'learning_rate': 0.0013306666666666666, 'seq/s': 26896.909816211748, 'global_steps': 251, 'samples_trained': 2313216, 'skipped_steps': 9, 'timestamp': 1593108069.3204668}
  0: {'training_steps': 261, 'average_loss': 2.064453125, 'step_loss': 2.064453125, 'learning_rate': 0.001328, 'seq/s': 24538.60386564512, 'global_steps': 252, 'samples_trained': 2322432, 'skipped_steps': 9, 'timestamp': 1593108069.6960392}
  0: {'training_steps': 262, 'average_loss': 2.44921875, 'step_loss': 2.44921875, 'learning_rate': 0.0013253333333333335, 'seq/s': 26955.57972824552, 'global_steps': 253, 'samples_trained': 2331648, 'skipped_steps': 9, 'timestamp': 1593108070.0379362}
  0: {'training_steps': 263, 'average_loss': 2.7109375, 'step_loss': 2.7109375, 'learning_rate': 0.0013226666666666667, 'seq/s': 26981.092963503528, 'global_steps': 254, 'samples_trained': 2340864, 'skipped_steps': 9, 'timestamp': 1593108070.3795094}
  0: {'training_steps': 264, 'average_loss': 2.140625, 'step_loss': 2.140625, 'learning_rate': 0.0013199999999999998, 'seq/s': 27093.407020298306, 'global_steps': 255, 'samples_trained': 2350080, 'skipped_steps': 9, 'timestamp': 1593108070.7196672}
  0: {'training_steps': 265, 'average_loss': 2.298828125, 'step_loss': 2.298828125, 'learning_rate': 0.0013173333333333336, 'seq/s': 24570.688648134634, 'global_steps': 256, 'samples_trained': 2359296, 'skipped_steps': 9, 'timestamp': 1593108071.0947495}
  0: {'training_steps': 266, 'average_loss': 2.05078125, 'step_loss': 2.05078125, 'learning_rate': 0.0013146666666666667, 'seq/s': 26979.755895388524, 'global_steps': 257, 'samples_trained': 2368512, 'skipped_steps': 9, 'timestamp': 1593108071.4363403}
  0: {'training_steps': 267, 'average_loss': 1.73046875, 'step_loss': 1.73046875, 'learning_rate': 0.001312, 'seq/s': 26949.885355967086, 'global_steps': 258, 'samples_trained': 2377728, 'skipped_steps': 9, 'timestamp': 1593108071.7783093}
  0: {'training_steps': 268, 'average_loss': 2.009765625, 'step_loss': 2.009765625, 'learning_rate': 0.0013093333333333336, 'seq/s': 24335.10341883893, 'global_steps': 259, 'samples_trained': 2386944, 'skipped_steps': 9, 'timestamp': 1593108072.1570222}
  0: {'training_steps': 269, 'average_loss': 2.2734375, 'step_loss': 2.2734375, 'learning_rate': 0.0013066666666666667, 'seq/s': 27159.65379275456, 'global_steps': 260, 'samples_trained': 2396160, 'skipped_steps': 9, 'timestamp': 1593108072.49635}
  0: {'training_steps': 270, 'average_loss': 2.12890625, 'step_loss': 2.12890625, 'learning_rate': 0.001304, 'seq/s': 27091.77397908335, 'global_steps': 261, 'samples_trained': 2405376, 'skipped_steps': 9, 'timestamp': 1593108072.836528}
  0: {'training_steps': 271, 'average_loss': 2.341796875, 'step_loss': 2.341796875, 'learning_rate': 0.0013013333333333334, 'seq/s': 26911.12241851761, 'global_steps': 262, 'samples_trained': 2414592, 'skipped_steps': 9, 'timestamp': 1593108073.17899}
  0: {'training_steps': 272, 'average_loss': 2.318359375, 'step_loss': 2.318359375, 'learning_rate': 0.0012986666666666667, 'seq/s': 27007.50365342936, 'global_steps': 263, 'samples_trained': 2423808, 'skipped_steps': 9, 'timestamp': 1593108073.5202296}
  0: {'training_steps': 273, 'average_loss': 2.16796875, 'step_loss': 2.16796875, 'learning_rate': 0.001296, 'seq/s': 27083.27015653836, 'global_steps': 264, 'samples_trained': 2433024, 'skipped_steps': 9, 'timestamp': 1593108073.8605146}
  0: {'training_steps': 274, 'average_loss': 1.9599609375, 'step_loss': 1.9599609375, 'learning_rate': 0.0012933333333333334, 'seq/s': 27057.336135100817, 'global_steps': 265, 'samples_trained': 2442240, 'skipped_steps': 9, 'timestamp': 1593108074.2011256}
  0: {'training_steps': 275, 'average_loss': 1.85546875, 'step_loss': 1.85546875, 'learning_rate': 0.0012906666666666667, 'seq/s': 27112.3913191992, 'global_steps': 266, 'samples_trained': 2451456, 'skipped_steps': 9, 'timestamp': 1593108074.5410452}
  0: {'training_steps': 276, 'average_loss': 2.09765625, 'step_loss': 2.09765625, 'learning_rate': 0.001288, 'seq/s': 27029.089034098655, 'global_steps': 267, 'samples_trained': 2460672, 'skipped_steps': 9, 'timestamp': 1593108074.8820124}
  0: {'training_steps': 277, 'average_loss': 1.8720703125, 'step_loss': 1.8720703125, 'learning_rate': 0.0012853333333333334, 'seq/s': 26917.53142596108, 'global_steps': 268, 'samples_trained': 2469888, 'skipped_steps': 9, 'timestamp': 1593108075.2243924}
  0: {'training_steps': 278, 'average_loss': 1.9560546875, 'step_loss': 1.9560546875, 'learning_rate': 0.0012826666666666665, 'seq/s': 27068.420260819574, 'global_steps': 269, 'samples_trained': 2479104, 'skipped_steps': 9, 'timestamp': 1593108075.5648642}
  0: {'training_steps': 279, 'average_loss': 1.88671875, 'step_loss': 1.88671875, 'learning_rate': 0.00128, 'seq/s': 27029.353635862084, 'global_steps': 270, 'samples_trained': 2488320, 'skipped_steps': 9, 'timestamp': 1593108075.9058278}
  0: {'training_steps': 280, 'average_loss': 1.7373046875, 'step_loss': 1.7373046875, 'learning_rate': 0.0012773333333333334, 'seq/s': 26956.030870486637, 'global_steps': 271, 'samples_trained': 2497536, 'skipped_steps': 9, 'timestamp': 1593108076.2477188}
  0: {'training_steps': 281, 'average_loss': 1.88671875, 'step_loss': 1.88671875, 'learning_rate': 0.0012746666666666666, 'seq/s': 27008.484916916223, 'global_steps': 272, 'samples_trained': 2506752, 'skipped_steps': 9, 'timestamp': 1593108076.5889459}
  0: {'training_steps': 282, 'average_loss': 2.099609375, 'step_loss': 2.099609375, 'learning_rate': 0.0012720000000000001, 'seq/s': 27024.251515166347, 'global_steps': 273, 'samples_trained': 2515968, 'skipped_steps': 9, 'timestamp': 1593108076.929974}
  0: {'training_steps': 283, 'average_loss': 1.6044921875, 'step_loss': 1.6044921875, 'learning_rate': 0.0012693333333333335, 'seq/s': 26937.2085187872, 'global_steps': 274, 'samples_trained': 2525184, 'skipped_steps': 9, 'timestamp': 1593108077.2721043}
  0: {'training_steps': 284, 'average_loss': 1.7529296875, 'step_loss': 1.7529296875, 'learning_rate': 0.0012666666666666666, 'seq/s': 27071.43443907673, 'global_steps': 275, 'samples_trained': 2534400, 'skipped_steps': 9, 'timestamp': 1593108077.612538}
  0: {'training_steps': 285, 'average_loss': 1.8857421875, 'step_loss': 1.8857421875, 'learning_rate': 0.001264, 'seq/s': 27012.448437312192, 'global_steps': 276, 'samples_trained': 2543616, 'skipped_steps': 9, 'timestamp': 1593108077.9537156}
  0: {'training_steps': 286, 'average_loss': 1.978515625, 'step_loss': 1.978515625, 'learning_rate': 0.0012613333333333335, 'seq/s': 27033.20984938076, 'global_steps': 277, 'samples_trained': 2552832, 'skipped_steps': 9, 'timestamp': 1593108078.2946308}
  0: {'training_steps': 287, 'average_loss': 2.048828125, 'step_loss': 2.048828125, 'learning_rate': 0.0012586666666666666, 'seq/s': 26953.737720807218, 'global_steps': 278, 'samples_trained': 2562048, 'skipped_steps': 9, 'timestamp': 1593108078.636551}
  0: {'training_steps': 288, 'average_loss': 1.7666015625, 'step_loss': 1.7666015625, 'learning_rate': 0.001256, 'seq/s': 27033.947191958923, 'global_steps': 279, 'samples_trained': 2571264, 'skipped_steps': 9, 'timestamp': 1593108078.9774568}
  0: {'training_steps': 289, 'average_loss': 1.5224609375, 'step_loss': 1.5224609375, 'learning_rate': 0.0012533333333333335, 'seq/s': 26879.60907550964, 'global_steps': 280, 'samples_trained': 2580480, 'skipped_steps': 9, 'timestamp': 1593108079.3203204}
  0: {'training_steps': 290, 'average_loss': 1.6865234375, 'step_loss': 1.6865234375, 'learning_rate': 0.0012506666666666666, 'seq/s': 27064.857192669224, 'global_steps': 281, 'samples_trained': 2589696, 'skipped_steps': 9, 'timestamp': 1593108079.6608367}
  0: {'training_steps': 291, 'average_loss': 2.0234375, 'step_loss': 2.0234375, 'learning_rate': 0.001248, 'seq/s': 26916.463046071338, 'global_steps': 282, 'samples_trained': 2598912, 'skipped_steps': 9, 'timestamp': 1593108080.0032306}
  0: {'training_steps': 292, 'average_loss': 1.4697265625, 'step_loss': 1.4697265625, 'learning_rate': 0.0012453333333333335, 'seq/s': 27019.264637031807, 'global_steps': 283, 'samples_trained': 2608128, 'skipped_steps': 9, 'timestamp': 1593108080.3443215}
  0: {'training_steps': 293, 'average_loss': 1.8994140625, 'step_loss': 1.8994140625, 'learning_rate': 0.0012426666666666667, 'seq/s': 26916.81916327781, 'global_steps': 284, 'samples_trained': 2617344, 'skipped_steps': 9, 'timestamp': 1593108080.686711}
  0: {'training_steps': 294, 'average_loss': 1.736328125, 'step_loss': 1.736328125, 'learning_rate': 0.00124, 'seq/s': 26943.91167097438, 'global_steps': 285, 'samples_trained': 2626560, 'skipped_steps': 9, 'timestamp': 1593108081.0287557}
  0: {'training_steps': 295, 'average_loss': 1.7158203125, 'step_loss': 1.7158203125, 'learning_rate': 0.0012373333333333333, 'seq/s': 26955.091007858213, 'global_steps': 286, 'samples_trained': 2635776, 'skipped_steps': 9, 'timestamp': 1593108081.370659}
  0: {'training_steps': 296, 'average_loss': 1.7373046875, 'step_loss': 1.7373046875, 'learning_rate': 0.0012346666666666667, 'seq/s': 27019.18909242653, 'global_steps': 287, 'samples_trained': 2644992, 'skipped_steps': 9, 'timestamp': 1593108081.7117512}
  0: {'training_steps': 297, 'average_loss': 1.837890625, 'step_loss': 1.837890625, 'learning_rate': 0.001232, 'seq/s': 27036.68894666272, 'global_steps': 288, 'samples_trained': 2654208, 'skipped_steps': 9, 'timestamp': 1593108082.0526226}
  0: {'training_steps': 298, 'average_loss': 2.07421875, 'step_loss': 2.07421875, 'learning_rate': 0.0012293333333333334, 'seq/s': 26952.891985401886, 'global_steps': 289, 'samples_trained': 2663424, 'skipped_steps': 9, 'timestamp': 1593108082.3945541}
  0: {'training_steps': 299, 'average_loss': 1.703125, 'step_loss': 1.703125, 'learning_rate': 0.0012266666666666667, 'seq/s': 27081.25887232373, 'global_steps': 290, 'samples_trained': 2672640, 'skipped_steps': 9, 'timestamp': 1593108082.7348642}
  0: {'training_steps': 300, 'average_loss': 1.951171875, 'step_loss': 1.951171875, 'learning_rate': 0.001224, 'seq/s': 27006.48472450451, 'global_steps': 291, 'samples_trained': 2681856, 'skipped_steps': 9, 'timestamp': 1593108083.076117}
  0: {'training_steps': 301, 'average_loss': 1.8115234375, 'step_loss': 1.8115234375, 'learning_rate': 0.0012213333333333334, 'seq/s': 26979.680571550616, 'global_steps': 292, 'samples_trained': 2691072, 'skipped_steps': 9, 'timestamp': 1593108083.4177086}
  0: {'training_steps': 302, 'average_loss': 1.5830078125, 'step_loss': 1.5830078125, 'learning_rate': 0.0012186666666666665, 'seq/s': 26939.31110942301, 'global_steps': 293, 'samples_trained': 2700288, 'skipped_steps': 9, 'timestamp': 1593108083.759812}
  0: {'training_steps': 303, 'average_loss': 1.69921875, 'step_loss': 1.69921875, 'learning_rate': 0.001216, 'seq/s': 26855.425395817456, 'global_steps': 294, 'samples_trained': 2709504, 'skipped_steps': 9, 'timestamp': 1593108084.102984}
  0: {'training_steps': 304, 'average_loss': 1.5888671875, 'step_loss': 1.5888671875, 'learning_rate': 0.0012133333333333334, 'seq/s': 26972.959612474016, 'global_steps': 295, 'samples_trained': 2718720, 'skipped_steps': 9, 'timestamp': 1593108084.4446602}
  0: {'training_steps': 305, 'average_loss': 1.8427734375, 'step_loss': 1.8427734375, 'learning_rate': 0.0012106666666666665, 'seq/s': 26960.279867342277, 'global_steps': 296, 'samples_trained': 2727936, 'skipped_steps': 9, 'timestamp': 1593108084.7864976}
  0: {'training_steps': 306, 'average_loss': 1.697265625, 'step_loss': 1.697265625, 'learning_rate': 0.001208, 'seq/s': 26962.06634999686, 'global_steps': 297, 'samples_trained': 2737152, 'skipped_steps': 9, 'timestamp': 1593108085.128312}
  0: {'training_steps': 307, 'average_loss': 1.5576171875, 'step_loss': 1.5576171875, 'learning_rate': 0.0012053333333333334, 'seq/s': 26994.64060225987, 'global_steps': 298, 'samples_trained': 2746368, 'skipped_steps': 9, 'timestamp': 1593108085.4697142}
  0: {'training_steps': 308, 'average_loss': 1.779296875, 'step_loss': 1.779296875, 'learning_rate': 0.0012026666666666666, 'seq/s': 27082.985522701045, 'global_steps': 299, 'samples_trained': 2755584, 'skipped_steps': 9, 'timestamp': 1593108085.8100023}
  0: {'training_steps': 309, 'average_loss': 1.6630859375, 'step_loss': 1.6630859375, 'learning_rate': 0.0012, 'seq/s': 26947.96897979671, 'global_steps': 300, 'samples_trained': 2764800, 'skipped_steps': 9, 'timestamp': 1593108086.1519954}
  0: {'training_steps': 310, 'average_loss': 1.5859375, 'step_loss': 1.5859375, 'learning_rate': 0.0011973333333333335, 'seq/s': 26984.483307608698, 'global_steps': 301, 'samples_trained': 2774016, 'skipped_steps': 9, 'timestamp': 1593108086.4935257}
  0: {'training_steps': 311, 'average_loss': 1.6767578125, 'step_loss': 1.6767578125, 'learning_rate': 0.0011946666666666666, 'seq/s': 26919.89340880664, 'global_steps': 302, 'samples_trained': 2783232, 'skipped_steps': 9, 'timestamp': 1593108086.8358757}
  0: {'training_steps': 312, 'average_loss': 1.8916015625, 'step_loss': 1.8916015625, 'learning_rate': 0.001192, 'seq/s': 26925.80017985499, 'global_steps': 303, 'samples_trained': 2792448, 'skipped_steps': 9, 'timestamp': 1593108087.1781504}
  0: {'training_steps': 313, 'average_loss': 1.591796875, 'step_loss': 1.591796875, 'learning_rate': 0.0011893333333333335, 'seq/s': 26948.73925510622, 'global_steps': 304, 'samples_trained': 2801664, 'skipped_steps': 9, 'timestamp': 1593108087.5201342}
  0: {'training_steps': 314, 'average_loss': 1.482421875, 'step_loss': 1.482421875, 'learning_rate': 0.0011866666666666666, 'seq/s': 27021.60672934357, 'global_steps': 305, 'samples_trained': 2810880, 'skipped_steps': 9, 'timestamp': 1593108087.8611956}
  0: {'training_steps': 315, 'average_loss': 1.5595703125, 'step_loss': 1.5595703125, 'learning_rate': 0.0011840000000000002, 'seq/s': 27013.80762640084, 'global_steps': 306, 'samples_trained': 2820096, 'skipped_steps': 9, 'timestamp': 1593108088.2023554}
  0: {'training_steps': 316, 'average_loss': 1.5224609375, 'step_loss': 1.5224609375, 'learning_rate': 0.0011813333333333333, 'seq/s': 26992.22778806593, 'global_steps': 307, 'samples_trained': 2829312, 'skipped_steps': 9, 'timestamp': 1593108088.5437882}
  0: {'training_steps': 317, 'average_loss': 1.7978515625, 'step_loss': 1.7978515625, 'learning_rate': 0.0011786666666666666, 'seq/s': 27031.016965580035, 'global_steps': 308, 'samples_trained': 2838528, 'skipped_steps': 9, 'timestamp': 1593108088.884731}
  0: {'training_steps': 318, 'average_loss': 1.505859375, 'step_loss': 1.505859375, 'learning_rate': 0.0011760000000000002, 'seq/s': 26865.74715494054, 'global_steps': 309, 'samples_trained': 2847744, 'skipped_steps': 9, 'timestamp': 1593108089.227771}
  0: {'training_steps': 319, 'average_loss': 1.544921875, 'step_loss': 1.544921875, 'learning_rate': 0.0011733333333333333, 'seq/s': 26994.150464466336, 'global_steps': 310, 'samples_trained': 2856960, 'skipped_steps': 9, 'timestamp': 1593108089.5691795}
  0: {'training_steps': 320, 'average_loss': 1.533203125, 'step_loss': 1.533203125, 'learning_rate': 0.0011706666666666666, 'seq/s': 26996.035707251936, 'global_steps': 311, 'samples_trained': 2866176, 'skipped_steps': 9, 'timestamp': 1593108089.9105644}
  0: {'training_steps': 321, 'average_loss': 1.3896484375, 'step_loss': 1.3896484375, 'learning_rate': 0.0011680000000000002, 'seq/s': 27020.73784127441, 'global_steps': 312, 'samples_trained': 2875392, 'skipped_steps': 9, 'timestamp': 1593108090.2516367}
  0: {'training_steps': 322, 'average_loss': 1.9033203125, 'step_loss': 1.9033203125, 'learning_rate': 0.0011653333333333333, 'seq/s': 27014.015292334334, 'global_steps': 313, 'samples_trained': 2884608, 'skipped_steps': 9, 'timestamp': 1593108090.5927944}
  0: {'training_steps': 323, 'average_loss': 1.4560546875, 'step_loss': 1.4560546875, 'learning_rate': 0.0011626666666666664, 'seq/s': 27056.446009537547, 'global_steps': 314, 'samples_trained': 2893824, 'skipped_steps': 9, 'timestamp': 1593108090.9334168}
  0: {'training_steps': 324, 'average_loss': 1.7001953125, 'step_loss': 1.7001953125, 'learning_rate': 0.0011600000000000002, 'seq/s': 26897.995361443478, 'global_steps': 315, 'samples_trained': 2903040, 'skipped_steps': 9, 'timestamp': 1593108091.2760458}
  0: {'training_steps': 325, 'average_loss': 1.3095703125, 'step_loss': 1.3095703125, 'learning_rate': 0.0011573333333333333, 'seq/s': 26981.16829522809, 'global_steps': 316, 'samples_trained': 2912256, 'skipped_steps': 9, 'timestamp': 1593108091.6176188}
  0: {'training_steps': 326, 'average_loss': 1.408203125, 'step_loss': 1.408203125, 'learning_rate': 0.0011546666666666665, 'seq/s': 26913.745619825044, 'global_steps': 317, 'samples_trained': 2921472, 'skipped_steps': 9, 'timestamp': 1593108091.960047}
  0: {'training_steps': 327, 'average_loss': 1.6611328125, 'step_loss': 1.6611328125, 'learning_rate': 0.0011520000000000002, 'seq/s': 26897.340280561122, 'global_steps': 318, 'samples_trained': 2930688, 'skipped_steps': 9, 'timestamp': 1593108092.3026843}
  0: {'training_steps': 328, 'average_loss': 1.5908203125, 'step_loss': 1.5908203125, 'learning_rate': 0.0011493333333333334, 'seq/s': 26893.129808083653, 'global_steps': 319, 'samples_trained': 2939904, 'skipped_steps': 9, 'timestamp': 1593108092.645375}
  0: {'training_steps': 329, 'average_loss': 1.9345703125, 'step_loss': 1.9345703125, 'learning_rate': 0.0011466666666666665, 'seq/s': 25274.458096584407, 'global_steps': 320, 'samples_trained': 2949120, 'skipped_steps': 9, 'timestamp': 1593108093.0100129}
  0: {'training_steps': 330, 'average_loss': 1.333984375, 'step_loss': 1.333984375, 'learning_rate': 0.001144, 'seq/s': 27156.829806068337, 'global_steps': 321, 'samples_trained': 2958336, 'skipped_steps': 9, 'timestamp': 1593108093.3493757}
  0: {'training_steps': 331, 'average_loss': 1.53125, 'step_loss': 1.53125, 'learning_rate': 0.0011413333333333334, 'seq/s': 26848.952680748207, 'global_steps': 322, 'samples_trained': 2967552, 'skipped_steps': 9, 'timestamp': 1593108093.6926303}
  0: {'training_steps': 332, 'average_loss': 1.5703125, 'step_loss': 1.5703125, 'learning_rate': 0.0011386666666666667, 'seq/s': 26913.595708831042, 'global_steps': 323, 'samples_trained': 2976768, 'skipped_steps': 9, 'timestamp': 1593108094.0350604}
  0: {'training_steps': 333, 'average_loss': 1.6044921875, 'step_loss': 1.6044921875, 'learning_rate': 0.001136, 'seq/s': 24192.289115546206, 'global_steps': 324, 'samples_trained': 2985984, 'skipped_steps': 9, 'timestamp': 1593108094.4160094}
  0: {'training_steps': 334, 'average_loss': 1.7783203125, 'step_loss': 1.7783203125, 'learning_rate': 0.0011333333333333334, 'seq/s': 26938.297319390775, 'global_steps': 325, 'samples_trained': 2995200, 'skipped_steps': 9, 'timestamp': 1593108094.7581258}
  0: :::MLLOG {"namespace": "", "time_ms": 1593108095466, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.6961079239845276, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 843, "epoch_num": 1}}
  0: {'global_steps': 326, 'eval_loss': 1.435320496559143, 'eval_mlm_accuracy': 0.6961079239845276}
  0: {'training_steps': 335, 'average_loss': 1.5478515625, 'step_loss': 1.5478515625, 'learning_rate': 0.0011306666666666668, 'seq/s': 12993.75490408314, 'global_steps': 326, 'samples_trained': 3004416, 'skipped_steps': 9, 'timestamp': 1593108095.4673908}
  0: {'training_steps': 336, 'average_loss': 1.7392578125, 'step_loss': 1.7392578125, 'learning_rate': 0.001128, 'seq/s': 28087.979636695512, 'global_steps': 327, 'samples_trained': 3013632, 'skipped_steps': 9, 'timestamp': 1593108095.7955036}
  0: {'training_steps': 337, 'average_loss': 1.4326171875, 'step_loss': 1.4326171875, 'learning_rate': 0.0011253333333333332, 'seq/s': 23895.69618512861, 'global_steps': 328, 'samples_trained': 3022848, 'skipped_steps': 9, 'timestamp': 1593108096.1811807}
  0: {'training_steps': 338, 'average_loss': 1.572265625, 'step_loss': 1.572265625, 'learning_rate': 0.0011226666666666668, 'seq/s': 27118.76338339229, 'global_steps': 329, 'samples_trained': 3032064, 'skipped_steps': 9, 'timestamp': 1593108096.5210202}
  0: {'training_steps': 339, 'average_loss': 1.771484375, 'step_loss': 1.771484375, 'learning_rate': 0.0011200000000000001, 'seq/s': 26995.451253826217, 'global_steps': 330, 'samples_trained': 3041280, 'skipped_steps': 9, 'timestamp': 1593108096.862412}
  0: {'training_steps': 340, 'average_loss': 1.431640625, 'step_loss': 1.431640625, 'learning_rate': 0.0011173333333333332, 'seq/s': 26991.907368626864, 'global_steps': 331, 'samples_trained': 3050496, 'skipped_steps': 9, 'timestamp': 1593108097.203849}
  0: {'training_steps': 341, 'average_loss': 1.3681640625, 'step_loss': 1.3681640625, 'learning_rate': 0.0011146666666666668, 'seq/s': 24117.16687671732, 'global_steps': 332, 'samples_trained': 3059712, 'skipped_steps': 9, 'timestamp': 1593108097.5859842}
  0: {'training_steps': 342, 'average_loss': 1.3837890625, 'step_loss': 1.3837890625, 'learning_rate': 0.0011120000000000001, 'seq/s': 27067.60522099602, 'global_steps': 333, 'samples_trained': 3068928, 'skipped_steps': 9, 'timestamp': 1593108097.926466}
  0: {'training_steps': 343, 'average_loss': 1.552734375, 'step_loss': 1.552734375, 'learning_rate': 0.0011093333333333333, 'seq/s': 26995.69634411726, 'global_steps': 334, 'samples_trained': 3078144, 'skipped_steps': 9, 'timestamp': 1593108098.2678552}
  0: {'training_steps': 344, 'average_loss': 1.84765625, 'step_loss': 1.84765625, 'learning_rate': 0.0011066666666666666, 'seq/s': 26982.750358620968, 'global_steps': 335, 'samples_trained': 3087360, 'skipped_steps': 9, 'timestamp': 1593108098.6094077}
  0: {'training_steps': 345, 'average_loss': 1.5400390625, 'step_loss': 1.5400390625, 'learning_rate': 0.0011040000000000002, 'seq/s': 26819.147625424095, 'global_steps': 336, 'samples_trained': 3096576, 'skipped_steps': 9, 'timestamp': 1593108098.953044}
  0: {'training_steps': 346, 'average_loss': 1.75, 'step_loss': 1.75, 'learning_rate': 0.0011013333333333333, 'seq/s': 27065.653115882207, 'global_steps': 337, 'samples_trained': 3105792, 'skipped_steps': 9, 'timestamp': 1593108099.2935503}
  0: {'training_steps': 347, 'average_loss': 1.509765625, 'step_loss': 1.509765625, 'learning_rate': 0.0010986666666666666, 'seq/s': 26875.21729705076, 'global_steps': 338, 'samples_trained': 3115008, 'skipped_steps': 9, 'timestamp': 1593108099.6364694}
  0: {'training_steps': 348, 'average_loss': 1.6650390625, 'step_loss': 1.6650390625, 'learning_rate': 0.0010960000000000002, 'seq/s': 26970.042591432317, 'global_steps': 339, 'samples_trained': 3124224, 'skipped_steps': 9, 'timestamp': 1593108099.978183}
  0: {'training_steps': 349, 'average_loss': 1.5283203125, 'step_loss': 1.5283203125, 'learning_rate': 0.0010933333333333333, 'seq/s': 27022.30565812451, 'global_steps': 340, 'samples_trained': 3133440, 'skipped_steps': 9, 'timestamp': 1593108100.3192353}
  0: {'training_steps': 350, 'average_loss': 1.6298828125, 'step_loss': 1.6298828125, 'learning_rate': 0.0010906666666666667, 'seq/s': 26946.297070286768, 'global_steps': 341, 'samples_trained': 3142656, 'skipped_steps': 9, 'timestamp': 1593108100.6612499}
  0: {'training_steps': 351, 'average_loss': 1.7490234375, 'step_loss': 1.7490234375, 'learning_rate': 0.0010880000000000002, 'seq/s': 26990.88961616156, 'global_steps': 342, 'samples_trained': 3151872, 'skipped_steps': 9, 'timestamp': 1593108101.0026999}
  0: {'training_steps': 352, 'average_loss': 1.5400390625, 'step_loss': 1.5400390625, 'learning_rate': 0.0010853333333333333, 'seq/s': 26958.02359603455, 'global_steps': 343, 'samples_trained': 3161088, 'skipped_steps': 9, 'timestamp': 1593108101.3445659}
  0: {'training_steps': 353, 'average_loss': 1.3955078125, 'step_loss': 1.3955078125, 'learning_rate': 0.0010826666666666667, 'seq/s': 26983.164739216598, 'global_steps': 344, 'samples_trained': 3170304, 'skipped_steps': 9, 'timestamp': 1593108101.6861134}
  0: {'training_steps': 354, 'average_loss': 1.7294921875, 'step_loss': 1.7294921875, 'learning_rate': 0.00108, 'seq/s': 26982.67501806188, 'global_steps': 345, 'samples_trained': 3179520, 'skipped_steps': 9, 'timestamp': 1593108102.027667}
  0: {'training_steps': 355, 'average_loss': 1.427734375, 'step_loss': 1.427734375, 'learning_rate': 0.0010773333333333334, 'seq/s': 27062.88653664527, 'global_steps': 346, 'samples_trained': 3188736, 'skipped_steps': 9, 'timestamp': 1593108102.368208}
  0: {'training_steps': 356, 'average_loss': 1.3984375, 'step_loss': 1.3984375, 'learning_rate': 0.0010746666666666667, 'seq/s': 27076.516460377836, 'global_steps': 347, 'samples_trained': 3197952, 'skipped_steps': 9, 'timestamp': 1593108102.7085779}
  0: {'training_steps': 357, 'average_loss': 1.353515625, 'step_loss': 1.353515625, 'learning_rate': 0.001072, 'seq/s': 26957.440787326454, 'global_steps': 348, 'samples_trained': 3207168, 'skipped_steps': 9, 'timestamp': 1593108103.0504515}
  0: {'training_steps': 358, 'average_loss': 1.5283203125, 'step_loss': 1.5283203125, 'learning_rate': 0.0010693333333333334, 'seq/s': 26910.223152923354, 'global_steps': 349, 'samples_trained': 3216384, 'skipped_steps': 9, 'timestamp': 1593108103.3929248}
  0: {'training_steps': 359, 'average_loss': 1.5087890625, 'step_loss': 1.5087890625, 'learning_rate': 0.0010666666666666667, 'seq/s': 26893.616284206917, 'global_steps': 350, 'samples_trained': 3225600, 'skipped_steps': 9, 'timestamp': 1593108103.735609}
  0: {'training_steps': 360, 'average_loss': 1.724609375, 'step_loss': 1.724609375, 'learning_rate': 0.001064, 'seq/s': 26830.42850162143, 'global_steps': 351, 'samples_trained': 3234816, 'skipped_steps': 9, 'timestamp': 1593108104.0791006}
  0: {'training_steps': 361, 'average_loss': 1.4287109375, 'step_loss': 1.4287109375, 'learning_rate': 0.0010613333333333332, 'seq/s': 26838.028894060666, 'global_steps': 352, 'samples_trained': 3244032, 'skipped_steps': 9, 'timestamp': 1593108104.422495}
  0: {'training_steps': 362, 'average_loss': 1.24609375, 'step_loss': 1.24609375, 'learning_rate': 0.0010586666666666667, 'seq/s': 27092.53350864823, 'global_steps': 353, 'samples_trained': 3253248, 'skipped_steps': 9, 'timestamp': 1593108104.7626634}
  0: {'training_steps': 363, 'average_loss': 1.587890625, 'step_loss': 1.587890625, 'learning_rate': 0.001056, 'seq/s': 26950.993972497297, 'global_steps': 354, 'samples_trained': 3262464, 'skipped_steps': 9, 'timestamp': 1593108105.1046185}
  0: {'training_steps': 364, 'average_loss': 1.35546875, 'step_loss': 1.35546875, 'learning_rate': 0.0010533333333333332, 'seq/s': 26950.29872773821, 'global_steps': 355, 'samples_trained': 3271680, 'skipped_steps': 9, 'timestamp': 1593108105.446582}
  0: {'training_steps': 365, 'average_loss': 1.29296875, 'step_loss': 1.29296875, 'learning_rate': 0.0010506666666666668, 'seq/s': 26932.685307064912, 'global_steps': 356, 'samples_trained': 3280896, 'skipped_steps': 9, 'timestamp': 1593108105.7887695}
  0: {'training_steps': 366, 'average_loss': 1.6982421875, 'step_loss': 1.6982421875, 'learning_rate': 0.001048, 'seq/s': 26955.598525537407, 'global_steps': 357, 'samples_trained': 3290112, 'skipped_steps': 9, 'timestamp': 1593108106.1306658}
  0: {'training_steps': 367, 'average_loss': 1.3740234375, 'step_loss': 1.3740234375, 'learning_rate': 0.0010453333333333332, 'seq/s': 26962.81862405223, 'global_steps': 358, 'samples_trained': 3299328, 'skipped_steps': 9, 'timestamp': 1593108106.4724708}
  0: {'training_steps': 368, 'average_loss': 1.3974609375, 'step_loss': 1.3974609375, 'learning_rate': 0.0010426666666666666, 'seq/s': 27026.651161619975, 'global_steps': 359, 'samples_trained': 3308544, 'skipped_steps': 9, 'timestamp': 1593108106.8134685}
  0: {'training_steps': 369, 'average_loss': 1.556640625, 'step_loss': 1.556640625, 'learning_rate': 0.0010400000000000001, 'seq/s': 26924.037253037728, 'global_steps': 360, 'samples_trained': 3317760, 'skipped_steps': 9, 'timestamp': 1593108107.155766}
  0: {'training_steps': 370, 'average_loss': 1.5859375, 'step_loss': 1.5859375, 'learning_rate': 0.0010373333333333332, 'seq/s': 26948.382292780665, 'global_steps': 361, 'samples_trained': 3326976, 'skipped_steps': 9, 'timestamp': 1593108107.4977543}
  0: {'training_steps': 371, 'average_loss': 1.4990234375, 'step_loss': 1.4990234375, 'learning_rate': 0.0010346666666666666, 'seq/s': 27163.031899501286, 'global_steps': 362, 'samples_trained': 3336192, 'skipped_steps': 9, 'timestamp': 1593108107.8370402}
  0: {'training_steps': 372, 'average_loss': 1.3017578125, 'step_loss': 1.3017578125, 'learning_rate': 0.0010320000000000001, 'seq/s': 26949.697463899472, 'global_steps': 363, 'samples_trained': 3345408, 'skipped_steps': 9, 'timestamp': 1593108108.1790118}
  0: {'training_steps': 373, 'average_loss': 1.7705078125, 'step_loss': 1.7705078125, 'learning_rate': 0.0010293333333333333, 'seq/s': 26993.84885005538, 'global_steps': 364, 'samples_trained': 3354624, 'skipped_steps': 9, 'timestamp': 1593108108.5204241}
  0: {'training_steps': 374, 'average_loss': 1.55078125, 'step_loss': 1.55078125, 'learning_rate': 0.0010266666666666666, 'seq/s': 26915.43223359055, 'global_steps': 365, 'samples_trained': 3363840, 'skipped_steps': 9, 'timestamp': 1593108108.8628306}
  0: {'training_steps': 375, 'average_loss': 1.5703125, 'step_loss': 1.5703125, 'learning_rate': 0.001024, 'seq/s': 26996.771023314966, 'global_steps': 366, 'samples_trained': 3373056, 'skipped_steps': 9, 'timestamp': 1593108109.204206}
  0: {'training_steps': 376, 'average_loss': 1.6142578125, 'step_loss': 1.6142578125, 'learning_rate': 0.0010213333333333333, 'seq/s': 26942.728519361903, 'global_steps': 367, 'samples_trained': 3382272, 'skipped_steps': 9, 'timestamp': 1593108109.5462656}
  0: {'training_steps': 377, 'average_loss': 1.76171875, 'step_loss': 1.76171875, 'learning_rate': 0.0010186666666666669, 'seq/s': 26961.558588739463, 'global_steps': 368, 'samples_trained': 3391488, 'skipped_steps': 9, 'timestamp': 1593108109.8880866}
  0: {'training_steps': 378, 'average_loss': 1.291015625, 'step_loss': 1.291015625, 'learning_rate': 0.001016, 'seq/s': 26980.659814250477, 'global_steps': 369, 'samples_trained': 3400704, 'skipped_steps': 9, 'timestamp': 1593108110.2296658}
  0: {'training_steps': 379, 'average_loss': 1.6220703125, 'step_loss': 1.6220703125, 'learning_rate': 0.0010133333333333333, 'seq/s': 27054.078939301256, 'global_steps': 370, 'samples_trained': 3409920, 'skipped_steps': 9, 'timestamp': 1593108110.570318}
  0: {'training_steps': 380, 'average_loss': 1.662109375, 'step_loss': 1.662109375, 'learning_rate': 0.0010106666666666669, 'seq/s': 26830.093289336874, 'global_steps': 371, 'samples_trained': 3419136, 'skipped_steps': 9, 'timestamp': 1593108110.913814}
  0: {'training_steps': 381, 'average_loss': 1.490234375, 'step_loss': 1.490234375, 'learning_rate': 0.001008, 'seq/s': 26879.683841609276, 'global_steps': 372, 'samples_trained': 3428352, 'skipped_steps': 9, 'timestamp': 1593108111.2566764}
  0: {'training_steps': 382, 'average_loss': 1.3984375, 'step_loss': 1.3984375, 'learning_rate': 0.0010053333333333331, 'seq/s': 26885.535578831106, 'global_steps': 373, 'samples_trained': 3437568, 'skipped_steps': 9, 'timestamp': 1593108111.599464}
  0: {'training_steps': 383, 'average_loss': 1.3720703125, 'step_loss': 1.3720703125, 'learning_rate': 0.001002666666666667, 'seq/s': 27099.618172551323, 'global_steps': 374, 'samples_trained': 3446784, 'skipped_steps': 9, 'timestamp': 1593108111.9395435}
  0: {'training_steps': 384, 'average_loss': 1.271484375, 'step_loss': 1.271484375, 'learning_rate': 0.001, 'seq/s': 26965.696459726943, 'global_steps': 375, 'samples_trained': 3456000, 'skipped_steps': 9, 'timestamp': 1593108112.2813122}
  0: {'training_steps': 385, 'average_loss': 1.2001953125, 'step_loss': 1.2001953125, 'learning_rate': 0.0009973333333333334, 'seq/s': 26923.999746464608, 'global_steps': 376, 'samples_trained': 3465216, 'skipped_steps': 9, 'timestamp': 1593108112.62361}
  0: {'training_steps': 386, 'average_loss': 1.443359375, 'step_loss': 1.443359375, 'learning_rate': 0.0009946666666666667, 'seq/s': 27065.861579697685, 'global_steps': 377, 'samples_trained': 3474432, 'skipped_steps': 9, 'timestamp': 1593108112.9641135}
  0: {'training_steps': 387, 'average_loss': 1.330078125, 'step_loss': 1.330078125, 'learning_rate': 0.000992, 'seq/s': 26926.100275497447, 'global_steps': 378, 'samples_trained': 3483648, 'skipped_steps': 9, 'timestamp': 1593108113.3063846}
  0: {'training_steps': 388, 'average_loss': 1.484375, 'step_loss': 1.484375, 'learning_rate': 0.0009893333333333334, 'seq/s': 26937.18974713483, 'global_steps': 379, 'samples_trained': 3492864, 'skipped_steps': 9, 'timestamp': 1593108113.6485145}
  0: :::MLLOG {"namespace": "", "time_ms": 1593108114123, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.7017184495925903, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 843, "epoch_num": 1}}
  0: {'global_steps': 380, 'eval_loss': 1.3980202674865723, 'eval_mlm_accuracy': 0.7017184495925903}
  0: {'training_steps': 389, 'average_loss': 1.3154296875, 'step_loss': 1.3154296875, 'learning_rate': 0.0009866666666666665, 'seq/s': 19372.52957095948, 'global_steps': 380, 'samples_trained': 3502080, 'skipped_steps': 9, 'timestamp': 1593108114.1242404}
  0: {'training_steps': 390, 'average_loss': 1.5341796875, 'step_loss': 1.5341796875, 'learning_rate': 0.000984, 'seq/s': 28113.208373152498, 'global_steps': 381, 'samples_trained': 3511296, 'skipped_steps': 9, 'timestamp': 1593108114.4520588}
  0: {'training_steps': 391, 'average_loss': 1.5595703125, 'step_loss': 1.5595703125, 'learning_rate': 0.0009813333333333334, 'seq/s': 26976.837404257843, 'global_steps': 382, 'samples_trained': 3520512, 'skipped_steps': 9, 'timestamp': 1593108114.7936864}
  0: {'training_steps': 392, 'average_loss': 1.595703125, 'step_loss': 1.595703125, 'learning_rate': 0.0009786666666666665, 'seq/s': 26907.488256119752, 'global_steps': 383, 'samples_trained': 3529728, 'skipped_steps': 9, 'timestamp': 1593108115.1361942}
  0: {'training_steps': 393, 'average_loss': 1.490234375, 'step_loss': 1.490234375, 'learning_rate': 0.000976, 'seq/s': 27000.203027985288, 'global_steps': 384, 'samples_trained': 3538944, 'skipped_steps': 9, 'timestamp': 1593108115.477526}
  0: {'training_steps': 394, 'average_loss': 1.3701171875, 'step_loss': 1.3701171875, 'learning_rate': 0.0009733333333333334, 'seq/s': 26913.576970074213, 'global_steps': 385, 'samples_trained': 3548160, 'skipped_steps': 9, 'timestamp': 1593108115.8199565}
  0: {'training_steps': 395, 'average_loss': 1.4462890625, 'step_loss': 1.4462890625, 'learning_rate': 0.0009706666666666666, 'seq/s': 26953.756515530167, 'global_steps': 386, 'samples_trained': 3557376, 'skipped_steps': 9, 'timestamp': 1593108116.1618764}
  0: {'training_steps': 396, 'average_loss': 1.443359375, 'step_loss': 1.443359375, 'learning_rate': 0.000968, 'seq/s': 26925.2937836264, 'global_steps': 387, 'samples_trained': 3566592, 'skipped_steps': 9, 'timestamp': 1593108116.5041583}
  0: {'training_steps': 397, 'average_loss': 1.814453125, 'step_loss': 1.814453125, 'learning_rate': 0.0009653333333333333, 'seq/s': 26846.06241835172, 'global_steps': 388, 'samples_trained': 3575808, 'skipped_steps': 9, 'timestamp': 1593108116.84745}
  0: {'training_steps': 398, 'average_loss': 1.6201171875, 'step_loss': 1.6201171875, 'learning_rate': 0.0009626666666666666, 'seq/s': 27010.42878395196, 'global_steps': 389, 'samples_trained': 3585024, 'skipped_steps': 9, 'timestamp': 1593108117.1886528}
  0: {'training_steps': 399, 'average_loss': 1.51953125, 'step_loss': 1.51953125, 'learning_rate': 0.00096, 'seq/s': 26871.275263223022, 'global_steps': 390, 'samples_trained': 3594240, 'skipped_steps': 9, 'timestamp': 1593108117.5316222}
  0: {'training_steps': 400, 'average_loss': 1.626953125, 'step_loss': 1.626953125, 'learning_rate': 0.0009573333333333334, 'seq/s': 26988.47747242658, 'global_steps': 391, 'samples_trained': 3603456, 'skipped_steps': 9, 'timestamp': 1593108117.8731024}
  0: {'training_steps': 401, 'average_loss': 1.5107421875, 'step_loss': 1.5107421875, 'learning_rate': 0.0009546666666666668, 'seq/s': 26977.759955780006, 'global_steps': 392, 'samples_trained': 3612672, 'skipped_steps': 9, 'timestamp': 1593108118.2147186}
  0: {'training_steps': 402, 'average_loss': 1.544921875, 'step_loss': 1.544921875, 'learning_rate': 0.0009519999999999999, 'seq/s': 26939.611506308956, 'global_steps': 393, 'samples_trained': 3621888, 'skipped_steps': 9, 'timestamp': 1593108118.5568182}
  0: {'training_steps': 403, 'average_loss': 1.3544921875, 'step_loss': 1.3544921875, 'learning_rate': 0.0009493333333333334, 'seq/s': 26845.89461577888, 'global_steps': 394, 'samples_trained': 3631104, 'skipped_steps': 9, 'timestamp': 1593108118.9001117}
  0: {'training_steps': 404, 'average_loss': 1.53515625, 'step_loss': 1.53515625, 'learning_rate': 0.0009466666666666668, 'seq/s': 27018.698062788982, 'global_steps': 395, 'samples_trained': 3640320, 'skipped_steps': 9, 'timestamp': 1593108119.2412102}
  0: {'training_steps': 405, 'average_loss': 1.4111328125, 'step_loss': 1.4111328125, 'learning_rate': 0.000944, 'seq/s': 27042.34439007873, 'global_steps': 396, 'samples_trained': 3649536, 'skipped_steps': 9, 'timestamp': 1593108119.5820103}
  0: {'training_steps': 406, 'average_loss': 1.67578125, 'step_loss': 1.67578125, 'learning_rate': 0.0009413333333333334, 'seq/s': 26918.61863641935, 'global_steps': 397, 'samples_trained': 3658752, 'skipped_steps': 9, 'timestamp': 1593108119.9243765}
  0: {'training_steps': 407, 'average_loss': 1.85546875, 'step_loss': 1.85546875, 'learning_rate': 0.0009386666666666668, 'seq/s': 26919.218514267886, 'global_steps': 398, 'samples_trained': 3667968, 'skipped_steps': 9, 'timestamp': 1593108120.266735}
  0: {'training_steps': 408, 'average_loss': 1.337890625, 'step_loss': 1.337890625, 'learning_rate': 0.000936, 'seq/s': 27019.453500393185, 'global_steps': 399, 'samples_trained': 3677184, 'skipped_steps': 9, 'timestamp': 1593108120.6078236}
  0: {'training_steps': 409, 'average_loss': 1.4677734375, 'step_loss': 1.4677734375, 'learning_rate': 0.0009333333333333333, 'seq/s': 26913.85805416651, 'global_steps': 400, 'samples_trained': 3686400, 'skipped_steps': 9, 'timestamp': 1593108120.9502504}
  0: {'training_steps': 410, 'average_loss': 1.4775390625, 'step_loss': 1.4775390625, 'learning_rate': 0.0009306666666666668, 'seq/s': 26905.746454129025, 'global_steps': 401, 'samples_trained': 3695616, 'skipped_steps': 9, 'timestamp': 1593108121.2927806}
  0: {'training_steps': 411, 'average_loss': 1.6103515625, 'step_loss': 1.6103515625, 'learning_rate': 0.000928, 'seq/s': 26949.47199687661, 'global_steps': 402, 'samples_trained': 3704832, 'skipped_steps': 9, 'timestamp': 1593108121.6347551}
  0: {'training_steps': 412, 'average_loss': 1.5498046875, 'step_loss': 1.5498046875, 'learning_rate': 0.0009253333333333333, 'seq/s': 26974.164243939576, 'global_steps': 403, 'samples_trained': 3714048, 'skipped_steps': 9, 'timestamp': 1593108121.9764168}
  0: {'training_steps': 413, 'average_loss': 1.673828125, 'step_loss': 1.673828125, 'learning_rate': 0.0009226666666666668, 'seq/s': 26882.001797015175, 'global_steps': 404, 'samples_trained': 3723264, 'skipped_steps': 9, 'timestamp': 1593108122.3192496}
  0: {'training_steps': 414, 'average_loss': 1.4248046875, 'step_loss': 1.4248046875, 'learning_rate': 0.0009199999999999999, 'seq/s': 27044.747250902023, 'global_steps': 405, 'samples_trained': 3732480, 'skipped_steps': 9, 'timestamp': 1593108122.6600192}
  0: {'training_steps': 415, 'average_loss': 1.515625, 'step_loss': 1.515625, 'learning_rate': 0.0009173333333333334, 'seq/s': 27062.166560834856, 'global_steps': 406, 'samples_trained': 3741696, 'skipped_steps': 9, 'timestamp': 1593108123.0005696}
  0: {'training_steps': 416, 'average_loss': 1.498046875, 'step_loss': 1.498046875, 'learning_rate': 0.0009146666666666667, 'seq/s': 26996.563622420974, 'global_steps': 407, 'samples_trained': 3750912, 'skipped_steps': 9, 'timestamp': 1593108123.3419473}
  0: {'training_steps': 417, 'average_loss': 1.3720703125, 'step_loss': 1.3720703125, 'learning_rate': 0.0009119999999999999, 'seq/s': 26945.470584913906, 'global_steps': 408, 'samples_trained': 3760128, 'skipped_steps': 9, 'timestamp': 1593108123.6839721}
  0: {'training_steps': 418, 'average_loss': 1.3251953125, 'step_loss': 1.3251953125, 'learning_rate': 0.0009093333333333334, 'seq/s': 26921.262048845172, 'global_steps': 409, 'samples_trained': 3769344, 'skipped_steps': 9, 'timestamp': 1593108124.0263045}
  0: {'training_steps': 419, 'average_loss': 1.333984375, 'step_loss': 1.333984375, 'learning_rate': 0.0009066666666666667, 'seq/s': 27044.65264200737, 'global_steps': 410, 'samples_trained': 3778560, 'skipped_steps': 9, 'timestamp': 1593108124.3670757}
  0: {'training_steps': 420, 'average_loss': 1.48046875, 'step_loss': 1.48046875, 'learning_rate': 0.000904, 'seq/s': 27012.429560648976, 'global_steps': 411, 'samples_trained': 3787776, 'skipped_steps': 9, 'timestamp': 1593108124.708253}
  0: {'training_steps': 421, 'average_loss': 1.43359375, 'step_loss': 1.43359375, 'learning_rate': 0.0009013333333333333, 'seq/s': 27011.37250961355, 'global_steps': 412, 'samples_trained': 3796992, 'skipped_steps': 9, 'timestamp': 1593108125.0494437}
  0: {'training_steps': 422, 'average_loss': 1.4931640625, 'step_loss': 1.4931640625, 'learning_rate': 0.0008986666666666668, 'seq/s': 26941.282586271307, 'global_steps': 413, 'samples_trained': 3806208, 'skipped_steps': 9, 'timestamp': 1593108125.3915224}
  0: {'training_steps': 423, 'average_loss': 1.6328125, 'step_loss': 1.6328125, 'learning_rate': 0.0008959999999999999, 'seq/s': 27031.905418730235, 'global_steps': 414, 'samples_trained': 3815424, 'skipped_steps': 9, 'timestamp': 1593108125.7324538}
  0: {'training_steps': 424, 'average_loss': 1.40234375, 'step_loss': 1.40234375, 'learning_rate': 0.0008933333333333333, 'seq/s': 26946.898182403762, 'global_steps': 415, 'samples_trained': 3824640, 'skipped_steps': 9, 'timestamp': 1593108126.074461}
  0: {'training_steps': 425, 'average_loss': 1.5478515625, 'step_loss': 1.5478515625, 'learning_rate': 0.0008906666666666668, 'seq/s': 26950.65574083667, 'global_steps': 416, 'samples_trained': 3833856, 'skipped_steps': 9, 'timestamp': 1593108126.41642}
  0: {'training_steps': 426, 'average_loss': 1.314453125, 'step_loss': 1.314453125, 'learning_rate': 0.0008879999999999999, 'seq/s': 26844.067567980488, 'global_steps': 417, 'samples_trained': 3843072, 'skipped_steps': 9, 'timestamp': 1593108126.7597373}
  0: {'training_steps': 427, 'average_loss': 1.578125, 'step_loss': 1.578125, 'learning_rate': 0.0008853333333333333, 'seq/s': 26967.013321408365, 'global_steps': 418, 'samples_trained': 3852288, 'skipped_steps': 9, 'timestamp': 1593108127.1014893}
  0: {'training_steps': 428, 'average_loss': 1.291015625, 'step_loss': 1.291015625, 'learning_rate': 0.0008826666666666667, 'seq/s': 26959.151684480323, 'global_steps': 419, 'samples_trained': 3861504, 'skipped_steps': 9, 'timestamp': 1593108127.4433408}
  0: {'training_steps': 429, 'average_loss': 1.44921875, 'step_loss': 1.44921875, 'learning_rate': 0.0008799999999999999, 'seq/s': 26897.246700182655, 'global_steps': 420, 'samples_trained': 3870720, 'skipped_steps': 9, 'timestamp': 1593108127.785979}
  0: {'training_steps': 430, 'average_loss': 1.30078125, 'step_loss': 1.30078125, 'learning_rate': 0.0008773333333333333, 'seq/s': 26997.6572266689, 'global_steps': 421, 'samples_trained': 3879936, 'skipped_steps': 9, 'timestamp': 1593108128.1273432}
  0: {'training_steps': 431, 'average_loss': 1.2685546875, 'step_loss': 1.2685546875, 'learning_rate': 0.0008746666666666667, 'seq/s': 26986.70632892127, 'global_steps': 422, 'samples_trained': 3889152, 'skipped_steps': 9, 'timestamp': 1593108128.4688456}
  0: {'training_steps': 432, 'average_loss': 1.5, 'step_loss': 1.5, 'learning_rate': 0.0008720000000000002, 'seq/s': 26957.70398813866, 'global_steps': 423, 'samples_trained': 3898368, 'skipped_steps': 9, 'timestamp': 1593108128.8107154}
  0: {'training_steps': 433, 'average_loss': 1.4873046875, 'step_loss': 1.4873046875, 'learning_rate': 0.0008693333333333333, 'seq/s': 26963.9094960002, 'global_steps': 424, 'samples_trained': 3907584, 'skipped_steps': 9, 'timestamp': 1593108129.1525066}
  0: {'training_steps': 434, 'average_loss': 1.669921875, 'step_loss': 1.669921875, 'learning_rate': 0.0008666666666666667, 'seq/s': 26986.517923079984, 'global_steps': 425, 'samples_trained': 3916800, 'skipped_steps': 9, 'timestamp': 1593108129.4940119}
  0: {'training_steps': 435, 'average_loss': 1.392578125, 'step_loss': 1.392578125, 'learning_rate': 0.0008640000000000001, 'seq/s': 26956.83920476893, 'global_steps': 426, 'samples_trained': 3926016, 'skipped_steps': 9, 'timestamp': 1593108129.8358927}
  0: {'training_steps': 436, 'average_loss': 1.435546875, 'step_loss': 1.435546875, 'learning_rate': 0.0008613333333333333, 'seq/s': 27008.918960049134, 'global_steps': 427, 'samples_trained': 3935232, 'skipped_steps': 9, 'timestamp': 1593108130.1771145}
  0: {'training_steps': 437, 'average_loss': 1.40234375, 'step_loss': 1.40234375, 'learning_rate': 0.0008586666666666668, 'seq/s': 27040.339179584756, 'global_steps': 428, 'samples_trained': 3944448, 'skipped_steps': 9, 'timestamp': 1593108130.5179398}
  0: {'training_steps': 438, 'average_loss': 1.408203125, 'step_loss': 1.408203125, 'learning_rate': 0.0008560000000000001, 'seq/s': 26955.203787913128, 'global_steps': 429, 'samples_trained': 3953664, 'skipped_steps': 9, 'timestamp': 1593108130.8598413}
  0: {'training_steps': 439, 'average_loss': 1.3251953125, 'step_loss': 1.3251953125, 'learning_rate': 0.0008533333333333333, 'seq/s': 27018.169281599898, 'global_steps': 430, 'samples_trained': 3962880, 'skipped_steps': 9, 'timestamp': 1593108131.2009463}
  0: {'training_steps': 440, 'average_loss': 1.4208984375, 'step_loss': 1.4208984375, 'learning_rate': 0.0008506666666666667, 'seq/s': 26946.203148941036, 'global_steps': 431, 'samples_trained': 3972096, 'skipped_steps': 9, 'timestamp': 1593108131.5429626}
  0: {'training_steps': 441, 'average_loss': 1.53125, 'step_loss': 1.53125, 'learning_rate': 0.0008480000000000001, 'seq/s': 26863.712039385122, 'global_steps': 432, 'samples_trained': 3981312, 'skipped_steps': 9, 'timestamp': 1593108131.8860283}
  0: {'training_steps': 442, 'average_loss': 1.3955078125, 'step_loss': 1.3955078125, 'learning_rate': 0.0008453333333333332, 'seq/s': 26964.041158928965, 'global_steps': 433, 'samples_trained': 3990528, 'skipped_steps': 9, 'timestamp': 1593108132.2278178}
  0: {'training_steps': 443, 'average_loss': 1.33984375, 'step_loss': 1.33984375, 'learning_rate': 0.0008426666666666667, 'seq/s': 26970.607126908995, 'global_steps': 434, 'samples_trained': 3999744, 'skipped_steps': 9, 'timestamp': 1593108132.5695245}
  0: :::MLLOG {"namespace": "", "time_ms": 1593108133045, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.7046310305595398, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 843, "epoch_num": 1}}
  0: {'global_steps': 435, 'eval_loss': 1.375322699546814, 'eval_mlm_accuracy': 0.7046310305595398}
  0: {'training_steps': 444, 'average_loss': 1.3486328125, 'step_loss': 1.3486328125, 'learning_rate': 0.0008400000000000001, 'seq/s': 19349.924518951357, 'global_steps': 435, 'samples_trained': 4008960, 'skipped_steps': 9, 'timestamp': 1593108133.0458064}
  0: {'training_steps': 445, 'average_loss': 1.6005859375, 'step_loss': 1.6005859375, 'learning_rate': 0.0008373333333333333, 'seq/s': 28101.517852651108, 'global_steps': 436, 'samples_trained': 4018176, 'skipped_steps': 9, 'timestamp': 1593108133.3737612}
  0: {'training_steps': 446, 'average_loss': 1.0859375, 'step_loss': 1.0859375, 'learning_rate': 0.0008346666666666667, 'seq/s': 27078.64085553885, 'global_steps': 437, 'samples_trained': 4027392, 'skipped_steps': 9, 'timestamp': 1593108133.7141042}
  0: {'training_steps': 447, 'average_loss': 1.6884765625, 'step_loss': 1.6884765625, 'learning_rate': 0.0008320000000000001, 'seq/s': 26995.564371869088, 'global_steps': 438, 'samples_trained': 4036608, 'skipped_steps': 9, 'timestamp': 1593108134.0554945}
  0: {'training_steps': 448, 'average_loss': 1.46484375, 'step_loss': 1.46484375, 'learning_rate': 0.0008293333333333333, 'seq/s': 27074.525142623002, 'global_steps': 439, 'samples_trained': 4045824, 'skipped_steps': 9, 'timestamp': 1593108134.395889}
  0: {'training_steps': 449, 'average_loss': 1.138671875, 'step_loss': 1.138671875, 'learning_rate': 0.0008266666666666666, 'seq/s': 25291.110497248414, 'global_steps': 440, 'samples_trained': 4055040, 'skipped_steps': 9, 'timestamp': 1593108134.760287}
  0: {'training_steps': 450, 'average_loss': 1.4462890625, 'step_loss': 1.4462890625, 'learning_rate': 0.0008240000000000001, 'seq/s': 24584.628028024974, 'global_steps': 441, 'samples_trained': 4064256, 'skipped_steps': 9, 'timestamp': 1593108135.1351562}
  0: {'training_steps': 451, 'average_loss': 1.720703125, 'step_loss': 1.720703125, 'learning_rate': 0.0008213333333333333, 'seq/s': 26960.61834061379, 'global_steps': 442, 'samples_trained': 4073472, 'skipped_steps': 9, 'timestamp': 1593108135.4769893}
  0: {'training_steps': 452, 'average_loss': 1.3505859375, 'step_loss': 1.3505859375, 'learning_rate': 0.0008186666666666667, 'seq/s': 26935.913336148544, 'global_steps': 443, 'samples_trained': 4082688, 'skipped_steps': 9, 'timestamp': 1593108135.819136}
  0: {'training_steps': 453, 'average_loss': 1.4384765625, 'step_loss': 1.4384765625, 'learning_rate': 0.0008160000000000001, 'seq/s': 24374.924354868308, 'global_steps': 444, 'samples_trained': 4091904, 'skipped_steps': 9, 'timestamp': 1593108136.1972306}
  0: {'training_steps': 454, 'average_loss': 1.404296875, 'step_loss': 1.404296875, 'learning_rate': 0.0008133333333333332, 'seq/s': 23987.842879829022, 'global_steps': 445, 'samples_trained': 4101120, 'skipped_steps': 9, 'timestamp': 1593108136.5814261}
  0: {'training_steps': 455, 'average_loss': 1.48046875, 'step_loss': 1.48046875, 'learning_rate': 0.0008106666666666667, 'seq/s': 26948.307144022387, 'global_steps': 446, 'samples_trained': 4110336, 'skipped_steps': 9, 'timestamp': 1593108136.9234157}
  0: {'training_steps': 456, 'average_loss': 1.4521484375, 'step_loss': 1.4521484375, 'learning_rate': 0.000808, 'seq/s': 26969.214648814963, 'global_steps': 447, 'samples_trained': 4119552, 'skipped_steps': 9, 'timestamp': 1593108137.2651393}
  0: {'training_steps': 457, 'average_loss': 1.3369140625, 'step_loss': 1.3369140625, 'learning_rate': 0.0008053333333333332, 'seq/s': 24676.440069404813, 'global_steps': 448, 'samples_trained': 4128768, 'skipped_steps': 9, 'timestamp': 1593108137.638614}
  0: {'training_steps': 458, 'average_loss': 1.29296875, 'step_loss': 1.29296875, 'learning_rate': 0.0008026666666666667, 'seq/s': 24205.33245499233, 'global_steps': 449, 'samples_trained': 4137984, 'skipped_steps': 9, 'timestamp': 1593108138.0193574}
  0: {'training_steps': 459, 'average_loss': 1.4326171875, 'step_loss': 1.4326171875, 'learning_rate': 0.0008, 'seq/s': 26909.24901634825, 'global_steps': 450, 'samples_trained': 4147200, 'skipped_steps': 9, 'timestamp': 1593108138.3618429}
  0: {'training_steps': 460, 'average_loss': 1.5888671875, 'step_loss': 1.5888671875, 'learning_rate': 0.0007973333333333333, 'seq/s': 26938.203453802947, 'global_steps': 451, 'samples_trained': 4156416, 'skipped_steps': 9, 'timestamp': 1593108138.7039604}
  0: {'training_steps': 461, 'average_loss': 1.384765625, 'step_loss': 1.384765625, 'learning_rate': 0.0007946666666666666, 'seq/s': 24840.759375361482, 'global_steps': 452, 'samples_trained': 4165632, 'skipped_steps': 9, 'timestamp': 1593108139.0749648}
  0: {'training_steps': 462, 'average_loss': 1.4345703125, 'step_loss': 1.4345703125, 'learning_rate': 0.0007920000000000001, 'seq/s': 24015.533122801044, 'global_steps': 453, 'samples_trained': 4174848, 'skipped_steps': 9, 'timestamp': 1593108139.4587173}
  0: {'training_steps': 463, 'average_loss': 1.224609375, 'step_loss': 1.224609375, 'learning_rate': 0.0007893333333333335, 'seq/s': 27019.094662263986, 'global_steps': 454, 'samples_trained': 4184064, 'skipped_steps': 9, 'timestamp': 1593108139.7998106}
  0: {'training_steps': 464, 'average_loss': 1.4443359375, 'step_loss': 1.4443359375, 'learning_rate': 0.0007866666666666666, 'seq/s': 26935.55671352349, 'global_steps': 455, 'samples_trained': 4193280, 'skipped_steps': 9, 'timestamp': 1593108140.141962}
  0: {'training_steps': 465, 'average_loss': 1.322265625, 'step_loss': 1.322265625, 'learning_rate': 0.0007840000000000001, 'seq/s': 24886.723286888064, 'global_steps': 456, 'samples_trained': 4202496, 'skipped_steps': 9, 'timestamp': 1593108140.5122812}
  0: {'training_steps': 466, 'average_loss': 1.306640625, 'step_loss': 1.306640625, 'learning_rate': 0.0007813333333333334, 'seq/s': 24810.72056195699, 'global_steps': 457, 'samples_trained': 4211712, 'skipped_steps': 9, 'timestamp': 1593108140.8837347}
  0: {'training_steps': 467, 'average_loss': 1.236328125, 'step_loss': 1.236328125, 'learning_rate': 0.0007786666666666667, 'seq/s': 27025.536312235938, 'global_steps': 458, 'samples_trained': 4220928, 'skipped_steps': 9, 'timestamp': 1593108141.2247467}
  0: {'training_steps': 468, 'average_loss': 1.2490234375, 'step_loss': 1.2490234375, 'learning_rate': 0.000776, 'seq/s': 26911.77817090415, 'global_steps': 459, 'samples_trained': 4230144, 'skipped_steps': 9, 'timestamp': 1593108141.5672002}
  0: {'training_steps': 469, 'average_loss': 1.400390625, 'step_loss': 1.400390625, 'learning_rate': 0.0007733333333333334, 'seq/s': 27004.843278498865, 'global_steps': 460, 'samples_trained': 4239360, 'skipped_steps': 9, 'timestamp': 1593108141.9084733}
  0: {'training_steps': 470, 'average_loss': 1.6083984375, 'step_loss': 1.6083984375, 'learning_rate': 0.0007706666666666667, 'seq/s': 26922.63082805742, 'global_steps': 461, 'samples_trained': 4248576, 'skipped_steps': 9, 'timestamp': 1593108142.2507887}
  0: {'training_steps': 471, 'average_loss': 1.685546875, 'step_loss': 1.685546875, 'learning_rate': 0.000768, 'seq/s': 26908.911831657737, 'global_steps': 462, 'samples_trained': 4257792, 'skipped_steps': 9, 'timestamp': 1593108142.5932784}
  0: {'training_steps': 472, 'average_loss': 1.40234375, 'step_loss': 1.40234375, 'learning_rate': 0.0007653333333333335, 'seq/s': 26960.900408165544, 'global_steps': 463, 'samples_trained': 4267008, 'skipped_steps': 9, 'timestamp': 1593108142.9351075}
  0: {'training_steps': 473, 'average_loss': 1.58984375, 'step_loss': 1.58984375, 'learning_rate': 0.0007626666666666666, 'seq/s': 26938.428732312703, 'global_steps': 464, 'samples_trained': 4276224, 'skipped_steps': 9, 'timestamp': 1593108143.277222}
  0: {'training_steps': 474, 'average_loss': 1.4736328125, 'step_loss': 1.4736328125, 'learning_rate': 0.00076, 'seq/s': 27021.890074491646, 'global_steps': 465, 'samples_trained': 4285440, 'skipped_steps': 9, 'timestamp': 1593108143.61828}
  0: {'training_steps': 475, 'average_loss': 1.6328125, 'step_loss': 1.6328125, 'learning_rate': 0.0007573333333333334, 'seq/s': 26939.911909894352, 'global_steps': 466, 'samples_trained': 4294656, 'skipped_steps': 9, 'timestamp': 1593108143.9603753}
  0: {'training_steps': 476, 'average_loss': 1.423828125, 'step_loss': 1.423828125, 'learning_rate': 0.0007546666666666666, 'seq/s': 26986.122279376035, 'global_steps': 467, 'samples_trained': 4303872, 'skipped_steps': 9, 'timestamp': 1593108144.3018851}
  0: {'training_steps': 477, 'average_loss': 1.4560546875, 'step_loss': 1.4560546875, 'learning_rate': 0.0007520000000000001, 'seq/s': 26995.07420052978, 'global_steps': 468, 'samples_trained': 4313088, 'skipped_steps': 9, 'timestamp': 1593108144.6432817}
  0: {'training_steps': 478, 'average_loss': 1.2900390625, 'step_loss': 1.2900390625, 'learning_rate': 0.0007493333333333334, 'seq/s': 26936.908175488796, 'global_steps': 469, 'samples_trained': 4322304, 'skipped_steps': 9, 'timestamp': 1593108144.9854155}
  0: {'training_steps': 479, 'average_loss': 1.3193359375, 'step_loss': 1.3193359375, 'learning_rate': 0.0007466666666666666, 'seq/s': 27062.71601259646, 'global_steps': 470, 'samples_trained': 4331520, 'skipped_steps': 9, 'timestamp': 1593108145.3259587}
  0: {'training_steps': 480, 'average_loss': 1.4814453125, 'step_loss': 1.4814453125, 'learning_rate': 0.000744, 'seq/s': 26893.279491170128, 'global_steps': 471, 'samples_trained': 4340736, 'skipped_steps': 9, 'timestamp': 1593108145.6686475}
  0: {'training_steps': 481, 'average_loss': 1.7744140625, 'step_loss': 1.7744140625, 'learning_rate': 0.0007413333333333334, 'seq/s': 26935.087487614157, 'global_steps': 472, 'samples_trained': 4349952, 'skipped_steps': 9, 'timestamp': 1593108146.0108047}
  0: {'training_steps': 482, 'average_loss': 1.6181640625, 'step_loss': 1.6181640625, 'learning_rate': 0.0007386666666666666, 'seq/s': 27028.748839443604, 'global_steps': 473, 'samples_trained': 4359168, 'skipped_steps': 9, 'timestamp': 1593108146.3517756}
  0: {'training_steps': 483, 'average_loss': 1.40234375, 'step_loss': 1.40234375, 'learning_rate': 0.000736, 'seq/s': 26926.60670206289, 'global_steps': 474, 'samples_trained': 4368384, 'skipped_steps': 9, 'timestamp': 1593108146.69404}
  0: {'training_steps': 484, 'average_loss': 1.556640625, 'step_loss': 1.556640625, 'learning_rate': 0.0007333333333333334, 'seq/s': 26985.42522107553, 'global_steps': 475, 'samples_trained': 4377600, 'skipped_steps': 9, 'timestamp': 1593108147.0355587}
  0: {'training_steps': 485, 'average_loss': 1.56640625, 'step_loss': 1.56640625, 'learning_rate': 0.0007306666666666666, 'seq/s': 27006.918703350617, 'global_steps': 476, 'samples_trained': 4386816, 'skipped_steps': 9, 'timestamp': 1593108147.3768055}
  0: {'training_steps': 486, 'average_loss': 1.453125, 'step_loss': 1.453125, 'learning_rate': 0.000728, 'seq/s': 27021.64450835332, 'global_steps': 477, 'samples_trained': 4396032, 'skipped_steps': 9, 'timestamp': 1593108147.7178664}
  0: {'training_steps': 487, 'average_loss': 1.39453125, 'step_loss': 1.39453125, 'learning_rate': 0.0007253333333333334, 'seq/s': 27001.560985014385, 'global_steps': 478, 'samples_trained': 4405248, 'skipped_steps': 9, 'timestamp': 1593108148.0591807}
  0: {'training_steps': 488, 'average_loss': 1.4609375, 'step_loss': 1.4609375, 'learning_rate': 0.0007226666666666666, 'seq/s': 26982.99521834369, 'global_steps': 479, 'samples_trained': 4414464, 'skipped_steps': 9, 'timestamp': 1593108148.4007301}
  0: {'training_steps': 489, 'average_loss': 1.54296875, 'step_loss': 1.54296875, 'learning_rate': 0.0007199999999999999, 'seq/s': 27042.268716358685, 'global_steps': 480, 'samples_trained': 4423680, 'skipped_steps': 9, 'timestamp': 1593108148.741531}
  0: {'training_steps': 490, 'average_loss': 1.5380859375, 'step_loss': 1.5380859375, 'learning_rate': 0.0007173333333333334, 'seq/s': 26939.930685340885, 'global_steps': 481, 'samples_trained': 4432896, 'skipped_steps': 9, 'timestamp': 1593108149.0836263}
  0: {'training_steps': 491, 'average_loss': 1.396484375, 'step_loss': 1.396484375, 'learning_rate': 0.0007146666666666666, 'seq/s': 27051.76907809836, 'global_steps': 482, 'samples_trained': 4442112, 'skipped_steps': 9, 'timestamp': 1593108149.4243073}
  0: {'training_steps': 492, 'average_loss': 1.5693359375, 'step_loss': 1.5693359375, 'learning_rate': 0.000712, 'seq/s': 26995.658637628974, 'global_steps': 483, 'samples_trained': 4451328, 'skipped_steps': 9, 'timestamp': 1593108149.7656965}
  0: {'training_steps': 493, 'average_loss': 1.4755859375, 'step_loss': 1.4755859375, 'learning_rate': 0.0007093333333333334, 'seq/s': 26942.390495129006, 'global_steps': 484, 'samples_trained': 4460544, 'skipped_steps': 9, 'timestamp': 1593108150.107761}
  0: {'training_steps': 494, 'average_loss': 1.328125, 'step_loss': 1.328125, 'learning_rate': 0.0007066666666666667, 'seq/s': 27020.888948228967, 'global_steps': 485, 'samples_trained': 4469760, 'skipped_steps': 9, 'timestamp': 1593108150.4488318}
  0: {'training_steps': 495, 'average_loss': 1.4013671875, 'step_loss': 1.4013671875, 'learning_rate': 0.000704, 'seq/s': 26919.218514267886, 'global_steps': 486, 'samples_trained': 4478976, 'skipped_steps': 9, 'timestamp': 1593108150.7911904}
  0: {'training_steps': 496, 'average_loss': 1.1923828125, 'step_loss': 1.1923828125, 'learning_rate': 0.0007013333333333334, 'seq/s': 26910.42922931369, 'global_steps': 487, 'samples_trained': 4488192, 'skipped_steps': 9, 'timestamp': 1593108151.133661}
  0: {'training_steps': 497, 'average_loss': 1.5361328125, 'step_loss': 1.5361328125, 'learning_rate': 0.0006986666666666668, 'seq/s': 27025.271785205656, 'global_steps': 488, 'samples_trained': 4497408, 'skipped_steps': 9, 'timestamp': 1593108151.4746761}
  0: :::MLLOG {"namespace": "", "time_ms": 1593108151951, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.7069154381752014, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 843, "epoch_num": 1}}
  0: {'global_steps': 489, 'eval_loss': 1.3662415742874146, 'eval_mlm_accuracy': 0.7069154381752014}
  0: {'training_steps': 498, 'average_loss': 1.3134765625, 'step_loss': 1.3134765625, 'learning_rate': 0.000696, 'seq/s': 19330.764711971664, 'global_steps': 489, 'samples_trained': 4506624, 'skipped_steps': 9, 'timestamp': 1593108151.9514298}
  0: {'training_steps': 499, 'average_loss': 1.5654296875, 'step_loss': 1.5654296875, 'learning_rate': 0.0006933333333333333, 'seq/s': 28109.201435176365, 'global_steps': 490, 'samples_trained': 4515840, 'skipped_steps': 9, 'timestamp': 1593108152.2792947}
  0: {'training_steps': 500, 'average_loss': 1.3447265625, 'step_loss': 1.3447265625, 'learning_rate': 0.0006906666666666668, 'seq/s': 27065.994240161326, 'global_steps': 491, 'samples_trained': 4525056, 'skipped_steps': 9, 'timestamp': 1593108152.6197968}
  0: {'training_steps': 501, 'average_loss': 1.6787109375, 'step_loss': 1.6787109375, 'learning_rate': 0.0006879999999999999, 'seq/s': 26948.04412666897, 'global_steps': 492, 'samples_trained': 4534272, 'skipped_steps': 9, 'timestamp': 1593108152.961789}
  0: {'training_steps': 502, 'average_loss': 1.1630859375, 'step_loss': 1.1630859375, 'learning_rate': 0.0006853333333333334, 'seq/s': 26905.727726300785, 'global_steps': 493, 'samples_trained': 4543488, 'skipped_steps': 9, 'timestamp': 1593108153.3043191}
  0: {'training_steps': 503, 'average_loss': 1.46484375, 'step_loss': 1.46484375, 'learning_rate': 0.0006826666666666668, 'seq/s': 27001.52326214135, 'global_steps': 494, 'samples_trained': 4552704, 'skipped_steps': 9, 'timestamp': 1593108153.6456344}
  0: {'training_steps': 504, 'average_loss': 1.51953125, 'step_loss': 1.51953125, 'learning_rate': 0.0006799999999999999, 'seq/s': 26971.02113461262, 'global_steps': 495, 'samples_trained': 4561920, 'skipped_steps': 9, 'timestamp': 1593108153.9873357}
  0: {'training_steps': 505, 'average_loss': 1.412109375, 'step_loss': 1.412109375, 'learning_rate': 0.0006773333333333334, 'seq/s': 26990.211157156074, 'global_steps': 496, 'samples_trained': 4571136, 'skipped_steps': 9, 'timestamp': 1593108154.3287938}
  0: {'training_steps': 506, 'average_loss': 1.701171875, 'step_loss': 1.701171875, 'learning_rate': 0.0006746666666666667, 'seq/s': 26964.041158928965, 'global_steps': 497, 'samples_trained': 4580352, 'skipped_steps': 9, 'timestamp': 1593108154.6705832}
  0: {'training_steps': 507, 'average_loss': 1.416015625, 'step_loss': 1.416015625, 'learning_rate': 0.000672, 'seq/s': 26930.546225723658, 'global_steps': 498, 'samples_trained': 4589568, 'skipped_steps': 9, 'timestamp': 1593108155.012798}
  0: {'training_steps': 508, 'average_loss': 1.3876953125, 'step_loss': 1.3876953125, 'learning_rate': 0.0006693333333333333, 'seq/s': 26995.97914613697, 'global_steps': 499, 'samples_trained': 4598784, 'skipped_steps': 9, 'timestamp': 1593108155.3541832}
  0: {'training_steps': 509, 'average_loss': 1.453125, 'step_loss': 1.453125, 'learning_rate': 0.0006666666666666668, 'seq/s': 27052.16664905399, 'global_steps': 500, 'samples_trained': 4608000, 'skipped_steps': 9, 'timestamp': 1593108155.694859}
  0: {'training_steps': 510, 'average_loss': 1.5146484375, 'step_loss': 1.5146484375, 'learning_rate': 0.000664, 'seq/s': 26890.95959055585, 'global_steps': 501, 'samples_trained': 4617216, 'skipped_steps': 9, 'timestamp': 1593108156.0375774}
  0: {'training_steps': 511, 'average_loss': 1.2158203125, 'step_loss': 1.2158203125, 'learning_rate': 0.0006613333333333333, 'seq/s': 26967.276709178852, 'global_steps': 502, 'samples_trained': 4626432, 'skipped_steps': 9, 'timestamp': 1593108156.3793256}
  0: {'training_steps': 512, 'average_loss': 1.5107421875, 'step_loss': 1.5107421875, 'learning_rate': 0.0006586666666666668, 'seq/s': 26946.09044419039, 'global_steps': 503, 'samples_trained': 4635648, 'skipped_steps': 9, 'timestamp': 1593108156.7213433}
  0: {'training_steps': 513, 'average_loss': 1.265625, 'step_loss': 1.265625, 'learning_rate': 0.0006559999999999999, 'seq/s': 26955.222584680698, 'global_steps': 504, 'samples_trained': 4644864, 'skipped_steps': 9, 'timestamp': 1593108157.0632443}
  0: {'training_steps': 514, 'average_loss': 1.51171875, 'step_loss': 1.51171875, 'learning_rate': 0.0006533333333333333, 'seq/s': 26986.95126044788, 'global_steps': 505, 'samples_trained': 4654080, 'skipped_steps': 9, 'timestamp': 1593108157.4047434}
  0: {'training_steps': 515, 'average_loss': 1.46875, 'step_loss': 1.46875, 'learning_rate': 0.0006506666666666667, 'seq/s': 27030.355389469445, 'global_steps': 506, 'samples_trained': 4663296, 'skipped_steps': 9, 'timestamp': 1593108157.7456944}
  0: {'training_steps': 516, 'average_loss': 1.5283203125, 'step_loss': 1.5283203125, 'learning_rate': 0.0006479999999999999, 'seq/s': 26992.41627364255, 'global_steps': 507, 'samples_trained': 4672512, 'skipped_steps': 9, 'timestamp': 1593108158.0871248}
  0: {'training_steps': 517, 'average_loss': 1.6572265625, 'step_loss': 1.6572265625, 'learning_rate': 0.0006453333333333334, 'seq/s': 26984.539820562437, 'global_steps': 508, 'samples_trained': 4681728, 'skipped_steps': 9, 'timestamp': 1593108158.4286547}
  0: {'training_steps': 518, 'average_loss': 1.271484375, 'step_loss': 1.271484375, 'learning_rate': 0.0006426666666666667, 'seq/s': 27076.66819183498, 'global_steps': 509, 'samples_trained': 4690944, 'skipped_steps': 9, 'timestamp': 1593108158.7690225}
  0: {'training_steps': 519, 'average_loss': 1.40234375, 'step_loss': 1.40234375, 'learning_rate': 0.0006399999999999999, 'seq/s': 27084.067163112417, 'global_steps': 510, 'samples_trained': 4700160, 'skipped_steps': 9, 'timestamp': 1593108159.1092975}
  0: {'training_steps': 520, 'average_loss': 1.4345703125, 'step_loss': 1.4345703125, 'learning_rate': 0.0006373333333333333, 'seq/s': 27031.60296003726, 'global_steps': 511, 'samples_trained': 4709376, 'skipped_steps': 9, 'timestamp': 1593108159.4502325}
  0: {'training_steps': 521, 'average_loss': 1.27734375, 'step_loss': 1.27734375, 'learning_rate': 0.0006346666666666667, 'seq/s': 27033.569062535404, 'global_steps': 512, 'samples_trained': 4718592, 'skipped_steps': 9, 'timestamp': 1593108159.7911432}
  0: {'training_steps': 522, 'average_loss': 1.349609375, 'step_loss': 1.349609375, 'learning_rate': 0.0006319999999999999, 'seq/s': 25289.753671801856, 'global_steps': 513, 'samples_trained': 4727808, 'skipped_steps': 9, 'timestamp': 1593108160.1555605}
  0: {'training_steps': 523, 'average_loss': 1.1875, 'step_loss': 1.1875, 'learning_rate': 0.0006293333333333333, 'seq/s': 26892.044655535017, 'global_steps': 514, 'samples_trained': 4737024, 'skipped_steps': 9, 'timestamp': 1593108160.498265}
  0: {'training_steps': 524, 'average_loss': 1.2392578125, 'step_loss': 1.2392578125, 'learning_rate': 0.0006266666666666668, 'seq/s': 26970.400127823435, 'global_steps': 515, 'samples_trained': 4746240, 'skipped_steps': 9, 'timestamp': 1593108160.8399737}
  0: {'training_steps': 525, 'average_loss': 1.6123046875, 'step_loss': 1.6123046875, 'learning_rate': 0.0006240000000000001, 'seq/s': 27066.771277564134, 'global_steps': 516, 'samples_trained': 4755456, 'skipped_steps': 9, 'timestamp': 1593108161.1804657}
  0: {'training_steps': 526, 'average_loss': 1.626953125, 'step_loss': 1.626953125, 'learning_rate': 0.0006213333333333333, 'seq/s': 24254.435175128332, 'global_steps': 517, 'samples_trained': 4764672, 'skipped_steps': 9, 'timestamp': 1593108161.5604386}
  0: {'training_steps': 527, 'average_loss': 1.291015625, 'step_loss': 1.291015625, 'learning_rate': 0.0006186666666666667, 'seq/s': 27152.060891884033, 'global_steps': 518, 'samples_trained': 4773888, 'skipped_steps': 9, 'timestamp': 1593108161.8998618}
  0: {'training_steps': 528, 'average_loss': 1.26953125, 'step_loss': 1.26953125, 'learning_rate': 0.0006160000000000001, 'seq/s': 27095.970912353627, 'global_steps': 519, 'samples_trained': 4783104, 'skipped_steps': 9, 'timestamp': 1593108162.2399871}
  0: {'training_steps': 529, 'average_loss': 1.3291015625, 'step_loss': 1.3291015625, 'learning_rate': 0.0006133333333333334, 'seq/s': 27025.933112491104, 'global_steps': 520, 'samples_trained': 4792320, 'skipped_steps': 9, 'timestamp': 1593108162.5809946}
  0: {'training_steps': 530, 'average_loss': 1.40234375, 'step_loss': 1.40234375, 'learning_rate': 0.0006106666666666667, 'seq/s': 24318.706555814548, 'global_steps': 521, 'samples_trained': 4801536, 'skipped_steps': 9, 'timestamp': 1593108162.9599628}
  0: {'training_steps': 531, 'average_loss': 1.2734375, 'step_loss': 1.2734375, 'learning_rate': 0.0006080000000000001, 'seq/s': 27002.654994184482, 'global_steps': 522, 'samples_trained': 4810752, 'skipped_steps': 9, 'timestamp': 1593108163.3012633}
  0: {'training_steps': 532, 'average_loss': 1.513671875, 'step_loss': 1.513671875, 'learning_rate': 0.0006053333333333333, 'seq/s': 27056.749024430705, 'global_steps': 523, 'samples_trained': 4819968, 'skipped_steps': 9, 'timestamp': 1593108163.641882}
  0: {'training_steps': 533, 'average_loss': 1.68359375, 'step_loss': 1.68359375, 'learning_rate': 0.0006026666666666667, 'seq/s': 26954.07602983066, 'global_steps': 524, 'samples_trained': 4829184, 'skipped_steps': 9, 'timestamp': 1593108163.9837978}
  0: {'training_steps': 534, 'average_loss': 1.5947265625, 'step_loss': 1.5947265625, 'learning_rate': 0.0006000000000000001, 'seq/s': 24153.394225385906, 'global_steps': 525, 'samples_trained': 4838400, 'skipped_steps': 9, 'timestamp': 1593108164.3653598}
  0: {'training_steps': 535, 'average_loss': 1.634765625, 'step_loss': 1.634765625, 'learning_rate': 0.0005973333333333333, 'seq/s': 27050.860388365276, 'global_steps': 526, 'samples_trained': 4847616, 'skipped_steps': 9, 'timestamp': 1593108164.7060523}
  0: {'training_steps': 536, 'average_loss': 1.447265625, 'step_loss': 1.447265625, 'learning_rate': 0.0005946666666666667, 'seq/s': 26963.664982543763, 'global_steps': 527, 'samples_trained': 4856832, 'skipped_steps': 9, 'timestamp': 1593108165.0478466}
  0: {'training_steps': 537, 'average_loss': 1.1005859375, 'step_loss': 1.1005859375, 'learning_rate': 0.0005920000000000001, 'seq/s': 26981.16829522809, 'global_steps': 528, 'samples_trained': 4866048, 'skipped_steps': 9, 'timestamp': 1593108165.389419}
  0: {'training_steps': 538, 'average_loss': 1.2783203125, 'step_loss': 1.2783203125, 'learning_rate': 0.0005893333333333333, 'seq/s': 24794.551420141117, 'global_steps': 529, 'samples_trained': 4875264, 'skipped_steps': 9, 'timestamp': 1593108165.7611146}
  0: {'training_steps': 539, 'average_loss': 1.5185546875, 'step_loss': 1.5185546875, 'learning_rate': 0.0005866666666666667, 'seq/s': 26759.5503447512, 'global_steps': 530, 'samples_trained': 4884480, 'skipped_steps': 9, 'timestamp': 1593108166.105516}
  0: {'training_steps': 540, 'average_loss': 1.3310546875, 'step_loss': 1.3310546875, 'learning_rate': 0.0005840000000000001, 'seq/s': 26984.539820562437, 'global_steps': 531, 'samples_trained': 4893696, 'skipped_steps': 9, 'timestamp': 1593108166.4470458}
  0: {'training_steps': 541, 'average_loss': 1.25390625, 'step_loss': 1.25390625, 'learning_rate': 0.0005813333333333332, 'seq/s': 26897.47129418494, 'global_steps': 532, 'samples_trained': 4902912, 'skipped_steps': 9, 'timestamp': 1593108166.7896814}
  0: {'training_steps': 542, 'average_loss': 1.2021484375, 'step_loss': 1.2021484375, 'learning_rate': 0.0005786666666666667, 'seq/s': 26849.698655941986, 'global_steps': 533, 'samples_trained': 4912128, 'skipped_steps': 9, 'timestamp': 1593108167.1329267}
  0: {'training_steps': 543, 'average_loss': 1.529296875, 'step_loss': 1.529296875, 'learning_rate': 0.0005760000000000001, 'seq/s': 26957.08359444466, 'global_steps': 534, 'samples_trained': 4921344, 'skipped_steps': 9, 'timestamp': 1593108167.4748042}
  0: {'training_steps': 544, 'average_loss': 1.2373046875, 'step_loss': 1.2373046875, 'learning_rate': 0.0005733333333333332, 'seq/s': 26816.20796576823, 'global_steps': 535, 'samples_trained': 4930560, 'skipped_steps': 9, 'timestamp': 1593108167.8184779}
  0: {'training_steps': 545, 'average_loss': 1.7646484375, 'step_loss': 1.7646484375, 'learning_rate': 0.0005706666666666667, 'seq/s': 26982.486668504836, 'global_steps': 536, 'samples_trained': 4939776, 'skipped_steps': 9, 'timestamp': 1593108168.160034}
  0: {'training_steps': 546, 'average_loss': 1.34375, 'step_loss': 1.34375, 'learning_rate': 0.000568, 'seq/s': 27028.843337099897, 'global_steps': 537, 'samples_trained': 4948992, 'skipped_steps': 9, 'timestamp': 1593108168.501004}
  0: {'training_steps': 547, 'average_loss': 1.533203125, 'step_loss': 1.533203125, 'learning_rate': 0.0005653333333333333, 'seq/s': 27021.134500634726, 'global_steps': 538, 'samples_trained': 4958208, 'skipped_steps': 9, 'timestamp': 1593108168.842071}
  0: {'training_steps': 548, 'average_loss': 1.349609375, 'step_loss': 1.349609375, 'learning_rate': 0.0005626666666666666, 'seq/s': 26832.216441946734, 'global_steps': 539, 'samples_trained': 4967424, 'skipped_steps': 9, 'timestamp': 1593108169.1855395}
  0: {'training_steps': 549, 'average_loss': 1.232421875, 'step_loss': 1.232421875, 'learning_rate': 0.0005600000000000001, 'seq/s': 26890.286145988477, 'global_steps': 540, 'samples_trained': 4976640, 'skipped_steps': 9, 'timestamp': 1593108169.5282667}
  0: {'training_steps': 550, 'average_loss': 1.458984375, 'step_loss': 1.458984375, 'learning_rate': 0.0005573333333333333, 'seq/s': 26921.52454366084, 'global_steps': 541, 'samples_trained': 4985856, 'skipped_steps': 9, 'timestamp': 1593108169.8705962}
  0: {'training_steps': 551, 'average_loss': 1.39453125, 'step_loss': 1.39453125, 'learning_rate': 0.0005546666666666666, 'seq/s': 27011.27813407982, 'global_steps': 542, 'samples_trained': 4995072, 'skipped_steps': 9, 'timestamp': 1593108170.211788}
  0: :::MLLOG {"namespace": "", "time_ms": 1593108170687, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.7090094089508057, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 843, "epoch_num": 1}}
  0: {'global_steps': 543, 'eval_loss': 1.351900577545166, 'eval_mlm_accuracy': 0.7090094089508057}
  0: {'training_steps': 552, 'average_loss': 1.421875, 'step_loss': 1.421875, 'learning_rate': 0.0005520000000000001, 'seq/s': 19379.29904308499, 'global_steps': 543, 'samples_trained': 5004288, 'skipped_steps': 9, 'timestamp': 1593108170.6873477}
  0: {'training_steps': 553, 'average_loss': 1.54296875, 'step_loss': 1.54296875, 'learning_rate': 0.0005493333333333332, 'seq/s': 27959.47827671957, 'global_steps': 544, 'samples_trained': 5013504, 'skipped_steps': 9, 'timestamp': 1593108171.0169685}
  0: {'training_steps': 554, 'average_loss': 1.5732421875, 'step_loss': 1.5732421875, 'learning_rate': 0.0005466666666666667, 'seq/s': 26954.18880139238, 'global_steps': 545, 'samples_trained': 5022720, 'skipped_steps': 9, 'timestamp': 1593108171.3588834}
  0: {'training_steps': 555, 'average_loss': 1.498046875, 'step_loss': 1.498046875, 'learning_rate': 0.0005440000000000001, 'seq/s': 26927.78843813458, 'global_steps': 546, 'samples_trained': 5031936, 'skipped_steps': 9, 'timestamp': 1593108171.7011328}
  0: {'training_steps': 556, 'average_loss': 1.548828125, 'step_loss': 1.548828125, 'learning_rate': 0.0005413333333333334, 'seq/s': 27004.937609071996, 'global_steps': 547, 'samples_trained': 5041152, 'skipped_steps': 9, 'timestamp': 1593108172.0424047}
  0: {'training_steps': 557, 'average_loss': 1.4599609375, 'step_loss': 1.4599609375, 'learning_rate': 0.0005386666666666667, 'seq/s': 26994.414382605002, 'global_steps': 548, 'samples_trained': 5050368, 'skipped_steps': 9, 'timestamp': 1593108172.3838096}
  0: {'training_steps': 558, 'average_loss': 1.7900390625, 'step_loss': 1.7900390625, 'learning_rate': 0.000536, 'seq/s': 26945.038578646236, 'global_steps': 549, 'samples_trained': 5059584, 'skipped_steps': 9, 'timestamp': 1593108172.7258399}
  0: {'training_steps': 559, 'average_loss': 1.6650390625, 'step_loss': 1.6650390625, 'learning_rate': 0.0005333333333333335, 'seq/s': 26880.375447748895, 'global_steps': 550, 'samples_trained': 5068800, 'skipped_steps': 9, 'timestamp': 1593108173.0686932}
  0: {'training_steps': 560, 'average_loss': 1.6982421875, 'step_loss': 1.6982421875, 'learning_rate': 0.0005306666666666666, 'seq/s': 26912.13416415448, 'global_steps': 551, 'samples_trained': 5078016, 'skipped_steps': 9, 'timestamp': 1593108173.4111419}
  0: {'training_steps': 561, 'average_loss': 1.5498046875, 'step_loss': 1.5498046875, 'learning_rate': 0.000528, 'seq/s': 26988.515158856775, 'global_steps': 552, 'samples_trained': 5087232, 'skipped_steps': 9, 'timestamp': 1593108173.7526217}
  0: {'training_steps': 562, 'average_loss': 1.3515625, 'step_loss': 1.3515625, 'learning_rate': 0.0005253333333333335, 'seq/s': 26904.229584507277, 'global_steps': 553, 'samples_trained': 5096448, 'skipped_steps': 9, 'timestamp': 1593108174.095171}
  0: {'training_steps': 563, 'average_loss': 1.29296875, 'step_loss': 1.29296875, 'learning_rate': 0.0005226666666666666, 'seq/s': 26917.925060984486, 'global_steps': 554, 'samples_trained': 5105664, 'skipped_steps': 9, 'timestamp': 1593108174.4375463}
  0: {'training_steps': 564, 'average_loss': 1.4345703125, 'step_loss': 1.4345703125, 'learning_rate': 0.0005200000000000001, 'seq/s': 26974.201890406657, 'global_steps': 555, 'samples_trained': 5114880, 'skipped_steps': 9, 'timestamp': 1593108174.7792075}
  0: {'training_steps': 565, 'average_loss': 1.3740234375, 'step_loss': 1.3740234375, 'learning_rate': 0.0005173333333333334, 'seq/s': 26968.44320258196, 'global_steps': 556, 'samples_trained': 5124096, 'skipped_steps': 9, 'timestamp': 1593108175.1209416}
  0: {'training_steps': 566, 'average_loss': 1.4306640625, 'step_loss': 1.4306640625, 'learning_rate': 0.0005146666666666666, 'seq/s': 27042.1362883677, 'global_steps': 557, 'samples_trained': 5133312, 'skipped_steps': 9, 'timestamp': 1593108175.461744}
  0: {'training_steps': 567, 'average_loss': 1.1884765625, 'step_loss': 1.1884765625, 'learning_rate': 0.000512, 'seq/s': 27057.77174980523, 'global_steps': 558, 'samples_trained': 5142528, 'skipped_steps': 9, 'timestamp': 1593108175.8023498}
  0: {'training_steps': 568, 'average_loss': 1.3134765625, 'step_loss': 1.3134765625, 'learning_rate': 0.0005093333333333334, 'seq/s': 27010.825140697736, 'global_steps': 559, 'samples_trained': 5151744, 'skipped_steps': 9, 'timestamp': 1593108176.1435473}
  0: {'training_steps': 569, 'average_loss': 1.3935546875, 'step_loss': 1.3935546875, 'learning_rate': 0.0005066666666666667, 'seq/s': 26926.60670206289, 'global_steps': 560, 'samples_trained': 5160960, 'skipped_steps': 9, 'timestamp': 1593108176.4858117}
  0: {'training_steps': 570, 'average_loss': 1.2158203125, 'step_loss': 1.2158203125, 'learning_rate': 0.000504, 'seq/s': 25323.668270849685, 'global_steps': 561, 'samples_trained': 5170176, 'skipped_steps': 9, 'timestamp': 1593108176.849741}
  0: {'training_steps': 571, 'average_loss': 1.2783203125, 'step_loss': 1.2783203125, 'learning_rate': 0.0005013333333333334, 'seq/s': 26935.293945000158, 'global_steps': 562, 'samples_trained': 5179392, 'skipped_steps': 9, 'timestamp': 1593108177.1918952}
  0: {'training_steps': 572, 'average_loss': 1.59765625, 'step_loss': 1.59765625, 'learning_rate': 0.0004986666666666666, 'seq/s': 26965.67764834898, 'global_steps': 563, 'samples_trained': 5188608, 'skipped_steps': 9, 'timestamp': 1593108177.533664}
  0: {'training_steps': 573, 'average_loss': 1.583984375, 'step_loss': 1.583984375, 'learning_rate': 0.000496, 'seq/s': 27003.183168271873, 'global_steps': 564, 'samples_trained': 5197824, 'skipped_steps': 9, 'timestamp': 1593108177.8749583}
  0: {'training_steps': 574, 'average_loss': 1.505859375, 'step_loss': 1.505859375, 'learning_rate': 0.0004933333333333334, 'seq/s': 24648.305859397416, 'global_steps': 565, 'samples_trained': 5207040, 'skipped_steps': 9, 'timestamp': 1593108178.2488592}
  0: {'training_steps': 575, 'average_loss': 1.3701171875, 'step_loss': 1.3701171875, 'learning_rate': 0.0004906666666666666, 'seq/s': 26857.272650465795, 'global_steps': 566, 'samples_trained': 5216256, 'skipped_steps': 9, 'timestamp': 1593108178.5920074}
  0: {'training_steps': 576, 'average_loss': 1.4775390625, 'step_loss': 1.4775390625, 'learning_rate': 0.000488, 'seq/s': 27010.542027549494, 'global_steps': 567, 'samples_trained': 5225472, 'skipped_steps': 9, 'timestamp': 1593108178.9332082}
  0: {'training_steps': 577, 'average_loss': 1.3408203125, 'step_loss': 1.3408203125, 'learning_rate': 0.0004853333333333334, 'seq/s': 26885.98437951016, 'global_steps': 568, 'samples_trained': 5234688, 'skipped_steps': 9, 'timestamp': 1593108179.2759898}
  0: {'training_steps': 578, 'average_loss': 1.3759765625, 'step_loss': 1.3759765625, 'learning_rate': 0.00048266666666666656, 'seq/s': 24692.676311714593, 'global_steps': 569, 'samples_trained': 5243904, 'skipped_steps': 9, 'timestamp': 1593108179.6492186}
  0: {'training_steps': 579, 'average_loss': 1.3359375, 'step_loss': 1.3359375, 'learning_rate': 0.00048, 'seq/s': 27002.86248868323, 'global_steps': 570, 'samples_trained': 5253120, 'skipped_steps': 9, 'timestamp': 1593108179.9905167}
  0: {'training_steps': 580, 'average_loss': 1.4599609375, 'step_loss': 1.4599609375, 'learning_rate': 0.0004773333333333334, 'seq/s': 26971.491613340786, 'global_steps': 571, 'samples_trained': 5262336, 'skipped_steps': 9, 'timestamp': 1593108180.332212}
  0: {'training_steps': 581, 'average_loss': 1.349609375, 'step_loss': 1.349609375, 'learning_rate': 0.0004746666666666666, 'seq/s': 26981.695629078607, 'global_steps': 572, 'samples_trained': 5271552, 'skipped_steps': 9, 'timestamp': 1593108180.6737778}
  0: {'training_steps': 582, 'average_loss': 1.2646484375, 'step_loss': 1.2646484375, 'learning_rate': 0.000472, 'seq/s': 24658.856532270464, 'global_steps': 573, 'samples_trained': 5280768, 'skipped_steps': 9, 'timestamp': 1593108181.0475185}
  0: {'training_steps': 583, 'average_loss': 1.2705078125, 'step_loss': 1.2705078125, 'learning_rate': 0.0004693333333333334, 'seq/s': 26974.74777598821, 'global_steps': 574, 'samples_trained': 5289984, 'skipped_steps': 9, 'timestamp': 1593108181.3891726}
  0: {'training_steps': 584, 'average_loss': 1.328125, 'step_loss': 1.328125, 'learning_rate': 0.00046666666666666655, 'seq/s': 26925.2937836264, 'global_steps': 575, 'samples_trained': 5299200, 'skipped_steps': 9, 'timestamp': 1593108181.7314541}
  0: {'training_steps': 585, 'average_loss': 1.6416015625, 'step_loss': 1.6416015625, 'learning_rate': 0.000464, 'seq/s': 26983.29659025704, 'global_steps': 576, 'samples_trained': 5308416, 'skipped_steps': 9, 'timestamp': 1593108182.073}
  0: {'training_steps': 586, 'average_loss': 1.4033203125, 'step_loss': 1.4033203125, 'learning_rate': 0.0004613333333333334, 'seq/s': 24654.38986952304, 'global_steps': 577, 'samples_trained': 5317632, 'skipped_steps': 9, 'timestamp': 1593108182.4468086}
  0: {'training_steps': 587, 'average_loss': 1.0830078125, 'step_loss': 1.0830078125, 'learning_rate': 0.0004586666666666666, 'seq/s': 26845.316645195344, 'global_steps': 578, 'samples_trained': 5326848, 'skipped_steps': 9, 'timestamp': 1593108182.7901096}
  0: {'training_steps': 588, 'average_loss': 1.6083984375, 'step_loss': 1.6083984375, 'learning_rate': 0.00045599999999999997, 'seq/s': 26952.215435289785, 'global_steps': 579, 'samples_trained': 5336064, 'skipped_steps': 9, 'timestamp': 1593108183.1320493}
  0: {'training_steps': 589, 'average_loss': 1.3271484375, 'step_loss': 1.3271484375, 'learning_rate': 0.00045333333333333337, 'seq/s': 24406.597669753584, 'global_steps': 580, 'samples_trained': 5345280, 'skipped_steps': 9, 'timestamp': 1593108183.509653}
  0: {'training_steps': 590, 'average_loss': 1.3642578125, 'step_loss': 1.3642578125, 'learning_rate': 0.00045066666666666676, 'seq/s': 27273.789631435902, 'global_steps': 581, 'samples_trained': 5354496, 'skipped_steps': 9, 'timestamp': 1593108183.847561}
  0: {'training_steps': 591, 'average_loss': 1.298828125, 'step_loss': 1.298828125, 'learning_rate': 0.00044799999999999994, 'seq/s': 27035.856907152753, 'global_steps': 582, 'samples_trained': 5363712, 'skipped_steps': 9, 'timestamp': 1593108184.1884427}
  0: {'training_steps': 592, 'average_loss': 1.4697265625, 'step_loss': 1.4697265625, 'learning_rate': 0.0004453333333333334, 'seq/s': 27015.110310967917, 'global_steps': 583, 'samples_trained': 5372928, 'skipped_steps': 9, 'timestamp': 1593108184.529586}
  0: {'training_steps': 593, 'average_loss': 1.31640625, 'step_loss': 1.31640625, 'learning_rate': 0.0004426666666666668, 'seq/s': 26975.61370878258, 'global_steps': 584, 'samples_trained': 5382144, 'skipped_steps': 9, 'timestamp': 1593108184.871229}
  0: {'training_steps': 594, 'average_loss': 1.548828125, 'step_loss': 1.548828125, 'learning_rate': 0.00043999999999999996, 'seq/s': 26878.225977685095, 'global_steps': 585, 'samples_trained': 5391360, 'skipped_steps': 9, 'timestamp': 1593108185.2141097}
  0: {'training_steps': 595, 'average_loss': 1.2998046875, 'step_loss': 1.2998046875, 'learning_rate': 0.00043733333333333336, 'seq/s': 26852.06740194631, 'global_steps': 586, 'samples_trained': 5400576, 'skipped_steps': 9, 'timestamp': 1593108185.557325}
  0: {'training_steps': 596, 'average_loss': 1.421875, 'step_loss': 1.421875, 'learning_rate': 0.00043466666666666675, 'seq/s': 26883.01135416855, 'global_steps': 587, 'samples_trained': 5409792, 'skipped_steps': 9, 'timestamp': 1593108185.9001446}
  0: {'training_steps': 597, 'average_loss': 1.3349609375, 'step_loss': 1.3349609375, 'learning_rate': 0.00043199999999999993, 'seq/s': 26930.827664381715, 'global_steps': 588, 'samples_trained': 5419008, 'skipped_steps': 9, 'timestamp': 1593108186.2423558}
  0: {'training_steps': 598, 'average_loss': 1.6455078125, 'step_loss': 1.6455078125, 'learning_rate': 0.0004293333333333334, 'seq/s': 26985.08612465243, 'global_steps': 589, 'samples_trained': 5428224, 'skipped_steps': 9, 'timestamp': 1593108186.5838788}
  0: {'training_steps': 599, 'average_loss': 1.3466796875, 'step_loss': 1.3466796875, 'learning_rate': 0.0004266666666666668, 'seq/s': 27004.730082681002, 'global_steps': 590, 'samples_trained': 5437440, 'skipped_steps': 9, 'timestamp': 1593108186.925153}
  0: {'training_steps': 600, 'average_loss': 1.5185546875, 'step_loss': 1.5185546875, 'learning_rate': 0.00042399999999999995, 'seq/s': 26963.928304911304, 'global_steps': 591, 'samples_trained': 5446656, 'skipped_steps': 9, 'timestamp': 1593108187.266944}
  0: {'training_steps': 601, 'average_loss': 1.4560546875, 'step_loss': 1.4560546875, 'learning_rate': 0.00042133333333333335, 'seq/s': 27036.802410563843, 'global_steps': 592, 'samples_trained': 5455872, 'skipped_steps': 9, 'timestamp': 1593108187.6078136}
  0: {'training_steps': 602, 'average_loss': 1.162109375, 'step_loss': 1.162109375, 'learning_rate': 0.00041866666666666674, 'seq/s': 27077.009593831255, 'global_steps': 593, 'samples_trained': 5465088, 'skipped_steps': 9, 'timestamp': 1593108187.948177}
  0: {'training_steps': 603, 'average_loss': 1.1611328125, 'step_loss': 1.1611328125, 'learning_rate': 0.0004159999999999999, 'seq/s': 26944.738060716914, 'global_steps': 594, 'samples_trained': 5474304, 'skipped_steps': 9, 'timestamp': 1593108188.2902114}
  0: {'training_steps': 604, 'average_loss': 1.3046875, 'step_loss': 1.3046875, 'learning_rate': 0.0004133333333333333, 'seq/s': 26992.114697983485, 'global_steps': 595, 'samples_trained': 5483520, 'skipped_steps': 9, 'timestamp': 1593108188.6316452}
  0: {'training_steps': 605, 'average_loss': 1.296875, 'step_loss': 1.296875, 'learning_rate': 0.00041066666666666676, 'seq/s': 26864.458834968056, 'global_steps': 596, 'samples_trained': 5492736, 'skipped_steps': 9, 'timestamp': 1593108188.9747021}
  0: :::MLLOG {"namespace": "", "time_ms": 1593108189450, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.7102692723274231, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 843, "epoch_num": 1}}
  0: {'global_steps': 597, 'eval_loss': 1.3434655666351318, 'eval_mlm_accuracy': 0.7102692723274231}
  0: {'training_steps': 606, 'average_loss': 1.5732421875, 'step_loss': 1.5732421875, 'learning_rate': 0.00040799999999999994, 'seq/s': 19359.18901834066, 'global_steps': 597, 'samples_trained': 5501952, 'skipped_steps': 9, 'timestamp': 1593108189.4507558}
  0: {'training_steps': 607, 'average_loss': 1.2783203125, 'step_loss': 1.2783203125, 'learning_rate': 0.00040533333333333334, 'seq/s': 27998.280226856245, 'global_steps': 598, 'samples_trained': 5511168, 'skipped_steps': 9, 'timestamp': 1593108189.7799203}
  0: {'training_steps': 608, 'average_loss': 1.6064453125, 'step_loss': 1.6064453125, 'learning_rate': 0.00040266666666666673, 'seq/s': 27056.654331547506, 'global_steps': 599, 'samples_trained': 5520384, 'skipped_steps': 9, 'timestamp': 1593108190.12054}
  0: {'training_steps': 609, 'average_loss': 1.333984375, 'step_loss': 1.333984375, 'learning_rate': 0.0003999999999999999, 'seq/s': 26905.090995652554, 'global_steps': 600, 'samples_trained': 5529600, 'skipped_steps': 9, 'timestamp': 1593108190.4630785}
  0: {'training_steps': 610, 'average_loss': 1.353515625, 'step_loss': 1.353515625, 'learning_rate': 0.0003973333333333333, 'seq/s': 27054.306160072007, 'global_steps': 601, 'samples_trained': 5538816, 'skipped_steps': 9, 'timestamp': 1593108190.8037276}
  0: {'training_steps': 611, 'average_loss': 1.4560546875, 'step_loss': 1.4560546875, 'learning_rate': 0.00039466666666666675, 'seq/s': 26956.01207259176, 'global_steps': 602, 'samples_trained': 5548032, 'skipped_steps': 9, 'timestamp': 1593108191.1456187}
  0: {'training_steps': 612, 'average_loss': 1.3701171875, 'step_loss': 1.3701171875, 'learning_rate': 0.00039199999999999993, 'seq/s': 27021.17227832404, 'global_steps': 603, 'samples_trained': 5557248, 'skipped_steps': 9, 'timestamp': 1593108191.4866855}
  0: {'training_steps': 613, 'average_loss': 1.3466796875, 'step_loss': 1.3466796875, 'learning_rate': 0.00038933333333333333, 'seq/s': 26937.90308831769, 'global_steps': 604, 'samples_trained': 5566464, 'skipped_steps': 9, 'timestamp': 1593108191.8288064}
  0: {'training_steps': 614, 'average_loss': 1.2529296875, 'step_loss': 1.2529296875, 'learning_rate': 0.0003866666666666667, 'seq/s': 26945.9026050344, 'global_steps': 605, 'samples_trained': 5575680, 'skipped_steps': 9, 'timestamp': 1593108192.170826}
  0: {'training_steps': 615, 'average_loss': 1.4072265625, 'step_loss': 1.4072265625, 'learning_rate': 0.0003839999999999999, 'seq/s': 27029.82615189682, 'global_steps': 606, 'samples_trained': 5584896, 'skipped_steps': 9, 'timestamp': 1593108192.5117838}
  0: {'training_steps': 616, 'average_loss': 1.255859375, 'step_loss': 1.255859375, 'learning_rate': 0.0003813333333333333, 'seq/s': 26946.879397244153, 'global_steps': 607, 'samples_trained': 5594112, 'skipped_steps': 9, 'timestamp': 1593108192.8537908}
  0: {'training_steps': 617, 'average_loss': 1.4462890625, 'step_loss': 1.4462890625, 'learning_rate': 0.0003786666666666667, 'seq/s': 26939.010719236292, 'global_steps': 608, 'samples_trained': 5603328, 'skipped_steps': 9, 'timestamp': 1593108193.1958976}
  0: {'training_steps': 618, 'average_loss': 1.20703125, 'step_loss': 1.20703125, 'learning_rate': 0.0003759999999999999, 'seq/s': 26944.77562509149, 'global_steps': 609, 'samples_trained': 5612544, 'skipped_steps': 9, 'timestamp': 1593108193.5379314}
  0: {'training_steps': 619, 'average_loss': 1.2978515625, 'step_loss': 1.2978515625, 'learning_rate': 0.0003733333333333333, 'seq/s': 26935.594252302486, 'global_steps': 610, 'samples_trained': 5621760, 'skipped_steps': 9, 'timestamp': 1593108193.8800817}
  0: {'training_steps': 620, 'average_loss': 1.2939453125, 'step_loss': 1.2939453125, 'learning_rate': 0.0003706666666666667, 'seq/s': 26930.640037956084, 'global_steps': 611, 'samples_trained': 5630976, 'skipped_steps': 9, 'timestamp': 1593108194.2222948}
  0: {'training_steps': 621, 'average_loss': 1.33203125, 'step_loss': 1.33203125, 'learning_rate': 0.0003680000000000001, 'seq/s': 26867.241013949802, 'global_steps': 612, 'samples_trained': 5640192, 'skipped_steps': 9, 'timestamp': 1593108194.5653157}
  0: {'training_steps': 622, 'average_loss': 1.3134765625, 'step_loss': 1.3134765625, 'learning_rate': 0.0003653333333333333, 'seq/s': 27005.42813867573, 'global_steps': 613, 'samples_trained': 5649408, 'skipped_steps': 9, 'timestamp': 1593108194.9065814}
  0: {'training_steps': 623, 'average_loss': 1.2392578125, 'step_loss': 1.2392578125, 'learning_rate': 0.0003626666666666667, 'seq/s': 27081.79012591324, 'global_steps': 614, 'samples_trained': 5658624, 'skipped_steps': 9, 'timestamp': 1593108195.2468846}
  0: {'training_steps': 624, 'average_loss': 1.5029296875, 'step_loss': 1.5029296875, 'learning_rate': 0.00036000000000000013, 'seq/s': 26828.51045559884, 'global_steps': 615, 'samples_trained': 5667840, 'skipped_steps': 9, 'timestamp': 1593108195.5904005}
  0: {'training_steps': 625, 'average_loss': 1.7626953125, 'step_loss': 1.7626953125, 'learning_rate': 0.0003573333333333333, 'seq/s': 26880.356755223125, 'global_steps': 616, 'samples_trained': 5677056, 'skipped_steps': 9, 'timestamp': 1593108195.9332542}
  0: {'training_steps': 626, 'average_loss': 1.3291015625, 'step_loss': 1.3291015625, 'learning_rate': 0.0003546666666666667, 'seq/s': 27035.42199778987, 'global_steps': 617, 'samples_trained': 5686272, 'skipped_steps': 9, 'timestamp': 1593108196.2741416}
  0: {'training_steps': 627, 'average_loss': 1.64453125, 'step_loss': 1.64453125, 'learning_rate': 0.0003520000000000001, 'seq/s': 26902.694162304066, 'global_steps': 618, 'samples_trained': 5695488, 'skipped_steps': 9, 'timestamp': 1593108196.6167104}
  0: {'training_steps': 628, 'average_loss': 1.640625, 'step_loss': 1.640625, 'learning_rate': 0.0003493333333333333, 'seq/s': 26926.081519323816, 'global_steps': 619, 'samples_trained': 5704704, 'skipped_steps': 9, 'timestamp': 1593108196.958982}
  0: {'training_steps': 629, 'average_loss': 1.58984375, 'step_loss': 1.58984375, 'learning_rate': 0.00034666666666666667, 'seq/s': 26967.577730073805, 'global_steps': 620, 'samples_trained': 5713920, 'skipped_steps': 9, 'timestamp': 1593108197.3007271}
  0: {'training_steps': 630, 'average_loss': 1.408203125, 'step_loss': 1.408203125, 'learning_rate': 0.00034400000000000007, 'seq/s': 27005.39040499661, 'global_steps': 621, 'samples_trained': 5723136, 'skipped_steps': 9, 'timestamp': 1593108197.6419933}
  0: {'training_steps': 631, 'average_loss': 1.2783203125, 'step_loss': 1.2783203125, 'learning_rate': 0.0003413333333333333, 'seq/s': 26865.44840307306, 'global_steps': 622, 'samples_trained': 5732352, 'skipped_steps': 9, 'timestamp': 1593108197.985037}
  0: {'training_steps': 632, 'average_loss': 1.404296875, 'step_loss': 1.404296875, 'learning_rate': 0.0003386666666666667, 'seq/s': 26892.400126897195, 'global_steps': 623, 'samples_trained': 5741568, 'skipped_steps': 9, 'timestamp': 1593108198.327737}
  0: {'training_steps': 633, 'average_loss': 1.609375, 'step_loss': 1.609375, 'learning_rate': 0.0003360000000000001, 'seq/s': 26968.970039133386, 'global_steps': 624, 'samples_trained': 5750784, 'skipped_steps': 9, 'timestamp': 1593108198.669464}
  0: {'training_steps': 634, 'average_loss': 1.51953125, 'step_loss': 1.51953125, 'learning_rate': 0.00033333333333333327, 'seq/s': 26953.85048953809, 'global_steps': 625, 'samples_trained': 5760000, 'skipped_steps': 9, 'timestamp': 1593108199.0113826}
  0: {'training_steps': 635, 'average_loss': 1.435546875, 'step_loss': 1.435546875, 'learning_rate': 0.00033066666666666666, 'seq/s': 26965.884574950105, 'global_steps': 626, 'samples_trained': 5769216, 'skipped_steps': 9, 'timestamp': 1593108199.3531485}
  0: {'training_steps': 636, 'average_loss': 1.4716796875, 'step_loss': 1.4716796875, 'learning_rate': 0.00032800000000000006, 'seq/s': 26786.25278155057, 'global_steps': 627, 'samples_trained': 5778432, 'skipped_steps': 9, 'timestamp': 1593108199.6972063}
  0: {'training_steps': 637, 'average_loss': 1.4794921875, 'step_loss': 1.4794921875, 'learning_rate': 0.00032533333333333324, 'seq/s': 26919.06854229923, 'global_steps': 628, 'samples_trained': 5787648, 'skipped_steps': 9, 'timestamp': 1593108200.039567}
  0: {'training_steps': 638, 'average_loss': 1.3466796875, 'step_loss': 1.3466796875, 'learning_rate': 0.0003226666666666667, 'seq/s': 26881.833544861467, 'global_steps': 629, 'samples_trained': 5796864, 'skipped_steps': 9, 'timestamp': 1593108200.382402}
  0: {'training_steps': 639, 'average_loss': 1.498046875, 'step_loss': 1.498046875, 'learning_rate': 0.0003200000000000001, 'seq/s': 26935.256407058198, 'global_steps': 630, 'samples_trained': 5806080, 'skipped_steps': 9, 'timestamp': 1593108200.7245574}
  0: {'training_steps': 640, 'average_loss': 1.35546875, 'step_loss': 1.35546875, 'learning_rate': 0.00031733333333333326, 'seq/s': 26805.97999747576, 'global_steps': 631, 'samples_trained': 5815296, 'skipped_steps': 9, 'timestamp': 1593108201.0683625}
  0: {'training_steps': 641, 'average_loss': 1.30859375, 'step_loss': 1.30859375, 'learning_rate': 0.00031466666666666665, 'seq/s': 27078.88745871235, 'global_steps': 632, 'samples_trained': 5824512, 'skipped_steps': 9, 'timestamp': 1593108201.4087021}
  0: {'training_steps': 642, 'average_loss': 1.3896484375, 'step_loss': 1.3896484375, 'learning_rate': 0.00031200000000000005, 'seq/s': 25209.333076814215, 'global_steps': 633, 'samples_trained': 5833728, 'skipped_steps': 9, 'timestamp': 1593108201.774282}
  0: {'training_steps': 643, 'average_loss': 1.462890625, 'step_loss': 1.462890625, 'learning_rate': 0.0003093333333333332, 'seq/s': 26954.508325941795, 'global_steps': 634, 'samples_trained': 5842944, 'skipped_steps': 9, 'timestamp': 1593108202.1161926}
  0: {'training_steps': 644, 'average_loss': 1.3896484375, 'step_loss': 1.3896484375, 'learning_rate': 0.0003066666666666667, 'seq/s': 26932.047299877097, 'global_steps': 635, 'samples_trained': 5852160, 'skipped_steps': 9, 'timestamp': 1593108202.4583883}
  0: {'training_steps': 645, 'average_loss': 1.3125, 'step_loss': 1.3125, 'learning_rate': 0.00030400000000000007, 'seq/s': 26915.038671471095, 'global_steps': 636, 'samples_trained': 5861376, 'skipped_steps': 9, 'timestamp': 1593108202.8008006}
  0: {'training_steps': 646, 'average_loss': 1.3603515625, 'step_loss': 1.3603515625, 'learning_rate': 0.00030133333333333325, 'seq/s': 24229.760216630937, 'global_steps': 637, 'samples_trained': 5870592, 'skipped_steps': 9, 'timestamp': 1593108203.1811602}
  0: {'training_steps': 647, 'average_loss': 1.2880859375, 'step_loss': 1.2880859375, 'learning_rate': 0.00029866666666666664, 'seq/s': 27106.288358968388, 'global_steps': 638, 'samples_trained': 5879808, 'skipped_steps': 9, 'timestamp': 1593108203.521156}
  0: {'training_steps': 648, 'average_loss': 1.37109375, 'step_loss': 1.37109375, 'learning_rate': 0.00029600000000000004, 'seq/s': 26932.403829579398, 'global_steps': 639, 'samples_trained': 5889024, 'skipped_steps': 9, 'timestamp': 1593108203.8633473}
  0: {'training_steps': 649, 'average_loss': 1.5400390625, 'step_loss': 1.5400390625, 'learning_rate': 0.0002933333333333332, 'seq/s': 27024.667171438577, 'global_steps': 640, 'samples_trained': 5898240, 'skipped_steps': 9, 'timestamp': 1593108204.20437}
  0: {'training_steps': 650, 'average_loss': 1.3818359375, 'step_loss': 1.3818359375, 'learning_rate': 0.0002906666666666666, 'seq/s': 24251.787395750398, 'global_steps': 641, 'samples_trained': 5907456, 'skipped_steps': 9, 'timestamp': 1593108204.584384}
  0: {'training_steps': 651, 'average_loss': 1.4599609375, 'step_loss': 1.4599609375, 'learning_rate': 0.00028800000000000006, 'seq/s': 27057.14674177815, 'global_steps': 642, 'samples_trained': 5916672, 'skipped_steps': 9, 'timestamp': 1593108204.9249973}
  0: {'training_steps': 652, 'average_loss': 1.2021484375, 'step_loss': 1.2021484375, 'learning_rate': 0.00028533333333333346, 'seq/s': 26998.14749181773, 'global_steps': 643, 'samples_trained': 5925888, 'skipped_steps': 9, 'timestamp': 1593108205.266355}
  0: {'training_steps': 653, 'average_loss': 1.349609375, 'step_loss': 1.349609375, 'learning_rate': 0.00028266666666666663, 'seq/s': 27015.865547956935, 'global_steps': 644, 'samples_trained': 5935104, 'skipped_steps': 9, 'timestamp': 1593108205.607489}
  0: {'training_steps': 654, 'average_loss': 1.45703125, 'step_loss': 1.45703125, 'learning_rate': 0.00028000000000000003, 'seq/s': 24216.371948262837, 'global_steps': 645, 'samples_trained': 5944320, 'skipped_steps': 9, 'timestamp': 1593108205.988059}
  0: {'training_steps': 655, 'average_loss': 1.3505859375, 'step_loss': 1.3505859375, 'learning_rate': 0.0002773333333333334, 'seq/s': 27079.45656010172, 'global_steps': 646, 'samples_trained': 5953536, 'skipped_steps': 9, 'timestamp': 1593108206.328392}
  0: {'training_steps': 656, 'average_loss': 1.6435546875, 'step_loss': 1.6435546875, 'learning_rate': 0.0002746666666666666, 'seq/s': 26949.39684204068, 'global_steps': 647, 'samples_trained': 5962752, 'skipped_steps': 9, 'timestamp': 1593108206.6703672}
  0: {'training_steps': 657, 'average_loss': 1.50390625, 'step_loss': 1.50390625, 'learning_rate': 0.00027200000000000005, 'seq/s': 26951.501335906603, 'global_steps': 648, 'samples_trained': 5971968, 'skipped_steps': 9, 'timestamp': 1593108207.012316}
  0: {'training_steps': 658, 'average_loss': 1.25, 'step_loss': 1.25, 'learning_rate': 0.00026933333333333345, 'seq/s': 24847.945015909747, 'global_steps': 649, 'samples_trained': 5981184, 'skipped_steps': 9, 'timestamp': 1593108207.3832126}
  0: {'training_steps': 659, 'average_loss': 1.4208984375, 'step_loss': 1.4208984375, 'learning_rate': 0.0002666666666666666, 'seq/s': 26950.317517665473, 'global_steps': 650, 'samples_trained': 5990400, 'skipped_steps': 9, 'timestamp': 1593108207.7251759}
  0: {'training_steps': 660, 'average_loss': 1.6357421875, 'step_loss': 1.6357421875, 'learning_rate': 0.000264, 'seq/s': 27071.45339829691, 'global_steps': 651, 'samples_trained': 5999616, 'skipped_steps': 9, 'timestamp': 1593108208.065609}
  0: :::MLLOG {"namespace": "", "time_ms": 1593108208543, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.7112626433372498, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 843, "epoch_num": 1}}
  0: {'global_steps': 652, 'eval_loss': 1.3375962972640991, 'eval_mlm_accuracy': 0.7112626433372498}
  0: {'training_steps': 661, 'average_loss': 1.45703125, 'step_loss': 1.45703125, 'learning_rate': 0.0002613333333333334, 'seq/s': 19277.520441658315, 'global_steps': 652, 'samples_trained': 6008832, 'skipped_steps': 9, 'timestamp': 1593108208.5436797}
  0: {'training_steps': 662, 'average_loss': 1.3681640625, 'step_loss': 1.3681640625, 'learning_rate': 0.0002586666666666666, 'seq/s': 25479.809570013815, 'global_steps': 653, 'samples_trained': 6018048, 'skipped_steps': 9, 'timestamp': 1593108208.9053788}
  0: {'training_steps': 663, 'average_loss': 1.6064453125, 'step_loss': 1.6064453125, 'learning_rate': 0.000256, 'seq/s': 27067.851623208157, 'global_steps': 654, 'samples_trained': 6027264, 'skipped_steps': 9, 'timestamp': 1593108209.2458575}
  0: {'training_steps': 664, 'average_loss': 1.1484375, 'step_loss': 1.1484375, 'learning_rate': 0.00025333333333333344, 'seq/s': 27074.771670838873, 'global_steps': 655, 'samples_trained': 6036480, 'skipped_steps': 9, 'timestamp': 1593108209.5862489}
  0: {'training_steps': 665, 'average_loss': 1.2197265625, 'step_loss': 1.2197265625, 'learning_rate': 0.0002506666666666666, 'seq/s': 26985.08612465243, 'global_steps': 656, 'samples_trained': 6045696, 'skipped_steps': 9, 'timestamp': 1593108209.9277718}
  0: {'training_steps': 666, 'average_loss': 1.546875, 'step_loss': 1.546875, 'learning_rate': 0.000248, 'seq/s': 26995.168462866433, 'global_steps': 657, 'samples_trained': 6054912, 'skipped_steps': 9, 'timestamp': 1593108210.2691674}
  0: {'training_steps': 667, 'average_loss': 1.328125, 'step_loss': 1.328125, 'learning_rate': 0.0002453333333333334, 'seq/s': 27065.956337039017, 'global_steps': 658, 'samples_trained': 6064128, 'skipped_steps': 9, 'timestamp': 1593108210.6096697}
  0: {'training_steps': 668, 'average_loss': 1.291015625, 'step_loss': 1.291015625, 'learning_rate': 0.00024266666666666658, 'seq/s': 27059.059736190535, 'global_steps': 659, 'samples_trained': 6073344, 'skipped_steps': 9, 'timestamp': 1593108210.9502587}
  0: {'training_steps': 669, 'average_loss': 1.212890625, 'step_loss': 1.212890625, 'learning_rate': 0.00024, 'seq/s': 26939.198462317938, 'global_steps': 660, 'samples_trained': 6082560, 'skipped_steps': 9, 'timestamp': 1593108211.2923634}
  0: {'training_steps': 670, 'average_loss': 1.49609375, 'step_loss': 1.49609375, 'learning_rate': 0.0002373333333333334, 'seq/s': 26907.993982810218, 'global_steps': 661, 'samples_trained': 6091776, 'skipped_steps': 9, 'timestamp': 1593108211.6348648}
  0: {'training_steps': 671, 'average_loss': 1.3046875, 'step_loss': 1.3046875, 'learning_rate': 0.00023466666666666658, 'seq/s': 26905.240811917327, 'global_steps': 662, 'samples_trained': 6100992, 'skipped_steps': 9, 'timestamp': 1593108211.9774015}
  0: {'training_steps': 672, 'average_loss': 1.33203125, 'step_loss': 1.33203125, 'learning_rate': 0.000232, 'seq/s': 26910.27935526225, 'global_steps': 663, 'samples_trained': 6110208, 'skipped_steps': 9, 'timestamp': 1593108212.3198738}
  0: {'training_steps': 673, 'average_loss': 1.1005859375, 'step_loss': 1.1005859375, 'learning_rate': 0.0002293333333333334, 'seq/s': 27014.506151778583, 'global_steps': 664, 'samples_trained': 6119424, 'skipped_steps': 9, 'timestamp': 1593108212.6610253}
  0: {'training_steps': 674, 'average_loss': 1.4521484375, 'step_loss': 1.4521484375, 'learning_rate': 0.00022666666666666657, 'seq/s': 26992.84980045879, 'global_steps': 665, 'samples_trained': 6128640, 'skipped_steps': 9, 'timestamp': 1593108213.0024502}
  0: {'training_steps': 675, 'average_loss': 1.3935546875, 'step_loss': 1.3935546875, 'learning_rate': 0.00022399999999999997, 'seq/s': 26932.32876991025, 'global_steps': 666, 'samples_trained': 6137856, 'skipped_steps': 9, 'timestamp': 1593108213.3446424}
  0: {'training_steps': 676, 'average_loss': 1.337890625, 'step_loss': 1.337890625, 'learning_rate': 0.0002213333333333334, 'seq/s': 26960.82518957465, 'global_steps': 667, 'samples_trained': 6147072, 'skipped_steps': 9, 'timestamp': 1593108213.6864724}
  0: {'training_steps': 677, 'average_loss': 1.349609375, 'step_loss': 1.349609375, 'learning_rate': 0.00021866666666666657, 'seq/s': 27013.599963660123, 'global_steps': 668, 'samples_trained': 6156288, 'skipped_steps': 9, 'timestamp': 1593108214.0276346}
  0: {'training_steps': 678, 'average_loss': 1.474609375, 'step_loss': 1.474609375, 'learning_rate': 0.00021599999999999996, 'seq/s': 26854.99627202154, 'global_steps': 669, 'samples_trained': 6165504, 'skipped_steps': 9, 'timestamp': 1593108214.370812}
  0: {'training_steps': 679, 'average_loss': 1.2919921875, 'step_loss': 1.2919921875, 'learning_rate': 0.0002133333333333334, 'seq/s': 27012.618328468394, 'global_steps': 670, 'samples_trained': 6174720, 'skipped_steps': 9, 'timestamp': 1593108214.711987}
  0: {'training_steps': 680, 'average_loss': 1.5712890625, 'step_loss': 1.5712890625, 'learning_rate': 0.00021066666666666657, 'seq/s': 26859.288127207892, 'global_steps': 671, 'samples_trained': 6183936, 'skipped_steps': 9, 'timestamp': 1593108215.0551097}
  0: {'training_steps': 681, 'average_loss': 1.4091796875, 'step_loss': 1.4091796875, 'learning_rate': 0.00020799999999999996, 'seq/s': 26918.674873831897, 'global_steps': 672, 'samples_trained': 6193152, 'skipped_steps': 9, 'timestamp': 1593108215.3974755}
  0: {'training_steps': 682, 'average_loss': 1.4189453125, 'step_loss': 1.4189453125, 'learning_rate': 0.00020533333333333338, 'seq/s': 26804.883278770827, 'global_steps': 673, 'samples_trained': 6202368, 'skipped_steps': 9, 'timestamp': 1593108215.7412944}
  0: {'training_steps': 683, 'average_loss': 1.380859375, 'step_loss': 1.380859375, 'learning_rate': 0.00020266666666666678, 'seq/s': 26972.432620040625, 'global_steps': 674, 'samples_trained': 6211584, 'skipped_steps': 9, 'timestamp': 1593108216.0829775}
  0: {'training_steps': 684, 'average_loss': 1.5302734375, 'step_loss': 1.5302734375, 'learning_rate': 0.00019999999999999996, 'seq/s': 26870.621483175062, 'global_steps': 675, 'samples_trained': 6220800, 'skipped_steps': 9, 'timestamp': 1593108216.4259553}
  0: {'training_steps': 685, 'average_loss': 1.341796875, 'step_loss': 1.341796875, 'learning_rate': 0.00019733333333333338, 'seq/s': 26836.81775888128, 'global_steps': 676, 'samples_trained': 6230016, 'skipped_steps': 9, 'timestamp': 1593108216.7693655}
  0: {'training_steps': 686, 'average_loss': 1.451171875, 'step_loss': 1.451171875, 'learning_rate': 0.00019466666666666677, 'seq/s': 27036.632215069287, 'global_steps': 677, 'samples_trained': 6239232, 'skipped_steps': 9, 'timestamp': 1593108217.1102374}
  0: {'training_steps': 687, 'average_loss': 1.337890625, 'step_loss': 1.337890625, 'learning_rate': 0.00019199999999999995, 'seq/s': 26890.379677939076, 'global_steps': 678, 'samples_trained': 6248448, 'skipped_steps': 9, 'timestamp': 1593108217.4529629}
  0: {'training_steps': 688, 'average_loss': 1.484375, 'step_loss': 1.484375, 'learning_rate': 0.00018933333333333335, 'seq/s': 26969.120567643087, 'global_steps': 679, 'samples_trained': 6257664, 'skipped_steps': 9, 'timestamp': 1593108217.7946882}
  0: {'training_steps': 689, 'average_loss': 1.2919921875, 'step_loss': 1.2919921875, 'learning_rate': 0.00018666666666666677, 'seq/s': 26898.014078508982, 'global_steps': 680, 'samples_trained': 6266880, 'skipped_steps': 9, 'timestamp': 1593108218.1373167}
  0: {'training_steps': 690, 'average_loss': 1.392578125, 'step_loss': 1.392578125, 'learning_rate': 0.00018399999999999995, 'seq/s': 26839.10969159264, 'global_steps': 681, 'samples_trained': 6276096, 'skipped_steps': 9, 'timestamp': 1593108218.4806976}
  0: {'training_steps': 691, 'average_loss': 1.4345703125, 'step_loss': 1.4345703125, 'learning_rate': 0.00018133333333333334, 'seq/s': 26806.983852578844, 'global_steps': 682, 'samples_trained': 6285312, 'skipped_steps': 9, 'timestamp': 1593108218.8244898}
  0: {'training_steps': 692, 'average_loss': 1.3857421875, 'step_loss': 1.3857421875, 'learning_rate': 0.00017866666666666676, 'seq/s': 27001.919357565275, 'global_steps': 683, 'samples_trained': 6294528, 'skipped_steps': 9, 'timestamp': 1593108219.1657996}
  0: {'training_steps': 693, 'average_loss': 1.423828125, 'step_loss': 1.423828125, 'learning_rate': 0.00017599999999999994, 'seq/s': 26838.532013116994, 'global_steps': 684, 'samples_trained': 6303744, 'skipped_steps': 9, 'timestamp': 1593108219.5091877}
  0: {'training_steps': 694, 'average_loss': 1.2080078125, 'step_loss': 1.2080078125, 'learning_rate': 0.00017333333333333334, 'seq/s': 26889.724967948427, 'global_steps': 685, 'samples_trained': 6312960, 'skipped_steps': 9, 'timestamp': 1593108219.8519216}
  0: {'training_steps': 695, 'average_loss': 1.3564453125, 'step_loss': 1.3564453125, 'learning_rate': 0.00017066666666666676, 'seq/s': 26929.364247591806, 'global_steps': 686, 'samples_trained': 6322176, 'skipped_steps': 9, 'timestamp': 1593108220.1941514}
  0: {'training_steps': 696, 'average_loss': 1.2041015625, 'step_loss': 1.2041015625, 'learning_rate': 0.00016799999999999994, 'seq/s': 26890.136496220883, 'global_steps': 687, 'samples_trained': 6331392, 'skipped_steps': 9, 'timestamp': 1593108220.5368807}
  0: {'training_steps': 697, 'average_loss': 1.318359375, 'step_loss': 1.318359375, 'learning_rate': 0.00016533333333333333, 'seq/s': 26905.634087550647, 'global_steps': 688, 'samples_trained': 6340608, 'skipped_steps': 9, 'timestamp': 1593108220.8794122}
  0: {'training_steps': 698, 'average_loss': 1.6220703125, 'step_loss': 1.6220703125, 'learning_rate': 0.00016266666666666675, 'seq/s': 26937.90308831769, 'global_steps': 689, 'samples_trained': 6349824, 'skipped_steps': 9, 'timestamp': 1593108221.2215333}
  0: {'training_steps': 699, 'average_loss': 1.3232421875, 'step_loss': 1.3232421875, 'learning_rate': 0.00015999999999999993, 'seq/s': 27053.151160764337, 'global_steps': 690, 'samples_trained': 6359040, 'skipped_steps': 9, 'timestamp': 1593108221.562197}
  0: {'training_steps': 700, 'average_loss': 1.4267578125, 'step_loss': 1.4267578125, 'learning_rate': 0.00015733333333333333, 'seq/s': 26963.89068711534, 'global_steps': 691, 'samples_trained': 6368256, 'skipped_steps': 9, 'timestamp': 1593108221.9039881}
  0: {'training_steps': 701, 'average_loss': 1.12890625, 'step_loss': 1.12890625, 'learning_rate': 0.00015466666666666672, 'seq/s': 26888.546570301685, 'global_steps': 692, 'samples_trained': 6377472, 'skipped_steps': 9, 'timestamp': 1593108222.2467375}
  0: {'training_steps': 702, 'average_loss': 1.1015625, 'step_loss': 1.1015625, 'learning_rate': 0.00015199999999999993, 'seq/s': 27095.1921975346, 'global_steps': 693, 'samples_trained': 6386688, 'skipped_steps': 9, 'timestamp': 1593108222.5868728}
  0: {'training_steps': 703, 'average_loss': 1.3271484375, 'step_loss': 1.3271484375, 'learning_rate': 0.00014933333333333332, 'seq/s': 26963.213584736146, 'global_steps': 694, 'samples_trained': 6395904, 'skipped_steps': 9, 'timestamp': 1593108222.9286726}
  0: {'training_steps': 704, 'average_loss': 1.5146484375, 'step_loss': 1.5146484375, 'learning_rate': 0.00014666666666666672, 'seq/s': 26923.305893710116, 'global_steps': 695, 'samples_trained': 6405120, 'skipped_steps': 9, 'timestamp': 1593108223.2709794}
  0: {'training_steps': 705, 'average_loss': 1.677734375, 'step_loss': 1.677734375, 'learning_rate': 0.00014399999999999992, 'seq/s': 26827.337416977243, 'global_steps': 696, 'samples_trained': 6414336, 'skipped_steps': 9, 'timestamp': 1593108223.6145105}
  0: {'training_steps': 706, 'average_loss': 1.32421875, 'step_loss': 1.32421875, 'learning_rate': 0.00014133333333333332, 'seq/s': 26912.827441197584, 'global_steps': 697, 'samples_trained': 6423552, 'skipped_steps': 9, 'timestamp': 1593108223.9569504}
  0: {'training_steps': 707, 'average_loss': 1.4541015625, 'step_loss': 1.4541015625, 'learning_rate': 0.0001386666666666667, 'seq/s': 26953.38062605116, 'global_steps': 698, 'samples_trained': 6432768, 'skipped_steps': 9, 'timestamp': 1593108224.298875}
  0: {'training_steps': 708, 'average_loss': 1.28515625, 'step_loss': 1.28515625, 'learning_rate': 0.0001359999999999999, 'seq/s': 27071.301725278998, 'global_steps': 699, 'samples_trained': 6441984, 'skipped_steps': 9, 'timestamp': 1593108224.6393106}
  0: {'training_steps': 709, 'average_loss': 1.2685546875, 'step_loss': 1.2685546875, 'learning_rate': 0.0001333333333333333, 'seq/s': 26849.624056557248, 'global_steps': 700, 'samples_trained': 6451200, 'skipped_steps': 9, 'timestamp': 1593108224.9825568}
  0: {'training_steps': 710, 'average_loss': 1.4619140625, 'step_loss': 1.4619140625, 'learning_rate': 0.0001306666666666667, 'seq/s': 27031.451733228903, 'global_steps': 701, 'samples_trained': 6460416, 'skipped_steps': 9, 'timestamp': 1593108225.323494}
  0: {'training_steps': 711, 'average_loss': 1.2783203125, 'step_loss': 1.2783203125, 'learning_rate': 0.00012799999999999989, 'seq/s': 26978.588423162608, 'global_steps': 702, 'samples_trained': 6469632, 'skipped_steps': 9, 'timestamp': 1593108225.6650999}
  0: {'training_steps': 712, 'average_loss': 1.4296875, 'step_loss': 1.4296875, 'learning_rate': 0.0001253333333333333, 'seq/s': 26918.50616229918, 'global_steps': 703, 'samples_trained': 6478848, 'skipped_steps': 9, 'timestamp': 1593108226.0074675}
  0: {'training_steps': 713, 'average_loss': 1.5849609375, 'step_loss': 1.5849609375, 'learning_rate': 0.0001226666666666667, 'seq/s': 26931.559432422906, 'global_steps': 704, 'samples_trained': 6488064, 'skipped_steps': 9, 'timestamp': 1593108226.3496695}
  0: {'training_steps': 714, 'average_loss': 1.5009765625, 'step_loss': 1.5009765625, 'learning_rate': 0.00012000000000000011, 'seq/s': 26884.84370713902, 'global_steps': 705, 'samples_trained': 6497280, 'skipped_steps': 9, 'timestamp': 1593108226.692466}
  0: :::MLLOG {"namespace": "", "time_ms": 1593108227168, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.7120344638824463, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 843, "epoch_num": 1}}
  0: {'global_steps': 706, 'eval_loss': 1.332696557044983, 'eval_mlm_accuracy': 0.7120344638824463}
  0: 0.712034 > 0.712000, Target MLM Accuracy reached at 706
  0: {'training_steps': 715, 'average_loss': 1.2294921875, 'step_loss': 1.2294921875, 'learning_rate': 0.00011733333333333329, 'seq/s': 19366.453702024388, 'global_steps': 706, 'samples_trained': 6506496, 'skipped_steps': 9, 'timestamp': 1593108227.1683414}
  0: (1, 715.0) {'final_loss': 0.0}
  0: :::MLLOG {"namespace": "", "time_ms": 1593108227251, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 954, "first_epoch_num": 1}}
  0: :::MLLOG {"namespace": "", "time_ms": 1593108227252, "event_type": "INTERVAL_END", "key": "epoch_stop", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 956, "epoch_num": 1}}
  0: :::MLLOG {"namespace": "", "time_ms": 1593108227252, "event_type": "POINT_IN_TIME", "key": "train_samples", "value": 6506496, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 961}}
  0: :::MLLOG {"namespace": "", "time_ms": 1593108227252, "event_type": "POINT_IN_TIME", "key": "eval_samples", "value": 10000, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 964}}
  0: :::MLLOG {"namespace": "", "time_ms": 1593108227252, "event_type": "INTERVAL_END", "key": "run_stop", "value": null, "metadata": {"file": "/workspace/bert/run_pretraining.py", "lineno": 966, "status": "success"}}
  0: {'e2e_time': 284.3058240413666, 'training_sequences_per_second': 27394.075075118548, 'final_loss': 0.0, 'raw_train_time': 255.34514236450195}
 25: ++ date +%s
 25: + END=1593108230
 25: ++ date '+%Y-%m-%d %r'
 25: ENDING TIMING RUN AT 2020-06-25 11:03:50 AM
 25: + END_FMT='2020-06-25 11:03:50 AM'
 25: + echo 'ENDING TIMING RUN AT 2020-06-25 11:03:50 AM'
 25: + RESULT=289
 25: + RESULT_NAME=bert
 25: + echo 'RESULT,bert,18263,289,root,2020-06-25 10:59:01 AM'
 25: RESULT,bert,18263,289,root,2020-06-25 10:59:01 AM
 25: + set +x
  4: ++ date +%s
  4: + END=1593108230
  4: ++ date '+%Y-%m-%d %r'
  4: + END_FMT='2020-06-25 11:03:50 AM'
  4: + echo 'ENDING TIMING RUN AT 2020-06-25 11:03:50 AM'
  4: ENDING TIMING RUN AT 2020-06-25 11:03:50 AM
  4: + RESULT=289
  4: + RESULT_NAME=bert
  4: RESULT,bert,18263,289,root,2020-06-25 10:59:01 AM
  4: + echo 'RESULT,bert,18263,289,root,2020-06-25 10:59:01 AM'
  4: + set +x
124: ++ date +%s
124: + END=1593108230
124: ++ date '+%Y-%m-%d %r'
124: + END_FMT='2020-06-25 11:03:50 AM'
124: + echo 'ENDING TIMING RUN AT 2020-06-25 11:03:50 AM'
124: ENDING TIMING RUN AT 2020-06-25 11:03:50 AM
124: + RESULT=290
124: + RESULT_NAME=bert
124: + echo 'RESULT,bert,18263,290,root,2020-06-25 10:59:00 AM'
124: RESULT,bert,18263,290,root,2020-06-25 10:59:00 AM
124: + set +x
442: ++ date +%s
442: + END=1593108230
442: ++ date '+%Y-%m-%d %r'
442: ENDING TIMING RUN AT 2020-06-25 11:03:50 AM
442: + END_FMT='2020-06-25 11:03:50 AM'
442: + echo 'ENDING TIMING RUN AT 2020-06-25 11:03:50 AM'
442: + RESULT=290
442: + RESULT_NAME=bert
442: RESULT,bert,18263,290,root,2020-06-25 10:59:00 AM
442: + echo 'RESULT,bert,18263,290,root,2020-06-25 10:59:00 AM'
442: + set +x
235: ++ date +%s
 67: ++ date +%s
235: + END=1593108230
 67: + END=1593108230
235: ++ date '+%Y-%m-%d %r'
 67: ++ date '+%Y-%m-%d %r'
 67: + END_FMT='2020-06-25 11:03:50 AM'
 67: + echo 'ENDING TIMING RUN AT 2020-06-25 11:03:50 AM'
 67: ENDING TIMING RUN AT 2020-06-25 11:03:50 AM
235: ENDING TIMING RUN AT 2020-06-25 11:03:50 AM
 67: + RESULT=290
 67: + RESULT_NAME=bert
 67: + echo 'RESULT,bert,18263,290,root,2020-06-25 10:59:00 AM'
 67: RESULT,bert,18263,290,root,2020-06-25 10:59:00 AM
 67: + set +x
235: + END_FMT='2020-06-25 11:03:50 AM'
235: + echo 'ENDING TIMING RUN AT 2020-06-25 11:03:50 AM'
235: + RESULT=290
235: + RESULT_NAME=bert
235: RESULT,bert,18263,290,root,2020-06-25 10:59:00 AM
235: + echo 'RESULT,bert,18263,290,root,2020-06-25 10:59:00 AM'
235: + set +x
268: ++ date +%s
268: + END=1593108230
268: ++ date '+%Y-%m-%d %r'
268: ENDING TIMING RUN AT 2020-06-25 11:03:50 AM
268: + END_FMT='2020-06-25 11:03:50 AM'
268: + echo 'ENDING TIMING RUN AT 2020-06-25 11:03:50 AM'
268: + RESULT=290
268: + RESULT_NAME=bert
268: RESULT,bert,18263,290,root,2020-06-25 10:59:00 AM
268: + echo 'RESULT,bert,18263,290,root,2020-06-25 10:59:00 AM'
268: + set +x
170: ++ date +%s
170: + END=1593108230
397: ++ date +%s
170: ++ date '+%Y-%m-%d %r'
397: + END=1593108230
170: ENDING TIMING RUN AT 2020-06-25 11:03:50 AM
397: ++ date '+%Y-%m-%d %r'
170: + END_FMT='2020-06-25 11:03:50 AM'
170: + echo 'ENDING TIMING RUN AT 2020-06-25 11:03:50 AM'
170: + RESULT=290
170: + RESULT_NAME=bert
170: RESULT,bert,18263,290,root,2020-06-25 10:59:00 AM
170: + echo 'RESULT,bert,18263,290,root,2020-06-25 10:59:00 AM'
170: + set +x
397: ENDING TIMING RUN AT 2020-06-25 11:03:50 AM
397: + END_FMT='2020-06-25 11:03:50 AM'
397: + echo 'ENDING TIMING RUN AT 2020-06-25 11:03:50 AM'
397: + RESULT=289
397: + RESULT_NAME=bert
397: RESULT,bert,18263,289,root,2020-06-25 10:59:01 AM
397: + echo 'RESULT,bert,18263,289,root,2020-06-25 10:59:01 AM'
397: + set +x
329: ++ date +%s
329: + END=1593108230
329: ++ date '+%Y-%m-%d %r'
329: + END_FMT='2020-06-25 11:03:50 AM'
329: ENDING TIMING RUN AT 2020-06-25 11:03:50 AM
329: + echo 'ENDING TIMING RUN AT 2020-06-25 11:03:50 AM'
329: + RESULT=290
329: + RESULT_NAME=bert
329: RESULT,bert,18263,290,root,2020-06-25 10:59:00 AM
329: + echo 'RESULT,bert,18263,290,root,2020-06-25 10:59:00 AM'
329: + set +x
418: ++ date +%s
418: + END=1593108230
418: ++ date '+%Y-%m-%d %r'
418: ENDING TIMING RUN AT 2020-06-25 11:03:50 AM
418: + END_FMT='2020-06-25 11:03:50 AM'
418: + echo 'ENDING TIMING RUN AT 2020-06-25 11:03:50 AM'
418: + RESULT=289
418: + RESULT_NAME=bert
418: + echo 'RESULT,bert,18263,289,root,2020-06-25 10:59:01 AM'
418: RESULT,bert,18263,289,root,2020-06-25 10:59:01 AM
418: + set +x
246: ++ date +%s
246: + END=1593108230
246: ++ date '+%Y-%m-%d %r'
246: + END_FMT='2020-06-25 11:03:50 AM'
246: + echo 'ENDING TIMING RUN AT 2020-06-25 11:03:50 AM'
246: ENDING TIMING RUN AT 2020-06-25 11:03:50 AM
246: + RESULT=289
246: + RESULT_NAME=bert
246: RESULT,bert,18263,289,root,2020-06-25 10:59:01 AM
246: + echo 'RESULT,bert,18263,289,root,2020-06-25 10:59:01 AM'
246: + set +x
354: ++ date +%s
354: + END=1593108230
354: ++ date '+%Y-%m-%d %r'
354: + END_FMT='2020-06-25 11:03:50 AM'
354: + echo 'ENDING TIMING RUN AT 2020-06-25 11:03:50 AM'
354: ENDING TIMING RUN AT 2020-06-25 11:03:50 AM
354: + RESULT=289
354: + RESULT_NAME=bert
354: RESULT,bert,18263,289,root,2020-06-25 10:59:01 AM
354: + echo 'RESULT,bert,18263,289,root,2020-06-25 10:59:01 AM'
354: + set +x
155: ++ date +%s
155: + END=1593108231
155: ++ date '+%Y-%m-%d %r'
155: ENDING TIMING RUN AT 2020-06-25 11:03:51 AM
155: + END_FMT='2020-06-25 11:03:51 AM'
155: + echo 'ENDING TIMING RUN AT 2020-06-25 11:03:51 AM'
155: + RESULT=290
155: + RESULT_NAME=bert
155: RESULT,bert,18263,290,root,2020-06-25 10:59:01 AM
155: + echo 'RESULT,bert,18263,290,root,2020-06-25 10:59:01 AM'
155: + set +x
404: ++ date +%s
404: + END=1593108231
404: ++ date '+%Y-%m-%d %r'
404: ENDING TIMING RUN AT 2020-06-25 11:03:51 AM
404: + END_FMT='2020-06-25 11:03:51 AM'
404: + echo 'ENDING TIMING RUN AT 2020-06-25 11:03:51 AM'
404: + RESULT=290
404: + RESULT_NAME=bert
404: RESULT,bert,18263,290,root,2020-06-25 10:59:01 AM
404: + echo 'RESULT,bert,18263,290,root,2020-06-25 10:59:01 AM'
404: + set +x
283: ++ date +%s
283: + END=1593108231
283: ++ date '+%Y-%m-%d %r'
283: + END_FMT='2020-06-25 11:03:51 AM'
283: ENDING TIMING RUN AT 2020-06-25 11:03:51 AM
283: + echo 'ENDING TIMING RUN AT 2020-06-25 11:03:51 AM'
283: + RESULT=290
283: + RESULT_NAME=bert
283: RESULT,bert,18263,290,root,2020-06-25 10:59:01 AM
283: + echo 'RESULT,bert,18263,290,root,2020-06-25 10:59:01 AM'
283: + set +x
211: ++ date +%s
211: + END=1593108231
211: ++ date '+%Y-%m-%d %r'
211: ENDING TIMING RUN AT 2020-06-25 11:03:51 AM
211: + END_FMT='2020-06-25 11:03:51 AM'
211: + echo 'ENDING TIMING RUN AT 2020-06-25 11:03:51 AM'
211: + RESULT=290
211: + RESULT_NAME=bert
211: RESULT,bert,18263,290,root,2020-06-25 10:59:01 AM
211: + echo 'RESULT,bert,18263,290,root,2020-06-25 10:59:01 AM'
211: + set +x
132: ++ date +%s
132: + END=1593108231
132: ++ date '+%Y-%m-%d %r'
132: ENDING TIMING RUN AT 2020-06-25 11:03:51 AM
132: + END_FMT='2020-06-25 11:03:51 AM'
132: + echo 'ENDING TIMING RUN AT 2020-06-25 11:03:51 AM'
132: + RESULT=291
132: + RESULT_NAME=bert
132: RESULT,bert,18263,291,root,2020-06-25 10:59:00 AM
132: + echo 'RESULT,bert,18263,291,root,2020-06-25 10:59:00 AM'
132: + set +x
 58: ++ date +%s
 58: + END=1593108231
 58: ++ date '+%Y-%m-%d %r'
 58: ENDING TIMING RUN AT 2020-06-25 11:03:51 AM
 58: + END_FMT='2020-06-25 11:03:51 AM'
 58: + echo 'ENDING TIMING RUN AT 2020-06-25 11:03:51 AM'
 58: + RESULT=291
 58: + RESULT_NAME=bert
 58: RESULT,bert,18263,291,root,2020-06-25 10:59:00 AM
 58: + echo 'RESULT,bert,18263,291,root,2020-06-25 10:59:00 AM'
 58: + set +x
204: ++ date +%s
204: + END=1593108231
204: ++ date '+%Y-%m-%d %r'
204: ENDING TIMING RUN AT 2020-06-25 11:03:51 AM
204: + END_FMT='2020-06-25 11:03:51 AM'
204: + echo 'ENDING TIMING RUN AT 2020-06-25 11:03:51 AM'
204: + RESULT=291
204: + RESULT_NAME=bert
204: RESULT,bert,18263,291,root,2020-06-25 10:59:00 AM
204: + echo 'RESULT,bert,18263,291,root,2020-06-25 10:59:00 AM'
204: + set +x
 46: ++ date +%s
 46: + END=1593108231
 46: ++ date '+%Y-%m-%d %r'
 46: + END_FMT='2020-06-25 11:03:51 AM'
 46: + echo 'ENDING TIMING RUN AT 2020-06-25 11:03:51 AM'
 46: ENDING TIMING RUN AT 2020-06-25 11:03:51 AM
 46: + RESULT=291
 46: + RESULT_NAME=bert
 46: RESULT,bert,18263,291,root,2020-06-25 10:59:00 AM
 46: + echo 'RESULT,bert,18263,291,root,2020-06-25 10:59:00 AM'
 46: + set +x
291: ++ date +%s
291: + END=1593108232
291: ++ date '+%Y-%m-%d %r'
291: ENDING TIMING RUN AT 2020-06-25 11:03:52 AM
291: + END_FMT='2020-06-25 11:03:52 AM'
291: + echo 'ENDING TIMING RUN AT 2020-06-25 11:03:52 AM'
291: + RESULT=291
291: + RESULT_NAME=bert
291: RESULT,bert,18263,291,root,2020-06-25 10:59:01 AM
291: + echo 'RESULT,bert,18263,291,root,2020-06-25 10:59:01 AM'
291: + set +x
350: ++ date +%s
350: + END=1593108232
350: ++ date '+%Y-%m-%d %r'
350: + END_FMT='2020-06-25 11:03:52 AM'
350: + echo 'ENDING TIMING RUN AT 2020-06-25 11:03:52 AM'
350: ENDING TIMING RUN AT 2020-06-25 11:03:52 AM
350: + RESULT=291
350: + RESULT_NAME=bert
350: RESULT,bert,18263,291,root,2020-06-25 10:59:01 AM
350: + echo 'RESULT,bert,18263,291,root,2020-06-25 10:59:01 AM'
350: + set +x
180: ++ date +%s
180: + END=1593108232
180: ++ date '+%Y-%m-%d %r'
180: + END_FMT='2020-06-25 11:03:52 AM'
180: ENDING TIMING RUN AT 2020-06-25 11:03:52 AM
180: + echo 'ENDING TIMING RUN AT 2020-06-25 11:03:52 AM'
180: + RESULT=291
180: + RESULT_NAME=bert
180: RESULT,bert,18263,291,root,2020-06-25 10:59:01 AM
180: + echo 'RESULT,bert,18263,291,root,2020-06-25 10:59:01 AM'
180: + set +x
381: ++ date +%s
381: + END=1593108232
381: ++ date '+%Y-%m-%d %r'
381: + END_FMT='2020-06-25 11:03:52 AM'
381: ENDING TIMING RUN AT 2020-06-25 11:03:52 AM
381: + echo 'ENDING TIMING RUN AT 2020-06-25 11:03:52 AM'
381: + RESULT=291
381: RESULT,bert,18263,291,root,2020-06-25 10:59:01 AM
381: + RESULT_NAME=bert
381: + echo 'RESULT,bert,18263,291,root,2020-06-25 10:59:01 AM'
381: + set +x
501: ++ date +%s
501: + END=1593108232
501: ++ date '+%Y-%m-%d %r'
501: ENDING TIMING RUN AT 2020-06-25 11:03:52 AM
501: + END_FMT='2020-06-25 11:03:52 AM'
501: + echo 'ENDING TIMING RUN AT 2020-06-25 11:03:52 AM'
501: + RESULT=292
501: + RESULT_NAME=bert
501: + echo 'RESULT,bert,18263,292,root,2020-06-25 10:59:00 AM'
501: + set +x
501: RESULT,bert,18263,292,root,2020-06-25 10:59:00 AM
102: ++ date +%s
102: + END=1593108232
102: ++ date '+%Y-%m-%d %r'
102: ENDING TIMING RUN AT 2020-06-25 11:03:52 AM
102: + END_FMT='2020-06-25 11:03:52 AM'
102: + echo 'ENDING TIMING RUN AT 2020-06-25 11:03:52 AM'
102: + RESULT=291
102: + RESULT_NAME=bert
102: + echo 'RESULT,bert,18263,291,root,2020-06-25 10:59:01 AM'
102: RESULT,bert,18263,291,root,2020-06-25 10:59:01 AM
102: + set +x
316: ++ date +%s
316: + END=1593108232
316: ++ date '+%Y-%m-%d %r'
316: ENDING TIMING RUN AT 2020-06-25 11:03:52 AM
316: + END_FMT='2020-06-25 11:03:52 AM'
316: + echo 'ENDING TIMING RUN AT 2020-06-25 11:03:52 AM'
316: + RESULT=291
316: + RESULT_NAME=bert
316: RESULT,bert,18263,291,root,2020-06-25 10:59:01 AM
316: + echo 'RESULT,bert,18263,291,root,2020-06-25 10:59:01 AM'
316: + set +x
472: ++ date +%s
472: + END=1593108232
472: ++ date '+%Y-%m-%d %r'
472: + END_FMT='2020-06-25 11:03:52 AM'
472: + echo 'ENDING TIMING RUN AT 2020-06-25 11:03:52 AM'
472: ENDING TIMING RUN AT 2020-06-25 11:03:52 AM
472: + RESULT=291
472: + RESULT_NAME=bert
472: RESULT,bert,18263,291,root,2020-06-25 10:59:01 AM
472: + echo 'RESULT,bert,18263,291,root,2020-06-25 10:59:01 AM'
472: + set +x
489: ++ date +%s
489: + END=1593108232
489: ++ date '+%Y-%m-%d %r'
489: + END_FMT='2020-06-25 11:03:52 AM'
489: + echo 'ENDING TIMING RUN AT 2020-06-25 11:03:52 AM'
489: ENDING TIMING RUN AT 2020-06-25 11:03:52 AM
489: + RESULT=291
489: + RESULT_NAME=bert
489: RESULT,bert,18263,291,root,2020-06-25 10:59:01 AM
489: + echo 'RESULT,bert,18263,291,root,2020-06-25 10:59:01 AM'
489: + set +x
 89: ++ date +%s
 89: + END=1593108232
 89: ++ date '+%Y-%m-%d %r'
 89: + END_FMT='2020-06-25 11:03:52 AM'
 89: + echo 'ENDING TIMING RUN AT 2020-06-25 11:03:52 AM'
 89: ENDING TIMING RUN AT 2020-06-25 11:03:52 AM
 89: + RESULT=292
 89: + RESULT_NAME=bert
 89: RESULT,bert,18263,292,root,2020-06-25 10:59:00 AM
 89: + echo 'RESULT,bert,18263,292,root,2020-06-25 10:59:00 AM'
 89: + set +x
448: ++ date +%s
448: + END=1593108232
448: ++ date '+%Y-%m-%d %r'
448: + END_FMT='2020-06-25 11:03:52 AM'
448: ENDING TIMING RUN AT 2020-06-25 11:03:52 AM
448: + echo 'ENDING TIMING RUN AT 2020-06-25 11:03:52 AM'
448: + RESULT=292
448: + RESULT_NAME=bert
448: RESULT,bert,18263,292,root,2020-06-25 10:59:00 AM
448: + echo 'RESULT,bert,18263,292,root,2020-06-25 10:59:00 AM'
448: + set +x
 24: ++ date +%s
 31: ++ date +%s
 20: ++ date +%s
 22: ++ date +%s
 18: ++ date +%s
 30: ++ date +%s
 16: ++ date +%s
 19: ++ date +%s
 29: ++ date +%s
 17: ++ date +%s
 21: ++ date +%s
 27: ++ date +%s
 23: ++ date +%s
 24: + END=1593108234
 31: + END=1593108234
 24: ++ date '+%Y-%m-%d %r'
 26: ++ date +%s
 20: + END=1593108234
 22: + END=1593108234
 31: ++ date '+%Y-%m-%d %r'
 30: + END=1593108234
 16: + END=1593108234
 19: + END=1593108234
 18: + END=1593108234
 20: ++ date '+%Y-%m-%d %r'
 29: + END=1593108234
 22: ++ date '+%Y-%m-%d %r'
 27: + END=1593108234
 17: + END=1593108234
 23: + END=1593108234
 21: + END=1593108234
 16: ++ date '+%Y-%m-%d %r'
 30: ++ date '+%Y-%m-%d %r'
 19: ++ date '+%Y-%m-%d %r'
 24: + END_FMT='2020-06-25 11:03:54 AM'
 18: ++ date '+%Y-%m-%d %r'
 24: ENDING TIMING RUN AT 2020-06-25 11:03:54 AM
 24: + echo 'ENDING TIMING RUN AT 2020-06-25 11:03:54 AM'
 24: + RESULT=293
 24: + RESULT_NAME=bert
 29: ++ date '+%Y-%m-%d %r'
 24: RESULT,bert,18263,293,root,2020-06-25 10:59:01 AM
 24: + echo 'RESULT,bert,18263,293,root,2020-06-25 10:59:01 AM'
 24: + set +x
 26: + END=1593108234
 27: ++ date '+%Y-%m-%d %r'
 31: + END_FMT='2020-06-25 11:03:54 AM'
 17: ++ date '+%Y-%m-%d %r'
 23: ++ date '+%Y-%m-%d %r'
 31: + echo 'ENDING TIMING RUN AT 2020-06-25 11:03:54 AM'
 21: ++ date '+%Y-%m-%d %r'
 31: ENDING TIMING RUN AT 2020-06-25 11:03:54 AM
 31: + RESULT=293
 31: + RESULT_NAME=bert
 31: RESULT,bert,18263,293,root,2020-06-25 10:59:01 AM
 31: + echo 'RESULT,bert,18263,293,root,2020-06-25 10:59:01 AM'
 31: + set +x
 26: ++ date '+%Y-%m-%d %r'
 20: + END_FMT='2020-06-25 11:03:54 AM'
 20: ENDING TIMING RUN AT 2020-06-25 11:03:54 AM
 20: + echo 'ENDING TIMING RUN AT 2020-06-25 11:03:54 AM'
 22: + END_FMT='2020-06-25 11:03:54 AM'
 20: + RESULT=293
 20: + RESULT_NAME=bert
 22: ENDING TIMING RUN AT 2020-06-25 11:03:54 AM
 22: + echo 'ENDING TIMING RUN AT 2020-06-25 11:03:54 AM'
 20: RESULT,bert,18263,293,root,2020-06-25 10:59:01 AM
 20: + echo 'RESULT,bert,18263,293,root,2020-06-25 10:59:01 AM'
 22: + RESULT=293
 22: + RESULT_NAME=bert
 20: + set +x
 22: RESULT,bert,18263,293,root,2020-06-25 10:59:01 AM
 22: + echo 'RESULT,bert,18263,293,root,2020-06-25 10:59:01 AM'
 22: + set +x
 16: + END_FMT='2020-06-25 11:03:54 AM'
 30: + END_FMT='2020-06-25 11:03:54 AM'
 16: + echo 'ENDING TIMING RUN AT 2020-06-25 11:03:54 AM'
 16: ENDING TIMING RUN AT 2020-06-25 11:03:54 AM
 16: + RESULT=293
 30: + echo 'ENDING TIMING RUN AT 2020-06-25 11:03:54 AM'
 16: + RESULT_NAME=bert
 30: ENDING TIMING RUN AT 2020-06-25 11:03:54 AM
 16: RESULT,bert,18263,293,root,2020-06-25 10:59:01 AM
 16: + echo 'RESULT,bert,18263,293,root,2020-06-25 10:59:01 AM'
 16: + set +x
 30: + RESULT=293
 30: + RESULT_NAME=bert
 30: RESULT,bert,18263,293,root,2020-06-25 10:59:01 AM
 30: + echo 'RESULT,bert,18263,293,root,2020-06-25 10:59:01 AM'
 30: + set +x
 18: + END_FMT='2020-06-25 11:03:54 AM'
 18: + echo 'ENDING TIMING RUN AT 2020-06-25 11:03:54 AM'
 18: ENDING TIMING RUN AT 2020-06-25 11:03:54 AM
 18: + RESULT=293
 18: + RESULT_NAME=bert
 18: + echo 'RESULT,bert,18263,293,root,2020-06-25 10:59:01 AM'
 18: RESULT,bert,18263,293,root,2020-06-25 10:59:01 AM
 18: + set +x
 19: + END_FMT='2020-06-25 11:03:54 AM'
 19: ENDING TIMING RUN AT 2020-06-25 11:03:54 AM
 19: + echo 'ENDING TIMING RUN AT 2020-06-25 11:03:54 AM'
 19: + RESULT=293
 19: + RESULT_NAME=bert
 19: RESULT,bert,18263,293,root,2020-06-25 10:59:01 AM
 19: + echo 'RESULT,bert,18263,293,root,2020-06-25 10:59:01 AM'
 19: + set +x
 27: + END_FMT='2020-06-25 11:03:54 AM'
 29: + END_FMT='2020-06-25 11:03:54 AM'
 29: + echo 'ENDING TIMING RUN AT 2020-06-25 11:03:54 AM'
 27: ENDING TIMING RUN AT 2020-06-25 11:03:54 AM
 27: + echo 'ENDING TIMING RUN AT 2020-06-25 11:03:54 AM'
 29: ENDING TIMING RUN AT 2020-06-25 11:03:54 AM
 29: + RESULT=293
 29: + RESULT_NAME=bert
 27: + RESULT=293
 27: + RESULT_NAME=bert
 29: RESULT,bert,18263,293,root,2020-06-25 10:59:01 AM
 29: + echo 'RESULT,bert,18263,293,root,2020-06-25 10:59:01 AM'
 29: + set +x
 27: RESULT,bert,18263,293,root,2020-06-25 10:59:01 AM
 27: + echo 'RESULT,bert,18263,293,root,2020-06-25 10:59:01 AM'
 27: + set +x
 23: + END_FMT='2020-06-25 11:03:54 AM'
 21: + END_FMT='2020-06-25 11:03:54 AM'
 23: + echo 'ENDING TIMING RUN AT 2020-06-25 11:03:54 AM'
 23: ENDING TIMING RUN AT 2020-06-25 11:03:54 AM
 21: ENDING TIMING RUN AT 2020-06-25 11:03:54 AM
 21: + echo 'ENDING TIMING RUN AT 2020-06-25 11:03:54 AM'
 23: + RESULT=293
 23: + RESULT_NAME=bert
 21: + RESULT=293
 21: + RESULT_NAME=bert
 23: RESULT,bert,18263,293,root,2020-06-25 10:59:01 AM
 23: + echo 'RESULT,bert,18263,293,root,2020-06-25 10:59:01 AM'
 23: + set +x
 21: + echo 'RESULT,bert,18263,293,root,2020-06-25 10:59:01 AM'
 21: RESULT,bert,18263,293,root,2020-06-25 10:59:01 AM
 21: + set +x
 26: + END_FMT='2020-06-25 11:03:54 AM'
 26: + echo 'ENDING TIMING RUN AT 2020-06-25 11:03:54 AM'
 26: ENDING TIMING RUN AT 2020-06-25 11:03:54 AM
 26: + RESULT=293
 26: + RESULT_NAME=bert
 26: RESULT,bert,18263,293,root,2020-06-25 10:59:01 AM
 26: + echo 'RESULT,bert,18263,293,root,2020-06-25 10:59:01 AM'
 26: + set +x
 17: ENDING TIMING RUN AT 2020-06-25 11:03:54 AM
 17: RESULT,bert,18263,293,root,2020-06-25 10:59:01 AM
 17: + END_FMT='2020-06-25 11:03:54 AM'
 17: + echo 'ENDING TIMING RUN AT 2020-06-25 11:03:54 AM'
 17: + RESULT=293
 17: + RESULT_NAME=bert
 17: + echo 'RESULT,bert,18263,293,root,2020-06-25 10:59:01 AM'
 17: + set +x
  1: ++ date +%s
  7: ++ date +%s
  2: ++ date +%s
  6: ++ date +%s
  5: ++ date +%s
  3: ++ date +%s
 13: ++ date +%s
 14: ++ date +%s
 15: ++ date +%s
  8: ++ date +%s
  1: + END=1593108234
  9: ++ date +%s
 10: ++ date +%s
 12: ++ date +%s
 11: ++ date +%s
  2: + END=1593108234
  7: + END=1593108234
  6: + END=1593108234
  1: ++ date '+%Y-%m-%d %r'
  5: + END=1593108234
  2: ++ date '+%Y-%m-%d %r'
  7: ++ date '+%Y-%m-%d %r'
  5: ++ date '+%Y-%m-%d %r'
  6: ++ date '+%Y-%m-%d %r'
 13: + END=1593108234
  3: + END=1593108234
  8: + END=1593108234
 14: + END=1593108234
 15: + END=1593108234
  9: + END=1593108234
 10: + END=1593108234
 11: + END=1593108234
 12: + END=1593108234
  1: + END_FMT='2020-06-25 11:03:54 AM'
  1: + echo 'ENDING TIMING RUN AT 2020-06-25 11:03:54 AM'
  1: ENDING TIMING RUN AT 2020-06-25 11:03:54 AM
 13: ++ date '+%Y-%m-%d %r'
 14: ++ date '+%Y-%m-%d %r'
  1: + RESULT=293
  1: + RESULT_NAME=bert
  1: + echo 'RESULT,bert,18263,293,root,2020-06-25 10:59:01 AM'
  3: ++ date '+%Y-%m-%d %r'
  1: RESULT,bert,18263,293,root,2020-06-25 10:59:01 AM
  1: + set +x
  8: ++ date '+%Y-%m-%d %r'
 15: ++ date '+%Y-%m-%d %r'
  9: ++ date '+%Y-%m-%d %r'
 10: ++ date '+%Y-%m-%d %r'
  2: ENDING TIMING RUN AT 2020-06-25 11:03:54 AM
  2: + END_FMT='2020-06-25 11:03:54 AM'
  2: + echo 'ENDING TIMING RUN AT 2020-06-25 11:03:54 AM'
  2: + RESULT=293
  2: + RESULT_NAME=bert
 11: ++ date '+%Y-%m-%d %r'
  2: RESULT,bert,18263,293,root,2020-06-25 10:59:01 AM
  2: + echo 'RESULT,bert,18263,293,root,2020-06-25 10:59:01 AM'
  2: + set +x
 12: ++ date '+%Y-%m-%d %r'
  7: + END_FMT='2020-06-25 11:03:54 AM'
  7: ENDING TIMING RUN AT 2020-06-25 11:03:54 AM
  7: + echo 'ENDING TIMING RUN AT 2020-06-25 11:03:54 AM'
  7: + RESULT=293
  7: + RESULT_NAME=bert
  5: + END_FMT='2020-06-25 11:03:54 AM'
  6: + END_FMT='2020-06-25 11:03:54 AM'
  7: + echo 'RESULT,bert,18263,293,root,2020-06-25 10:59:01 AM'
  7: + set +x
  5: ENDING TIMING RUN AT 2020-06-25 11:03:54 AM
  5: + echo 'ENDING TIMING RUN AT 2020-06-25 11:03:54 AM'
  7: RESULT,bert,18263,293,root,2020-06-25 10:59:01 AM
  5: + RESULT=293
  5: + RESULT_NAME=bert
  6: ENDING TIMING RUN AT 2020-06-25 11:03:54 AM
  6: + echo 'ENDING TIMING RUN AT 2020-06-25 11:03:54 AM'
  6: + RESULT=293
  6: + RESULT_NAME=bert
  5: RESULT,bert,18263,293,root,2020-06-25 10:59:01 AM
  5: + echo 'RESULT,bert,18263,293,root,2020-06-25 10:59:01 AM'
  5: + set +x
  6: RESULT,bert,18263,293,root,2020-06-25 10:59:01 AM
  6: + echo 'RESULT,bert,18263,293,root,2020-06-25 10:59:01 AM'
  6: + set +x
 14: ENDING TIMING RUN AT 2020-06-25 11:03:54 AM
 14: RESULT,bert,18263,293,root,2020-06-25 10:59:01 AM
 14: + END_FMT='2020-06-25 11:03:54 AM'
 14: + echo 'ENDING TIMING RUN AT 2020-06-25 11:03:54 AM'
 14: + RESULT=293
 14: + RESULT_NAME=bert
 14: + echo 'RESULT,bert,18263,293,root,2020-06-25 10:59:01 AM'
 14: + set +x
  8: ENDING TIMING RUN AT 2020-06-25 11:03:54 AM
  8: + END_FMT='2020-06-25 11:03:54 AM'
  8: + echo 'ENDING TIMING RUN AT 2020-06-25 11:03:54 AM'
  8: + RESULT=293
  8: + RESULT_NAME=bert
 13: + END_FMT='2020-06-25 11:03:54 AM'
  8: RESULT,bert,18263,293,root,2020-06-25 10:59:01 AM
  8: + echo 'RESULT,bert,18263,293,root,2020-06-25 10:59:01 AM'
  8: + set +x
  3: + END_FMT='2020-06-25 11:03:54 AM'
 13: ENDING TIMING RUN AT 2020-06-25 11:03:54 AM
 13: + echo 'ENDING TIMING RUN AT 2020-06-25 11:03:54 AM'
 13: + RESULT=293
 13: + RESULT_NAME=bert
  3: ENDING TIMING RUN AT 2020-06-25 11:03:54 AM
  3: + echo 'ENDING TIMING RUN AT 2020-06-25 11:03:54 AM'
 13: RESULT,bert,18263,293,root,2020-06-25 10:59:01 AM
 13: + echo 'RESULT,bert,18263,293,root,2020-06-25 10:59:01 AM'
 13: + set +x
 15: + END_FMT='2020-06-25 11:03:54 AM'
  3: + RESULT=293
  3: + RESULT_NAME=bert
  3: RESULT,bert,18263,293,root,2020-06-25 10:59:01 AM
  3: + echo 'RESULT,bert,18263,293,root,2020-06-25 10:59:01 AM'
  3: + set +x
 15: ENDING TIMING RUN AT 2020-06-25 11:03:54 AM
 15: + echo 'ENDING TIMING RUN AT 2020-06-25 11:03:54 AM'
 15: + RESULT=293
 15: + RESULT_NAME=bert
 15: RESULT,bert,18263,293,root,2020-06-25 10:59:01 AM
 15: + echo 'RESULT,bert,18263,293,root,2020-06-25 10:59:01 AM'
 15: + set +x
  9: + END_FMT='2020-06-25 11:03:54 AM'
 11: + END_FMT='2020-06-25 11:03:54 AM'
 10: + END_FMT='2020-06-25 11:03:54 AM'
 11: ENDING TIMING RUN AT 2020-06-25 11:03:54 AM
 11: + echo 'ENDING TIMING RUN AT 2020-06-25 11:03:54 AM'
  9: ENDING TIMING RUN AT 2020-06-25 11:03:54 AM
  9: + echo 'ENDING TIMING RUN AT 2020-06-25 11:03:54 AM'
 10: ENDING TIMING RUN AT 2020-06-25 11:03:54 AM
 10: + echo 'ENDING TIMING RUN AT 2020-06-25 11:03:54 AM'
 10: + RESULT=293
 10: + RESULT_NAME=bert
  9: + RESULT=293
  9: + RESULT_NAME=bert
  9: + echo 'RESULT,bert,18263,293,root,2020-06-25 10:59:01 AM'
 11: + RESULT=293
 11: + RESULT_NAME=bert
 11: + echo 'RESULT,bert,18263,293,root,2020-06-25 10:59:01 AM'
 12: + END_FMT='2020-06-25 11:03:54 AM'
  9: RESULT,bert,18263,293,root,2020-06-25 10:59:01 AM
  9: + set +x
 10: RESULT,bert,18263,293,root,2020-06-25 10:59:01 AM
 10: + echo 'RESULT,bert,18263,293,root,2020-06-25 10:59:01 AM'
 10: + set +x
 11: RESULT,bert,18263,293,root,2020-06-25 10:59:01 AM
 11: + set +x
 12: ENDING TIMING RUN AT 2020-06-25 11:03:54 AM
 12: + echo 'ENDING TIMING RUN AT 2020-06-25 11:03:54 AM'
 12: + RESULT=293
 12: + RESULT_NAME=bert
 12: + echo 'RESULT,bert,18263,293,root,2020-06-25 10:59:01 AM'
 12: RESULT,bert,18263,293,root,2020-06-25 10:59:01 AM
 12: + set +x
161: ++ date +%s
162: ++ date +%s
166: ++ date +%s
167: ++ date +%s
164: ++ date +%s
163: ++ date +%s
160: ++ date +%s
172: ++ date +%s
174: ++ date +%s
175: ++ date +%s
168: ++ date +%s
171: ++ date +%s
173: ++ date +%s
169: ++ date +%s
161: + END=1593108234
166: + END=1593108234
162: + END=1593108234
167: + END=1593108234
163: + END=1593108234
166: ++ date '+%Y-%m-%d %r'
164: + END=1593108234
161: ++ date '+%Y-%m-%d %r'
162: ++ date '+%Y-%m-%d %r'
167: ++ date '+%Y-%m-%d %r'
168: + END=1593108234
169: + END=1593108234
173: + END=1593108234
160: + END=1593108234
172: + END=1593108234
171: + END=1593108234
175: + END=1593108234
163: ++ date '+%Y-%m-%d %r'
164: ++ date '+%Y-%m-%d %r'
174: + END=1593108234
169: ++ date '+%Y-%m-%d %r'
175: ++ date '+%Y-%m-%d %r'
168: ++ date '+%Y-%m-%d %r'
172: ++ date '+%Y-%m-%d %r'
166: + END_FMT='2020-06-25 11:03:54 AM'
160: ++ date '+%Y-%m-%d %r'
171: ++ date '+%Y-%m-%d %r'
173: ++ date '+%Y-%m-%d %r'
166: + echo 'ENDING TIMING RUN AT 2020-06-25 11:03:54 AM'
166: ENDING TIMING RUN AT 2020-06-25 11:03:54 AM
166: + RESULT=294
166: + RESULT_NAME=bert
174: ++ date '+%Y-%m-%d %r'
166: + echo 'RESULT,bert,18263,294,root,2020-06-25 10:59:00 AM'
166: RESULT,bert,18263,294,root,2020-06-25 10:59:00 AM
166: + set +x
161: + END_FMT='2020-06-25 11:03:54 AM'
161: + echo 'ENDING TIMING RUN AT 2020-06-25 11:03:54 AM'
161: ENDING TIMING RUN AT 2020-06-25 11:03:54 AM
161: + RESULT=294
161: + RESULT_NAME=bert
161: RESULT,bert,18263,294,root,2020-06-25 10:59:00 AM
161: + echo 'RESULT,bert,18263,294,root,2020-06-25 10:59:00 AM'
161: + set +x
167: + END_FMT='2020-06-25 11:03:54 AM'
162: + END_FMT='2020-06-25 11:03:54 AM'
162: + echo 'ENDING TIMING RUN AT 2020-06-25 11:03:54 AM'
167: + echo 'ENDING TIMING RUN AT 2020-06-25 11:03:54 AM'
162: ENDING TIMING RUN AT 2020-06-25 11:03:54 AM
167: ENDING TIMING RUN AT 2020-06-25 11:03:54 AM
162: + RESULT=294
162: + RESULT_NAME=bert
167: + RESULT=294
167: + RESULT_NAME=bert
162: + echo 'RESULT,bert,18263,294,root,2020-06-25 10:59:00 AM'
162: RESULT,bert,18263,294,root,2020-06-25 10:59:00 AM
162: + set +x
167: RESULT,bert,18263,294,root,2020-06-25 10:59:00 AM
167: + echo 'RESULT,bert,18263,294,root,2020-06-25 10:59:00 AM'
167: + set +x
163: + END_FMT='2020-06-25 11:03:54 AM'
164: + END_FMT='2020-06-25 11:03:54 AM'
163: + echo 'ENDING TIMING RUN AT 2020-06-25 11:03:54 AM'
163: ENDING TIMING RUN AT 2020-06-25 11:03:54 AM
164: + echo 'ENDING TIMING RUN AT 2020-06-25 11:03:54 AM'
163: + RESULT=294
164: ENDING TIMING RUN AT 2020-06-25 11:03:54 AM
169: + END_FMT='2020-06-25 11:03:54 AM'
163: + RESULT_NAME=bert
164: + RESULT=294
164: + RESULT_NAME=bert
163: RESULT,bert,18263,294,root,2020-06-25 10:59:00 AM
163: + echo 'RESULT,bert,18263,294,root,2020-06-25 10:59:00 AM'
163: + set +x
169: ENDING TIMING RUN AT 2020-06-25 11:03:54 AM
169: + echo 'ENDING TIMING RUN AT 2020-06-25 11:03:54 AM'
164: RESULT,bert,18263,294,root,2020-06-25 10:59:00 AM
164: + echo 'RESULT,bert,18263,294,root,2020-06-25 10:59:00 AM'
164: + set +x
169: + RESULT=294
169: + RESULT_NAME=bert
169: RESULT,bert,18263,294,root,2020-06-25 10:59:00 AM
169: + echo 'RESULT,bert,18263,294,root,2020-06-25 10:59:00 AM'
169: + set +x
175: + END_FMT='2020-06-25 11:03:54 AM'
175: + echo 'ENDING TIMING RUN AT 2020-06-25 11:03:54 AM'
175: ENDING TIMING RUN AT 2020-06-25 11:03:54 AM
175: + RESULT=294
175: + RESULT_NAME=bert
175: RESULT,bert,18263,294,root,2020-06-25 10:59:00 AM
175: + echo 'RESULT,bert,18263,294,root,2020-06-25 10:59:00 AM'
175: + set +x
171: + END_FMT='2020-06-25 11:03:54 AM'
173: + END_FMT='2020-06-25 11:03:54 AM'
173: + echo 'ENDING TIMING RUN AT 2020-06-25 11:03:54 AM'
160: + END_FMT='2020-06-25 11:03:54 AM'
168: + END_FMT='2020-06-25 11:03:54 AM'
171: ENDING TIMING RUN AT 2020-06-25 11:03:54 AM
171: + echo 'ENDING TIMING RUN AT 2020-06-25 11:03:54 AM'
171: + RESULT=294
171: + RESULT_NAME=bert
172: + END_FMT='2020-06-25 11:03:54 AM'
172: + echo 'ENDING TIMING RUN AT 2020-06-25 11:03:54 AM'
173: ENDING TIMING RUN AT 2020-06-25 11:03:54 AM
173: RESULT,bert,18263,294,root,2020-06-25 10:59:00 AM
173: + RESULT=294
173: + RESULT_NAME=bert
173: + echo 'RESULT,bert,18263,294,root,2020-06-25 10:59:00 AM'
173: + set +x
174: + END_FMT='2020-06-25 11:03:54 AM'
174: + echo 'ENDING TIMING RUN AT 2020-06-25 11:03:54 AM'
160: ENDING TIMING RUN AT 2020-06-25 11:03:54 AM
160: + echo 'ENDING TIMING RUN AT 2020-06-25 11:03:54 AM'
160: + RESULT=294
168: ENDING TIMING RUN AT 2020-06-25 11:03:54 AM
168: + echo 'ENDING TIMING RUN AT 2020-06-25 11:03:54 AM'
168: + RESULT=294
168: + RESULT_NAME=bert
171: RESULT,bert,18263,294,root,2020-06-25 10:59:00 AM
171: + echo 'RESULT,bert,18263,294,root,2020-06-25 10:59:00 AM'
171: + set +x
172: ENDING TIMING RUN AT 2020-06-25 11:03:54 AM
172: RESULT,bert,18263,294,root,2020-06-25 10:59:00 AM
172: + RESULT=294
172: + RESULT_NAME=bert
172: + echo 'RESULT,bert,18263,294,root,2020-06-25 10:59:00 AM'
172: + set +x
174: ENDING TIMING RUN AT 2020-06-25 11:03:54 AM
174: RESULT,bert,18263,294,root,2020-06-25 10:59:00 AM
174: + RESULT=294
174: + RESULT_NAME=bert
174: + echo 'RESULT,bert,18263,294,root,2020-06-25 10:59:00 AM'
174: + set +x
160: RESULT,bert,18263,294,root,2020-06-25 10:59:00 AM
160: + RESULT_NAME=bert
160: + echo 'RESULT,bert,18263,294,root,2020-06-25 10:59:00 AM'
160: + set +x
168: RESULT,bert,18263,294,root,2020-06-25 10:59:00 AM
168: + echo 'RESULT,bert,18263,294,root,2020-06-25 10:59:00 AM'
168: + set +x
227: ++ date +%s
226: ++ date +%s
225: ++ date +%s
229: ++ date +%s
230: ++ date +%s
232: ++ date +%s
237: ++ date +%s
238: ++ date +%s
239: ++ date +%s
233: ++ date +%s
234: ++ date +%s
224: ++ date +%s
227: + END=1593108235
236: ++ date +%s
226: + END=1593108235
227: ++ date '+%Y-%m-%d %r'
231: ++ date +%s
225: + END=1593108235
226: ++ date '+%Y-%m-%d %r'
229: + END=1593108235
230: + END=1593108235
239: + END=1593108235
232: + END=1593108235
233: + END=1593108235
238: + END=1593108235
225: ++ date '+%Y-%m-%d %r'
224: + END=1593108235
234: + END=1593108235
229: ++ date '+%Y-%m-%d %r'
232: ++ date '+%Y-%m-%d %r'
233: ++ date '+%Y-%m-%d %r'
237: + END=1593108235
230: ++ date '+%Y-%m-%d %r'
238: ++ date '+%Y-%m-%d %r'
224: ++ date '+%Y-%m-%d %r'
231: + END=1593108235
239: ++ date '+%Y-%m-%d %r'
227: + END_FMT='2020-06-25 11:03:55 AM'
227: + echo 'ENDING TIMING RUN AT 2020-06-25 11:03:55 AM'
234: ++ date '+%Y-%m-%d %r'
227: ENDING TIMING RUN AT 2020-06-25 11:03:55 AM
236: + END=1593108235
227: + RESULT=295
227: + RESULT_NAME=bert
227: RESULT,bert,18263,295,root,2020-06-25 10:59:00 AM
227: + echo 'RESULT,bert,18263,295,root,2020-06-25 10:59:00 AM'
227: + set +x
226: + END_FMT='2020-06-25 11:03:55 AM'
226: ENDING TIMING RUN AT 2020-06-25 11:03:55 AM
226: + echo 'ENDING TIMING RUN AT 2020-06-25 11:03:55 AM'
226: + RESULT=295
226: + RESULT_NAME=bert
226: RESULT,bert,18263,295,root,2020-06-25 10:59:00 AM
226: + echo 'RESULT,bert,18263,295,root,2020-06-25 10:59:00 AM'
226: + set +x
237: ++ date '+%Y-%m-%d %r'
225: + END_FMT='2020-06-25 11:03:55 AM'
231: ++ date '+%Y-%m-%d %r'
225: ENDING TIMING RUN AT 2020-06-25 11:03:55 AM
225: + echo 'ENDING TIMING RUN AT 2020-06-25 11:03:55 AM'
225: + RESULT=295
225: + RESULT_NAME=bert
225: + echo 'RESULT,bert,18263,295,root,2020-06-25 10:59:00 AM'
225: RESULT,bert,18263,295,root,2020-06-25 10:59:00 AM
225: + set +x
236: ++ date '+%Y-%m-%d %r'
229: + END_FMT='2020-06-25 11:03:55 AM'
232: + END_FMT='2020-06-25 11:03:55 AM'
229: ENDING TIMING RUN AT 2020-06-25 11:03:55 AM
229: + echo 'ENDING TIMING RUN AT 2020-06-25 11:03:55 AM'
232: ENDING TIMING RUN AT 2020-06-25 11:03:55 AM
232: + echo 'ENDING TIMING RUN AT 2020-06-25 11:03:55 AM'
229: + RESULT=295
229: + RESULT_NAME=bert
232: + RESULT=295
232: + RESULT_NAME=bert
233: + END_FMT='2020-06-25 11:03:55 AM'
229: RESULT,bert,18263,295,root,2020-06-25 10:59:00 AM
229: + echo 'RESULT,bert,18263,295,root,2020-06-25 10:59:00 AM'
229: + set +x
232: RESULT,bert,18263,295,root,2020-06-25 10:59:00 AM
232: + echo 'RESULT,bert,18263,295,root,2020-06-25 10:59:00 AM'
232: + set +x
230: + END_FMT='2020-06-25 11:03:55 AM'
233: ENDING TIMING RUN AT 2020-06-25 11:03:55 AM
233: + echo 'ENDING TIMING RUN AT 2020-06-25 11:03:55 AM'
233: + RESULT=295
224: + END_FMT='2020-06-25 11:03:55 AM'
233: + RESULT_NAME=bert
238: + END_FMT='2020-06-25 11:03:55 AM'
230: ENDING TIMING RUN AT 2020-06-25 11:03:55 AM
230: + echo 'ENDING TIMING RUN AT 2020-06-25 11:03:55 AM'
233: RESULT,bert,18263,295,root,2020-06-25 10:59:00 AM
233: + echo 'RESULT,bert,18263,295,root,2020-06-25 10:59:00 AM'
233: + set +x
238: ENDING TIMING RUN AT 2020-06-25 11:03:55 AM
238: + echo 'ENDING TIMING RUN AT 2020-06-25 11:03:55 AM'
238: + RESULT=295
238: + RESULT_NAME=bert
224: ENDING TIMING RUN AT 2020-06-25 11:03:55 AM
224: + echo 'ENDING TIMING RUN AT 2020-06-25 11:03:55 AM'
224: + RESULT=295
224: + RESULT_NAME=bert
224: + echo 'RESULT,bert,18263,295,root,2020-06-25 10:59:00 AM'
230: + RESULT=295
230: + RESULT_NAME=bert
230: + echo 'RESULT,bert,18263,295,root,2020-06-25 10:59:00 AM'
230: + set +x
224: RESULT,bert,18263,295,root,2020-06-25 10:59:00 AM
224: + set +x
230: RESULT,bert,18263,295,root,2020-06-25 10:59:00 AM
238: RESULT,bert,18263,295,root,2020-06-25 10:59:00 AM
238: + echo 'RESULT,bert,18263,295,root,2020-06-25 10:59:00 AM'
238: + set +x
239: + END_FMT='2020-06-25 11:03:55 AM'
234: + END_FMT='2020-06-25 11:03:55 AM'
237: + END_FMT='2020-06-25 11:03:55 AM'
234: ENDING TIMING RUN AT 2020-06-25 11:03:55 AM
234: + echo 'ENDING TIMING RUN AT 2020-06-25 11:03:55 AM'
239: ENDING TIMING RUN AT 2020-06-25 11:03:55 AM
239: + echo 'ENDING TIMING RUN AT 2020-06-25 11:03:55 AM'
234: + RESULT=295
234: + RESULT_NAME=bert
237: + echo 'ENDING TIMING RUN AT 2020-06-25 11:03:55 AM'
239: + RESULT=295
239: + RESULT_NAME=bert
237: ENDING TIMING RUN AT 2020-06-25 11:03:55 AM
231: + END_FMT='2020-06-25 11:03:55 AM'
234: RESULT,bert,18263,295,root,2020-06-25 10:59:00 AM
234: + echo 'RESULT,bert,18263,295,root,2020-06-25 10:59:00 AM'
234: + set +x
237: + RESULT=295
237: + RESULT_NAME=bert
239: RESULT,bert,18263,295,root,2020-06-25 10:59:00 AM
239: + echo 'RESULT,bert,18263,295,root,2020-06-25 10:59:00 AM'
239: + set +x
231: ENDING TIMING RUN AT 2020-06-25 11:03:55 AM
231: + echo 'ENDING TIMING RUN AT 2020-06-25 11:03:55 AM'
237: RESULT,bert,18263,295,root,2020-06-25 10:59:00 AM
237: + echo 'RESULT,bert,18263,295,root,2020-06-25 10:59:00 AM'
237: + set +x
231: + RESULT=295
231: + RESULT_NAME=bert
231: RESULT,bert,18263,295,root,2020-06-25 10:59:00 AM
231: + echo 'RESULT,bert,18263,295,root,2020-06-25 10:59:00 AM'
231: + set +x
236: ENDING TIMING RUN AT 2020-06-25 11:03:55 AM
236: + END_FMT='2020-06-25 11:03:55 AM'
236: + echo 'ENDING TIMING RUN AT 2020-06-25 11:03:55 AM'
236: + RESULT=295
236: + RESULT_NAME=bert
236: + echo 'RESULT,bert,18263,295,root,2020-06-25 10:59:00 AM'
236: + set +x
236: RESULT,bert,18263,295,root,2020-06-25 10:59:00 AM
263: ++ date +%s
267: ++ date +%s
261: ++ date +%s
265: ++ date +%s
269: ++ date +%s
260: ++ date +%s
266: ++ date +%s
259: ++ date +%s
257: ++ date +%s
264: ++ date +%s
258: ++ date +%s
262: ++ date +%s
270: ++ date +%s
271: ++ date +%s
269: + END=1593108235
425: ++ date +%s
260: + END=1593108235
267: + END=1593108235
259: + END=1593108235
265: + END=1593108235
266: + END=1593108235
263: + END=1593108235
264: + END=1593108235
257: + END=1593108235
261: + END=1593108235
265: ++ date '+%Y-%m-%d %r'
269: ++ date '+%Y-%m-%d %r'
258: + END=1593108235
262: + END=1593108235
266: ++ date '+%Y-%m-%d %r'
270: + END=1593108235
259: ++ date '+%Y-%m-%d %r'
264: ++ date '+%Y-%m-%d %r'
263: ++ date '+%Y-%m-%d %r'
257: ++ date '+%Y-%m-%d %r'
260: ++ date '+%Y-%m-%d %r'
267: ++ date '+%Y-%m-%d %r'
261: ++ date '+%Y-%m-%d %r'
271: + END=1593108235
262: ++ date '+%Y-%m-%d %r'
258: ++ date '+%Y-%m-%d %r'
270: ++ date '+%Y-%m-%d %r'
425: + END=1593108235
271: ++ date '+%Y-%m-%d %r'
425: ++ date '+%Y-%m-%d %r'
265: ENDING TIMING RUN AT 2020-06-25 11:03:55 AM
423: ++ date +%s
265: + END_FMT='2020-06-25 11:03:55 AM'
265: + echo 'ENDING TIMING RUN AT 2020-06-25 11:03:55 AM'
265: + RESULT=295
265: + RESULT_NAME=bert
265: + echo 'RESULT,bert,18263,295,root,2020-06-25 10:59:00 AM'
265: + set +x
266: ENDING TIMING RUN AT 2020-06-25 11:03:55 AM
266: RESULT,bert,18263,295,root,2020-06-25 10:59:00 AM
266: + END_FMT='2020-06-25 11:03:55 AM'
266: + echo 'ENDING TIMING RUN AT 2020-06-25 11:03:55 AM'
266: + RESULT=295
266: + RESULT_NAME=bert
266: + echo 'RESULT,bert,18263,295,root,2020-06-25 10:59:00 AM'
266: + set +x
259: + END_FMT='2020-06-25 11:03:55 AM'
259: + echo 'ENDING TIMING RUN AT 2020-06-25 11:03:55 AM'
264: + END_FMT='2020-06-25 11:03:55 AM'
265: RESULT,bert,18263,295,root,2020-06-25 10:59:00 AM
259: ENDING TIMING RUN AT 2020-06-25 11:03:55 AM
259: + RESULT=295
259: + RESULT_NAME=bert
259: + echo 'RESULT,bert,18263,295,root,2020-06-25 10:59:00 AM'
259: + set +x
264: ENDING TIMING RUN AT 2020-06-25 11:03:55 AM
264: RESULT,bert,18263,295,root,2020-06-25 10:59:00 AM
264: + echo 'ENDING TIMING RUN AT 2020-06-25 11:03:55 AM'
264: + RESULT=295
264: + RESULT_NAME=bert
264: + echo 'RESULT,bert,18263,295,root,2020-06-25 10:59:00 AM'
264: + set +x
259: RESULT,bert,18263,295,root,2020-06-25 10:59:00 AM
263: + END_FMT='2020-06-25 11:03:55 AM'
269: + END_FMT='2020-06-25 11:03:55 AM'
263: ENDING TIMING RUN AT 2020-06-25 11:03:55 AM
263: + echo 'ENDING TIMING RUN AT 2020-06-25 11:03:55 AM'
263: + RESULT=295
263: RESULT,bert,18263,295,root,2020-06-25 10:59:00 AM
263: + RESULT_NAME=bert
263: + echo 'RESULT,bert,18263,295,root,2020-06-25 10:59:00 AM'
263: + set +x
269: ENDING TIMING RUN AT 2020-06-25 11:03:55 AM
269: + echo 'ENDING TIMING RUN AT 2020-06-25 11:03:55 AM'
269: + RESULT=295
269: + RESULT_NAME=bert
257: + END_FMT='2020-06-25 11:03:55 AM'
269: RESULT,bert,18263,295,root,2020-06-25 10:59:00 AM
269: + echo 'RESULT,bert,18263,295,root,2020-06-25 10:59:00 AM'
269: + set +x
257: ENDING TIMING RUN AT 2020-06-25 11:03:55 AM
257: + echo 'ENDING TIMING RUN AT 2020-06-25 11:03:55 AM'
257: + RESULT=295
257: + RESULT_NAME=bert
257: RESULT,bert,18263,295,root,2020-06-25 10:59:00 AM
425: + END_FMT='2020-06-25 11:03:55 AM'
257: + echo 'RESULT,bert,18263,295,root,2020-06-25 10:59:00 AM'
257: + set +x
261: + END_FMT='2020-06-25 11:03:55 AM'
425: ENDING TIMING RUN AT 2020-06-25 11:03:55 AM
261: + echo 'ENDING TIMING RUN AT 2020-06-25 11:03:55 AM'
261: ENDING TIMING RUN AT 2020-06-25 11:03:55 AM
262: + END_FMT='2020-06-25 11:03:55 AM'
425: + echo 'ENDING TIMING RUN AT 2020-06-25 11:03:55 AM'
261: + RESULT=295
261: + RESULT_NAME=bert
262: ENDING TIMING RUN AT 2020-06-25 11:03:55 AM
425: + RESULT=294
262: + echo 'ENDING TIMING RUN AT 2020-06-25 11:03:55 AM'
261: RESULT,bert,18263,295,root,2020-06-25 10:59:00 AM
425: + RESULT_NAME=bert
425: RESULT,bert,18263,294,root,2020-06-25 10:59:01 AM
261: + echo 'RESULT,bert,18263,295,root,2020-06-25 10:59:00 AM'
261: + set +x
425: + echo 'RESULT,bert,18263,294,root,2020-06-25 10:59:01 AM'
425: + set +x
426: ++ date +%s
258: + END_FMT='2020-06-25 11:03:55 AM'
262: + RESULT=295
262: + RESULT_NAME=bert
270: + END_FMT='2020-06-25 11:03:55 AM'
262: RESULT,bert,18263,295,root,2020-06-25 10:59:00 AM
420: ++ date +%s
262: + echo 'RESULT,bert,18263,295,root,2020-06-25 10:59:00 AM'
258: ENDING TIMING RUN AT 2020-06-25 11:03:55 AM
258: + echo 'ENDING TIMING RUN AT 2020-06-25 11:03:55 AM'
430: ++ date +%s
270: ENDING TIMING RUN AT 2020-06-25 11:03:55 AM
262: + set +x
270: + echo 'ENDING TIMING RUN AT 2020-06-25 11:03:55 AM'
258: + RESULT=295
258: + RESULT_NAME=bert
270: + RESULT=295
270: + RESULT_NAME=bert
258: RESULT,bert,18263,295,root,2020-06-25 10:59:00 AM
419: ++ date +%s
258: + echo 'RESULT,bert,18263,295,root,2020-06-25 10:59:00 AM'
270: RESULT,bert,18263,295,root,2020-06-25 10:59:00 AM
270: + echo 'RESULT,bert,18263,295,root,2020-06-25 10:59:00 AM'
270: + set +x
271: + END_FMT='2020-06-25 11:03:55 AM'
258: + set +x
271: ENDING TIMING RUN AT 2020-06-25 11:03:55 AM
271: + echo 'ENDING TIMING RUN AT 2020-06-25 11:03:55 AM'
271: + RESULT=295
271: + RESULT_NAME=bert
271: RESULT,bert,18263,295,root,2020-06-25 10:59:00 AM
421: ++ date +%s
271: + echo 'RESULT,bert,18263,295,root,2020-06-25 10:59:00 AM'
271: + set +x
260: + END_FMT='2020-06-25 11:03:55 AM'
267: + END_FMT='2020-06-25 11:03:55 AM'
427: ++ date +%s
260: + echo 'ENDING TIMING RUN AT 2020-06-25 11:03:55 AM'
424: ++ date +%s
260: ENDING TIMING RUN AT 2020-06-25 11:03:55 AM
267: + echo 'ENDING TIMING RUN AT 2020-06-25 11:03:55 AM'
267: ENDING TIMING RUN AT 2020-06-25 11:03:55 AM
429: ++ date +%s
417: ++ date +%s
428: ++ date +%s
260: + RESULT=295
416: ++ date +%s
267: + RESULT=295
422: ++ date +%s
260: + RESULT_NAME=bert
267: + RESULT_NAME=bert
260: RESULT,bert,18263,295,root,2020-06-25 10:59:00 AM
260: + echo 'RESULT,bert,18263,295,root,2020-06-25 10:59:00 AM'
267: RESULT,bert,18263,295,root,2020-06-25 10:59:00 AM
267: + echo 'RESULT,bert,18263,295,root,2020-06-25 10:59:00 AM'
267: + set +x
423: + END=1593108235
260: + set +x
423: ++ date '+%Y-%m-%d %r'
420: + END=1593108235
419: + END=1593108235
430: + END=1593108235
426: + END=1593108235
427: + END=1593108235
417: + END=1593108235
429: + END=1593108235
421: + END=1593108235
424: + END=1593108235
428: + END=1593108235
419: ++ date '+%Y-%m-%d %r'
423: + END_FMT='2020-06-25 11:03:55 AM'
426: ++ date '+%Y-%m-%d %r'
430: ++ date '+%Y-%m-%d %r'
420: ++ date '+%Y-%m-%d %r'
423: + echo 'ENDING TIMING RUN AT 2020-06-25 11:03:55 AM'
423: ENDING TIMING RUN AT 2020-06-25 11:03:55 AM
423: + RESULT=294
423: + RESULT_NAME=bert
423: + echo 'RESULT,bert,18263,294,root,2020-06-25 10:59:01 AM'
423: RESULT,bert,18263,294,root,2020-06-25 10:59:01 AM
423: + set +x
417: ++ date '+%Y-%m-%d %r'
427: ++ date '+%Y-%m-%d %r'
422: + END=1593108235
416: + END=1593108235
429: ++ date '+%Y-%m-%d %r'
424: ++ date '+%Y-%m-%d %r'
421: ++ date '+%Y-%m-%d %r'
428: ++ date '+%Y-%m-%d %r'
422: ++ date '+%Y-%m-%d %r'
416: ++ date '+%Y-%m-%d %r'
419: + END_FMT='2020-06-25 11:03:55 AM'
419: + echo 'ENDING TIMING RUN AT 2020-06-25 11:03:55 AM'
419: ENDING TIMING RUN AT 2020-06-25 11:03:55 AM
419: + RESULT=294
419: + RESULT_NAME=bert
419: + echo 'RESULT,bert,18263,294,root,2020-06-25 10:59:01 AM'
419: RESULT,bert,18263,294,root,2020-06-25 10:59:01 AM
419: + set +x
426: + END_FMT='2020-06-25 11:03:55 AM'
426: + echo 'ENDING TIMING RUN AT 2020-06-25 11:03:55 AM'
426: ENDING TIMING RUN AT 2020-06-25 11:03:55 AM
426: + RESULT=294
426: + RESULT_NAME=bert
426: RESULT,bert,18263,294,root,2020-06-25 10:59:01 AM
426: + echo 'RESULT,bert,18263,294,root,2020-06-25 10:59:01 AM'
426: + set +x
430: + END_FMT='2020-06-25 11:03:55 AM'
430: + echo 'ENDING TIMING RUN AT 2020-06-25 11:03:55 AM'
430: ENDING TIMING RUN AT 2020-06-25 11:03:55 AM
430: + RESULT=294
430: + RESULT_NAME=bert
430: + echo 'RESULT,bert,18263,294,root,2020-06-25 10:59:01 AM'
430: RESULT,bert,18263,294,root,2020-06-25 10:59:01 AM
430: + set +x
420: + END_FMT='2020-06-25 11:03:55 AM'
420: + echo 'ENDING TIMING RUN AT 2020-06-25 11:03:55 AM'
417: + END_FMT='2020-06-25 11:03:55 AM'
420: ENDING TIMING RUN AT 2020-06-25 11:03:55 AM
420: + RESULT=294
420: + RESULT_NAME=bert
417: + echo 'ENDING TIMING RUN AT 2020-06-25 11:03:55 AM'
417: ENDING TIMING RUN AT 2020-06-25 11:03:55 AM
420: RESULT,bert,18263,294,root,2020-06-25 10:59:01 AM
420: + echo 'RESULT,bert,18263,294,root,2020-06-25 10:59:01 AM'
420: + set +x
427: + END_FMT='2020-06-25 11:03:55 AM'
417: + RESULT=294
417: + RESULT_NAME=bert
427: ENDING TIMING RUN AT 2020-06-25 11:03:55 AM
427: + echo 'ENDING TIMING RUN AT 2020-06-25 11:03:55 AM'
417: RESULT,bert,18263,294,root,2020-06-25 10:59:01 AM
417: + echo 'RESULT,bert,18263,294,root,2020-06-25 10:59:01 AM'
417: + set +x
427: + RESULT=294
427: + RESULT_NAME=bert
427: RESULT,bert,18263,294,root,2020-06-25 10:59:01 AM
427: + echo 'RESULT,bert,18263,294,root,2020-06-25 10:59:01 AM'
427: + set +x
429: + END_FMT='2020-06-25 11:03:55 AM'
424: + END_FMT='2020-06-25 11:03:55 AM'
429: + echo 'ENDING TIMING RUN AT 2020-06-25 11:03:55 AM'
424: ENDING TIMING RUN AT 2020-06-25 11:03:55 AM
424: + echo 'ENDING TIMING RUN AT 2020-06-25 11:03:55 AM'
429: ENDING TIMING RUN AT 2020-06-25 11:03:55 AM
424: + RESULT=294
424: + RESULT_NAME=bert
429: + RESULT=294
429: + RESULT_NAME=bert
429: RESULT,bert,18263,294,root,2020-06-25 10:59:01 AM
429: + echo 'RESULT,bert,18263,294,root,2020-06-25 10:59:01 AM'
429: + set +x
424: RESULT,bert,18263,294,root,2020-06-25 10:59:01 AM
424: + echo 'RESULT,bert,18263,294,root,2020-06-25 10:59:01 AM'
424: + set +x
421: + END_FMT='2020-06-25 11:03:55 AM'
421: ENDING TIMING RUN AT 2020-06-25 11:03:55 AM
421: + echo 'ENDING TIMING RUN AT 2020-06-25 11:03:55 AM'
421: + RESULT=294
421: + RESULT_NAME=bert
421: RESULT,bert,18263,294,root,2020-06-25 10:59:01 AM
421: + echo 'RESULT,bert,18263,294,root,2020-06-25 10:59:01 AM'
421: + set +x
416: + END_FMT='2020-06-25 11:03:55 AM'
428: + END_FMT='2020-06-25 11:03:55 AM'
422: + END_FMT='2020-06-25 11:03:55 AM'
416: + echo 'ENDING TIMING RUN AT 2020-06-25 11:03:55 AM'
428: ENDING TIMING RUN AT 2020-06-25 11:03:55 AM
428: + echo 'ENDING TIMING RUN AT 2020-06-25 11:03:55 AM'
416: ENDING TIMING RUN AT 2020-06-25 11:03:55 AM
422: ENDING TIMING RUN AT 2020-06-25 11:03:55 AM
422: + echo 'ENDING TIMING RUN AT 2020-06-25 11:03:55 AM'
422: + RESULT=294
416: + RESULT=294
416: + RESULT_NAME=bert
422: + RESULT_NAME=bert
428: + RESULT=294
428: + RESULT_NAME=bert
428: + echo 'RESULT,bert,18263,294,root,2020-06-25 10:59:01 AM'
416: RESULT,bert,18263,294,root,2020-06-25 10:59:01 AM
416: + echo 'RESULT,bert,18263,294,root,2020-06-25 10:59:01 AM'
416: + set +x
422: RESULT,bert,18263,294,root,2020-06-25 10:59:01 AM
422: + echo 'RESULT,bert,18263,294,root,2020-06-25 10:59:01 AM'
422: + set +x
428: RESULT,bert,18263,294,root,2020-06-25 10:59:01 AM
428: + set +x
127: ++ date +%s
127: + END=1593108235
127: ++ date '+%Y-%m-%d %r'
117: ++ date +%s
118: ++ date +%s
122: ++ date +%s
115: ++ date +%s
116: ++ date +%s
127: + END_FMT='2020-06-25 11:03:55 AM'
127: ENDING TIMING RUN AT 2020-06-25 11:03:55 AM
127: + echo 'ENDING TIMING RUN AT 2020-06-25 11:03:55 AM'
127: + RESULT=295
127: RESULT,bert,18263,295,root,2020-06-25 10:59:00 AM
127: + RESULT_NAME=bert
127: + echo 'RESULT,bert,18263,295,root,2020-06-25 10:59:00 AM'
127: + set +x
121: ++ date +%s
112: ++ date +%s
125: ++ date +%s
114: ++ date +%s
113: ++ date +%s
120: ++ date +%s
119: ++ date +%s
115: + END=1593108235
116: + END=1593108235
118: + END=1593108235
117: + END=1593108235
122: + END=1593108235
121: + END=1593108235
125: + END=1593108235
118: ++ date '+%Y-%m-%d %r'
120: + END=1593108235
114: + END=1593108235
112: + END=1593108235
115: ++ date '+%Y-%m-%d %r'
116: ++ date '+%Y-%m-%d %r'
122: ++ date '+%Y-%m-%d %r'
113: + END=1593108235
117: ++ date '+%Y-%m-%d %r'
126: ++ date +%s
125: ++ date '+%Y-%m-%d %r'
119: + END=1593108235
120: ++ date '+%Y-%m-%d %r'
121: ++ date '+%Y-%m-%d %r'
114: ++ date '+%Y-%m-%d %r'
112: ++ date '+%Y-%m-%d %r'
113: ++ date '+%Y-%m-%d %r'
118: ENDING TIMING RUN AT 2020-06-25 11:03:55 AM
118: + END_FMT='2020-06-25 11:03:55 AM'
118: + echo 'ENDING TIMING RUN AT 2020-06-25 11:03:55 AM'
118: + RESULT=295
118: + RESULT_NAME=bert
118: + echo 'RESULT,bert,18263,295,root,2020-06-25 10:59:00 AM'
119: ++ date '+%Y-%m-%d %r'
118: RESULT,bert,18263,295,root,2020-06-25 10:59:00 AM
118: + set +x
115: + END_FMT='2020-06-25 11:03:55 AM'
116: + END_FMT='2020-06-25 11:03:55 AM'
115: + echo 'ENDING TIMING RUN AT 2020-06-25 11:03:55 AM'
115: ENDING TIMING RUN AT 2020-06-25 11:03:55 AM
445: ++ date +%s
116: ENDING TIMING RUN AT 2020-06-25 11:03:55 AM
435: ++ date +%s
116: + echo 'ENDING TIMING RUN AT 2020-06-25 11:03:55 AM'
434: ++ date +%s
122: + END_FMT='2020-06-25 11:03:55 AM'
438: ++ date +%s
115: + RESULT=295
447: ++ date +%s
122: ENDING TIMING RUN AT 2020-06-25 11:03:55 AM
116: + RESULT=295
116: + RESULT_NAME=bert
115: + RESULT_NAME=bert
115: RESULT,bert,18263,295,root,2020-06-25 10:59:00 AM
117: + END_FMT='2020-06-25 11:03:55 AM'
122: + echo 'ENDING TIMING RUN AT 2020-06-25 11:03:55 AM'
116: RESULT,bert,18263,295,root,2020-06-25 10:59:00 AM
115: + echo 'RESULT,bert,18263,295,root,2020-06-25 10:59:00 AM'
115: + set +x
116: + echo 'RESULT,bert,18263,295,root,2020-06-25 10:59:00 AM'
116: + set +x
122: + RESULT=295
122: + RESULT_NAME=bert
117: ENDING TIMING RUN AT 2020-06-25 11:03:55 AM
117: + echo 'ENDING TIMING RUN AT 2020-06-25 11:03:55 AM'
117: + RESULT=295
117: + RESULT_NAME=bert
122: RESULT,bert,18263,295,root,2020-06-25 10:59:00 AM
122: + echo 'RESULT,bert,18263,295,root,2020-06-25 10:59:00 AM'
122: + set +x
117: + echo 'RESULT,bert,18263,295,root,2020-06-25 10:59:00 AM'
117: RESULT,bert,18263,295,root,2020-06-25 10:59:00 AM
117: + set +x
436: ++ date +%s
443: ++ date +%s
441: ++ date +%s
125: ENDING TIMING RUN AT 2020-06-25 11:03:55 AM
125: RESULT,bert,18263,295,root,2020-06-25 10:59:00 AM
444: ++ date +%s
433: ++ date +%s
125: + END_FMT='2020-06-25 11:03:55 AM'
125: + echo 'ENDING TIMING RUN AT 2020-06-25 11:03:55 AM'
125: + RESULT=295
125: + RESULT_NAME=bert
125: + echo 'RESULT,bert,18263,295,root,2020-06-25 10:59:00 AM'
125: + set +x
440: ++ date +%s
121: ENDING TIMING RUN AT 2020-06-25 11:03:55 AM
126: + END=1593108235
446: ++ date +%s
121: + END_FMT='2020-06-25 11:03:55 AM'
439: ++ date +%s
121: + echo 'ENDING TIMING RUN AT 2020-06-25 11:03:55 AM'
432: ++ date +%s
121: RESULT,bert,18263,295,root,2020-06-25 10:59:00 AM
121: + RESULT=295
121: + RESULT_NAME=bert
121: + echo 'RESULT,bert,18263,295,root,2020-06-25 10:59:00 AM'
121: + set +x
114: + END_FMT='2020-06-25 11:03:55 AM'
112: + END_FMT='2020-06-25 11:03:55 AM'
113: + END_FMT='2020-06-25 11:03:55 AM'
114: + echo 'ENDING TIMING RUN AT 2020-06-25 11:03:55 AM'
120: + END_FMT='2020-06-25 11:03:55 AM'
114: ENDING TIMING RUN AT 2020-06-25 11:03:55 AM
114: + RESULT=295
114: + RESULT_NAME=bert
112: ENDING TIMING RUN AT 2020-06-25 11:03:55 AM
112: + echo 'ENDING TIMING RUN AT 2020-06-25 11:03:55 AM'
113: ENDING TIMING RUN AT 2020-06-25 11:03:55 AM
113: + echo 'ENDING TIMING RUN AT 2020-06-25 11:03:55 AM'
120: + echo 'ENDING TIMING RUN AT 2020-06-25 11:03:55 AM'
114: RESULT,bert,18263,295,root,2020-06-25 10:59:00 AM
114: + echo 'RESULT,bert,18263,295,root,2020-06-25 10:59:00 AM'
114: + set +x
120: ENDING TIMING RUN AT 2020-06-25 11:03:55 AM
120: + RESULT=295
120: + RESULT_NAME=bert
112: + RESULT=295
112: + RESULT_NAME=bert
113: + RESULT=295
113: + RESULT_NAME=bert
120: RESULT,bert,18263,295,root,2020-06-25 10:59:00 AM
120: + echo 'RESULT,bert,18263,295,root,2020-06-25 10:59:00 AM'
120: + set +x
113: RESULT,bert,18263,295,root,2020-06-25 10:59:00 AM
113: + echo 'RESULT,bert,18263,295,root,2020-06-25 10:59:00 AM'
112: RESULT,bert,18263,295,root,2020-06-25 10:59:00 AM
112: + echo 'RESULT,bert,18263,295,root,2020-06-25 10:59:00 AM'
112: + set +x
113: + set +x
119: + END_FMT='2020-06-25 11:03:55 AM'
119: ENDING TIMING RUN AT 2020-06-25 11:03:55 AM
119: + echo 'ENDING TIMING RUN AT 2020-06-25 11:03:55 AM'
119: + RESULT=295
119: + RESULT_NAME=bert
119: + echo 'RESULT,bert,18263,295,root,2020-06-25 10:59:00 AM'
119: RESULT,bert,18263,295,root,2020-06-25 10:59:00 AM
119: + set +x
126: ++ date '+%Y-%m-%d %r'
126: + END_FMT='2020-06-25 11:03:55 AM'
126: + echo 'ENDING TIMING RUN AT 2020-06-25 11:03:55 AM'
126: ENDING TIMING RUN AT 2020-06-25 11:03:55 AM
435: + END=1593108235
435: ++ date '+%Y-%m-%d %r'
445: + END=1593108235
445: ++ date '+%Y-%m-%d %r'
434: + END=1593108235
126: + RESULT=295
126: RESULT,bert,18263,295,root,2020-06-25 10:59:00 AM
438: + END=1593108235
126: + RESULT_NAME=bert
443: + END=1593108235
447: + END=1593108235
126: + echo 'RESULT,bert,18263,295,root,2020-06-25 10:59:00 AM'
126: + set +x
444: + END=1593108235
436: + END=1593108235
433: + END=1593108235
446: + END=1593108235
443: ++ date '+%Y-%m-%d %r'
439: + END=1593108235
440: + END=1593108235
447: ++ date '+%Y-%m-%d %r'
432: + END=1593108235
441: + END=1593108235
434: ++ date '+%Y-%m-%d %r'
438: ++ date '+%Y-%m-%d %r'
444: ++ date '+%Y-%m-%d %r'
436: ++ date '+%Y-%m-%d %r'
433: ++ date '+%Y-%m-%d %r'
435: + END_FMT='2020-06-25 11:03:55 AM'
435: + echo 'ENDING TIMING RUN AT 2020-06-25 11:03:55 AM'
440: ++ date '+%Y-%m-%d %r'
445: + END_FMT='2020-06-25 11:03:55 AM'
445: + echo 'ENDING TIMING RUN AT 2020-06-25 11:03:55 AM'
446: ++ date '+%Y-%m-%d %r'
435: ENDING TIMING RUN AT 2020-06-25 11:03:55 AM
435: + RESULT=295
435: + RESULT_NAME=bert
445: ENDING TIMING RUN AT 2020-06-25 11:03:55 AM
445: + RESULT=295
445: + RESULT_NAME=bert
432: ++ date '+%Y-%m-%d %r'
435: RESULT,bert,18263,295,root,2020-06-25 10:59:00 AM
435: + echo 'RESULT,bert,18263,295,root,2020-06-25 10:59:00 AM'
435: + set +x
439: ++ date '+%Y-%m-%d %r'
441: ++ date '+%Y-%m-%d %r'
445: RESULT,bert,18263,295,root,2020-06-25 10:59:00 AM
445: + echo 'RESULT,bert,18263,295,root,2020-06-25 10:59:00 AM'
445: + set +x
443: + END_FMT='2020-06-25 11:03:55 AM'
443: + echo 'ENDING TIMING RUN AT 2020-06-25 11:03:55 AM'
443: ENDING TIMING RUN AT 2020-06-25 11:03:55 AM
443: + RESULT=295
443: + RESULT_NAME=bert
443: RESULT,bert,18263,295,root,2020-06-25 10:59:00 AM
443: + echo 'RESULT,bert,18263,295,root,2020-06-25 10:59:00 AM'
443: + set +x
447: + END_FMT='2020-06-25 11:03:55 AM'
434: + END_FMT='2020-06-25 11:03:55 AM'
447: + echo 'ENDING TIMING RUN AT 2020-06-25 11:03:55 AM'
447: ENDING TIMING RUN AT 2020-06-25 11:03:55 AM
434: ENDING TIMING RUN AT 2020-06-25 11:03:55 AM
434: + echo 'ENDING TIMING RUN AT 2020-06-25 11:03:55 AM'
447: + RESULT=295
447: + RESULT_NAME=bert
434: + RESULT=295
447: RESULT,bert,18263,295,root,2020-06-25 10:59:00 AM
447: + echo 'RESULT,bert,18263,295,root,2020-06-25 10:59:00 AM'
447: + set +x
434: + RESULT_NAME=bert
434: + echo 'RESULT,bert,18263,295,root,2020-06-25 10:59:00 AM'
434: RESULT,bert,18263,295,root,2020-06-25 10:59:00 AM
434: + set +x
444: + END_FMT='2020-06-25 11:03:55 AM'
444: + echo 'ENDING TIMING RUN AT 2020-06-25 11:03:55 AM'
444: ENDING TIMING RUN AT 2020-06-25 11:03:55 AM
438: + END_FMT='2020-06-25 11:03:55 AM'
444: + RESULT=295
444: + RESULT_NAME=bert
438: ENDING TIMING RUN AT 2020-06-25 11:03:55 AM
438: + echo 'ENDING TIMING RUN AT 2020-06-25 11:03:55 AM'
444: RESULT,bert,18263,295,root,2020-06-25 10:59:00 AM
444: + echo 'RESULT,bert,18263,295,root,2020-06-25 10:59:00 AM'
444: + set +x
438: + RESULT=295
438: + RESULT_NAME=bert
436: + END_FMT='2020-06-25 11:03:55 AM'
438: RESULT,bert,18263,295,root,2020-06-25 10:59:00 AM
438: + echo 'RESULT,bert,18263,295,root,2020-06-25 10:59:00 AM'
438: + set +x
436: + echo 'ENDING TIMING RUN AT 2020-06-25 11:03:55 AM'
436: ENDING TIMING RUN AT 2020-06-25 11:03:55 AM
436: + RESULT=295
436: + RESULT_NAME=bert
436: RESULT,bert,18263,295,root,2020-06-25 10:59:00 AM
436: + echo 'RESULT,bert,18263,295,root,2020-06-25 10:59:00 AM'
436: + set +x
433: + END_FMT='2020-06-25 11:03:55 AM'
433: + echo 'ENDING TIMING RUN AT 2020-06-25 11:03:55 AM'
433: ENDING TIMING RUN AT 2020-06-25 11:03:55 AM
433: + RESULT=295
433: + RESULT_NAME=bert
433: + echo 'RESULT,bert,18263,295,root,2020-06-25 10:59:00 AM'
433: RESULT,bert,18263,295,root,2020-06-25 10:59:00 AM
433: + set +x
446: + END_FMT='2020-06-25 11:03:55 AM'
446: + echo 'ENDING TIMING RUN AT 2020-06-25 11:03:55 AM'
446: ENDING TIMING RUN AT 2020-06-25 11:03:55 AM
440: + END_FMT='2020-06-25 11:03:55 AM'
446: + RESULT=295
446: + RESULT_NAME=bert
446: RESULT,bert,18263,295,root,2020-06-25 10:59:00 AM
446: + echo 'RESULT,bert,18263,295,root,2020-06-25 10:59:00 AM'
440: ENDING TIMING RUN AT 2020-06-25 11:03:55 AM
440: + echo 'ENDING TIMING RUN AT 2020-06-25 11:03:55 AM'
446: + set +x
432: + END_FMT='2020-06-25 11:03:55 AM'
439: + END_FMT='2020-06-25 11:03:55 AM'
440: + RESULT=295
440: + RESULT_NAME=bert
441: + END_FMT='2020-06-25 11:03:55 AM'
440: RESULT,bert,18263,295,root,2020-06-25 10:59:00 AM
440: + echo 'RESULT,bert,18263,295,root,2020-06-25 10:59:00 AM'
440: + set +x
432: ENDING TIMING RUN AT 2020-06-25 11:03:55 AM
432: + echo 'ENDING TIMING RUN AT 2020-06-25 11:03:55 AM'
439: ENDING TIMING RUN AT 2020-06-25 11:03:55 AM
439: + echo 'ENDING TIMING RUN AT 2020-06-25 11:03:55 AM'
441: ENDING TIMING RUN AT 2020-06-25 11:03:55 AM
441: + echo 'ENDING TIMING RUN AT 2020-06-25 11:03:55 AM'
441: + RESULT=295
432: + RESULT=295
432: + RESULT_NAME=bert
439: + RESULT=295
439: + RESULT_NAME=bert
441: + RESULT_NAME=bert
441: + echo 'RESULT,bert,18263,295,root,2020-06-25 10:59:00 AM'
441: RESULT,bert,18263,295,root,2020-06-25 10:59:00 AM
441: + set +x
432: RESULT,bert,18263,295,root,2020-06-25 10:59:00 AM
432: + echo 'RESULT,bert,18263,295,root,2020-06-25 10:59:00 AM'
432: + set +x
439: RESULT,bert,18263,295,root,2020-06-25 10:59:00 AM
439: + echo 'RESULT,bert,18263,295,root,2020-06-25 10:59:00 AM'
439: + set +x
 64: ++ date +%s
 65: ++ date +%s
 69: ++ date +%s
 68: ++ date +%s
 71: ++ date +%s
 77: ++ date +%s
 70: ++ date +%s
 66: ++ date +%s
 73: ++ date +%s
 75: ++ date +%s
 79: ++ date +%s
 72: ++ date +%s
 74: ++ date +%s
 78: ++ date +%s
 69: + END=1593108235
 65: + END=1593108235
 68: + END=1593108235
 77: + END=1593108235
 64: + END=1593108235
 71: + END=1593108235
 69: ++ date '+%Y-%m-%d %r'
 70: + END=1593108235
 66: + END=1593108235
 72: + END=1593108235
 78: + END=1593108235
 74: + END=1593108235
 79: + END=1593108235
 65: ++ date '+%Y-%m-%d %r'
 75: + END=1593108235
 73: + END=1593108235
 68: ++ date '+%Y-%m-%d %r'
 77: ++ date '+%Y-%m-%d %r'
 70: ++ date '+%Y-%m-%d %r'
 64: ++ date '+%Y-%m-%d %r'
 71: ++ date '+%Y-%m-%d %r'
 66: ++ date '+%Y-%m-%d %r'
 72: ++ date '+%Y-%m-%d %r'
 74: ++ date '+%Y-%m-%d %r'
 78: ++ date '+%Y-%m-%d %r'
 79: ++ date '+%Y-%m-%d %r'
 75: ++ date '+%Y-%m-%d %r'
 73: ++ date '+%Y-%m-%d %r'
 69: ENDING TIMING RUN AT 2020-06-25 11:03:55 AM
 69: + END_FMT='2020-06-25 11:03:55 AM'
 69: + echo 'ENDING TIMING RUN AT 2020-06-25 11:03:55 AM'
 69: + RESULT=295
 69: + RESULT_NAME=bert
 69: RESULT,bert,18263,295,root,2020-06-25 10:59:00 AM
 69: + echo 'RESULT,bert,18263,295,root,2020-06-25 10:59:00 AM'
 69: + set +x
 65: + END_FMT='2020-06-25 11:03:55 AM'
 65: + echo 'ENDING TIMING RUN AT 2020-06-25 11:03:55 AM'
 65: ENDING TIMING RUN AT 2020-06-25 11:03:55 AM
 65: + RESULT=295
 65: + RESULT_NAME=bert
 65: RESULT,bert,18263,295,root,2020-06-25 10:59:00 AM
 65: + echo 'RESULT,bert,18263,295,root,2020-06-25 10:59:00 AM'
 65: + set +x
 68: + END_FMT='2020-06-25 11:03:55 AM'
 77: + END_FMT='2020-06-25 11:03:55 AM'
 68: + echo 'ENDING TIMING RUN AT 2020-06-25 11:03:55 AM'
 68: ENDING TIMING RUN AT 2020-06-25 11:03:55 AM
 68: + RESULT=295
 77: ENDING TIMING RUN AT 2020-06-25 11:03:55 AM
 77: + echo 'ENDING TIMING RUN AT 2020-06-25 11:03:55 AM'
 68: + RESULT_NAME=bert
 70: + END_FMT='2020-06-25 11:03:55 AM'
 77: + RESULT=295
 77: + RESULT_NAME=bert
 68: RESULT,bert,18263,295,root,2020-06-25 10:59:00 AM
 68: + echo 'RESULT,bert,18263,295,root,2020-06-25 10:59:00 AM'
 68: + set +x
 70: ENDING TIMING RUN AT 2020-06-25 11:03:55 AM
 70: + echo 'ENDING TIMING RUN AT 2020-06-25 11:03:55 AM'
 77: RESULT,bert,18263,295,root,2020-06-25 10:59:00 AM
 77: + echo 'RESULT,bert,18263,295,root,2020-06-25 10:59:00 AM'
 77: + set +x
 70: + RESULT=295
 70: + RESULT_NAME=bert
 70: RESULT,bert,18263,295,root,2020-06-25 10:59:00 AM
 70: + echo 'RESULT,bert,18263,295,root,2020-06-25 10:59:00 AM'
 70: + set +x
 71: + END_FMT='2020-06-25 11:03:55 AM'
 64: + END_FMT='2020-06-25 11:03:55 AM'
 71: + echo 'ENDING TIMING RUN AT 2020-06-25 11:03:55 AM'
 71: ENDING TIMING RUN AT 2020-06-25 11:03:55 AM
 64: ENDING TIMING RUN AT 2020-06-25 11:03:55 AM
 64: + echo 'ENDING TIMING RUN AT 2020-06-25 11:03:55 AM'
 64: + RESULT=295
 64: + RESULT_NAME=bert
 71: + RESULT=295
 71: + RESULT_NAME=bert
 64: RESULT,bert,18263,295,root,2020-06-25 10:59:00 AM
 64: + echo 'RESULT,bert,18263,295,root,2020-06-25 10:59:00 AM'
 64: + set +x
 71: RESULT,bert,18263,295,root,2020-06-25 10:59:00 AM
 71: + echo 'RESULT,bert,18263,295,root,2020-06-25 10:59:00 AM'
 71: + set +x
 66: + END_FMT='2020-06-25 11:03:55 AM'
 66: + echo 'ENDING TIMING RUN AT 2020-06-25 11:03:55 AM'
 66: ENDING TIMING RUN AT 2020-06-25 11:03:55 AM
 66: + RESULT=295
 66: + RESULT_NAME=bert
 66: RESULT,bert,18263,295,root,2020-06-25 10:59:00 AM
 66: + echo 'RESULT,bert,18263,295,root,2020-06-25 10:59:00 AM'
 66: + set +x
 74: + END_FMT='2020-06-25 11:03:55 AM'
 79: + END_FMT='2020-06-25 11:03:55 AM'
 78: + END_FMT='2020-06-25 11:03:55 AM'
 79: + echo 'ENDING TIMING RUN AT 2020-06-25 11:03:55 AM'
 74: ENDING TIMING RUN AT 2020-06-25 11:03:55 AM
 74: + echo 'ENDING TIMING RUN AT 2020-06-25 11:03:55 AM'
 79: ENDING TIMING RUN AT 2020-06-25 11:03:55 AM
 73: + END_FMT='2020-06-25 11:03:55 AM'
 79: + RESULT=295
 79: + RESULT_NAME=bert
 74: + RESULT=295
 74: + RESULT_NAME=bert
 78: ENDING TIMING RUN AT 2020-06-25 11:03:55 AM
 78: + echo 'ENDING TIMING RUN AT 2020-06-25 11:03:55 AM'
 73: ENDING TIMING RUN AT 2020-06-25 11:03:55 AM
 73: + echo 'ENDING TIMING RUN AT 2020-06-25 11:03:55 AM'
 74: RESULT,bert,18263,295,root,2020-06-25 10:59:00 AM
 74: + echo 'RESULT,bert,18263,295,root,2020-06-25 10:59:00 AM'
 74: + set +x
 78: + RESULT=295
 78: + RESULT_NAME=bert
 79: RESULT,bert,18263,295,root,2020-06-25 10:59:00 AM
 79: + echo 'RESULT,bert,18263,295,root,2020-06-25 10:59:00 AM'
 79: + set +x
 73: + RESULT=295
 73: + RESULT_NAME=bert
 78: RESULT,bert,18263,295,root,2020-06-25 10:59:00 AM
 78: + echo 'RESULT,bert,18263,295,root,2020-06-25 10:59:00 AM'
 78: + set +x
 73: RESULT,bert,18263,295,root,2020-06-25 10:59:00 AM
 73: + echo 'RESULT,bert,18263,295,root,2020-06-25 10:59:00 AM'
 73: + set +x
 75: + END_FMT='2020-06-25 11:03:55 AM'
 72: + END_FMT='2020-06-25 11:03:55 AM'
 75: ENDING TIMING RUN AT 2020-06-25 11:03:55 AM
 75: + echo 'ENDING TIMING RUN AT 2020-06-25 11:03:55 AM'
 75: + RESULT=295
 72: ENDING TIMING RUN AT 2020-06-25 11:03:55 AM
 72: + echo 'ENDING TIMING RUN AT 2020-06-25 11:03:55 AM'
 75: + RESULT_NAME=bert
 75: + echo 'RESULT,bert,18263,295,root,2020-06-25 10:59:00 AM'
 72: + RESULT=295
 72: + RESULT_NAME=bert
 75: RESULT,bert,18263,295,root,2020-06-25 10:59:00 AM
 75: + set +x
 72: RESULT,bert,18263,295,root,2020-06-25 10:59:00 AM
 72: + echo 'RESULT,bert,18263,295,root,2020-06-25 10:59:00 AM'
 72: + set +x
389: ++ date +%s
387: ++ date +%s
393: ++ date +%s
385: ++ date +%s
391: ++ date +%s
395: ++ date +%s
396: ++ date +%s
398: ++ date +%s
390: ++ date +%s
384: ++ date +%s
394: ++ date +%s
388: ++ date +%s
399: ++ date +%s
386: ++ date +%s
387: + END=1593108235
395: + END=1593108235
390: + END=1593108235
393: + END=1593108235
399: + END=1593108235
385: + END=1593108235
389: + END=1593108235
388: + END=1593108235
391: + END=1593108235
398: + END=1593108235
384: + END=1593108235
394: + END=1593108235
396: + END=1593108235
387: ++ date '+%Y-%m-%d %r'
386: + END=1593108235
395: ++ date '+%Y-%m-%d %r'
388: ++ date '+%Y-%m-%d %r'
390: ++ date '+%Y-%m-%d %r'
399: ++ date '+%Y-%m-%d %r'
389: ++ date '+%Y-%m-%d %r'
394: ++ date '+%Y-%m-%d %r'
398: ++ date '+%Y-%m-%d %r'
393: ++ date '+%Y-%m-%d %r'
396: ++ date '+%Y-%m-%d %r'
391: ++ date '+%Y-%m-%d %r'
385: ++ date '+%Y-%m-%d %r'
387: + END_FMT='2020-06-25 11:03:55 AM'
387: + echo 'ENDING TIMING RUN AT 2020-06-25 11:03:55 AM'
387: ENDING TIMING RUN AT 2020-06-25 11:03:55 AM
387: + RESULT=294
387: + RESULT_NAME=bert
387: RESULT,bert,18263,294,root,2020-06-25 10:59:01 AM
387: + echo 'RESULT,bert,18263,294,root,2020-06-25 10:59:01 AM'
387: + set +x
384: ++ date '+%Y-%m-%d %r'
386: ++ date '+%Y-%m-%d %r'
395: ENDING TIMING RUN AT 2020-06-25 11:03:55 AM
395: + END_FMT='2020-06-25 11:03:55 AM'
395: + echo 'ENDING TIMING RUN AT 2020-06-25 11:03:55 AM'
395: + RESULT=294
395: + RESULT_NAME=bert
395: RESULT,bert,18263,294,root,2020-06-25 10:59:01 AM
395: + echo 'RESULT,bert,18263,294,root,2020-06-25 10:59:01 AM'
395: + set +x
390: + END_FMT='2020-06-25 11:03:55 AM'
390: ENDING TIMING RUN AT 2020-06-25 11:03:55 AM
390: + echo 'ENDING TIMING RUN AT 2020-06-25 11:03:55 AM'
390: + RESULT=294
390: + RESULT_NAME=bert
390: + echo 'RESULT,bert,18263,294,root,2020-06-25 10:59:01 AM'
390: + set +x
390: RESULT,bert,18263,294,root,2020-06-25 10:59:01 AM
388: + END_FMT='2020-06-25 11:03:55 AM'
388: + echo 'ENDING TIMING RUN AT 2020-06-25 11:03:55 AM'
388: ENDING TIMING RUN AT 2020-06-25 11:03:55 AM
388: + RESULT=294
388: + RESULT_NAME=bert
388: RESULT,bert,18263,294,root,2020-06-25 10:59:01 AM
388: + echo 'RESULT,bert,18263,294,root,2020-06-25 10:59:01 AM'
388: + set +x
399: + END_FMT='2020-06-25 11:03:55 AM'
393: + END_FMT='2020-06-25 11:03:55 AM'
399: ENDING TIMING RUN AT 2020-06-25 11:03:55 AM
399: + echo 'ENDING TIMING RUN AT 2020-06-25 11:03:55 AM'
389: + END_FMT='2020-06-25 11:03:55 AM'
393: ENDING TIMING RUN AT 2020-06-25 11:03:55 AM
393: + echo 'ENDING TIMING RUN AT 2020-06-25 11:03:55 AM'
394: + END_FMT='2020-06-25 11:03:55 AM'
399: + RESULT=294
399: + RESULT_NAME=bert
389: + echo 'ENDING TIMING RUN AT 2020-06-25 11:03:55 AM'
393: + RESULT=294
393: + RESULT_NAME=bert
399: RESULT,bert,18263,294,root,2020-06-25 10:59:01 AM
399: + echo 'RESULT,bert,18263,294,root,2020-06-25 10:59:01 AM'
399: + set +x
389: ENDING TIMING RUN AT 2020-06-25 11:03:55 AM
393: RESULT,bert,18263,294,root,2020-06-25 10:59:01 AM
393: + echo 'RESULT,bert,18263,294,root,2020-06-25 10:59:01 AM'
393: + set +x
394: ENDING TIMING RUN AT 2020-06-25 11:03:55 AM
394: RESULT,bert,18263,294,root,2020-06-25 10:59:01 AM
394: + echo 'ENDING TIMING RUN AT 2020-06-25 11:03:55 AM'
394: + RESULT=294
394: + RESULT_NAME=bert
394: + echo 'RESULT,bert,18263,294,root,2020-06-25 10:59:01 AM'
394: + set +x
389: RESULT,bert,18263,294,root,2020-06-25 10:59:01 AM
389: + RESULT=294
389: + RESULT_NAME=bert
389: + echo 'RESULT,bert,18263,294,root,2020-06-25 10:59:01 AM'
389: + set +x
385: + END_FMT='2020-06-25 11:03:55 AM'
384: slurmstepd: error: _is_a_lwp: open() /proc/84965/status failed: No such file or directory
391: + END_FMT='2020-06-25 11:03:55 AM'
385: ENDING TIMING RUN AT 2020-06-25 11:03:55 AM
385: + echo 'ENDING TIMING RUN AT 2020-06-25 11:03:55 AM'
396: + END_FMT='2020-06-25 11:03:55 AM'
385: + RESULT=294
385: + RESULT_NAME=bert
391: ENDING TIMING RUN AT 2020-06-25 11:03:55 AM
391: + echo 'ENDING TIMING RUN AT 2020-06-25 11:03:55 AM'
396: + echo 'ENDING TIMING RUN AT 2020-06-25 11:03:55 AM'
385: RESULT,bert,18263,294,root,2020-06-25 10:59:01 AM
385: + echo 'RESULT,bert,18263,294,root,2020-06-25 10:59:01 AM'
385: + set +x
391: + RESULT=294
391: + RESULT_NAME=bert
396: ENDING TIMING RUN AT 2020-06-25 11:03:55 AM
398: + END_FMT='2020-06-25 11:03:55 AM'
398: + echo 'ENDING TIMING RUN AT 2020-06-25 11:03:55 AM'
391: RESULT,bert,18263,294,root,2020-06-25 10:59:01 AM
391: + echo 'RESULT,bert,18263,294,root,2020-06-25 10:59:01 AM'
391: + set +x
396: RESULT,bert,18263,294,root,2020-06-25 10:59:01 AM
396: + RESULT=294
396: + RESULT_NAME=bert
396: + echo 'RESULT,bert,18263,294,root,2020-06-25 10:59:01 AM'
396: + set +x
398: ENDING TIMING RUN AT 2020-06-25 11:03:55 AM
398: RESULT,bert,18263,294,root,2020-06-25 10:59:01 AM
398: + RESULT=294
398: + RESULT_NAME=bert
398: + echo 'RESULT,bert,18263,294,root,2020-06-25 10:59:01 AM'
398: + set +x
386: + END_FMT='2020-06-25 11:03:55 AM'
384: + END_FMT='2020-06-25 11:03:55 AM'
386: ENDING TIMING RUN AT 2020-06-25 11:03:55 AM
386: + echo 'ENDING TIMING RUN AT 2020-06-25 11:03:55 AM'
384: ENDING TIMING RUN AT 2020-06-25 11:03:55 AM
384: + echo 'ENDING TIMING RUN AT 2020-06-25 11:03:55 AM'
384: + RESULT=294
384: + RESULT_NAME=bert
386: + RESULT=294
386: + RESULT_NAME=bert
386: RESULT,bert,18263,294,root,2020-06-25 10:59:01 AM
386: + echo 'RESULT,bert,18263,294,root,2020-06-25 10:59:01 AM'
386: + set +x
384: RESULT,bert,18263,294,root,2020-06-25 10:59:01 AM
384: + echo 'RESULT,bert,18263,294,root,2020-06-25 10:59:01 AM'
384: + set +x
403: ++ date +%s
405: ++ date +%s
409: ++ date +%s
411: ++ date +%s
407: ++ date +%s
401: ++ date +%s
413: ++ date +%s
402: ++ date +%s
412: ++ date +%s
406: ++ date +%s
410: ++ date +%s
400: ++ date +%s
415: ++ date +%s
414: ++ date +%s
403: + END=1593108235
405: + END=1593108235
407: + END=1593108235
409: + END=1593108235
411: + END=1593108235
401: + END=1593108235
413: + END=1593108235
412: + END=1593108235
402: + END=1593108235
407: ++ date '+%Y-%m-%d %r'
403: ++ date '+%Y-%m-%d %r'
405: ++ date '+%Y-%m-%d %r'
410: + END=1593108235
409: ++ date '+%Y-%m-%d %r'
406: + END=1593108235
400: + END=1593108235
411: ++ date '+%Y-%m-%d %r'
413: ++ date '+%Y-%m-%d %r'
415: + END=1593108235
401: ++ date '+%Y-%m-%d %r'
414: + END=1593108235
412: ++ date '+%Y-%m-%d %r'
402: ++ date '+%Y-%m-%d %r'
410: ++ date '+%Y-%m-%d %r'
406: ++ date '+%Y-%m-%d %r'
414: ++ date '+%Y-%m-%d %r'
415: ++ date '+%Y-%m-%d %r'
400: ++ date '+%Y-%m-%d %r'
405: + END_FMT='2020-06-25 11:03:55 AM'
331: ++ date +%s
405: ENDING TIMING RUN AT 2020-06-25 11:03:55 AM
405: + echo 'ENDING TIMING RUN AT 2020-06-25 11:03:55 AM'
405: + RESULT=294
405: + RESULT_NAME=bert
407: + END_FMT='2020-06-25 11:03:55 AM'
407: + echo 'ENDING TIMING RUN AT 2020-06-25 11:03:55 AM'
405: RESULT,bert,18263,294,root,2020-06-25 10:59:01 AM
405: + echo 'RESULT,bert,18263,294,root,2020-06-25 10:59:01 AM'
405: + set +x
407: ENDING TIMING RUN AT 2020-06-25 11:03:55 AM
407: + RESULT=294
407: + RESULT_NAME=bert
403: + END_FMT='2020-06-25 11:03:55 AM'
407: RESULT,bert,18263,294,root,2020-06-25 10:59:01 AM
327: ++ date +%s
407: + echo 'RESULT,bert,18263,294,root,2020-06-25 10:59:01 AM'
407: + set +x
409: + END_FMT='2020-06-25 11:03:55 AM'
321: ++ date +%s
403: ENDING TIMING RUN AT 2020-06-25 11:03:55 AM
403: + echo 'ENDING TIMING RUN AT 2020-06-25 11:03:55 AM'
409: ENDING TIMING RUN AT 2020-06-25 11:03:55 AM
323: ++ date +%s
409: + echo 'ENDING TIMING RUN AT 2020-06-25 11:03:55 AM'
325: ++ date +%s
403: + RESULT=294
403: + RESULT_NAME=bert
409: + RESULT=294
409: + RESULT_NAME=bert
403: + echo 'RESULT,bert,18263,294,root,2020-06-25 10:59:01 AM'
403: RESULT,bert,18263,294,root,2020-06-25 10:59:01 AM
324: ++ date +%s
403: + set +x
409: RESULT,bert,18263,294,root,2020-06-25 10:59:01 AM
409: + echo 'RESULT,bert,18263,294,root,2020-06-25 10:59:01 AM'
409: + set +x
326: ++ date +%s
411: + END_FMT='2020-06-25 11:03:55 AM'
411: ENDING TIMING RUN AT 2020-06-25 11:03:55 AM
411: + echo 'ENDING TIMING RUN AT 2020-06-25 11:03:55 AM'
411: + RESULT=294
411: + RESULT_NAME=bert
401: + END_FMT='2020-06-25 11:03:55 AM'
411: RESULT,bert,18263,294,root,2020-06-25 10:59:01 AM
411: + echo 'RESULT,bert,18263,294,root,2020-06-25 10:59:01 AM'
411: + set +x
401: ENDING TIMING RUN AT 2020-06-25 11:03:55 AM
401: + echo 'ENDING TIMING RUN AT 2020-06-25 11:03:55 AM'
328: ++ date +%s
332: ++ date +%s
320: ++ date +%s
335: ++ date +%s
413: + END_FMT='2020-06-25 11:03:55 AM'
334: ++ date +%s
401: + RESULT=294
322: ++ date +%s
413: ENDING TIMING RUN AT 2020-06-25 11:03:55 AM
412: + END_FMT='2020-06-25 11:03:55 AM'
333: ++ date +%s
401: RESULT,bert,18263,294,root,2020-06-25 10:59:01 AM
401: + RESULT_NAME=bert
413: + echo 'ENDING TIMING RUN AT 2020-06-25 11:03:55 AM'
412: ENDING TIMING RUN AT 2020-06-25 11:03:55 AM
401: + echo 'RESULT,bert,18263,294,root,2020-06-25 10:59:01 AM'
401: + set +x
412: + echo 'ENDING TIMING RUN AT 2020-06-25 11:03:55 AM'
412: + RESULT=294
412: + RESULT_NAME=bert
413: + RESULT=294
413: + RESULT_NAME=bert
413: RESULT,bert,18263,294,root,2020-06-25 10:59:01 AM
413: + echo 'RESULT,bert,18263,294,root,2020-06-25 10:59:01 AM'
413: + set +x
412: RESULT,bert,18263,294,root,2020-06-25 10:59:01 AM
412: + echo 'RESULT,bert,18263,294,root,2020-06-25 10:59:01 AM'
412: + set +x
410: + END_FMT='2020-06-25 11:03:55 AM'
402: + END_FMT='2020-06-25 11:03:55 AM'
331: + END=1593108235
410: ENDING TIMING RUN AT 2020-06-25 11:03:55 AM
402: + echo 'ENDING TIMING RUN AT 2020-06-25 11:03:55 AM'
410: + echo 'ENDING TIMING RUN AT 2020-06-25 11:03:55 AM'
402: ENDING TIMING RUN AT 2020-06-25 11:03:55 AM
410: + RESULT=294
410: + RESULT_NAME=bert
402: + RESULT=294
402: + RESULT_NAME=bert
402: RESULT,bert,18263,294,root,2020-06-25 10:59:01 AM
402: + echo 'RESULT,bert,18263,294,root,2020-06-25 10:59:01 AM'
402: + set +x
410: RESULT,bert,18263,294,root,2020-06-25 10:59:01 AM
410: + echo 'RESULT,bert,18263,294,root,2020-06-25 10:59:01 AM'
410: + set +x
414: + END_FMT='2020-06-25 11:03:55 AM'
414: + echo 'ENDING TIMING RUN AT 2020-06-25 11:03:55 AM'
414: ENDING TIMING RUN AT 2020-06-25 11:03:55 AM
414: + RESULT=294
414: + RESULT_NAME=bert
414: RESULT,bert,18263,294,root,2020-06-25 10:59:01 AM
414: + echo 'RESULT,bert,18263,294,root,2020-06-25 10:59:01 AM'
414: + set +x
415: + END_FMT='2020-06-25 11:03:55 AM'
400: + END_FMT='2020-06-25 11:03:55 AM'
415: + echo 'ENDING TIMING RUN AT 2020-06-25 11:03:55 AM'
415: ENDING TIMING RUN AT 2020-06-25 11:03:55 AM
415: + RESULT=294
415: + RESULT_NAME=bert
400: ENDING TIMING RUN AT 2020-06-25 11:03:55 AM
400: + echo 'ENDING TIMING RUN AT 2020-06-25 11:03:55 AM'
415: RESULT,bert,18263,294,root,2020-06-25 10:59:01 AM
415: + echo 'RESULT,bert,18263,294,root,2020-06-25 10:59:01 AM'
415: + set +x
400: + RESULT=294
400: + RESULT_NAME=bert
400: + echo 'RESULT,bert,18263,294,root,2020-06-25 10:59:01 AM'
400: RESULT,bert,18263,294,root,2020-06-25 10:59:01 AM
400: + set +x
400: slurmstepd: error: _is_a_lwp: open() /proc/82327/status failed: No such file or directory
406: + END_FMT='2020-06-25 11:03:55 AM'
406: + echo 'ENDING TIMING RUN AT 2020-06-25 11:03:55 AM'
406: ENDING TIMING RUN AT 2020-06-25 11:03:55 AM
406: + RESULT=294
406: + RESULT_NAME=bert
406: RESULT,bert,18263,294,root,2020-06-25 10:59:01 AM
406: + echo 'RESULT,bert,18263,294,root,2020-06-25 10:59:01 AM'
406: + set +x
331: ++ date '+%Y-%m-%d %r'
327: + END=1593108235
321: + END=1593108235
325: + END=1593108235
323: + END=1593108235
327: ++ date '+%Y-%m-%d %r'
333: + END=1593108235
332: + END=1593108235
320: + END=1593108235
322: + END=1593108235
326: + END=1593108235
321: ++ date '+%Y-%m-%d %r'
325: ++ date '+%Y-%m-%d %r'
328: + END=1593108235
335: + END=1593108235
323: ++ date '+%Y-%m-%d %r'
324: + END=1593108235
334: + END=1593108235
331: + END_FMT='2020-06-25 11:03:55 AM'
331: + echo 'ENDING TIMING RUN AT 2020-06-25 11:03:55 AM'
331: ENDING TIMING RUN AT 2020-06-25 11:03:55 AM
331: + RESULT=295
331: + RESULT_NAME=bert
331: RESULT,bert,18263,295,root,2020-06-25 10:59:00 AM
331: + echo 'RESULT,bert,18263,295,root,2020-06-25 10:59:00 AM'
331: + set +x
333: ++ date '+%Y-%m-%d %r'
320: ++ date '+%Y-%m-%d %r'
332: ++ date '+%Y-%m-%d %r'
326: ++ date '+%Y-%m-%d %r'
335: ++ date '+%Y-%m-%d %r'
322: ++ date '+%Y-%m-%d %r'
324: ++ date '+%Y-%m-%d %r'
328: ++ date '+%Y-%m-%d %r'
334: ++ date '+%Y-%m-%d %r'
327: + END_FMT='2020-06-25 11:03:55 AM'
327: ENDING TIMING RUN AT 2020-06-25 11:03:55 AM
327: + echo 'ENDING TIMING RUN AT 2020-06-25 11:03:55 AM'
327: + RESULT=295
327: + RESULT_NAME=bert
327: RESULT,bert,18263,295,root,2020-06-25 10:59:00 AM
327: + echo 'RESULT,bert,18263,295,root,2020-06-25 10:59:00 AM'
327: + set +x
325: + END_FMT='2020-06-25 11:03:55 AM'
323: + END_FMT='2020-06-25 11:03:55 AM'
323: ENDING TIMING RUN AT 2020-06-25 11:03:55 AM
323: + echo 'ENDING TIMING RUN AT 2020-06-25 11:03:55 AM'
325: ENDING TIMING RUN AT 2020-06-25 11:03:55 AM
325: + echo 'ENDING TIMING RUN AT 2020-06-25 11:03:55 AM'
325: + RESULT=295
325: + RESULT_NAME=bert
323: + RESULT=295
323: + RESULT_NAME=bert
325: RESULT,bert,18263,295,root,2020-06-25 10:59:00 AM
325: + echo 'RESULT,bert,18263,295,root,2020-06-25 10:59:00 AM'
325: + set +x
323: RESULT,bert,18263,295,root,2020-06-25 10:59:00 AM
323: + echo 'RESULT,bert,18263,295,root,2020-06-25 10:59:00 AM'
323: + set +x
321: + END_FMT='2020-06-25 11:03:55 AM'
321: + echo 'ENDING TIMING RUN AT 2020-06-25 11:03:55 AM'
321: ENDING TIMING RUN AT 2020-06-25 11:03:55 AM
321: + RESULT=295
321: + RESULT_NAME=bert
321: RESULT,bert,18263,295,root,2020-06-25 10:59:00 AM
321: + echo 'RESULT,bert,18263,295,root,2020-06-25 10:59:00 AM'
321: + set +x
333: + END_FMT='2020-06-25 11:03:55 AM'
333: ENDING TIMING RUN AT 2020-06-25 11:03:55 AM
333: + echo 'ENDING TIMING RUN AT 2020-06-25 11:03:55 AM'
333: + RESULT=295
333: + RESULT_NAME=bert
333: RESULT,bert,18263,295,root,2020-06-25 10:59:00 AM
333: + echo 'RESULT,bert,18263,295,root,2020-06-25 10:59:00 AM'
333: + set +x
320: + END_FMT='2020-06-25 11:03:55 AM'
320: slurmstepd: error: _is_a_lwp: open() /proc/77148/status failed: No such file or directory
320: + echo 'ENDING TIMING RUN AT 2020-06-25 11:03:55 AM'
320: ENDING TIMING RUN AT 2020-06-25 11:03:55 AM
320: + RESULT=295
320: + RESULT_NAME=bert
320: + echo 'RESULT,bert,18263,295,root,2020-06-25 10:59:00 AM'
320: RESULT,bert,18263,295,root,2020-06-25 10:59:00 AM
320: + set +x
335: + END_FMT='2020-06-25 11:03:55 AM'
335: + echo 'ENDING TIMING RUN AT 2020-06-25 11:03:55 AM'
335: ENDING TIMING RUN AT 2020-06-25 11:03:55 AM
335: + RESULT=295
335: + RESULT_NAME=bert
335: RESULT,bert,18263,295,root,2020-06-25 10:59:00 AM
335: + echo 'RESULT,bert,18263,295,root,2020-06-25 10:59:00 AM'
335: + set +x
324: ENDING TIMING RUN AT 2020-06-25 11:03:55 AM
324: + END_FMT='2020-06-25 11:03:55 AM'
324: + echo 'ENDING TIMING RUN AT 2020-06-25 11:03:55 AM'
324: + RESULT=295
324: + RESULT_NAME=bert
324: + echo 'RESULT,bert,18263,295,root,2020-06-25 10:59:00 AM'
324: + set +x
324: RESULT,bert,18263,295,root,2020-06-25 10:59:00 AM
332: + END_FMT='2020-06-25 11:03:55 AM'
334: ENDING TIMING RUN AT 2020-06-25 11:03:55 AM
334: + END_FMT='2020-06-25 11:03:55 AM'
334: + echo 'ENDING TIMING RUN AT 2020-06-25 11:03:55 AM'
334: + RESULT=295
322: + END_FMT='2020-06-25 11:03:55 AM'
326: + END_FMT='2020-06-25 11:03:55 AM'
328: + END_FMT='2020-06-25 11:03:55 AM'
328: + echo 'ENDING TIMING RUN AT 2020-06-25 11:03:55 AM'
332: ENDING TIMING RUN AT 2020-06-25 11:03:55 AM
332: + echo 'ENDING TIMING RUN AT 2020-06-25 11:03:55 AM'
332: + RESULT=295
334: + RESULT_NAME=bert
334: + echo 'RESULT,bert,18263,295,root,2020-06-25 10:59:00 AM'
334: + set +x
322: ENDING TIMING RUN AT 2020-06-25 11:03:55 AM
322: + echo 'ENDING TIMING RUN AT 2020-06-25 11:03:55 AM'
326: ENDING TIMING RUN AT 2020-06-25 11:03:55 AM
326: + echo 'ENDING TIMING RUN AT 2020-06-25 11:03:55 AM'
326: + RESULT=295
326: + RESULT_NAME=bert
328: ENDING TIMING RUN AT 2020-06-25 11:03:55 AM
332: + RESULT_NAME=bert
332: + echo 'RESULT,bert,18263,295,root,2020-06-25 10:59:00 AM'
334: RESULT,bert,18263,295,root,2020-06-25 10:59:00 AM
322: + RESULT=295
322: + RESULT_NAME=bert
322: + echo 'RESULT,bert,18263,295,root,2020-06-25 10:59:00 AM'
326: RESULT,bert,18263,295,root,2020-06-25 10:59:00 AM
326: + echo 'RESULT,bert,18263,295,root,2020-06-25 10:59:00 AM'
326: + set +x
328: + RESULT=295
328: + RESULT_NAME=bert
328: + echo 'RESULT,bert,18263,295,root,2020-06-25 10:59:00 AM'
328: + set +x
332: RESULT,bert,18263,295,root,2020-06-25 10:59:00 AM
332: + set +x
322: RESULT,bert,18263,295,root,2020-06-25 10:59:00 AM
322: + set +x
328: RESULT,bert,18263,295,root,2020-06-25 10:59:00 AM
 59: ++ date +%s
 56: ++ date +%s
 63: ++ date +%s
 59: + END=1593108235
 50: ++ date +%s
 51: ++ date +%s
 54: ++ date +%s
 49: ++ date +%s
 56: + END=1593108235
 53: ++ date +%s
 59: ++ date '+%Y-%m-%d %r'
 60: ++ date +%s
 61: ++ date +%s
 52: ++ date +%s
 55: ++ date +%s
 62: ++ date +%s
 57: ++ date +%s
 63: + END=1593108235
 56: ++ date '+%Y-%m-%d %r'
 63: ++ date '+%Y-%m-%d %r'
 59: + END_FMT='2020-06-25 11:03:55 AM'
 59: ENDING TIMING RUN AT 2020-06-25 11:03:55 AM
 59: + echo 'ENDING TIMING RUN AT 2020-06-25 11:03:55 AM'
 59: + RESULT=295
 59: + RESULT_NAME=bert
 59: RESULT,bert,18263,295,root,2020-06-25 10:59:00 AM
 59: + echo 'RESULT,bert,18263,295,root,2020-06-25 10:59:00 AM'
 59: + set +x
 50: + END=1593108235
 49: + END=1593108235
 51: + END=1593108235
 61: + END=1593108235
 54: + END=1593108235
 52: + END=1593108235
 53: + END=1593108235
 55: + END=1593108235
 57: + END=1593108235
 60: + END=1593108235
 62: + END=1593108235
 56: + END_FMT='2020-06-25 11:03:55 AM'
 56: ENDING TIMING RUN AT 2020-06-25 11:03:55 AM
 56: + echo 'ENDING TIMING RUN AT 2020-06-25 11:03:55 AM'
 56: + RESULT=295
 63: + END_FMT='2020-06-25 11:03:55 AM'
 56: + RESULT_NAME=bert
 56: RESULT,bert,18263,295,root,2020-06-25 10:59:00 AM
 56: + echo 'RESULT,bert,18263,295,root,2020-06-25 10:59:00 AM'
 56: + set +x
 63: ENDING TIMING RUN AT 2020-06-25 11:03:55 AM
 63: + echo 'ENDING TIMING RUN AT 2020-06-25 11:03:55 AM'
 63: + RESULT=295
 63: + RESULT_NAME=bert
 50: ++ date '+%Y-%m-%d %r'
 63: RESULT,bert,18263,295,root,2020-06-25 10:59:00 AM
 63: + echo 'RESULT,bert,18263,295,root,2020-06-25 10:59:00 AM'
 63: + set +x
 54: ++ date '+%Y-%m-%d %r'
 51: ++ date '+%Y-%m-%d %r'
 61: ++ date '+%Y-%m-%d %r'
 49: ++ date '+%Y-%m-%d %r'
 52: ++ date '+%Y-%m-%d %r'
 53: ++ date '+%Y-%m-%d %r'
 55: ++ date '+%Y-%m-%d %r'
 60: ++ date '+%Y-%m-%d %r'
 62: ++ date '+%Y-%m-%d %r'
 57: ++ date '+%Y-%m-%d %r'
 50: + END_FMT='2020-06-25 11:03:55 AM'
 50: + echo 'ENDING TIMING RUN AT 2020-06-25 11:03:55 AM'
 50: ENDING TIMING RUN AT 2020-06-25 11:03:55 AM
 50: + RESULT=295
 50: + RESULT_NAME=bert
 54: + END_FMT='2020-06-25 11:03:55 AM'
 50: RESULT,bert,18263,295,root,2020-06-25 10:59:00 AM
 50: + echo 'RESULT,bert,18263,295,root,2020-06-25 10:59:00 AM'
 50: + set +x
 61: + END_FMT='2020-06-25 11:03:55 AM'
 54: ENDING TIMING RUN AT 2020-06-25 11:03:55 AM
 54: + echo 'ENDING TIMING RUN AT 2020-06-25 11:03:55 AM'
 61: + echo 'ENDING TIMING RUN AT 2020-06-25 11:03:55 AM'
 54: + RESULT=295
 54: + RESULT_NAME=bert
 61: ENDING TIMING RUN AT 2020-06-25 11:03:55 AM
 61: + RESULT=295
 61: + RESULT_NAME=bert
 51: + END_FMT='2020-06-25 11:03:55 AM'
 54: RESULT,bert,18263,295,root,2020-06-25 10:59:00 AM
 54: + echo 'RESULT,bert,18263,295,root,2020-06-25 10:59:00 AM'
 54: + set +x
 61: RESULT,bert,18263,295,root,2020-06-25 10:59:00 AM
 61: + echo 'RESULT,bert,18263,295,root,2020-06-25 10:59:00 AM'
 61: + set +x
 51: ENDING TIMING RUN AT 2020-06-25 11:03:55 AM
 51: + echo 'ENDING TIMING RUN AT 2020-06-25 11:03:55 AM'
 51: + RESULT=295
 51: + RESULT_NAME=bert
 51: RESULT,bert,18263,295,root,2020-06-25 10:59:00 AM
 51: + echo 'RESULT,bert,18263,295,root,2020-06-25 10:59:00 AM'
 51: + set +x
 53: + END_FMT='2020-06-25 11:03:55 AM'
 53: + echo 'ENDING TIMING RUN AT 2020-06-25 11:03:55 AM'
 49: + END_FMT='2020-06-25 11:03:55 AM'
 53: ENDING TIMING RUN AT 2020-06-25 11:03:55 AM
 52: + END_FMT='2020-06-25 11:03:55 AM'
 53: + RESULT=295
 53: + RESULT_NAME=bert
 49: + echo 'ENDING TIMING RUN AT 2020-06-25 11:03:55 AM'
 49: ENDING TIMING RUN AT 2020-06-25 11:03:55 AM
 49: + RESULT=295
 49: + RESULT_NAME=bert
 52: ENDING TIMING RUN AT 2020-06-25 11:03:55 AM
 52: + echo 'ENDING TIMING RUN AT 2020-06-25 11:03:55 AM'
 53: RESULT,bert,18263,295,root,2020-06-25 10:59:00 AM
 53: + echo 'RESULT,bert,18263,295,root,2020-06-25 10:59:00 AM'
 53: + set +x
 49: RESULT,bert,18263,295,root,2020-06-25 10:59:00 AM
 49: + echo 'RESULT,bert,18263,295,root,2020-06-25 10:59:00 AM'
 49: + set +x
 52: + RESULT=295
 52: + RESULT_NAME=bert
 52: + echo 'RESULT,bert,18263,295,root,2020-06-25 10:59:00 AM'
 52: RESULT,bert,18263,295,root,2020-06-25 10:59:00 AM
 52: + set +x
 60: + END_FMT='2020-06-25 11:03:55 AM'
 55: + END_FMT='2020-06-25 11:03:55 AM'
 62: + END_FMT='2020-06-25 11:03:55 AM'
 60: ENDING TIMING RUN AT 2020-06-25 11:03:55 AM
 60: + echo 'ENDING TIMING RUN AT 2020-06-25 11:03:55 AM'
 55: ENDING TIMING RUN AT 2020-06-25 11:03:55 AM
 55: + echo 'ENDING TIMING RUN AT 2020-06-25 11:03:55 AM'
 60: + RESULT=295
 60: + RESULT_NAME=bert
 62: ENDING TIMING RUN AT 2020-06-25 11:03:55 AM
 62: + echo 'ENDING TIMING RUN AT 2020-06-25 11:03:55 AM'
 62: + RESULT=295
 62: + RESULT_NAME=bert
 55: + RESULT=295
 55: + RESULT_NAME=bert
 57: + END_FMT='2020-06-25 11:03:55 AM'
 60: RESULT,bert,18263,295,root,2020-06-25 10:59:00 AM
 60: + echo 'RESULT,bert,18263,295,root,2020-06-25 10:59:00 AM'
 60: + set +x
 55: RESULT,bert,18263,295,root,2020-06-25 10:59:00 AM
 55: + echo 'RESULT,bert,18263,295,root,2020-06-25 10:59:00 AM'
 55: + set +x
 62: RESULT,bert,18263,295,root,2020-06-25 10:59:00 AM
 62: + echo 'RESULT,bert,18263,295,root,2020-06-25 10:59:00 AM'
 62: + set +x
 57: ENDING TIMING RUN AT 2020-06-25 11:03:55 AM
 57: + echo 'ENDING TIMING RUN AT 2020-06-25 11:03:55 AM'
 57: + RESULT=295
 57: + RESULT_NAME=bert
 57: RESULT,bert,18263,295,root,2020-06-25 10:59:00 AM
 57: + echo 'RESULT,bert,18263,295,root,2020-06-25 10:59:00 AM'
 57: + set +x
355: ++ date +%s
352: ++ date +%s
353: ++ date +%s
357: ++ date +%s
358: ++ date +%s
359: ++ date +%s
355: + END=1593108235
361: ++ date +%s
365: ++ date +%s
367: ++ date +%s
363: ++ date +%s
360: ++ date +%s
366: ++ date +%s
352: + END=1593108235
353: + END=1593108235
355: ++ date '+%Y-%m-%d %r'
357: + END=1593108235
352: ++ date '+%Y-%m-%d %r'
255: ++ date +%s
353: ++ date '+%Y-%m-%d %r'
358: + END=1593108235
359: + END=1593108235
361: + END=1593108235
357: ++ date '+%Y-%m-%d %r'
367: + END=1593108235
363: + END=1593108235
365: + END=1593108235
366: + END=1593108235
360: + END=1593108235
362: ++ date +%s
355: + END_FMT='2020-06-25 11:03:55 AM'
361: ++ date '+%Y-%m-%d %r'
355: + echo 'ENDING TIMING RUN AT 2020-06-25 11:03:55 AM'
355: ENDING TIMING RUN AT 2020-06-25 11:03:55 AM
355: + RESULT=294
355: + RESULT_NAME=bert
355: RESULT,bert,18263,294,root,2020-06-25 10:59:01 AM
355: + echo 'RESULT,bert,18263,294,root,2020-06-25 10:59:01 AM'
355: + set +x
358: ++ date '+%Y-%m-%d %r'
359: ++ date '+%Y-%m-%d %r'
364: ++ date +%s
255: + END=1593108235
367: ++ date '+%Y-%m-%d %r'
365: ++ date '+%Y-%m-%d %r'
363: ++ date '+%Y-%m-%d %r'
366: ++ date '+%Y-%m-%d %r'
352: + END_FMT='2020-06-25 11:03:55 AM'
353: + END_FMT='2020-06-25 11:03:55 AM'
352: + echo 'ENDING TIMING RUN AT 2020-06-25 11:03:55 AM'
353: ENDING TIMING RUN AT 2020-06-25 11:03:55 AM
353: + echo 'ENDING TIMING RUN AT 2020-06-25 11:03:55 AM'
352: ENDING TIMING RUN AT 2020-06-25 11:03:55 AM
353: + RESULT=294
353: + RESULT_NAME=bert
360: ++ date '+%Y-%m-%d %r'
352: + RESULT=294
352: + RESULT_NAME=bert
353: RESULT,bert,18263,294,root,2020-06-25 10:59:01 AM
353: + echo 'RESULT,bert,18263,294,root,2020-06-25 10:59:01 AM'
353: + set +x
352: RESULT,bert,18263,294,root,2020-06-25 10:59:01 AM
352: + echo 'RESULT,bert,18263,294,root,2020-06-25 10:59:01 AM'
352: + set +x
255: ++ date '+%Y-%m-%d %r'
357: + END_FMT='2020-06-25 11:03:55 AM'
357: + echo 'ENDING TIMING RUN AT 2020-06-25 11:03:55 AM'
357: ENDING TIMING RUN AT 2020-06-25 11:03:55 AM
357: + RESULT=294
357: + RESULT_NAME=bert
357: RESULT,bert,18263,294,root,2020-06-25 10:59:01 AM
357: + echo 'RESULT,bert,18263,294,root,2020-06-25 10:59:01 AM'
357: + set +x
362: + END=1593108235
364: + END=1593108235
358: + END_FMT='2020-06-25 11:03:55 AM'
361: + END_FMT='2020-06-25 11:03:55 AM'
359: + END_FMT='2020-06-25 11:03:55 AM'
361: + echo 'ENDING TIMING RUN AT 2020-06-25 11:03:55 AM'
358: ENDING TIMING RUN AT 2020-06-25 11:03:55 AM
358: + echo 'ENDING TIMING RUN AT 2020-06-25 11:03:55 AM'
361: ENDING TIMING RUN AT 2020-06-25 11:03:55 AM
359: ENDING TIMING RUN AT 2020-06-25 11:03:55 AM
359: + echo 'ENDING TIMING RUN AT 2020-06-25 11:03:55 AM'
358: + RESULT=294
361: + RESULT=294
361: + RESULT_NAME=bert
358: + RESULT_NAME=bert
359: + RESULT=294
359: + RESULT_NAME=bert
361: RESULT,bert,18263,294,root,2020-06-25 10:59:01 AM
361: + echo 'RESULT,bert,18263,294,root,2020-06-25 10:59:01 AM'
361: + set +x
358: RESULT,bert,18263,294,root,2020-06-25 10:59:01 AM
358: + echo 'RESULT,bert,18263,294,root,2020-06-25 10:59:01 AM'
358: + set +x
359: RESULT,bert,18263,294,root,2020-06-25 10:59:01 AM
359: + echo 'RESULT,bert,18263,294,root,2020-06-25 10:59:01 AM'
359: + set +x
365: + END_FMT='2020-06-25 11:03:55 AM'
365: ENDING TIMING RUN AT 2020-06-25 11:03:55 AM
365: + echo 'ENDING TIMING RUN AT 2020-06-25 11:03:55 AM'
365: + RESULT=294
365: + RESULT_NAME=bert
365: RESULT,bert,18263,294,root,2020-06-25 10:59:01 AM
365: + echo 'RESULT,bert,18263,294,root,2020-06-25 10:59:01 AM'
365: + set +x
243: ++ date +%s
247: ++ date +%s
255: ENDING TIMING RUN AT 2020-06-25 11:03:55 AM
255: RESULT,bert,18263,294,root,2020-06-25 10:59:01 AM
255: + END_FMT='2020-06-25 11:03:55 AM'
255: + echo 'ENDING TIMING RUN AT 2020-06-25 11:03:55 AM'
255: + RESULT=294
255: + RESULT_NAME=bert
255: + echo 'RESULT,bert,18263,294,root,2020-06-25 10:59:01 AM'
255: + set +x
252: ++ date +%s
241: ++ date +%s
253: ++ date +%s
362: ++ date '+%Y-%m-%d %r'
245: ++ date +%s
367: ENDING TIMING RUN AT 2020-06-25 11:03:55 AM
367: + END_FMT='2020-06-25 11:03:55 AM'
367: + echo 'ENDING TIMING RUN AT 2020-06-25 11:03:55 AM'
367: + RESULT=294
367: + RESULT_NAME=bert
367: + echo 'RESULT,bert,18263,294,root,2020-06-25 10:59:01 AM'
367: RESULT,bert,18263,294,root,2020-06-25 10:59:01 AM
367: + set +x
363: + END_FMT='2020-06-25 11:03:55 AM'
364: ++ date '+%Y-%m-%d %r'
366: + END_FMT='2020-06-25 11:03:55 AM'
360: + END_FMT='2020-06-25 11:03:55 AM'
363: ENDING TIMING RUN AT 2020-06-25 11:03:55 AM
254: ++ date +%s
363: + echo 'ENDING TIMING RUN AT 2020-06-25 11:03:55 AM'
360: + echo 'ENDING TIMING RUN AT 2020-06-25 11:03:55 AM'
366: ENDING TIMING RUN AT 2020-06-25 11:03:55 AM
366: + echo 'ENDING TIMING RUN AT 2020-06-25 11:03:55 AM'
360: ENDING TIMING RUN AT 2020-06-25 11:03:55 AM
363: + RESULT=294
363: + RESULT_NAME=bert
360: + RESULT=294
360: + RESULT_NAME=bert
363: RESULT,bert,18263,294,root,2020-06-25 10:59:01 AM
363: + echo 'RESULT,bert,18263,294,root,2020-06-25 10:59:01 AM'
363: + set +x
366: + RESULT=294
366: + RESULT_NAME=bert
360: RESULT,bert,18263,294,root,2020-06-25 10:59:01 AM
360: + echo 'RESULT,bert,18263,294,root,2020-06-25 10:59:01 AM'
360: + set +x
366: RESULT,bert,18263,294,root,2020-06-25 10:59:01 AM
248: ++ date +%s
366: + echo 'RESULT,bert,18263,294,root,2020-06-25 10:59:01 AM'
366: + set +x
244: ++ date +%s
251: ++ date +%s
242: ++ date +%s
249: ++ date +%s
250: ++ date +%s
362: + END_FMT='2020-06-25 11:03:55 AM'
362: ENDING TIMING RUN AT 2020-06-25 11:03:55 AM
362: + echo 'ENDING TIMING RUN AT 2020-06-25 11:03:55 AM'
362: + RESULT=294
362: + RESULT_NAME=bert
362: RESULT,bert,18263,294,root,2020-06-25 10:59:01 AM
362: + echo 'RESULT,bert,18263,294,root,2020-06-25 10:59:01 AM'
362: + set +x
243: + END=1593108235
364: + END_FMT='2020-06-25 11:03:55 AM'
364: + echo 'ENDING TIMING RUN AT 2020-06-25 11:03:55 AM'
364: ENDING TIMING RUN AT 2020-06-25 11:03:55 AM
364: + RESULT=294
247: + END=1593108235
364: RESULT,bert,18263,294,root,2020-06-25 10:59:01 AM
364: + RESULT_NAME=bert
364: + echo 'RESULT,bert,18263,294,root,2020-06-25 10:59:01 AM'
364: + set +x
241: + END=1593108235
245: + END=1593108235
252: + END=1593108235
253: + END=1593108235
243: ++ date '+%Y-%m-%d %r'
254: + END=1593108235
248: + END=1593108235
247: ++ date '+%Y-%m-%d %r'
244: + END=1593108235
242: + END=1593108235
251: + END=1593108235
245: ++ date '+%Y-%m-%d %r'
249: + END=1593108235
250: + END=1593108235
253: ++ date '+%Y-%m-%d %r'
241: ++ date '+%Y-%m-%d %r'
252: ++ date '+%Y-%m-%d %r'
248: ++ date '+%Y-%m-%d %r'
254: ++ date '+%Y-%m-%d %r'
242: ++ date '+%Y-%m-%d %r'
244: ++ date '+%Y-%m-%d %r'
249: ++ date '+%Y-%m-%d %r'
250: ++ date '+%Y-%m-%d %r'
251: ++ date '+%Y-%m-%d %r'
243: + END_FMT='2020-06-25 11:03:55 AM'
243: + echo 'ENDING TIMING RUN AT 2020-06-25 11:03:55 AM'
243: ENDING TIMING RUN AT 2020-06-25 11:03:55 AM
243: + RESULT=294
243: + RESULT_NAME=bert
243: RESULT,bert,18263,294,root,2020-06-25 10:59:01 AM
243: + echo 'RESULT,bert,18263,294,root,2020-06-25 10:59:01 AM'
243: + set +x
247: ENDING TIMING RUN AT 2020-06-25 11:03:55 AM
247: + END_FMT='2020-06-25 11:03:55 AM'
247: + echo 'ENDING TIMING RUN AT 2020-06-25 11:03:55 AM'
247: + RESULT=294
247: + RESULT_NAME=bert
247: RESULT,bert,18263,294,root,2020-06-25 10:59:01 AM
247: + echo 'RESULT,bert,18263,294,root,2020-06-25 10:59:01 AM'
247: + set +x
245: + END_FMT='2020-06-25 11:03:55 AM'
245: ENDING TIMING RUN AT 2020-06-25 11:03:55 AM
245: + echo 'ENDING TIMING RUN AT 2020-06-25 11:03:55 AM'
245: + RESULT=294
245: + RESULT_NAME=bert
245: RESULT,bert,18263,294,root,2020-06-25 10:59:01 AM
245: + echo 'RESULT,bert,18263,294,root,2020-06-25 10:59:01 AM'
245: + set +x
253: + END_FMT='2020-06-25 11:03:55 AM'
241: + END_FMT='2020-06-25 11:03:55 AM'
252: + END_FMT='2020-06-25 11:03:55 AM'
254: + END_FMT='2020-06-25 11:03:55 AM'
242: + END_FMT='2020-06-25 11:03:55 AM'
253: ENDING TIMING RUN AT 2020-06-25 11:03:55 AM
253: + echo 'ENDING TIMING RUN AT 2020-06-25 11:03:55 AM'
241: ENDING TIMING RUN AT 2020-06-25 11:03:55 AM
241: + echo 'ENDING TIMING RUN AT 2020-06-25 11:03:55 AM'
241: + RESULT=294
252: ENDING TIMING RUN AT 2020-06-25 11:03:55 AM
252: + echo 'ENDING TIMING RUN AT 2020-06-25 11:03:55 AM'
253: + RESULT=294
253: + RESULT_NAME=bert
254: ENDING TIMING RUN AT 2020-06-25 11:03:55 AM
254: + echo 'ENDING TIMING RUN AT 2020-06-25 11:03:55 AM'
254: + RESULT=294
254: + RESULT_NAME=bert
241: RESULT,bert,18263,294,root,2020-06-25 10:59:01 AM
241: + RESULT_NAME=bert
241: + echo 'RESULT,bert,18263,294,root,2020-06-25 10:59:01 AM'
241: + set +x
242: ENDING TIMING RUN AT 2020-06-25 11:03:55 AM
242: + echo 'ENDING TIMING RUN AT 2020-06-25 11:03:55 AM'
242: + RESULT=294
242: + RESULT_NAME=bert
242: + echo 'RESULT,bert,18263,294,root,2020-06-25 10:59:01 AM'
248: + END_FMT='2020-06-25 11:03:55 AM'
248: + echo 'ENDING TIMING RUN AT 2020-06-25 11:03:55 AM'
252: + RESULT=294
252: + RESULT_NAME=bert
252: + echo 'RESULT,bert,18263,294,root,2020-06-25 10:59:01 AM'
252: + set +x
253: RESULT,bert,18263,294,root,2020-06-25 10:59:01 AM
253: + echo 'RESULT,bert,18263,294,root,2020-06-25 10:59:01 AM'
253: + set +x
254: RESULT,bert,18263,294,root,2020-06-25 10:59:01 AM
254: + echo 'RESULT,bert,18263,294,root,2020-06-25 10:59:01 AM'
254: + set +x
242: RESULT,bert,18263,294,root,2020-06-25 10:59:01 AM
242: + set +x
244: ENDING TIMING RUN AT 2020-06-25 11:03:55 AM
244: + END_FMT='2020-06-25 11:03:55 AM'
244: + echo 'ENDING TIMING RUN AT 2020-06-25 11:03:55 AM'
244: + RESULT=294
248: ENDING TIMING RUN AT 2020-06-25 11:03:55 AM
248: RESULT,bert,18263,294,root,2020-06-25 10:59:01 AM
248: + RESULT=294
248: + RESULT_NAME=bert
248: + echo 'RESULT,bert,18263,294,root,2020-06-25 10:59:01 AM'
248: + set +x
252: RESULT,bert,18263,294,root,2020-06-25 10:59:01 AM
244: RESULT,bert,18263,294,root,2020-06-25 10:59:01 AM
244: + RESULT_NAME=bert
244: + echo 'RESULT,bert,18263,294,root,2020-06-25 10:59:01 AM'
244: + set +x
249: + END_FMT='2020-06-25 11:03:55 AM'
251: + END_FMT='2020-06-25 11:03:55 AM'
249: + echo 'ENDING TIMING RUN AT 2020-06-25 11:03:55 AM'
249: ENDING TIMING RUN AT 2020-06-25 11:03:55 AM
251: ENDING TIMING RUN AT 2020-06-25 11:03:55 AM
251: + echo 'ENDING TIMING RUN AT 2020-06-25 11:03:55 AM'
249: + RESULT=294
249: + RESULT_NAME=bert
251: + RESULT=294
251: + RESULT_NAME=bert
249: RESULT,bert,18263,294,root,2020-06-25 10:59:01 AM
249: + echo 'RESULT,bert,18263,294,root,2020-06-25 10:59:01 AM'
249: + set +x
250: + END_FMT='2020-06-25 11:03:55 AM'
251: RESULT,bert,18263,294,root,2020-06-25 10:59:01 AM
251: + echo 'RESULT,bert,18263,294,root,2020-06-25 10:59:01 AM'
251: + set +x
250: + echo 'ENDING TIMING RUN AT 2020-06-25 11:03:55 AM'
250: ENDING TIMING RUN AT 2020-06-25 11:03:55 AM
250: + RESULT=294
250: + RESULT_NAME=bert
250: RESULT,bert,18263,294,root,2020-06-25 10:59:01 AM
250: + echo 'RESULT,bert,18263,294,root,2020-06-25 10:59:01 AM'
250: + set +x
146: ++ date +%s
153: ++ date +%s
157: ++ date +%s
148: ++ date +%s
145: ++ date +%s
151: ++ date +%s
146: + END=1593108235
147: ++ date +%s
152: ++ date +%s
156: ++ date +%s
144: ++ date +%s
150: ++ date +%s
158: ++ date +%s
159: ++ date +%s
153: + END=1593108235
157: + END=1593108235
148: + END=1593108235
146: ++ date '+%Y-%m-%d %r'
149: ++ date +%s
145: + END=1593108235
129: ++ date +%s
153: ++ date '+%Y-%m-%d %r'
157: ++ date '+%Y-%m-%d %r'
133: ++ date +%s
141: ++ date +%s
148: ++ date '+%Y-%m-%d %r'
151: + END=1593108235
156: + END=1593108235
147: + END=1593108235
152: + END=1593108235
145: ++ date '+%Y-%m-%d %r'
135: ++ date +%s
139: ++ date +%s
150: + END=1593108235
158: + END=1593108235
128: ++ date +%s
131: ++ date +%s
144: + END=1593108235
137: ++ date +%s
140: ++ date +%s
159: + END=1593108235
146: ENDING TIMING RUN AT 2020-06-25 11:03:55 AM
143: ++ date +%s
146: + END_FMT='2020-06-25 11:03:55 AM'
142: ++ date +%s
134: ++ date +%s
138: ++ date +%s
146: + echo 'ENDING TIMING RUN AT 2020-06-25 11:03:55 AM'
130: ++ date +%s
146: + RESULT=294
146: + RESULT_NAME=bert
151: ++ date '+%Y-%m-%d %r'
146: RESULT,bert,18263,294,root,2020-06-25 10:59:01 AM
156: ++ date '+%Y-%m-%d %r'
129: + END=1593108235
146: + echo 'RESULT,bert,18263,294,root,2020-06-25 10:59:01 AM'
146: + set +x
152: ++ date '+%Y-%m-%d %r'
149: + END=1593108235
147: ++ date '+%Y-%m-%d %r'
150: ++ date '+%Y-%m-%d %r'
133: + END=1593108235
141: + END=1593108235
153: + END_FMT='2020-06-25 11:03:55 AM'
153: ENDING TIMING RUN AT 2020-06-25 11:03:55 AM
153: + echo 'ENDING TIMING RUN AT 2020-06-25 11:03:55 AM'
153: + RESULT=294
153: + RESULT_NAME=bert
153: RESULT,bert,18263,294,root,2020-06-25 10:59:01 AM
153: + echo 'RESULT,bert,18263,294,root,2020-06-25 10:59:01 AM'
153: + set +x
144: ++ date '+%Y-%m-%d %r'
148: + END_FMT='2020-06-25 11:03:55 AM'
157: + END_FMT='2020-06-25 11:03:55 AM'
157: ENDING TIMING RUN AT 2020-06-25 11:03:55 AM
157: + echo 'ENDING TIMING RUN AT 2020-06-25 11:03:55 AM'
148: ENDING TIMING RUN AT 2020-06-25 11:03:55 AM
148: + echo 'ENDING TIMING RUN AT 2020-06-25 11:03:55 AM'
157: + RESULT=294
157: + RESULT_NAME=bert
158: ++ date '+%Y-%m-%d %r'
148: + RESULT=294
148: + RESULT_NAME=bert
157: + echo 'RESULT,bert,18263,294,root,2020-06-25 10:59:01 AM'
159: ++ date '+%Y-%m-%d %r'
148: RESULT,bert,18263,294,root,2020-06-25 10:59:01 AM
129: ++ date '+%Y-%m-%d %r'
148: + echo 'RESULT,bert,18263,294,root,2020-06-25 10:59:01 AM'
148: + set +x
157: RESULT,bert,18263,294,root,2020-06-25 10:59:01 AM
157: + set +x
149: ++ date '+%Y-%m-%d %r'
145: + END_FMT='2020-06-25 11:03:55 AM'
133: ++ date '+%Y-%m-%d %r'
145: ENDING TIMING RUN AT 2020-06-25 11:03:55 AM
135: + END=1593108235
145: + echo 'ENDING TIMING RUN AT 2020-06-25 11:03:55 AM'
145: + RESULT=294
145: + RESULT_NAME=bert
145: RESULT,bert,18263,294,root,2020-06-25 10:59:01 AM
141: ++ date '+%Y-%m-%d %r'
145: + echo 'RESULT,bert,18263,294,root,2020-06-25 10:59:01 AM'
145: + set +x
128: + END=1593108235
135: ++ date '+%Y-%m-%d %r'
152: + END_FMT='2020-06-25 11:03:55 AM'
140: + END=1593108235
147: + END_FMT='2020-06-25 11:03:55 AM'
147: + echo 'ENDING TIMING RUN AT 2020-06-25 11:03:55 AM'
152: ENDING TIMING RUN AT 2020-06-25 11:03:55 AM
142: + END=1593108235
152: + echo 'ENDING TIMING RUN AT 2020-06-25 11:03:55 AM'
147: ENDING TIMING RUN AT 2020-06-25 11:03:55 AM
134: + END=1593108235
147: + RESULT=294
147: + RESULT_NAME=bert
139: + END=1593108235
150: + END_FMT='2020-06-25 11:03:55 AM'
147: RESULT,bert,18263,294,root,2020-06-25 10:59:01 AM
143: + END=1593108235
152: + RESULT=294
152: + RESULT_NAME=bert
150: ENDING TIMING RUN AT 2020-06-25 11:03:55 AM
130: + END=1593108235
131: + END=1593108235
147: + echo 'RESULT,bert,18263,294,root,2020-06-25 10:59:01 AM'
147: + set +x
152: RESULT,bert,18263,294,root,2020-06-25 10:59:01 AM
137: + END=1593108235
150: + echo 'ENDING TIMING RUN AT 2020-06-25 11:03:55 AM'
150: + RESULT=294
150: RESULT,bert,18263,294,root,2020-06-25 10:59:01 AM
138: + END=1593108235
128: ++ date '+%Y-%m-%d %r'
152: + echo 'RESULT,bert,18263,294,root,2020-06-25 10:59:01 AM'
152: + set +x
129: ENDING TIMING RUN AT 2020-06-25 11:03:55 AM
150: + RESULT_NAME=bert
150: + echo 'RESULT,bert,18263,294,root,2020-06-25 10:59:01 AM'
150: + set +x
129: + END_FMT='2020-06-25 11:03:55 AM'
151: + END_FMT='2020-06-25 11:03:55 AM'
156: + END_FMT='2020-06-25 11:03:55 AM'
144: ENDING TIMING RUN AT 2020-06-25 11:03:55 AM
144: + END_FMT='2020-06-25 11:03:55 AM'
151: + echo 'ENDING TIMING RUN AT 2020-06-25 11:03:55 AM'
156: + echo 'ENDING TIMING RUN AT 2020-06-25 11:03:55 AM'
129: + echo 'ENDING TIMING RUN AT 2020-06-25 11:03:55 AM'
151: ENDING TIMING RUN AT 2020-06-25 11:03:55 AM
142: ++ date '+%Y-%m-%d %r'
144: + echo 'ENDING TIMING RUN AT 2020-06-25 11:03:55 AM'
129: + RESULT=295
129: + RESULT_NAME=bert
140: ++ date '+%Y-%m-%d %r'
129: RESULT,bert,18263,295,root,2020-06-25 10:59:00 AM
129: + echo 'RESULT,bert,18263,295,root,2020-06-25 10:59:00 AM'
156: ENDING TIMING RUN AT 2020-06-25 11:03:55 AM
129: + set +x
144: + RESULT=294
144: + RESULT_NAME=bert
144: + echo 'RESULT,bert,18263,294,root,2020-06-25 10:59:01 AM'
151: + RESULT=294
151: + RESULT_NAME=bert
156: + RESULT=294
156: + RESULT_NAME=bert
144: RESULT,bert,18263,294,root,2020-06-25 10:59:01 AM
134: ++ date '+%Y-%m-%d %r'
144: + set +x
151: RESULT,bert,18263,294,root,2020-06-25 10:59:01 AM
151: + echo 'RESULT,bert,18263,294,root,2020-06-25 10:59:01 AM'
151: + set +x
156: RESULT,bert,18263,294,root,2020-06-25 10:59:01 AM
139: ++ date '+%Y-%m-%d %r'
156: + echo 'RESULT,bert,18263,294,root,2020-06-25 10:59:01 AM'
156: + set +x
158: + END_FMT='2020-06-25 11:03:55 AM'
158: ENDING TIMING RUN AT 2020-06-25 11:03:55 AM
159: + END_FMT='2020-06-25 11:03:55 AM'
158: + echo 'ENDING TIMING RUN AT 2020-06-25 11:03:55 AM'
141: + END_FMT='2020-06-25 11:03:55 AM'
159: + echo 'ENDING TIMING RUN AT 2020-06-25 11:03:55 AM'
159: ENDING TIMING RUN AT 2020-06-25 11:03:55 AM
158: + RESULT=294
158: + RESULT_NAME=bert
133: + END_FMT='2020-06-25 11:03:55 AM'
159: + RESULT=294
159: + RESULT_NAME=bert
141: + echo 'ENDING TIMING RUN AT 2020-06-25 11:03:55 AM'
158: RESULT,bert,18263,294,root,2020-06-25 10:59:01 AM
158: + echo 'RESULT,bert,18263,294,root,2020-06-25 10:59:01 AM'
158: + set +x
131: ++ date '+%Y-%m-%d %r'
133: ENDING TIMING RUN AT 2020-06-25 11:03:55 AM
159: + echo 'RESULT,bert,18263,294,root,2020-06-25 10:59:01 AM'
159: + set +x
149: + END_FMT='2020-06-25 11:03:55 AM'
141: ENDING TIMING RUN AT 2020-06-25 11:03:55 AM
133: + echo 'ENDING TIMING RUN AT 2020-06-25 11:03:55 AM'
159: RESULT,bert,18263,294,root,2020-06-25 10:59:01 AM
143: ++ date '+%Y-%m-%d %r'
149: ENDING TIMING RUN AT 2020-06-25 11:03:55 AM
149: + echo 'ENDING TIMING RUN AT 2020-06-25 11:03:55 AM'
130: ++ date '+%Y-%m-%d %r'
149: + RESULT=294
149: + RESULT_NAME=bert
149: RESULT,bert,18263,294,root,2020-06-25 10:59:01 AM
133: + RESULT=295
133: + RESULT_NAME=bert
133: + echo 'RESULT,bert,18263,295,root,2020-06-25 10:59:00 AM'
149: + echo 'RESULT,bert,18263,294,root,2020-06-25 10:59:01 AM'
141: RESULT,bert,18263,295,root,2020-06-25 10:59:00 AM
137: ++ date '+%Y-%m-%d %r'
138: ++ date '+%Y-%m-%d %r'
149: + set +x
141: + RESULT=295
141: + RESULT_NAME=bert
141: + echo 'RESULT,bert,18263,295,root,2020-06-25 10:59:00 AM'
141: + set +x
133: RESULT,bert,18263,295,root,2020-06-25 10:59:00 AM
133: + set +x
135: + END_FMT='2020-06-25 11:03:55 AM'
135: ENDING TIMING RUN AT 2020-06-25 11:03:55 AM
135: + echo 'ENDING TIMING RUN AT 2020-06-25 11:03:55 AM'
135: + RESULT=295
135: + RESULT_NAME=bert
135: RESULT,bert,18263,295,root,2020-06-25 10:59:00 AM
135: + echo 'RESULT,bert,18263,295,root,2020-06-25 10:59:00 AM'
135: + set +x
128: ENDING TIMING RUN AT 2020-06-25 11:03:55 AM
128: + END_FMT='2020-06-25 11:03:55 AM'
128: + echo 'ENDING TIMING RUN AT 2020-06-25 11:03:55 AM'
128: + RESULT=295
128: + RESULT_NAME=bert
128: + echo 'RESULT,bert,18263,295,root,2020-06-25 10:59:00 AM'
128: + set +x
128: RESULT,bert,18263,295,root,2020-06-25 10:59:00 AM
142: + END_FMT='2020-06-25 11:03:55 AM'
140: + END_FMT='2020-06-25 11:03:55 AM'
142: ENDING TIMING RUN AT 2020-06-25 11:03:55 AM
142: + echo 'ENDING TIMING RUN AT 2020-06-25 11:03:55 AM'
142: + RESULT=295
142: + RESULT_NAME=bert
140: ENDING TIMING RUN AT 2020-06-25 11:03:55 AM
140: + echo 'ENDING TIMING RUN AT 2020-06-25 11:03:55 AM'
140: + RESULT=295
140: RESULT,bert,18263,295,root,2020-06-25 10:59:00 AM
140: + RESULT_NAME=bert
140: + echo 'RESULT,bert,18263,295,root,2020-06-25 10:59:00 AM'
140: + set +x
142: RESULT,bert,18263,295,root,2020-06-25 10:59:00 AM
142: + echo 'RESULT,bert,18263,295,root,2020-06-25 10:59:00 AM'
142: + set +x
134: + END_FMT='2020-06-25 11:03:55 AM'
134: ENDING TIMING RUN AT 2020-06-25 11:03:55 AM
134: + echo 'ENDING TIMING RUN AT 2020-06-25 11:03:55 AM'
139: + END_FMT='2020-06-25 11:03:55 AM'
134: + RESULT=295
134: + RESULT_NAME=bert
139: + echo 'ENDING TIMING RUN AT 2020-06-25 11:03:55 AM'
139: ENDING TIMING RUN AT 2020-06-25 11:03:55 AM
134: + echo 'RESULT,bert,18263,295,root,2020-06-25 10:59:00 AM'
134: RESULT,bert,18263,295,root,2020-06-25 10:59:00 AM
139: + RESULT=295
139: + RESULT_NAME=bert
134: + set +x
139: RESULT,bert,18263,295,root,2020-06-25 10:59:00 AM
139: + echo 'RESULT,bert,18263,295,root,2020-06-25 10:59:00 AM'
139: + set +x
137: + END_FMT='2020-06-25 11:03:55 AM'
137: ENDING TIMING RUN AT 2020-06-25 11:03:55 AM
137: + echo 'ENDING TIMING RUN AT 2020-06-25 11:03:55 AM'
138: + END_FMT='2020-06-25 11:03:55 AM'
137: + RESULT=295
137: + RESULT_NAME=bert
137: RESULT,bert,18263,295,root,2020-06-25 10:59:00 AM
137: + echo 'RESULT,bert,18263,295,root,2020-06-25 10:59:00 AM'
137: + set +x
138: ENDING TIMING RUN AT 2020-06-25 11:03:55 AM
138: + echo 'ENDING TIMING RUN AT 2020-06-25 11:03:55 AM'
131: + END_FMT='2020-06-25 11:03:55 AM'
138: + RESULT=295
138: + RESULT_NAME=bert
138: + echo 'RESULT,bert,18263,295,root,2020-06-25 10:59:00 AM'
143: + END_FMT='2020-06-25 11:03:55 AM'
130: + END_FMT='2020-06-25 11:03:55 AM'
131: ENDING TIMING RUN AT 2020-06-25 11:03:55 AM
131: + echo 'ENDING TIMING RUN AT 2020-06-25 11:03:55 AM'
138: RESULT,bert,18263,295,root,2020-06-25 10:59:00 AM
138: + set +x
130: ENDING TIMING RUN AT 2020-06-25 11:03:55 AM
130: + echo 'ENDING TIMING RUN AT 2020-06-25 11:03:55 AM'
131: + RESULT=295
131: + RESULT_NAME=bert
131: + echo 'RESULT,bert,18263,295,root,2020-06-25 10:59:00 AM'
143: ENDING TIMING RUN AT 2020-06-25 11:03:55 AM
143: + echo 'ENDING TIMING RUN AT 2020-06-25 11:03:55 AM'
143: + RESULT=295
143: + RESULT_NAME=bert
130: + RESULT=295
130: + RESULT_NAME=bert
131: RESULT,bert,18263,295,root,2020-06-25 10:59:00 AM
131: + set +x
143: + echo 'RESULT,bert,18263,295,root,2020-06-25 10:59:00 AM'
143: + set +x
130: RESULT,bert,18263,295,root,2020-06-25 10:59:00 AM
130: + echo 'RESULT,bert,18263,295,root,2020-06-25 10:59:00 AM'
130: + set +x
143: RESULT,bert,18263,295,root,2020-06-25 10:59:00 AM
490: ++ date +%s
488: ++ date +%s
495: ++ date +%s
482: ++ date +%s
492: ++ date +%s
493: ++ date +%s
481: ++ date +%s
487: ++ date +%s
490: + END=1593108235
483: ++ date +%s
485: ++ date +%s
486: ++ date +%s
488: + END=1593108235
491: ++ date +%s
480: ++ date +%s
490: ++ date '+%Y-%m-%d %r'
495: + END=1593108235
482: + END=1593108235
488: ++ date '+%Y-%m-%d %r'
492: + END=1593108235
493: + END=1593108235
481: + END=1593108235
482: ++ date '+%Y-%m-%d %r'
487: + END=1593108235
483: + END=1593108235
495: ++ date '+%Y-%m-%d %r'
492: ++ date '+%Y-%m-%d %r'
485: + END=1593108235
486: + END=1593108235
491: + END=1593108235
493: ++ date '+%Y-%m-%d %r'
480: + END=1593108235
490: + END_FMT='2020-06-25 11:03:55 AM'
481: ++ date '+%Y-%m-%d %r'
490: + echo 'ENDING TIMING RUN AT 2020-06-25 11:03:55 AM'
490: ENDING TIMING RUN AT 2020-06-25 11:03:55 AM
487: ++ date '+%Y-%m-%d %r'
490: + RESULT=294
490: + RESULT_NAME=bert
490: RESULT,bert,18263,294,root,2020-06-25 10:59:01 AM
490: + echo 'RESULT,bert,18263,294,root,2020-06-25 10:59:01 AM'
490: + set +x
484: ++ date +%s
488: + END_FMT='2020-06-25 11:03:55 AM'
483: ++ date '+%Y-%m-%d %r'
488: + echo 'ENDING TIMING RUN AT 2020-06-25 11:03:55 AM'
488: ENDING TIMING RUN AT 2020-06-25 11:03:55 AM
488: + RESULT=294
488: + RESULT_NAME=bert
488: RESULT,bert,18263,294,root,2020-06-25 10:59:01 AM
488: + echo 'RESULT,bert,18263,294,root,2020-06-25 10:59:01 AM'
488: + set +x
485: ++ date '+%Y-%m-%d %r'
486: ++ date '+%Y-%m-%d %r'
491: ++ date '+%Y-%m-%d %r'
480: ++ date '+%Y-%m-%d %r'
482: + END_FMT='2020-06-25 11:03:55 AM'
482: ENDING TIMING RUN AT 2020-06-25 11:03:55 AM
482: + echo 'ENDING TIMING RUN AT 2020-06-25 11:03:55 AM'
482: + RESULT=294
482: + RESULT_NAME=bert
482: RESULT,bert,18263,294,root,2020-06-25 10:59:01 AM
482: + echo 'RESULT,bert,18263,294,root,2020-06-25 10:59:01 AM'
482: + set +x
492: + END_FMT='2020-06-25 11:03:55 AM'
495: + END_FMT='2020-06-25 11:03:55 AM'
493: + END_FMT='2020-06-25 11:03:55 AM'
492: + echo 'ENDING TIMING RUN AT 2020-06-25 11:03:55 AM'
495: ENDING TIMING RUN AT 2020-06-25 11:03:55 AM
495: + echo 'ENDING TIMING RUN AT 2020-06-25 11:03:55 AM'
492: ENDING TIMING RUN AT 2020-06-25 11:03:55 AM
493: ENDING TIMING RUN AT 2020-06-25 11:03:55 AM
493: + echo 'ENDING TIMING RUN AT 2020-06-25 11:03:55 AM'
492: + RESULT=294
492: + RESULT_NAME=bert
495: + RESULT=294
495: + RESULT_NAME=bert
495: + echo 'RESULT,bert,18263,294,root,2020-06-25 10:59:01 AM'
493: + RESULT=294
493: + RESULT_NAME=bert
495: RESULT,bert,18263,294,root,2020-06-25 10:59:01 AM
495: + set +x
492: RESULT,bert,18263,294,root,2020-06-25 10:59:01 AM
492: + echo 'RESULT,bert,18263,294,root,2020-06-25 10:59:01 AM'
492: + set +x
493: RESULT,bert,18263,294,root,2020-06-25 10:59:01 AM
493: + echo 'RESULT,bert,18263,294,root,2020-06-25 10:59:01 AM'
493: + set +x
481: + END_FMT='2020-06-25 11:03:55 AM'
481: ENDING TIMING RUN AT 2020-06-25 11:03:55 AM
481: + echo 'ENDING TIMING RUN AT 2020-06-25 11:03:55 AM'
481: + RESULT=294
481: + RESULT_NAME=bert
481: RESULT,bert,18263,294,root,2020-06-25 10:59:01 AM
481: + echo 'RESULT,bert,18263,294,root,2020-06-25 10:59:01 AM'
481: + set +x
487: + END_FMT='2020-06-25 11:03:55 AM'
487: + echo 'ENDING TIMING RUN AT 2020-06-25 11:03:55 AM'
487: ENDING TIMING RUN AT 2020-06-25 11:03:55 AM
487: + RESULT=294
487: + RESULT_NAME=bert
487: RESULT,bert,18263,294,root,2020-06-25 10:59:01 AM
487: + echo 'RESULT,bert,18263,294,root,2020-06-25 10:59:01 AM'
487: + set +x
484: + END=1593108235
483: + END_FMT='2020-06-25 11:03:55 AM'
483: + echo 'ENDING TIMING RUN AT 2020-06-25 11:03:55 AM'
483: ENDING TIMING RUN AT 2020-06-25 11:03:55 AM
485: + END_FMT='2020-06-25 11:03:55 AM'
483: + RESULT=294
483: + RESULT_NAME=bert
483: RESULT,bert,18263,294,root,2020-06-25 10:59:01 AM
483: + echo 'RESULT,bert,18263,294,root,2020-06-25 10:59:01 AM'
485: + echo 'ENDING TIMING RUN AT 2020-06-25 11:03:55 AM'
483: + set +x
485: ENDING TIMING RUN AT 2020-06-25 11:03:55 AM
485: + RESULT=294
485: + RESULT_NAME=bert
485: RESULT,bert,18263,294,root,2020-06-25 10:59:01 AM
485: + echo 'RESULT,bert,18263,294,root,2020-06-25 10:59:01 AM'
485: + set +x
486: + END_FMT='2020-06-25 11:03:55 AM'
480: + END_FMT='2020-06-25 11:03:55 AM'
486: + echo 'ENDING TIMING RUN AT 2020-06-25 11:03:55 AM'
480: + echo 'ENDING TIMING RUN AT 2020-06-25 11:03:55 AM'
486: ENDING TIMING RUN AT 2020-06-25 11:03:55 AM
480: ENDING TIMING RUN AT 2020-06-25 11:03:55 AM
491: + END_FMT='2020-06-25 11:03:55 AM'
480: + RESULT=294
486: + RESULT=294
486: + RESULT_NAME=bert
480: + RESULT_NAME=bert
486: RESULT,bert,18263,294,root,2020-06-25 10:59:01 AM
486: + echo 'RESULT,bert,18263,294,root,2020-06-25 10:59:01 AM'
486: + set +x
491: ENDING TIMING RUN AT 2020-06-25 11:03:55 AM
491: + echo 'ENDING TIMING RUN AT 2020-06-25 11:03:55 AM'
480: RESULT,bert,18263,294,root,2020-06-25 10:59:01 AM
480: + echo 'RESULT,bert,18263,294,root,2020-06-25 10:59:01 AM'
480: + set +x
491: + RESULT=294
491: + RESULT_NAME=bert
491: RESULT,bert,18263,294,root,2020-06-25 10:59:01 AM
491: + echo 'RESULT,bert,18263,294,root,2020-06-25 10:59:01 AM'
491: + set +x
484: ++ date '+%Y-%m-%d %r'
484: + END_FMT='2020-06-25 11:03:55 AM'
484: ENDING TIMING RUN AT 2020-06-25 11:03:55 AM
484: + echo 'ENDING TIMING RUN AT 2020-06-25 11:03:55 AM'
484: + RESULT=294
484: + RESULT_NAME=bert
484: RESULT,bert,18263,294,root,2020-06-25 10:59:01 AM
484: + echo 'RESULT,bert,18263,294,root,2020-06-25 10:59:01 AM'
484: + set +x
505: ++ date +%s
505: + END=1593108235
496: ++ date +%s
499: ++ date +%s
498: ++ date +%s
504: ++ date +%s
507: ++ date +%s
505: ++ date '+%Y-%m-%d %r'
509: ++ date +%s
506: ++ date +%s
510: ++ date +%s
500: ++ date +%s
502: ++ date +%s
372: ++ date +%s
508: ++ date +%s
497: ++ date +%s
503: ++ date +%s
380: ++ date +%s
499: + END=1593108235
498: + END=1593108235
496: + END=1593108235
504: + END=1593108235
507: + END=1593108235
499: ++ date '+%Y-%m-%d %r'
377: ++ date +%s
505: + END_FMT='2020-06-25 11:03:55 AM'
505: ENDING TIMING RUN AT 2020-06-25 11:03:55 AM
505: + echo 'ENDING TIMING RUN AT 2020-06-25 11:03:55 AM'
505: + RESULT=295
505: + RESULT_NAME=bert
505: RESULT,bert,18263,295,root,2020-06-25 10:59:00 AM
370: ++ date +%s
505: + echo 'RESULT,bert,18263,295,root,2020-06-25 10:59:00 AM'
505: + set +x
378: ++ date +%s
383: ++ date +%s
509: + END=1593108235
379: ++ date +%s
372: + END=1593108235
498: ++ date '+%Y-%m-%d %r'
375: ++ date +%s
380: + END=1593108235
496: ++ date '+%Y-%m-%d %r'
504: ++ date '+%Y-%m-%d %r'
507: ++ date '+%Y-%m-%d %r'
376: ++ date +%s
506: + END=1593108235
510: + END=1593108235
502: + END=1593108235
371: ++ date +%s
509: ++ date '+%Y-%m-%d %r'
373: ++ date +%s
374: ++ date +%s
368: ++ date +%s
382: ++ date +%s
508: + END=1593108235
497: + END=1593108235
500: + END=1593108235
372: ++ date '+%Y-%m-%d %r'
503: + END=1593108235
380: ++ date '+%Y-%m-%d %r'
377: + END=1593108235
499: + END_FMT='2020-06-25 11:03:55 AM'
506: ++ date '+%Y-%m-%d %r'
499: + echo 'ENDING TIMING RUN AT 2020-06-25 11:03:55 AM'
499: ENDING TIMING RUN AT 2020-06-25 11:03:55 AM
510: ++ date '+%Y-%m-%d %r'
499: + RESULT=295
499: + RESULT_NAME=bert
499: RESULT,bert,18263,295,root,2020-06-25 10:59:00 AM
378: + END=1593108235
499: + echo 'RESULT,bert,18263,295,root,2020-06-25 10:59:00 AM'
499: + set +x
502: ++ date '+%Y-%m-%d %r'
508: ++ date '+%Y-%m-%d %r'
500: ++ date '+%Y-%m-%d %r'
498: + END_FMT='2020-06-25 11:03:55 AM'
498: + echo 'ENDING TIMING RUN AT 2020-06-25 11:03:55 AM'
498: ENDING TIMING RUN AT 2020-06-25 11:03:55 AM
496: + END_FMT='2020-06-25 11:03:55 AM'
498: + RESULT=295
377: ++ date '+%Y-%m-%d %r'
498: + RESULT_NAME=bert
503: ++ date '+%Y-%m-%d %r'
496: ENDING TIMING RUN AT 2020-06-25 11:03:55 AM
496: + echo 'ENDING TIMING RUN AT 2020-06-25 11:03:55 AM'
498: RESULT,bert,18263,295,root,2020-06-25 10:59:00 AM
498: + echo 'RESULT,bert,18263,295,root,2020-06-25 10:59:00 AM'
498: + set +x
496: + RESULT=295
496: + RESULT_NAME=bert
370: + END=1593108235
497: ++ date '+%Y-%m-%d %r'
496: RESULT,bert,18263,295,root,2020-06-25 10:59:00 AM
496: + echo 'RESULT,bert,18263,295,root,2020-06-25 10:59:00 AM'
496: + set +x
375: + END=1593108235
378: ++ date '+%Y-%m-%d %r'
379: + END=1593108235
383: + END=1593108235
504: + END_FMT='2020-06-25 11:03:55 AM'
504: ENDING TIMING RUN AT 2020-06-25 11:03:55 AM
507: + END_FMT='2020-06-25 11:03:55 AM'
504: + echo 'ENDING TIMING RUN AT 2020-06-25 11:03:55 AM'
504: + RESULT=295
507: + echo 'ENDING TIMING RUN AT 2020-06-25 11:03:55 AM'
504: + RESULT_NAME=bert
507: ENDING TIMING RUN AT 2020-06-25 11:03:55 AM
504: RESULT,bert,18263,295,root,2020-06-25 10:59:00 AM
504: + echo 'RESULT,bert,18263,295,root,2020-06-25 10:59:00 AM'
376: + END=1593108235
507: + RESULT=295
507: + RESULT_NAME=bert
504: + set +x
507: RESULT,bert,18263,295,root,2020-06-25 10:59:00 AM
507: + echo 'RESULT,bert,18263,295,root,2020-06-25 10:59:00 AM'
507: + set +x
509: + END_FMT='2020-06-25 11:03:55 AM'
509: ENDING TIMING RUN AT 2020-06-25 11:03:55 AM
509: + echo 'ENDING TIMING RUN AT 2020-06-25 11:03:55 AM'
509: + RESULT=295
509: + RESULT_NAME=bert
509: RESULT,bert,18263,295,root,2020-06-25 10:59:00 AM
509: + echo 'RESULT,bert,18263,295,root,2020-06-25 10:59:00 AM'
509: + set +x
371: + END=1593108235
506: + END_FMT='2020-06-25 11:03:55 AM'
370: ++ date '+%Y-%m-%d %r'
374: + END=1593108235
510: + END_FMT='2020-06-25 11:03:55 AM'
383: ++ date '+%Y-%m-%d %r'
506: ENDING TIMING RUN AT 2020-06-25 11:03:55 AM
506: + echo 'ENDING TIMING RUN AT 2020-06-25 11:03:55 AM'
373: + END=1593108235
506: + RESULT=295
375: ++ date '+%Y-%m-%d %r'
379: ++ date '+%Y-%m-%d %r'
510: ENDING TIMING RUN AT 2020-06-25 11:03:55 AM
510: + echo 'ENDING TIMING RUN AT 2020-06-25 11:03:55 AM'
506: + RESULT_NAME=bert
506: RESULT,bert,18263,295,root,2020-06-25 10:59:00 AM
368: + END=1593108235
506: + echo 'RESULT,bert,18263,295,root,2020-06-25 10:59:00 AM'
510: + RESULT=295
510: + RESULT_NAME=bert
506: + set +x
510: RESULT,bert,18263,295,root,2020-06-25 10:59:00 AM
380: + END_FMT='2020-06-25 11:03:55 AM'
382: + END=1593108235
510: + echo 'RESULT,bert,18263,295,root,2020-06-25 10:59:00 AM'
372: + END_FMT='2020-06-25 11:03:55 AM'
372: + echo 'ENDING TIMING RUN AT 2020-06-25 11:03:55 AM'
372: ENDING TIMING RUN AT 2020-06-25 11:03:55 AM
510: + set +x
380: ENDING TIMING RUN AT 2020-06-25 11:03:55 AM
508: + END_FMT='2020-06-25 11:03:55 AM'
380: + echo 'ENDING TIMING RUN AT 2020-06-25 11:03:55 AM'
372: + RESULT=294
508: + echo 'ENDING TIMING RUN AT 2020-06-25 11:03:55 AM'
372: + RESULT_NAME=bert
380: + RESULT=294
380: + RESULT_NAME=bert
372: + echo 'RESULT,bert,18263,294,root,2020-06-25 10:59:01 AM'
372: RESULT,bert,18263,294,root,2020-06-25 10:59:01 AM
372: + set +x
380: RESULT,bert,18263,294,root,2020-06-25 10:59:01 AM
508: ENDING TIMING RUN AT 2020-06-25 11:03:55 AM
380: + echo 'RESULT,bert,18263,294,root,2020-06-25 10:59:01 AM'
380: + set +x
376: ++ date '+%Y-%m-%d %r'
508: + RESULT=295
508: + RESULT_NAME=bert
508: RESULT,bert,18263,295,root,2020-06-25 10:59:00 AM
508: + echo 'RESULT,bert,18263,295,root,2020-06-25 10:59:00 AM'
508: + set +x
500: + END_FMT='2020-06-25 11:03:55 AM'
371: ++ date '+%Y-%m-%d %r'
502: + END_FMT='2020-06-25 11:03:55 AM'
374: ++ date '+%Y-%m-%d %r'
500: + echo 'ENDING TIMING RUN AT 2020-06-25 11:03:55 AM'
377: + END_FMT='2020-06-25 11:03:55 AM'
500: ENDING TIMING RUN AT 2020-06-25 11:03:55 AM
502: ENDING TIMING RUN AT 2020-06-25 11:03:55 AM
377: + echo 'ENDING TIMING RUN AT 2020-06-25 11:03:55 AM'
377: ENDING TIMING RUN AT 2020-06-25 11:03:55 AM
502: + echo 'ENDING TIMING RUN AT 2020-06-25 11:03:55 AM'
377: + RESULT=294
377: + RESULT_NAME=bert
500: + RESULT=295
500: + RESULT_NAME=bert
502: + RESULT=295
502: + RESULT_NAME=bert
377: RESULT,bert,18263,294,root,2020-06-25 10:59:01 AM
377: + echo 'RESULT,bert,18263,294,root,2020-06-25 10:59:01 AM'
377: + set +x
500: RESULT,bert,18263,295,root,2020-06-25 10:59:00 AM
500: + echo 'RESULT,bert,18263,295,root,2020-06-25 10:59:00 AM'
500: + set +x
502: RESULT,bert,18263,295,root,2020-06-25 10:59:00 AM
502: + echo 'RESULT,bert,18263,295,root,2020-06-25 10:59:00 AM'
502: + set +x
503: + END_FMT='2020-06-25 11:03:55 AM'
503: ENDING TIMING RUN AT 2020-06-25 11:03:55 AM
503: + echo 'ENDING TIMING RUN AT 2020-06-25 11:03:55 AM'
503: + RESULT=295
373: ++ date '+%Y-%m-%d %r'
503: + RESULT_NAME=bert
503: RESULT,bert,18263,295,root,2020-06-25 10:59:00 AM
378: + END_FMT='2020-06-25 11:03:55 AM'
382: ++ date '+%Y-%m-%d %r'
503: + echo 'RESULT,bert,18263,295,root,2020-06-25 10:59:00 AM'
503: + set +x
378: ENDING TIMING RUN AT 2020-06-25 11:03:55 AM
497: + END_FMT='2020-06-25 11:03:55 AM'
378: + echo 'ENDING TIMING RUN AT 2020-06-25 11:03:55 AM'
368: ++ date '+%Y-%m-%d %r'
378: + RESULT=294
378: + RESULT_NAME=bert
497: + echo 'ENDING TIMING RUN AT 2020-06-25 11:03:55 AM'
497: ENDING TIMING RUN AT 2020-06-25 11:03:55 AM
378: + echo 'RESULT,bert,18263,294,root,2020-06-25 10:59:01 AM'
378: RESULT,bert,18263,294,root,2020-06-25 10:59:01 AM
497: + RESULT=295
497: + RESULT_NAME=bert
378: + set +x
497: RESULT,bert,18263,295,root,2020-06-25 10:59:00 AM
497: + echo 'RESULT,bert,18263,295,root,2020-06-25 10:59:00 AM'
497: + set +x
370: ENDING TIMING RUN AT 2020-06-25 11:03:55 AM
370: RESULT,bert,18263,294,root,2020-06-25 10:59:01 AM
370: + END_FMT='2020-06-25 11:03:55 AM'
370: + echo 'ENDING TIMING RUN AT 2020-06-25 11:03:55 AM'
370: + RESULT=294
370: + RESULT_NAME=bert
370: + echo 'RESULT,bert,18263,294,root,2020-06-25 10:59:01 AM'
370: + set +x
383: + END_FMT='2020-06-25 11:03:55 AM'
383: + echo 'ENDING TIMING RUN AT 2020-06-25 11:03:55 AM'
383: ENDING TIMING RUN AT 2020-06-25 11:03:55 AM
375: + END_FMT='2020-06-25 11:03:55 AM'
383: + RESULT=294
383: + RESULT_NAME=bert
383: RESULT,bert,18263,294,root,2020-06-25 10:59:01 AM
383: + echo 'RESULT,bert,18263,294,root,2020-06-25 10:59:01 AM'
383: + set +x
375: ENDING TIMING RUN AT 2020-06-25 11:03:55 AM
375: + echo 'ENDING TIMING RUN AT 2020-06-25 11:03:55 AM'
376: + END_FMT='2020-06-25 11:03:55 AM'
375: + RESULT=294
375: + RESULT_NAME=bert
379: + END_FMT='2020-06-25 11:03:55 AM'
375: RESULT,bert,18263,294,root,2020-06-25 10:59:01 AM
375: + echo 'RESULT,bert,18263,294,root,2020-06-25 10:59:01 AM'
375: + set +x
376: ENDING TIMING RUN AT 2020-06-25 11:03:55 AM
376: + echo 'ENDING TIMING RUN AT 2020-06-25 11:03:55 AM'
376: + RESULT=294
376: + RESULT_NAME=bert
379: ENDING TIMING RUN AT 2020-06-25 11:03:55 AM
379: + echo 'ENDING TIMING RUN AT 2020-06-25 11:03:55 AM'
376: RESULT,bert,18263,294,root,2020-06-25 10:59:01 AM
376: + echo 'RESULT,bert,18263,294,root,2020-06-25 10:59:01 AM'
376: + set +x
379: + RESULT=294
379: + RESULT_NAME=bert
379: RESULT,bert,18263,294,root,2020-06-25 10:59:01 AM
379: + echo 'RESULT,bert,18263,294,root,2020-06-25 10:59:01 AM'
379: + set +x
374: + END_FMT='2020-06-25 11:03:55 AM'
371: + END_FMT='2020-06-25 11:03:55 AM'
374: + echo 'ENDING TIMING RUN AT 2020-06-25 11:03:55 AM'
374: ENDING TIMING RUN AT 2020-06-25 11:03:55 AM
371: ENDING TIMING RUN AT 2020-06-25 11:03:55 AM
371: + echo 'ENDING TIMING RUN AT 2020-06-25 11:03:55 AM'
374: + RESULT=294
374: + RESULT_NAME=bert
371: + RESULT=294
371: + RESULT_NAME=bert
371: + echo 'RESULT,bert,18263,294,root,2020-06-25 10:59:01 AM'
374: RESULT,bert,18263,294,root,2020-06-25 10:59:01 AM
374: + echo 'RESULT,bert,18263,294,root,2020-06-25 10:59:01 AM'
374: + set +x
371: RESULT,bert,18263,294,root,2020-06-25 10:59:01 AM
371: + set +x
368: + END_FMT='2020-06-25 11:03:55 AM'
373: + END_FMT='2020-06-25 11:03:55 AM'
382: + END_FMT='2020-06-25 11:03:55 AM'
373: ENDING TIMING RUN AT 2020-06-25 11:03:55 AM
373: + echo 'ENDING TIMING RUN AT 2020-06-25 11:03:55 AM'
382: ENDING TIMING RUN AT 2020-06-25 11:03:55 AM
382: + echo 'ENDING TIMING RUN AT 2020-06-25 11:03:55 AM'
368: ENDING TIMING RUN AT 2020-06-25 11:03:55 AM
368: + echo 'ENDING TIMING RUN AT 2020-06-25 11:03:55 AM'
373: + RESULT=294
373: + RESULT_NAME=bert
382: + RESULT=294
382: + RESULT_NAME=bert
368: + RESULT=294
368: + RESULT_NAME=bert
373: RESULT,bert,18263,294,root,2020-06-25 10:59:01 AM
373: + echo 'RESULT,bert,18263,294,root,2020-06-25 10:59:01 AM'
373: + set +x
382: RESULT,bert,18263,294,root,2020-06-25 10:59:01 AM
382: + echo 'RESULT,bert,18263,294,root,2020-06-25 10:59:01 AM'
382: + set +x
368: RESULT,bert,18263,294,root,2020-06-25 10:59:01 AM
368: + echo 'RESULT,bert,18263,294,root,2020-06-25 10:59:01 AM'
368: + set +x
178: ++ date +%s
187: ++ date +%s
176: ++ date +%s
183: ++ date +%s
182: ++ date +%s
179: ++ date +%s
181: ++ date +%s
189: ++ date +%s
188: ++ date +%s
178: + END=1593108235
184: ++ date +%s
191: ++ date +%s
177: ++ date +%s
185: ++ date +%s
187: + END=1593108235
176: + END=1593108235
178: ++ date '+%Y-%m-%d %r'
190: ++ date +%s
187: ++ date '+%Y-%m-%d %r'
183: + END=1593108235
182: + END=1593108235
179: + END=1593108235
176: ++ date '+%Y-%m-%d %r'
189: + END=1593108235
181: + END=1593108235
183: ++ date '+%Y-%m-%d %r'
177: + END=1593108235
182: ++ date '+%Y-%m-%d %r'
179: ++ date '+%Y-%m-%d %r'
184: + END=1593108235
185: + END=1593108235
188: + END=1593108235
191: + END=1593108235
189: ++ date '+%Y-%m-%d %r'
177: ++ date '+%Y-%m-%d %r'
178: + END_FMT='2020-06-25 11:03:55 AM'
190: + END=1593108235
187: + END_FMT='2020-06-25 11:03:55 AM'
178: ENDING TIMING RUN AT 2020-06-25 11:03:55 AM
178: + echo 'ENDING TIMING RUN AT 2020-06-25 11:03:55 AM'
187: ENDING TIMING RUN AT 2020-06-25 11:03:55 AM
187: + echo 'ENDING TIMING RUN AT 2020-06-25 11:03:55 AM'
178: + RESULT=295
178: + RESULT_NAME=bert
178: RESULT,bert,18263,295,root,2020-06-25 10:59:00 AM
178: + echo 'RESULT,bert,18263,295,root,2020-06-25 10:59:00 AM'
178: + set +x
187: + RESULT=294
187: + RESULT_NAME=bert
187: RESULT,bert,18263,294,root,2020-06-25 10:59:01 AM
187: + echo 'RESULT,bert,18263,294,root,2020-06-25 10:59:01 AM'
187: + set +x
184: ++ date '+%Y-%m-%d %r'
176: + END_FMT='2020-06-25 11:03:55 AM'
185: ++ date '+%Y-%m-%d %r'
191: ++ date '+%Y-%m-%d %r'
181: ++ date '+%Y-%m-%d %r'
176: ENDING TIMING RUN AT 2020-06-25 11:03:55 AM
176: + echo 'ENDING TIMING RUN AT 2020-06-25 11:03:55 AM'
176: + RESULT=294
176: + RESULT_NAME=bert
188: ++ date '+%Y-%m-%d %r'
176: RESULT,bert,18263,294,root,2020-06-25 10:59:01 AM
176: + echo 'RESULT,bert,18263,294,root,2020-06-25 10:59:01 AM'
176: + set +x
190: ++ date '+%Y-%m-%d %r'
183: + END_FMT='2020-06-25 11:03:55 AM'
183: + echo 'ENDING TIMING RUN AT 2020-06-25 11:03:55 AM'
183: ENDING TIMING RUN AT 2020-06-25 11:03:55 AM
183: + RESULT=294
183: + RESULT_NAME=bert
183: RESULT,bert,18263,294,root,2020-06-25 10:59:01 AM
183: + echo 'RESULT,bert,18263,294,root,2020-06-25 10:59:01 AM'
183: + set +x
182: + END_FMT='2020-06-25 11:03:55 AM'
182: + echo 'ENDING TIMING RUN AT 2020-06-25 11:03:55 AM'
182: ENDING TIMING RUN AT 2020-06-25 11:03:55 AM
182: + RESULT=294
182: + RESULT_NAME=bert
182: RESULT,bert,18263,294,root,2020-06-25 10:59:01 AM
182: + echo 'RESULT,bert,18263,294,root,2020-06-25 10:59:01 AM'
182: + set +x
179: + END_FMT='2020-06-25 11:03:55 AM'
179: ENDING TIMING RUN AT 2020-06-25 11:03:55 AM
179: RESULT,bert,18263,294,root,2020-06-25 10:59:01 AM
179: + echo 'ENDING TIMING RUN AT 2020-06-25 11:03:55 AM'
179: + RESULT=294
179: + RESULT_NAME=bert
179: + echo 'RESULT,bert,18263,294,root,2020-06-25 10:59:01 AM'
179: + set +x
189: + END_FMT='2020-06-25 11:03:55 AM'
189: + echo 'ENDING TIMING RUN AT 2020-06-25 11:03:55 AM'
189: + RESULT=294
177: + END_FMT='2020-06-25 11:03:55 AM'
189: ENDING TIMING RUN AT 2020-06-25 11:03:55 AM
189: + RESULT_NAME=bert
189: + echo 'RESULT,bert,18263,294,root,2020-06-25 10:59:01 AM'
177: ENDING TIMING RUN AT 2020-06-25 11:03:55 AM
177: + echo 'ENDING TIMING RUN AT 2020-06-25 11:03:55 AM'
177: + RESULT=294
177: + RESULT_NAME=bert
189: RESULT,bert,18263,294,root,2020-06-25 10:59:01 AM
189: + set +x
177: RESULT,bert,18263,294,root,2020-06-25 10:59:01 AM
177: + echo 'RESULT,bert,18263,294,root,2020-06-25 10:59:01 AM'
177: + set +x
184: + END_FMT='2020-06-25 11:03:55 AM'
184: + echo 'ENDING TIMING RUN AT 2020-06-25 11:03:55 AM'
184: ENDING TIMING RUN AT 2020-06-25 11:03:55 AM
181: ENDING TIMING RUN AT 2020-06-25 11:03:55 AM
181: RESULT,bert,18263,294,root,2020-06-25 10:59:01 AM
181: + END_FMT='2020-06-25 11:03:55 AM'
181: + echo 'ENDING TIMING RUN AT 2020-06-25 11:03:55 AM'
181: + RESULT=294
181: + RESULT_NAME=bert
181: + echo 'RESULT,bert,18263,294,root,2020-06-25 10:59:01 AM'
181: + set +x
184: RESULT,bert,18263,294,root,2020-06-25 10:59:01 AM
184: + RESULT=294
184: + RESULT_NAME=bert
184: + echo 'RESULT,bert,18263,294,root,2020-06-25 10:59:01 AM'
184: + set +x
185: ENDING TIMING RUN AT 2020-06-25 11:03:55 AM
185: RESULT,bert,18263,295,root,2020-06-25 10:59:00 AM
185: + END_FMT='2020-06-25 11:03:55 AM'
185: + echo 'ENDING TIMING RUN AT 2020-06-25 11:03:55 AM'
185: + RESULT=295
185: + RESULT_NAME=bert
185: + echo 'RESULT,bert,18263,295,root,2020-06-25 10:59:00 AM'
185: + set +x
190: ENDING TIMING RUN AT 2020-06-25 11:03:55 AM
190: + END_FMT='2020-06-25 11:03:55 AM'
190: + echo 'ENDING TIMING RUN AT 2020-06-25 11:03:55 AM'
191: ENDING TIMING RUN AT 2020-06-25 11:03:55 AM
191: + END_FMT='2020-06-25 11:03:55 AM'
191: + echo 'ENDING TIMING RUN AT 2020-06-25 11:03:55 AM'
191: + RESULT=294
190: + RESULT=294
190: + RESULT_NAME=bert
190: + echo 'RESULT,bert,18263,294,root,2020-06-25 10:59:01 AM'
191: + RESULT_NAME=bert
191: + echo 'RESULT,bert,18263,294,root,2020-06-25 10:59:01 AM'
190: RESULT,bert,18263,294,root,2020-06-25 10:59:01 AM
190: + set +x
191: RESULT,bert,18263,294,root,2020-06-25 10:59:01 AM
191: + set +x
188: + END_FMT='2020-06-25 11:03:55 AM'
188: + echo 'ENDING TIMING RUN AT 2020-06-25 11:03:55 AM'
188: ENDING TIMING RUN AT 2020-06-25 11:03:55 AM
188: + RESULT=294
188: + RESULT_NAME=bert
188: + echo 'RESULT,bert,18263,294,root,2020-06-25 10:59:01 AM'
188: RESULT,bert,18263,294,root,2020-06-25 10:59:01 AM
188: + set +x
203: ++ date +%s
207: ++ date +%s
197: ++ date +%s
193: ++ date +%s
201: ++ date +%s
200: ++ date +%s
205: ++ date +%s
199: ++ date +%s
198: ++ date +%s
192: ++ date +%s
194: ++ date +%s
195: ++ date +%s
202: ++ date +%s
196: ++ date +%s
203: + END=1593108235
207: + END=1593108235
197: + END=1593108235
207: ++ date '+%Y-%m-%d %r'
203: ++ date '+%Y-%m-%d %r'
200: + END=1593108235
205: + END=1593108235
193: + END=1593108235
201: + END=1593108235
197: ++ date '+%Y-%m-%d %r'
199: + END=1593108235
196: + END=1593108235
192: + END=1593108235
195: + END=1593108235
198: + END=1593108235
194: + END=1593108235
202: + END=1593108235
200: ++ date '+%Y-%m-%d %r'
201: ++ date '+%Y-%m-%d %r'
205: ++ date '+%Y-%m-%d %r'
193: ++ date '+%Y-%m-%d %r'
199: ++ date '+%Y-%m-%d %r'
202: ++ date '+%Y-%m-%d %r'
194: ++ date '+%Y-%m-%d %r'
198: ++ date '+%Y-%m-%d %r'
195: ++ date '+%Y-%m-%d %r'
196: ++ date '+%Y-%m-%d %r'
203: + END_FMT='2020-06-25 11:03:55 AM'
207: + END_FMT='2020-06-25 11:03:55 AM'
192: ++ date '+%Y-%m-%d %r'
203: ENDING TIMING RUN AT 2020-06-25 11:03:55 AM
203: + echo 'ENDING TIMING RUN AT 2020-06-25 11:03:55 AM'
207: + echo 'ENDING TIMING RUN AT 2020-06-25 11:03:55 AM'
203: + RESULT=295
207: ENDING TIMING RUN AT 2020-06-25 11:03:55 AM
207: + RESULT=295
207: + RESULT_NAME=bert
203: + RESULT_NAME=bert
207: RESULT,bert,18263,295,root,2020-06-25 10:59:00 AM
207: + echo 'RESULT,bert,18263,295,root,2020-06-25 10:59:00 AM'
207: + set +x
203: RESULT,bert,18263,295,root,2020-06-25 10:59:00 AM
203: + echo 'RESULT,bert,18263,295,root,2020-06-25 10:59:00 AM'
203: + set +x
197: + END_FMT='2020-06-25 11:03:55 AM'
197: ENDING TIMING RUN AT 2020-06-25 11:03:55 AM
197: + echo 'ENDING TIMING RUN AT 2020-06-25 11:03:55 AM'
197: + RESULT=295
197: + RESULT_NAME=bert
197: RESULT,bert,18263,295,root,2020-06-25 10:59:00 AM
197: + echo 'RESULT,bert,18263,295,root,2020-06-25 10:59:00 AM'
197: + set +x
200: + END_FMT='2020-06-25 11:03:55 AM'
200: + echo 'ENDING TIMING RUN AT 2020-06-25 11:03:55 AM'
200: ENDING TIMING RUN AT 2020-06-25 11:03:55 AM
200: + RESULT=295
200: + RESULT_NAME=bert
200: RESULT,bert,18263,295,root,2020-06-25 10:59:00 AM
200: + echo 'RESULT,bert,18263,295,root,2020-06-25 10:59:00 AM'
200: + set +x
205: + END_FMT='2020-06-25 11:03:55 AM'
201: + END_FMT='2020-06-25 11:03:55 AM'
193: + END_FMT='2020-06-25 11:03:55 AM'
205: ENDING TIMING RUN AT 2020-06-25 11:03:55 AM
205: + echo 'ENDING TIMING RUN AT 2020-06-25 11:03:55 AM'
205: + RESULT=295
205: + RESULT_NAME=bert
193: ENDING TIMING RUN AT 2020-06-25 11:03:55 AM
193: + echo 'ENDING TIMING RUN AT 2020-06-25 11:03:55 AM'
193: + RESULT=295
201: ENDING TIMING RUN AT 2020-06-25 11:03:55 AM
201: + echo 'ENDING TIMING RUN AT 2020-06-25 11:03:55 AM'
201: + RESULT=295
201: + RESULT_NAME=bert
193: + RESULT_NAME=bert
201: RESULT,bert,18263,295,root,2020-06-25 10:59:00 AM
201: + echo 'RESULT,bert,18263,295,root,2020-06-25 10:59:00 AM'
201: + set +x
205: RESULT,bert,18263,295,root,2020-06-25 10:59:00 AM
205: + echo 'RESULT,bert,18263,295,root,2020-06-25 10:59:00 AM'
205: + set +x
193: RESULT,bert,18263,295,root,2020-06-25 10:59:00 AM
193: + echo 'RESULT,bert,18263,295,root,2020-06-25 10:59:00 AM'
193: + set +x
194: + END_FMT='2020-06-25 11:03:55 AM'
194: + echo 'ENDING TIMING RUN AT 2020-06-25 11:03:55 AM'
199: + END_FMT='2020-06-25 11:03:55 AM'
194: ENDING TIMING RUN AT 2020-06-25 11:03:55 AM
194: + RESULT=295
194: + RESULT_NAME=bert
199: ENDING TIMING RUN AT 2020-06-25 11:03:55 AM
199: + echo 'ENDING TIMING RUN AT 2020-06-25 11:03:55 AM'
194: RESULT,bert,18263,295,root,2020-06-25 10:59:00 AM
194: + echo 'RESULT,bert,18263,295,root,2020-06-25 10:59:00 AM'
194: + set +x
199: + RESULT=295
199: + RESULT_NAME=bert
199: RESULT,bert,18263,295,root,2020-06-25 10:59:00 AM
199: + echo 'RESULT,bert,18263,295,root,2020-06-25 10:59:00 AM'
199: + set +x
198: + END_FMT='2020-06-25 11:03:55 AM'
192: + END_FMT='2020-06-25 11:03:55 AM'
195: + END_FMT='2020-06-25 11:03:55 AM'
196: + END_FMT='2020-06-25 11:03:55 AM'
198: ENDING TIMING RUN AT 2020-06-25 11:03:55 AM
198: + echo 'ENDING TIMING RUN AT 2020-06-25 11:03:55 AM'
192: ENDING TIMING RUN AT 2020-06-25 11:03:55 AM
192: + echo 'ENDING TIMING RUN AT 2020-06-25 11:03:55 AM'
195: + echo 'ENDING TIMING RUN AT 2020-06-25 11:03:55 AM'
196: ENDING TIMING RUN AT 2020-06-25 11:03:55 AM
196: + echo 'ENDING TIMING RUN AT 2020-06-25 11:03:55 AM'
196: + RESULT=295
198: + RESULT=295
198: + RESULT_NAME=bert
198: + echo 'RESULT,bert,18263,295,root,2020-06-25 10:59:00 AM'
192: + RESULT=295
192: + RESULT_NAME=bert
192: + echo 'RESULT,bert,18263,295,root,2020-06-25 10:59:00 AM'
195: ENDING TIMING RUN AT 2020-06-25 11:03:55 AM
196: + RESULT_NAME=bert
196: + echo 'RESULT,bert,18263,295,root,2020-06-25 10:59:00 AM'
198: RESULT,bert,18263,295,root,2020-06-25 10:59:00 AM
198: + set +x
202: + END_FMT='2020-06-25 11:03:55 AM'
202: + echo 'ENDING TIMING RUN AT 2020-06-25 11:03:55 AM'
192: RESULT,bert,18263,295,root,2020-06-25 10:59:00 AM
192: + set +x
195: + RESULT=295
195: + RESULT_NAME=bert
195: + echo 'RESULT,bert,18263,295,root,2020-06-25 10:59:00 AM'
196: RESULT,bert,18263,295,root,2020-06-25 10:59:00 AM
196: + set +x
202: ENDING TIMING RUN AT 2020-06-25 11:03:55 AM
195: RESULT,bert,18263,295,root,2020-06-25 10:59:00 AM
195: + set +x
202: + RESULT=295
202: + RESULT_NAME=bert
202: RESULT,bert,18263,295,root,2020-06-25 10:59:00 AM
202: + echo 'RESULT,bert,18263,295,root,2020-06-25 10:59:00 AM'
202: + set +x
285: ++ date +%s
273: ++ date +%s
279: ++ date +%s
281: ++ date +%s
275: ++ date +%s
274: ++ date +%s
272: ++ date +%s
277: ++ date +%s
287: ++ date +%s
280: ++ date +%s
284: ++ date +%s
276: ++ date +%s
282: ++ date +%s
286: ++ date +%s
285: + END=1593108235
281: + END=1593108235
273: + END=1593108235
285: ++ date '+%Y-%m-%d %r'
279: + END=1593108235
274: + END=1593108235
275: + END=1593108235
272: + END=1593108235
277: + END=1593108235
281: ++ date '+%Y-%m-%d %r'
287: + END=1593108235
280: + END=1593108235
273: ++ date '+%Y-%m-%d %r'
284: + END=1593108235
279: ++ date '+%Y-%m-%d %r'
286: + END=1593108235
274: ++ date '+%Y-%m-%d %r'
275: ++ date '+%Y-%m-%d %r'
272: ++ date '+%Y-%m-%d %r'
276: + END=1593108235
282: + END=1593108235
277: ++ date '+%Y-%m-%d %r'
287: ++ date '+%Y-%m-%d %r'
280: ++ date '+%Y-%m-%d %r'
285: + END_FMT='2020-06-25 11:03:55 AM'
285: ENDING TIMING RUN AT 2020-06-25 11:03:55 AM
285: + echo 'ENDING TIMING RUN AT 2020-06-25 11:03:55 AM'
284: ++ date '+%Y-%m-%d %r'
285: + RESULT=294
285: + RESULT_NAME=bert
285: + echo 'RESULT,bert,18263,294,root,2020-06-25 10:59:01 AM'
285: RESULT,bert,18263,294,root,2020-06-25 10:59:01 AM
285: + set +x
286: ++ date '+%Y-%m-%d %r'
276: ++ date '+%Y-%m-%d %r'
282: ++ date '+%Y-%m-%d %r'
281: + END_FMT='2020-06-25 11:03:55 AM'
281: ENDING TIMING RUN AT 2020-06-25 11:03:55 AM
281: + echo 'ENDING TIMING RUN AT 2020-06-25 11:03:55 AM'
281: + RESULT=294
281: + RESULT_NAME=bert
281: RESULT,bert,18263,294,root,2020-06-25 10:59:01 AM
281: + echo 'RESULT,bert,18263,294,root,2020-06-25 10:59:01 AM'
281: + set +x
272: + END_FMT='2020-06-25 11:03:55 AM'
273: + END_FMT='2020-06-25 11:03:55 AM'
279: + END_FMT='2020-06-25 11:03:55 AM'
272: ENDING TIMING RUN AT 2020-06-25 11:03:55 AM
272: + echo 'ENDING TIMING RUN AT 2020-06-25 11:03:55 AM'
273: ENDING TIMING RUN AT 2020-06-25 11:03:55 AM
273: + echo 'ENDING TIMING RUN AT 2020-06-25 11:03:55 AM'
272: + RESULT=294
272: + RESULT_NAME=bert
273: + RESULT=294
273: + RESULT_NAME=bert
279: ENDING TIMING RUN AT 2020-06-25 11:03:55 AM
279: + echo 'ENDING TIMING RUN AT 2020-06-25 11:03:55 AM'
279: + RESULT=294
279: + RESULT_NAME=bert
272: RESULT,bert,18263,294,root,2020-06-25 10:59:01 AM
272: + echo 'RESULT,bert,18263,294,root,2020-06-25 10:59:01 AM'
272: + set +x
273: RESULT,bert,18263,294,root,2020-06-25 10:59:01 AM
273: + echo 'RESULT,bert,18263,294,root,2020-06-25 10:59:01 AM'
273: + set +x
279: RESULT,bert,18263,294,root,2020-06-25 10:59:01 AM
279: + echo 'RESULT,bert,18263,294,root,2020-06-25 10:59:01 AM'
279: + set +x
277: + END_FMT='2020-06-25 11:03:55 AM'
277: + echo 'ENDING TIMING RUN AT 2020-06-25 11:03:55 AM'
277: ENDING TIMING RUN AT 2020-06-25 11:03:55 AM
277: + RESULT=294
277: + RESULT_NAME=bert
277: RESULT,bert,18263,294,root,2020-06-25 10:59:01 AM
277: + echo 'RESULT,bert,18263,294,root,2020-06-25 10:59:01 AM'
277: + set +x
287: + END_FMT='2020-06-25 11:03:55 AM'
287: ENDING TIMING RUN AT 2020-06-25 11:03:55 AM
287: + echo 'ENDING TIMING RUN AT 2020-06-25 11:03:55 AM'
287: + RESULT=294
287: + RESULT_NAME=bert
287: RESULT,bert,18263,294,root,2020-06-25 10:59:01 AM
287: + echo 'RESULT,bert,18263,294,root,2020-06-25 10:59:01 AM'
287: + set +x
284: + END_FMT='2020-06-25 11:03:55 AM'
284: + echo 'ENDING TIMING RUN AT 2020-06-25 11:03:55 AM'
284: ENDING TIMING RUN AT 2020-06-25 11:03:55 AM
280: + END_FMT='2020-06-25 11:03:55 AM'
284: + RESULT=294
284: + RESULT_NAME=bert
280: + echo 'ENDING TIMING RUN AT 2020-06-25 11:03:55 AM'
284: RESULT,bert,18263,294,root,2020-06-25 10:59:01 AM
284: + echo 'RESULT,bert,18263,294,root,2020-06-25 10:59:01 AM'
284: + set +x
280: ENDING TIMING RUN AT 2020-06-25 11:03:55 AM
280: + RESULT=294
280: + RESULT_NAME=bert
280: + echo 'RESULT,bert,18263,294,root,2020-06-25 10:59:01 AM'
280: RESULT,bert,18263,294,root,2020-06-25 10:59:01 AM
280: + set +x
275: + END_FMT='2020-06-25 11:03:55 AM'
275: ENDING TIMING RUN AT 2020-06-25 11:03:55 AM
275: + echo 'ENDING TIMING RUN AT 2020-06-25 11:03:55 AM'
274: + END_FMT='2020-06-25 11:03:55 AM'
275: + RESULT=294
275: + RESULT_NAME=bert
276: + END_FMT='2020-06-25 11:03:55 AM'
286: + END_FMT='2020-06-25 11:03:55 AM'
275: RESULT,bert,18263,294,root,2020-06-25 10:59:01 AM
275: + echo 'RESULT,bert,18263,294,root,2020-06-25 10:59:01 AM'
275: + set +x
282: + END_FMT='2020-06-25 11:03:55 AM'
282: + echo 'ENDING TIMING RUN AT 2020-06-25 11:03:55 AM'
274: ENDING TIMING RUN AT 2020-06-25 11:03:55 AM
274: + echo 'ENDING TIMING RUN AT 2020-06-25 11:03:55 AM'
276: ENDING TIMING RUN AT 2020-06-25 11:03:55 AM
276: + echo 'ENDING TIMING RUN AT 2020-06-25 11:03:55 AM'
276: + RESULT=294
276: + RESULT_NAME=bert
282: ENDING TIMING RUN AT 2020-06-25 11:03:55 AM
286: ENDING TIMING RUN AT 2020-06-25 11:03:55 AM
286: RESULT,bert,18263,294,root,2020-06-25 10:59:01 AM
286: + echo 'ENDING TIMING RUN AT 2020-06-25 11:03:55 AM'
286: + RESULT=294
286: + RESULT_NAME=bert
286: + echo 'RESULT,bert,18263,294,root,2020-06-25 10:59:01 AM'
286: + set +x
274: RESULT,bert,18263,294,root,2020-06-25 10:59:01 AM
274: + RESULT=294
274: + RESULT_NAME=bert
274: + echo 'RESULT,bert,18263,294,root,2020-06-25 10:59:01 AM'
274: + set +x
276: RESULT,bert,18263,294,root,2020-06-25 10:59:01 AM
276: + echo 'RESULT,bert,18263,294,root,2020-06-25 10:59:01 AM'
276: + set +x
282: RESULT,bert,18263,294,root,2020-06-25 10:59:01 AM
282: + RESULT=294
282: + RESULT_NAME=bert
282: + echo 'RESULT,bert,18263,294,root,2020-06-25 10:59:01 AM'
282: + set +x
293: ++ date +%s
292: ++ date +%s
290: ++ date +%s
294: ++ date +%s
293: + END=1593108235
289: ++ date +%s
292: + END=1593108235
290: + END=1593108235
293: ++ date '+%Y-%m-%d %r'
297: ++ date +%s
294: + END=1593108235
296: ++ date +%s
299: ++ date +%s
302: ++ date +%s
288: ++ date +%s
290: ++ date '+%Y-%m-%d %r'
292: ++ date '+%Y-%m-%d %r'
295: ++ date +%s
300: ++ date +%s
301: ++ date +%s
289: + END=1593108235
294: ++ date '+%Y-%m-%d %r'
303: ++ date +%s
289: ++ date '+%Y-%m-%d %r'
293: + END_FMT='2020-06-25 11:03:55 AM'
293: + echo 'ENDING TIMING RUN AT 2020-06-25 11:03:55 AM'
293: ENDING TIMING RUN AT 2020-06-25 11:03:55 AM
293: + RESULT=294
293: + RESULT_NAME=bert
293: + echo 'RESULT,bert,18263,294,root,2020-06-25 10:59:01 AM'
293: RESULT,bert,18263,294,root,2020-06-25 10:59:01 AM
293: + set +x
297: + END=1593108235
299: + END=1593108235
302: + END=1593108235
296: + END=1593108235
288: + END=1593108235
297: ++ date '+%Y-%m-%d %r'
299: ++ date '+%Y-%m-%d %r'
301: + END=1593108235
302: ++ date '+%Y-%m-%d %r'
295: + END=1593108235
296: ++ date '+%Y-%m-%d %r'
290: + END_FMT='2020-06-25 11:03:55 AM'
292: + END_FMT='2020-06-25 11:03:55 AM'
294: + END_FMT='2020-06-25 11:03:55 AM'
300: + END=1593108235
290: + echo 'ENDING TIMING RUN AT 2020-06-25 11:03:55 AM'
292: + echo 'ENDING TIMING RUN AT 2020-06-25 11:03:55 AM'
294: + echo 'ENDING TIMING RUN AT 2020-06-25 11:03:55 AM'
290: ENDING TIMING RUN AT 2020-06-25 11:03:55 AM
292: ENDING TIMING RUN AT 2020-06-25 11:03:55 AM
294: ENDING TIMING RUN AT 2020-06-25 11:03:55 AM
290: + RESULT=294
292: + RESULT=294
292: + RESULT_NAME=bert
294: + RESULT=294
294: + RESULT_NAME=bert
290: + RESULT_NAME=bert
294: RESULT,bert,18263,294,root,2020-06-25 10:59:01 AM
294: + echo 'RESULT,bert,18263,294,root,2020-06-25 10:59:01 AM'
294: + set +x
288: ++ date '+%Y-%m-%d %r'
290: RESULT,bert,18263,294,root,2020-06-25 10:59:01 AM
290: + echo 'RESULT,bert,18263,294,root,2020-06-25 10:59:01 AM'
290: + set +x
292: RESULT,bert,18263,294,root,2020-06-25 10:59:01 AM
292: + echo 'RESULT,bert,18263,294,root,2020-06-25 10:59:01 AM'
292: + set +x
303: + END=1593108235
289: + END_FMT='2020-06-25 11:03:55 AM'
289: ENDING TIMING RUN AT 2020-06-25 11:03:55 AM
289: + echo 'ENDING TIMING RUN AT 2020-06-25 11:03:55 AM'
289: + RESULT=294
289: + RESULT_NAME=bert
289: RESULT,bert,18263,294,root,2020-06-25 10:59:01 AM
289: + echo 'RESULT,bert,18263,294,root,2020-06-25 10:59:01 AM'
289: + set +x
301: ++ date '+%Y-%m-%d %r'
295: ++ date '+%Y-%m-%d %r'
300: ++ date '+%Y-%m-%d %r'
303: ++ date '+%Y-%m-%d %r'
297: + END_FMT='2020-06-25 11:03:55 AM'
297: + echo 'ENDING TIMING RUN AT 2020-06-25 11:03:55 AM'
297: ENDING TIMING RUN AT 2020-06-25 11:03:55 AM
297: + RESULT=294
297: + RESULT_NAME=bert
299: + END_FMT='2020-06-25 11:03:55 AM'
297: RESULT,bert,18263,294,root,2020-06-25 10:59:01 AM
297: + echo 'RESULT,bert,18263,294,root,2020-06-25 10:59:01 AM'
297: + set +x
299: + echo 'ENDING TIMING RUN AT 2020-06-25 11:03:55 AM'
299: ENDING TIMING RUN AT 2020-06-25 11:03:55 AM
302: + END_FMT='2020-06-25 11:03:55 AM'
299: + RESULT=294
299: + RESULT_NAME=bert
302: + echo 'ENDING TIMING RUN AT 2020-06-25 11:03:55 AM'
296: + END_FMT='2020-06-25 11:03:55 AM'
302: ENDING TIMING RUN AT 2020-06-25 11:03:55 AM
299: RESULT,bert,18263,294,root,2020-06-25 10:59:01 AM
299: + echo 'RESULT,bert,18263,294,root,2020-06-25 10:59:01 AM'
299: + set +x
296: ENDING TIMING RUN AT 2020-06-25 11:03:55 AM
296: + echo 'ENDING TIMING RUN AT 2020-06-25 11:03:55 AM'
302: + RESULT=294
302: + RESULT_NAME=bert
302: + echo 'RESULT,bert,18263,294,root,2020-06-25 10:59:01 AM'
302: RESULT,bert,18263,294,root,2020-06-25 10:59:01 AM
302: + set +x
296: + RESULT=294
296: + RESULT_NAME=bert
296: RESULT,bert,18263,294,root,2020-06-25 10:59:01 AM
296: + echo 'RESULT,bert,18263,294,root,2020-06-25 10:59:01 AM'
296: + set +x
288: + END_FMT='2020-06-25 11:03:55 AM'
288: + echo 'ENDING TIMING RUN AT 2020-06-25 11:03:55 AM'
288: ENDING TIMING RUN AT 2020-06-25 11:03:55 AM
288: + RESULT=294
288: + RESULT_NAME=bert
288: RESULT,bert,18263,294,root,2020-06-25 10:59:01 AM
288: + echo 'RESULT,bert,18263,294,root,2020-06-25 10:59:01 AM'
288: + set +x
301: + END_FMT='2020-06-25 11:03:55 AM'
301: + echo 'ENDING TIMING RUN AT 2020-06-25 11:03:55 AM'
301: ENDING TIMING RUN AT 2020-06-25 11:03:55 AM
301: + RESULT=294
301: + RESULT_NAME=bert
301: RESULT,bert,18263,294,root,2020-06-25 10:59:01 AM
301: + echo 'RESULT,bert,18263,294,root,2020-06-25 10:59:01 AM'
301: + set +x
300: + END_FMT='2020-06-25 11:03:55 AM'
300: + echo 'ENDING TIMING RUN AT 2020-06-25 11:03:55 AM'
300: ENDING TIMING RUN AT 2020-06-25 11:03:55 AM
300: + RESULT=294
295: + END_FMT='2020-06-25 11:03:55 AM'
300: + RESULT_NAME=bert
300: RESULT,bert,18263,294,root,2020-06-25 10:59:01 AM
300: + echo 'RESULT,bert,18263,294,root,2020-06-25 10:59:01 AM'
300: + set +x
295: + echo 'ENDING TIMING RUN AT 2020-06-25 11:03:55 AM'
295: ENDING TIMING RUN AT 2020-06-25 11:03:55 AM
295: + RESULT=294
295: + RESULT_NAME=bert
303: + END_FMT='2020-06-25 11:03:55 AM'
295: + echo 'RESULT,bert,18263,294,root,2020-06-25 10:59:01 AM'
295: RESULT,bert,18263,294,root,2020-06-25 10:59:01 AM
295: + set +x
303: ENDING TIMING RUN AT 2020-06-25 11:03:55 AM
303: + echo 'ENDING TIMING RUN AT 2020-06-25 11:03:55 AM'
303: + RESULT=294
303: + RESULT_NAME=bert
303: RESULT,bert,18263,294,root,2020-06-25 10:59:01 AM
303: + echo 'RESULT,bert,18263,294,root,2020-06-25 10:59:01 AM'
303: + set +x
110: ++ date +%s
 98: ++ date +%s
110: + END=1593108235
100: ++ date +%s
107: ++ date +%s
 96: ++ date +%s
103: ++ date +%s
104: ++ date +%s
108: ++ date +%s
110: ++ date '+%Y-%m-%d %r'
101: ++ date +%s
 97: ++ date +%s
 99: ++ date +%s
111: ++ date +%s
105: ++ date +%s
106: ++ date +%s
 98: + END=1593108235
 98: ++ date '+%Y-%m-%d %r'
107: + END=1593108235
100: + END=1593108235
110: + END_FMT='2020-06-25 11:03:55 AM'
110: + echo 'ENDING TIMING RUN AT 2020-06-25 11:03:55 AM'
110: ENDING TIMING RUN AT 2020-06-25 11:03:55 AM
110: + RESULT=294
110: + RESULT_NAME=bert
104: + END=1593108235
110: RESULT,bert,18263,294,root,2020-06-25 10:59:01 AM
110: + echo 'RESULT,bert,18263,294,root,2020-06-25 10:59:01 AM'
110: + set +x
 96: + END=1593108235
108: + END=1593108235
 97: + END=1593108235
103: + END=1593108235
106: + END=1593108235
104: ++ date '+%Y-%m-%d %r'
107: ++ date '+%Y-%m-%d %r'
 98: + END_FMT='2020-06-25 11:03:55 AM'
111: + END=1593108235
 99: + END=1593108235
101: + END=1593108235
105: + END=1593108235
 98: ENDING TIMING RUN AT 2020-06-25 11:03:55 AM
 98: + echo 'ENDING TIMING RUN AT 2020-06-25 11:03:55 AM'
 98: + RESULT=294
 98: + RESULT_NAME=bert
 98: RESULT,bert,18263,294,root,2020-06-25 10:59:01 AM
 98: + echo 'RESULT,bert,18263,294,root,2020-06-25 10:59:01 AM'
 98: + set +x
100: ++ date '+%Y-%m-%d %r'
108: ++ date '+%Y-%m-%d %r'
103: ++ date '+%Y-%m-%d %r'
 96: ++ date '+%Y-%m-%d %r'
 97: ++ date '+%Y-%m-%d %r'
106: ++ date '+%Y-%m-%d %r'
111: ++ date '+%Y-%m-%d %r'
101: ++ date '+%Y-%m-%d %r'
 99: ++ date '+%Y-%m-%d %r'
105: ++ date '+%Y-%m-%d %r'
104: + END_FMT='2020-06-25 11:03:55 AM'
107: + END_FMT='2020-06-25 11:03:55 AM'
104: + echo 'ENDING TIMING RUN AT 2020-06-25 11:03:55 AM'
104: ENDING TIMING RUN AT 2020-06-25 11:03:55 AM
107: ENDING TIMING RUN AT 2020-06-25 11:03:55 AM
107: + echo 'ENDING TIMING RUN AT 2020-06-25 11:03:55 AM'
104: + RESULT=294
104: + RESULT_NAME=bert
107: + RESULT=294
107: + RESULT_NAME=bert
100: + END_FMT='2020-06-25 11:03:55 AM'
104: RESULT,bert,18263,294,root,2020-06-25 10:59:01 AM
104: + echo 'RESULT,bert,18263,294,root,2020-06-25 10:59:01 AM'
104: + set +x
107: + echo 'RESULT,bert,18263,294,root,2020-06-25 10:59:01 AM'
107: + set +x
100: ENDING TIMING RUN AT 2020-06-25 11:03:55 AM
100: + echo 'ENDING TIMING RUN AT 2020-06-25 11:03:55 AM'
107: RESULT,bert,18263,294,root,2020-06-25 10:59:01 AM
100: + RESULT=294
100: + RESULT_NAME=bert
100: RESULT,bert,18263,294,root,2020-06-25 10:59:01 AM
100: + echo 'RESULT,bert,18263,294,root,2020-06-25 10:59:01 AM'
100: + set +x
103: + END_FMT='2020-06-25 11:03:55 AM'
103: + echo 'ENDING TIMING RUN AT 2020-06-25 11:03:55 AM'
108: + END_FMT='2020-06-25 11:03:55 AM'
103: ENDING TIMING RUN AT 2020-06-25 11:03:55 AM
108: + echo 'ENDING TIMING RUN AT 2020-06-25 11:03:55 AM'
103: + RESULT=294
103: + RESULT_NAME=bert
108: ENDING TIMING RUN AT 2020-06-25 11:03:55 AM
103: RESULT,bert,18263,294,root,2020-06-25 10:59:01 AM
103: + echo 'RESULT,bert,18263,294,root,2020-06-25 10:59:01 AM'
103: + set +x
108: + RESULT=294
108: + RESULT_NAME=bert
 97: + END_FMT='2020-06-25 11:03:55 AM'
108: RESULT,bert,18263,294,root,2020-06-25 10:59:01 AM
108: + echo 'RESULT,bert,18263,294,root,2020-06-25 10:59:01 AM'
108: + set +x
 97: + echo 'ENDING TIMING RUN AT 2020-06-25 11:03:55 AM'
 97: ENDING TIMING RUN AT 2020-06-25 11:03:55 AM
 97: + RESULT=294
 97: + RESULT_NAME=bert
 97: RESULT,bert,18263,294,root,2020-06-25 10:59:01 AM
 97: + echo 'RESULT,bert,18263,294,root,2020-06-25 10:59:01 AM'
 97: + set +x
106: + END_FMT='2020-06-25 11:03:55 AM'
111: + END_FMT='2020-06-25 11:03:55 AM'
 96: + END_FMT='2020-06-25 11:03:55 AM'
106: ENDING TIMING RUN AT 2020-06-25 11:03:55 AM
106: + echo 'ENDING TIMING RUN AT 2020-06-25 11:03:55 AM'
111: + echo 'ENDING TIMING RUN AT 2020-06-25 11:03:55 AM'
 96: ENDING TIMING RUN AT 2020-06-25 11:03:55 AM
 96: + echo 'ENDING TIMING RUN AT 2020-06-25 11:03:55 AM'
111: ENDING TIMING RUN AT 2020-06-25 11:03:55 AM
 96: + RESULT=294
 96: + RESULT_NAME=bert
106: + RESULT=294
106: + RESULT_NAME=bert
111: + RESULT=294
111: + RESULT_NAME=bert
106: RESULT,bert,18263,294,root,2020-06-25 10:59:01 AM
106: + echo 'RESULT,bert,18263,294,root,2020-06-25 10:59:01 AM'
106: + set +x
111: RESULT,bert,18263,294,root,2020-06-25 10:59:01 AM
111: + echo 'RESULT,bert,18263,294,root,2020-06-25 10:59:01 AM'
111: + set +x
 96: RESULT,bert,18263,294,root,2020-06-25 10:59:01 AM
 96: + echo 'RESULT,bert,18263,294,root,2020-06-25 10:59:01 AM'
 96: + set +x
 99: + END_FMT='2020-06-25 11:03:55 AM'
101: + END_FMT='2020-06-25 11:03:55 AM'
105: + END_FMT='2020-06-25 11:03:55 AM'
 99: + echo 'ENDING TIMING RUN AT 2020-06-25 11:03:55 AM'
101: ENDING TIMING RUN AT 2020-06-25 11:03:55 AM
101: + echo 'ENDING TIMING RUN AT 2020-06-25 11:03:55 AM'
105: ENDING TIMING RUN AT 2020-06-25 11:03:55 AM
105: + echo 'ENDING TIMING RUN AT 2020-06-25 11:03:55 AM'
105: + RESULT=294
 99: ENDING TIMING RUN AT 2020-06-25 11:03:55 AM
 99: + RESULT=294
 99: + RESULT_NAME=bert
101: + RESULT=294
101: + RESULT_NAME=bert
101: + echo 'RESULT,bert,18263,294,root,2020-06-25 10:59:01 AM'
105: + RESULT_NAME=bert
105: + echo 'RESULT,bert,18263,294,root,2020-06-25 10:59:01 AM'
 99: RESULT,bert,18263,294,root,2020-06-25 10:59:01 AM
 99: + echo 'RESULT,bert,18263,294,root,2020-06-25 10:59:01 AM'
 99: + set +x
101: RESULT,bert,18263,294,root,2020-06-25 10:59:01 AM
101: + set +x
105: RESULT,bert,18263,294,root,2020-06-25 10:59:01 AM
105: + set +x
313: ++ date +%s
314: ++ date +%s
318: ++ date +%s
317: ++ date +%s
319: ++ date +%s
306: ++ date +%s
310: ++ date +%s
311: ++ date +%s
304: ++ date +%s
307: ++ date +%s
312: ++ date +%s
315: ++ date +%s
308: ++ date +%s
305: ++ date +%s
318: + END=1593108235
314: + END=1593108235
317: + END=1593108235
313: + END=1593108235
306: + END=1593108235
310: + END=1593108235
311: + END=1593108235
319: + END=1593108235
318: ++ date '+%Y-%m-%d %r'
313: ++ date '+%Y-%m-%d %r'
317: ++ date '+%Y-%m-%d %r'
312: + END=1593108235
315: + END=1593108235
314: ++ date '+%Y-%m-%d %r'
307: + END=1593108235
306: ++ date '+%Y-%m-%d %r'
310: ++ date '+%Y-%m-%d %r'
311: ++ date '+%Y-%m-%d %r'
308: + END=1593108235
304: + END=1593108235
305: + END=1593108235
319: ++ date '+%Y-%m-%d %r'
312: ++ date '+%Y-%m-%d %r'
315: ++ date '+%Y-%m-%d %r'
304: ++ date '+%Y-%m-%d %r'
307: ++ date '+%Y-%m-%d %r'
308: ++ date '+%Y-%m-%d %r'
318: + END_FMT='2020-06-25 11:03:55 AM'
305: ++ date '+%Y-%m-%d %r'
318: ENDING TIMING RUN AT 2020-06-25 11:03:55 AM
318: + echo 'ENDING TIMING RUN AT 2020-06-25 11:03:55 AM'
318: + RESULT=294
318: + RESULT_NAME=bert
318: + echo 'RESULT,bert,18263,294,root,2020-06-25 10:59:01 AM'
318: RESULT,bert,18263,294,root,2020-06-25 10:59:01 AM
318: + set +x
313: + END_FMT='2020-06-25 11:03:55 AM'
317: + END_FMT='2020-06-25 11:03:55 AM'
313: + echo 'ENDING TIMING RUN AT 2020-06-25 11:03:55 AM'
314: + END_FMT='2020-06-25 11:03:55 AM'
313: ENDING TIMING RUN AT 2020-06-25 11:03:55 AM
314: + echo 'ENDING TIMING RUN AT 2020-06-25 11:03:55 AM'
317: ENDING TIMING RUN AT 2020-06-25 11:03:55 AM
317: + echo 'ENDING TIMING RUN AT 2020-06-25 11:03:55 AM'
313: + RESULT=294
313: + RESULT_NAME=bert
314: ENDING TIMING RUN AT 2020-06-25 11:03:55 AM
317: + RESULT=294
317: + RESULT_NAME=bert
314: + RESULT=294
314: + RESULT_NAME=bert
317: RESULT,bert,18263,294,root,2020-06-25 10:59:01 AM
317: + echo 'RESULT,bert,18263,294,root,2020-06-25 10:59:01 AM'
317: + set +x
313: RESULT,bert,18263,294,root,2020-06-25 10:59:01 AM
313: + echo 'RESULT,bert,18263,294,root,2020-06-25 10:59:01 AM'
313: + set +x
314: RESULT,bert,18263,294,root,2020-06-25 10:59:01 AM
314: + echo 'RESULT,bert,18263,294,root,2020-06-25 10:59:01 AM'
314: + set +x
306: + END_FMT='2020-06-25 11:03:55 AM'
310: + END_FMT='2020-06-25 11:03:55 AM'
311: + END_FMT='2020-06-25 11:03:55 AM'
306: ENDING TIMING RUN AT 2020-06-25 11:03:55 AM
306: + echo 'ENDING TIMING RUN AT 2020-06-25 11:03:55 AM'
311: ENDING TIMING RUN AT 2020-06-25 11:03:55 AM
311: + echo 'ENDING TIMING RUN AT 2020-06-25 11:03:55 AM'
306: + RESULT=294
310: ENDING TIMING RUN AT 2020-06-25 11:03:55 AM
310: + echo 'ENDING TIMING RUN AT 2020-06-25 11:03:55 AM'
310: + RESULT=294
310: + RESULT_NAME=bert
311: + RESULT=294
311: + RESULT_NAME=bert
306: RESULT,bert,18263,294,root,2020-06-25 10:59:01 AM
306: + RESULT_NAME=bert
306: + echo 'RESULT,bert,18263,294,root,2020-06-25 10:59:01 AM'
306: + set +x
311: RESULT,bert,18263,294,root,2020-06-25 10:59:01 AM
311: + echo 'RESULT,bert,18263,294,root,2020-06-25 10:59:01 AM'
311: + set +x
319: + END_FMT='2020-06-25 11:03:55 AM'
310: RESULT,bert,18263,294,root,2020-06-25 10:59:01 AM
310: + echo 'RESULT,bert,18263,294,root,2020-06-25 10:59:01 AM'
310: + set +x
319: + echo 'ENDING TIMING RUN AT 2020-06-25 11:03:55 AM'
319: + RESULT=294
319: ENDING TIMING RUN AT 2020-06-25 11:03:55 AM
319: + RESULT_NAME=bert
319: RESULT,bert,18263,294,root,2020-06-25 10:59:01 AM
319: + echo 'RESULT,bert,18263,294,root,2020-06-25 10:59:01 AM'
319: + set +x
315: + END_FMT='2020-06-25 11:03:55 AM'
312: + END_FMT='2020-06-25 11:03:55 AM'
315: ENDING TIMING RUN AT 2020-06-25 11:03:55 AM
315: + echo 'ENDING TIMING RUN AT 2020-06-25 11:03:55 AM'
312: ENDING TIMING RUN AT 2020-06-25 11:03:55 AM
312: + echo 'ENDING TIMING RUN AT 2020-06-25 11:03:55 AM'
315: + RESULT=294
315: + RESULT_NAME=bert
312: + RESULT=294
312: + RESULT_NAME=bert
315: RESULT,bert,18263,294,root,2020-06-25 10:59:01 AM
315: + echo 'RESULT,bert,18263,294,root,2020-06-25 10:59:01 AM'
315: + set +x
304: + END_FMT='2020-06-25 11:03:55 AM'
312: RESULT,bert,18263,294,root,2020-06-25 10:59:01 AM
312: + echo 'RESULT,bert,18263,294,root,2020-06-25 10:59:01 AM'
312: + set +x
304: ENDING TIMING RUN AT 2020-06-25 11:03:55 AM
304: + echo 'ENDING TIMING RUN AT 2020-06-25 11:03:55 AM'
304: + RESULT=294
304: + RESULT_NAME=bert
304: RESULT,bert,18263,294,root,2020-06-25 10:59:01 AM
304: + echo 'RESULT,bert,18263,294,root,2020-06-25 10:59:01 AM'
304: + set +x
307: + END_FMT='2020-06-25 11:03:55 AM'
305: + END_FMT='2020-06-25 11:03:55 AM'
308: + END_FMT='2020-06-25 11:03:55 AM'
305: ENDING TIMING RUN AT 2020-06-25 11:03:55 AM
305: + echo 'ENDING TIMING RUN AT 2020-06-25 11:03:55 AM'
305: + RESULT=294
305: + RESULT_NAME=bert
305: + echo 'RESULT,bert,18263,294,root,2020-06-25 10:59:01 AM'
305: + set +x
307: ENDING TIMING RUN AT 2020-06-25 11:03:55 AM
307: RESULT,bert,18263,294,root,2020-06-25 10:59:01 AM
307: + echo 'ENDING TIMING RUN AT 2020-06-25 11:03:55 AM'
307: + RESULT=294
307: + RESULT_NAME=bert
307: + echo 'RESULT,bert,18263,294,root,2020-06-25 10:59:01 AM'
307: + set +x
308: ENDING TIMING RUN AT 2020-06-25 11:03:55 AM
308: RESULT,bert,18263,294,root,2020-06-25 10:59:01 AM
308: + echo 'ENDING TIMING RUN AT 2020-06-25 11:03:55 AM'
308: + RESULT=294
308: + RESULT_NAME=bert
308: + echo 'RESULT,bert,18263,294,root,2020-06-25 10:59:01 AM'
308: + set +x
305: RESULT,bert,18263,294,root,2020-06-25 10:59:01 AM
217: ++ date +%s
220: ++ date +%s
219: ++ date +%s
215: ++ date +%s
221: ++ date +%s
213: ++ date +%s
214: ++ date +%s
212: ++ date +%s
217: + END=1593108235
218: ++ date +%s
223: ++ date +%s
220: + END=1593108235
222: ++ date +%s
208: ++ date +%s
209: ++ date +%s
216: ++ date +%s
217: ++ date '+%Y-%m-%d %r'
220: ++ date '+%Y-%m-%d %r'
213: + END=1593108235
215: + END=1593108235
221: + END=1593108235
219: + END=1593108235
214: + END=1593108235
223: + END=1593108235
208: + END=1593108235
212: + END=1593108235
218: + END=1593108235
219: ++ date '+%Y-%m-%d %r'
222: + END=1593108235
221: ++ date '+%Y-%m-%d %r'
214: ++ date '+%Y-%m-%d %r'
209: + END=1593108235
213: ++ date '+%Y-%m-%d %r'
215: ++ date '+%Y-%m-%d %r'
216: + END=1593108235
217: + END_FMT='2020-06-25 11:03:55 AM'
217: ENDING TIMING RUN AT 2020-06-25 11:03:55 AM
217: + echo 'ENDING TIMING RUN AT 2020-06-25 11:03:55 AM'
217: + RESULT=294
217: + RESULT_NAME=bert
217: RESULT,bert,18263,294,root,2020-06-25 10:59:01 AM
217: + echo 'RESULT,bert,18263,294,root,2020-06-25 10:59:01 AM'
217: + set +x
220: + END_FMT='2020-06-25 11:03:55 AM'
220: + echo 'ENDING TIMING RUN AT 2020-06-25 11:03:55 AM'
220: ENDING TIMING RUN AT 2020-06-25 11:03:55 AM
220: + RESULT=294
220: + RESULT_NAME=bert
223: ++ date '+%Y-%m-%d %r'
220: RESULT,bert,18263,294,root,2020-06-25 10:59:01 AM
220: + echo 'RESULT,bert,18263,294,root,2020-06-25 10:59:01 AM'
220: + set +x
212: ++ date '+%Y-%m-%d %r'
208: ++ date '+%Y-%m-%d %r'
218: ++ date '+%Y-%m-%d %r'
222: ++ date '+%Y-%m-%d %r'
209: ++ date '+%Y-%m-%d %r'
216: ++ date '+%Y-%m-%d %r'
208: slurmstepd: error: _is_a_lwp: open() /proc/85032/status failed: No such file or directory
219: ENDING TIMING RUN AT 2020-06-25 11:03:55 AM
219: RESULT,bert,18263,294,root,2020-06-25 10:59:01 AM
219: + END_FMT='2020-06-25 11:03:55 AM'
219: + echo 'ENDING TIMING RUN AT 2020-06-25 11:03:55 AM'
219: + RESULT=294
219: + RESULT_NAME=bert
219: + echo 'RESULT,bert,18263,294,root,2020-06-25 10:59:01 AM'
219: + set +x
221: ENDING TIMING RUN AT 2020-06-25 11:03:55 AM
221: RESULT,bert,18263,294,root,2020-06-25 10:59:01 AM
221: + END_FMT='2020-06-25 11:03:55 AM'
221: + echo 'ENDING TIMING RUN AT 2020-06-25 11:03:55 AM'
221: + RESULT=294
221: + RESULT_NAME=bert
221: + echo 'RESULT,bert,18263,294,root,2020-06-25 10:59:01 AM'
221: + set +x
213: + END_FMT='2020-06-25 11:03:55 AM'
214: + END_FMT='2020-06-25 11:03:55 AM'
215: + END_FMT='2020-06-25 11:03:55 AM'
223: + END_FMT='2020-06-25 11:03:55 AM'
213: + echo 'ENDING TIMING RUN AT 2020-06-25 11:03:55 AM'
214: ENDING TIMING RUN AT 2020-06-25 11:03:55 AM
214: + echo 'ENDING TIMING RUN AT 2020-06-25 11:03:55 AM'
213: ENDING TIMING RUN AT 2020-06-25 11:03:55 AM
214: + RESULT=294
214: + RESULT_NAME=bert
215: ENDING TIMING RUN AT 2020-06-25 11:03:55 AM
215: + echo 'ENDING TIMING RUN AT 2020-06-25 11:03:55 AM'
223: ENDING TIMING RUN AT 2020-06-25 11:03:55 AM
223: + echo 'ENDING TIMING RUN AT 2020-06-25 11:03:55 AM'
223: + RESULT=294
223: + RESULT_NAME=bert
213: RESULT,bert,18263,294,root,2020-06-25 10:59:01 AM
213: + RESULT=294
213: + RESULT_NAME=bert
213: + echo 'RESULT,bert,18263,294,root,2020-06-25 10:59:01 AM'
213: + set +x
214: RESULT,bert,18263,294,root,2020-06-25 10:59:01 AM
214: + echo 'RESULT,bert,18263,294,root,2020-06-25 10:59:01 AM'
214: + set +x
215: + RESULT=294
215: + RESULT_NAME=bert
215: + echo 'RESULT,bert,18263,294,root,2020-06-25 10:59:01 AM'
215: + set +x
223: + echo 'RESULT,bert,18263,294,root,2020-06-25 10:59:01 AM'
223: + set +x
215: RESULT,bert,18263,294,root,2020-06-25 10:59:01 AM
223: RESULT,bert,18263,294,root,2020-06-25 10:59:01 AM
212: + END_FMT='2020-06-25 11:03:55 AM'
218: + END_FMT='2020-06-25 11:03:55 AM'
212: ENDING TIMING RUN AT 2020-06-25 11:03:55 AM
212: + echo 'ENDING TIMING RUN AT 2020-06-25 11:03:55 AM'
212: + RESULT=294
212: + RESULT_NAME=bert
218: + echo 'ENDING TIMING RUN AT 2020-06-25 11:03:55 AM'
218: ENDING TIMING RUN AT 2020-06-25 11:03:55 AM
212: RESULT,bert,18263,294,root,2020-06-25 10:59:01 AM
212: + echo 'RESULT,bert,18263,294,root,2020-06-25 10:59:01 AM'
212: + set +x
218: + RESULT=294
218: + RESULT_NAME=bert
218: RESULT,bert,18263,294,root,2020-06-25 10:59:01 AM
218: + echo 'RESULT,bert,18263,294,root,2020-06-25 10:59:01 AM'
218: + set +x
208: ENDING TIMING RUN AT 2020-06-25 11:03:55 AM
208: + END_FMT='2020-06-25 11:03:55 AM'
208: + echo 'ENDING TIMING RUN AT 2020-06-25 11:03:55 AM'
208: + RESULT=294
208: + RESULT_NAME=bert
208: + echo 'RESULT,bert,18263,294,root,2020-06-25 10:59:01 AM'
208: + set +x
209: ENDING TIMING RUN AT 2020-06-25 11:03:55 AM
209: RESULT,bert,18263,294,root,2020-06-25 10:59:01 AM
209: + END_FMT='2020-06-25 11:03:55 AM'
209: + echo 'ENDING TIMING RUN AT 2020-06-25 11:03:55 AM'
209: + RESULT=294
209: + RESULT_NAME=bert
209: + echo 'RESULT,bert,18263,294,root,2020-06-25 10:59:01 AM'
209: + set +x
216: ENDING TIMING RUN AT 2020-06-25 11:03:55 AM
216: RESULT,bert,18263,294,root,2020-06-25 10:59:01 AM
216: + END_FMT='2020-06-25 11:03:55 AM'
216: + echo 'ENDING TIMING RUN AT 2020-06-25 11:03:55 AM'
216: + RESULT=294
216: + RESULT_NAME=bert
216: + echo 'RESULT,bert,18263,294,root,2020-06-25 10:59:01 AM'
216: + set +x
222: ENDING TIMING RUN AT 2020-06-25 11:03:55 AM
222: RESULT,bert,18263,294,root,2020-06-25 10:59:01 AM
222: + END_FMT='2020-06-25 11:03:55 AM'
222: + echo 'ENDING TIMING RUN AT 2020-06-25 11:03:55 AM'
222: + RESULT=294
222: + RESULT_NAME=bert
222: + echo 'RESULT,bert,18263,294,root,2020-06-25 10:59:01 AM'
222: + set +x
208: RESULT,bert,18263,294,root,2020-06-25 10:59:01 AM
 40: ++ date +%s
 40: + END=1593108235
 40: ++ date '+%Y-%m-%d %r'
 43: ++ date +%s
 40: ENDING TIMING RUN AT 2020-06-25 11:03:55 AM
 40: + END_FMT='2020-06-25 11:03:55 AM'
 40: + echo 'ENDING TIMING RUN AT 2020-06-25 11:03:55 AM'
 43: + END=1593108235
 40: + RESULT=295
 40: + RESULT_NAME=bert
 47: ++ date +%s
 40: RESULT,bert,18263,295,root,2020-06-25 10:59:00 AM
 40: + echo 'RESULT,bert,18263,295,root,2020-06-25 10:59:00 AM'
 40: + set +x
 35: ++ date +%s
 36: ++ date +%s
 33: ++ date +%s
 37: ++ date +%s
 39: ++ date +%s
 32: ++ date +%s
 38: ++ date +%s
 34: ++ date +%s
 42: ++ date +%s
 43: ++ date '+%Y-%m-%d %r'
 41: ++ date +%s
 45: ++ date +%s
 47: + END=1593108235
 33: + END=1593108235
 36: + END=1593108235
 47: ++ date '+%Y-%m-%d %r'
 39: + END=1593108235
 37: + END=1593108235
 32: + END=1593108235
 38: + END=1593108235
 33: ++ date '+%Y-%m-%d %r'
 35: + END=1593108235
 36: ++ date '+%Y-%m-%d %r'
 41: + END=1593108235
 42: + END=1593108235
 43: + END_FMT='2020-06-25 11:03:55 AM'
 45: + END=1593108235
 34: + END=1593108235
 43: ENDING TIMING RUN AT 2020-06-25 11:03:55 AM
 43: + echo 'ENDING TIMING RUN AT 2020-06-25 11:03:55 AM'
 43: + RESULT=295
 43: + RESULT_NAME=bert
 43: RESULT,bert,18263,295,root,2020-06-25 10:59:00 AM
 43: + echo 'RESULT,bert,18263,295,root,2020-06-25 10:59:00 AM'
 39: ++ date '+%Y-%m-%d %r'
 43: + set +x
 37: ++ date '+%Y-%m-%d %r'
 47: + END_FMT='2020-06-25 11:03:55 AM'
 47: + echo 'ENDING TIMING RUN AT 2020-06-25 11:03:55 AM'
 47: ENDING TIMING RUN AT 2020-06-25 11:03:55 AM
 47: + RESULT=295
 47: + RESULT_NAME=bert
 47: RESULT,bert,18263,295,root,2020-06-25 10:59:00 AM
 47: + echo 'RESULT,bert,18263,295,root,2020-06-25 10:59:00 AM'
 47: + set +x
 38: ++ date '+%Y-%m-%d %r'
 35: ++ date '+%Y-%m-%d %r'
 42: ++ date '+%Y-%m-%d %r'
 45: ++ date '+%Y-%m-%d %r'
 32: ++ date '+%Y-%m-%d %r'
 34: ++ date '+%Y-%m-%d %r'
 41: ++ date '+%Y-%m-%d %r'
 39: + END_FMT='2020-06-25 11:03:55 AM'
 37: + END_FMT='2020-06-25 11:03:55 AM'
 39: + echo 'ENDING TIMING RUN AT 2020-06-25 11:03:55 AM'
 39: ENDING TIMING RUN AT 2020-06-25 11:03:55 AM
 37: ENDING TIMING RUN AT 2020-06-25 11:03:55 AM
 37: + echo 'ENDING TIMING RUN AT 2020-06-25 11:03:55 AM'
 39: + RESULT=295
 39: + RESULT_NAME=bert
 37: + RESULT=295
 37: + RESULT_NAME=bert
 39: RESULT,bert,18263,295,root,2020-06-25 10:59:00 AM
 39: + echo 'RESULT,bert,18263,295,root,2020-06-25 10:59:00 AM'
 39: + set +x
 37: RESULT,bert,18263,295,root,2020-06-25 10:59:00 AM
 37: + echo 'RESULT,bert,18263,295,root,2020-06-25 10:59:00 AM'
 37: + set +x
 33: + END_FMT='2020-06-25 11:03:55 AM'
 36: + END_FMT='2020-06-25 11:03:55 AM'
 33: + echo 'ENDING TIMING RUN AT 2020-06-25 11:03:55 AM'
 36: + echo 'ENDING TIMING RUN AT 2020-06-25 11:03:55 AM'
 33: ENDING TIMING RUN AT 2020-06-25 11:03:55 AM
 36: ENDING TIMING RUN AT 2020-06-25 11:03:55 AM
 33: + RESULT=295
 36: + RESULT=295
 33: + RESULT_NAME=bert
 36: + RESULT_NAME=bert
 33: RESULT,bert,18263,295,root,2020-06-25 10:59:00 AM
 33: + echo 'RESULT,bert,18263,295,root,2020-06-25 10:59:00 AM'
 33: + set +x
 36: RESULT,bert,18263,295,root,2020-06-25 10:59:00 AM
 36: + echo 'RESULT,bert,18263,295,root,2020-06-25 10:59:00 AM'
 36: + set +x
 42: + END_FMT='2020-06-25 11:03:55 AM'
 38: + END_FMT='2020-06-25 11:03:55 AM'
 42: ENDING TIMING RUN AT 2020-06-25 11:03:55 AM
 42: + echo 'ENDING TIMING RUN AT 2020-06-25 11:03:55 AM'
 38: ENDING TIMING RUN AT 2020-06-25 11:03:55 AM
 38: + echo 'ENDING TIMING RUN AT 2020-06-25 11:03:55 AM'
 32: + END_FMT='2020-06-25 11:03:55 AM'
 42: + RESULT=295
 42: + RESULT_NAME=bert
 38: + RESULT=295
 38: + RESULT_NAME=bert
 42: RESULT,bert,18263,295,root,2020-06-25 10:59:00 AM
 42: + echo 'RESULT,bert,18263,295,root,2020-06-25 10:59:00 AM'
 42: + set +x
 32: ENDING TIMING RUN AT 2020-06-25 11:03:55 AM
 32: + echo 'ENDING TIMING RUN AT 2020-06-25 11:03:55 AM'
 38: RESULT,bert,18263,295,root,2020-06-25 10:59:00 AM
 38: + echo 'RESULT,bert,18263,295,root,2020-06-25 10:59:00 AM'
 38: + set +x
 32: + RESULT=295
 32: + RESULT_NAME=bert
 32: RESULT,bert,18263,295,root,2020-06-25 10:59:00 AM
 32: + echo 'RESULT,bert,18263,295,root,2020-06-25 10:59:00 AM'
 32: + set +x
 35: + END_FMT='2020-06-25 11:03:55 AM'
 45: + END_FMT='2020-06-25 11:03:55 AM'
 34: + END_FMT='2020-06-25 11:03:55 AM'
 35: + echo 'ENDING TIMING RUN AT 2020-06-25 11:03:55 AM'
 34: ENDING TIMING RUN AT 2020-06-25 11:03:55 AM
 34: + echo 'ENDING TIMING RUN AT 2020-06-25 11:03:55 AM'
 35: ENDING TIMING RUN AT 2020-06-25 11:03:55 AM
 45: ENDING TIMING RUN AT 2020-06-25 11:03:55 AM
 45: + echo 'ENDING TIMING RUN AT 2020-06-25 11:03:55 AM'
 35: + RESULT=295
 35: + RESULT_NAME=bert
 34: + RESULT=295
 45: + RESULT=295
 45: + RESULT_NAME=bert
 34: + RESULT_NAME=bert
 35: RESULT,bert,18263,295,root,2020-06-25 10:59:00 AM
 35: + echo 'RESULT,bert,18263,295,root,2020-06-25 10:59:00 AM'
 35: + set +x
 34: + echo 'RESULT,bert,18263,295,root,2020-06-25 10:59:00 AM'
 45: RESULT,bert,18263,295,root,2020-06-25 10:59:00 AM
 45: + echo 'RESULT,bert,18263,295,root,2020-06-25 10:59:00 AM'
 34: RESULT,bert,18263,295,root,2020-06-25 10:59:00 AM
 34: + set +x
 41: + END_FMT='2020-06-25 11:03:55 AM'
 45: + set +x
 41: ENDING TIMING RUN AT 2020-06-25 11:03:55 AM
 41: + echo 'ENDING TIMING RUN AT 2020-06-25 11:03:55 AM'
 41: + RESULT=295
 41: + RESULT_NAME=bert
 41: RESULT,bert,18263,295,root,2020-06-25 10:59:00 AM
 41: + echo 'RESULT,bert,18263,295,root,2020-06-25 10:59:00 AM'
 41: + set +x
347: ++ date +%s
351: ++ date +%s
339: ++ date +%s
345: ++ date +%s
343: ++ date +%s
349: ++ date +%s
348: ++ date +%s
337: ++ date +%s
341: ++ date +%s
338: ++ date +%s
344: ++ date +%s
346: ++ date +%s
336: ++ date +%s
347: + END=1593108235
351: + END=1593108235
339: + END=1593108235
345: + END=1593108235
349: + END=1593108235
343: + END=1593108235
348: + END=1593108235
351: ++ date '+%Y-%m-%d %r'
339: ++ date '+%Y-%m-%d %r'
347: ++ date '+%Y-%m-%d %r'
338: + END=1593108235
345: ++ date '+%Y-%m-%d %r'
336: + END=1593108235
346: + END=1593108235
344: + END=1593108235
349: ++ date '+%Y-%m-%d %r'
343: ++ date '+%Y-%m-%d %r'
348: ++ date '+%Y-%m-%d %r'
342: ++ date +%s
341: + END=1593108235
337: + END=1593108235
338: ++ date '+%Y-%m-%d %r'
344: ++ date '+%Y-%m-%d %r'
336: ++ date '+%Y-%m-%d %r'
346: ++ date '+%Y-%m-%d %r'
351: + END_FMT='2020-06-25 11:03:55 AM'
341: ++ date '+%Y-%m-%d %r'
351: ENDING TIMING RUN AT 2020-06-25 11:03:55 AM
351: + echo 'ENDING TIMING RUN AT 2020-06-25 11:03:55 AM'
337: ++ date '+%Y-%m-%d %r'
351: + RESULT=294
351: + RESULT_NAME=bert
351: RESULT,bert,18263,294,root,2020-06-25 10:59:01 AM
351: + echo 'RESULT,bert,18263,294,root,2020-06-25 10:59:01 AM'
351: + set +x
339: + END_FMT='2020-06-25 11:03:55 AM'
345: + END_FMT='2020-06-25 11:03:55 AM'
339: ENDING TIMING RUN AT 2020-06-25 11:03:55 AM
339: + echo 'ENDING TIMING RUN AT 2020-06-25 11:03:55 AM'
345: ENDING TIMING RUN AT 2020-06-25 11:03:55 AM
345: + echo 'ENDING TIMING RUN AT 2020-06-25 11:03:55 AM'
339: + RESULT=294
339: + RESULT_NAME=bert
345: + RESULT=294
345: + RESULT_NAME=bert
347: + END_FMT='2020-06-25 11:03:55 AM'
339: RESULT,bert,18263,294,root,2020-06-25 10:59:01 AM
339: + echo 'RESULT,bert,18263,294,root,2020-06-25 10:59:01 AM'
339: + set +x
345: RESULT,bert,18263,294,root,2020-06-25 10:59:01 AM
345: + echo 'RESULT,bert,18263,294,root,2020-06-25 10:59:01 AM'
345: + set +x
347: + echo 'ENDING TIMING RUN AT 2020-06-25 11:03:55 AM'
347: ENDING TIMING RUN AT 2020-06-25 11:03:55 AM
347: + RESULT=294
347: + RESULT_NAME=bert
347: RESULT,bert,18263,294,root,2020-06-25 10:59:01 AM
347: + echo 'RESULT,bert,18263,294,root,2020-06-25 10:59:01 AM'
347: + set +x
349: + END_FMT='2020-06-25 11:03:55 AM'
349: + echo 'ENDING TIMING RUN AT 2020-06-25 11:03:55 AM'
349: ENDING TIMING RUN AT 2020-06-25 11:03:55 AM
349: + RESULT=294
349: + RESULT_NAME=bert
349: RESULT,bert,18263,294,root,2020-06-25 10:59:01 AM
349: + echo 'RESULT,bert,18263,294,root,2020-06-25 10:59:01 AM'
349: + set +x
343: + END_FMT='2020-06-25 11:03:55 AM'
343: + echo 'ENDING TIMING RUN AT 2020-06-25 11:03:55 AM'
343: ENDING TIMING RUN AT 2020-06-25 11:03:55 AM
343: + RESULT=294
343: + RESULT_NAME=bert
343: RESULT,bert,18263,294,root,2020-06-25 10:59:01 AM
343: + echo 'RESULT,bert,18263,294,root,2020-06-25 10:59:01 AM'
343: + set +x
348: + END_FMT='2020-06-25 11:03:55 AM'
348: ENDING TIMING RUN AT 2020-06-25 11:03:55 AM
348: + echo 'ENDING TIMING RUN AT 2020-06-25 11:03:55 AM'
348: + RESULT=294
348: + RESULT_NAME=bert
348: RESULT,bert,18263,294,root,2020-06-25 10:59:01 AM
348: + echo 'RESULT,bert,18263,294,root,2020-06-25 10:59:01 AM'
348: + set +x
342: + END=1593108235
338: + END_FMT='2020-06-25 11:03:55 AM'
338: ENDING TIMING RUN AT 2020-06-25 11:03:55 AM
338: + echo 'ENDING TIMING RUN AT 2020-06-25 11:03:55 AM'
338: + RESULT=294
338: + RESULT_NAME=bert
338: RESULT,bert,18263,294,root,2020-06-25 10:59:01 AM
338: + echo 'RESULT,bert,18263,294,root,2020-06-25 10:59:01 AM'
338: + set +x
344: + END_FMT='2020-06-25 11:03:55 AM'
346: + END_FMT='2020-06-25 11:03:55 AM'
346: + echo 'ENDING TIMING RUN AT 2020-06-25 11:03:55 AM'
344: ENDING TIMING RUN AT 2020-06-25 11:03:55 AM
344: + echo 'ENDING TIMING RUN AT 2020-06-25 11:03:55 AM'
346: ENDING TIMING RUN AT 2020-06-25 11:03:55 AM
344: + RESULT=294
346: + RESULT=294
346: + RESULT_NAME=bert
344: + RESULT_NAME=bert
336: + END_FMT='2020-06-25 11:03:55 AM'
344: RESULT,bert,18263,294,root,2020-06-25 10:59:01 AM
344: + echo 'RESULT,bert,18263,294,root,2020-06-25 10:59:01 AM'
344: + set +x
346: RESULT,bert,18263,294,root,2020-06-25 10:59:01 AM
346: + echo 'RESULT,bert,18263,294,root,2020-06-25 10:59:01 AM'
346: + set +x
336: + echo 'ENDING TIMING RUN AT 2020-06-25 11:03:55 AM'
336: ENDING TIMING RUN AT 2020-06-25 11:03:55 AM
336: + RESULT=294
336: + RESULT_NAME=bert
336: RESULT,bert,18263,294,root,2020-06-25 10:59:01 AM
336: + echo 'RESULT,bert,18263,294,root,2020-06-25 10:59:01 AM'
336: + set +x
337: + END_FMT='2020-06-25 11:03:55 AM'
337: + echo 'ENDING TIMING RUN AT 2020-06-25 11:03:55 AM'
337: ENDING TIMING RUN AT 2020-06-25 11:03:55 AM
341: + END_FMT='2020-06-25 11:03:55 AM'
337: + RESULT=294
337: + RESULT_NAME=bert
341: + echo 'ENDING TIMING RUN AT 2020-06-25 11:03:55 AM'
341: ENDING TIMING RUN AT 2020-06-25 11:03:55 AM
337: RESULT,bert,18263,294,root,2020-06-25 10:59:01 AM
337: + echo 'RESULT,bert,18263,294,root,2020-06-25 10:59:01 AM'
337: + set +x
342: ++ date '+%Y-%m-%d %r'
341: + RESULT=294
341: + RESULT_NAME=bert
341: RESULT,bert,18263,294,root,2020-06-25 10:59:01 AM
341: + echo 'RESULT,bert,18263,294,root,2020-06-25 10:59:01 AM'
341: + set +x
342: + END_FMT='2020-06-25 11:03:55 AM'
342: ENDING TIMING RUN AT 2020-06-25 11:03:55 AM
342: + echo 'ENDING TIMING RUN AT 2020-06-25 11:03:55 AM'
342: + RESULT=294
342: + RESULT_NAME=bert
342: RESULT,bert,18263,294,root,2020-06-25 10:59:01 AM
342: + echo 'RESULT,bert,18263,294,root,2020-06-25 10:59:01 AM'
342: + set +x
 91: ++ date +%s
 93: ++ date +%s
 94: ++ date +%s
 80: ++ date +%s
 81: ++ date +%s
 83: ++ date +%s
 87: ++ date +%s
 85: ++ date +%s
 84: ++ date +%s
 92: ++ date +%s
 95: ++ date +%s
 86: ++ date +%s
 88: ++ date +%s
 82: ++ date +%s
 80: + END=1593108235
 91: + END=1593108235
 93: + END=1593108235
 94: + END=1593108235
 80: ++ date '+%Y-%m-%d %r'
 93: ++ date '+%Y-%m-%d %r'
 91: ++ date '+%Y-%m-%d %r'
 81: + END=1593108235
 83: + END=1593108235
 84: + END=1593108235
 85: + END=1593108235
 92: + END=1593108235
 81: ++ date '+%Y-%m-%d %r'
 95: + END=1593108235
 83: ++ date '+%Y-%m-%d %r'
 87: + END=1593108235
 88: + END=1593108235
 94: ++ date '+%Y-%m-%d %r'
 82: + END=1593108235
 86: + END=1593108235
 84: ++ date '+%Y-%m-%d %r'
 93: + END_FMT='2020-06-25 11:03:55 AM'
 95: ++ date '+%Y-%m-%d %r'
 93: + echo 'ENDING TIMING RUN AT 2020-06-25 11:03:55 AM'
 93: ENDING TIMING RUN AT 2020-06-25 11:03:55 AM
 92: ++ date '+%Y-%m-%d %r'
 93: + RESULT=295
 93: + RESULT_NAME=bert
 93: RESULT,bert,18263,295,root,2020-06-25 10:59:00 AM
 93: + echo 'RESULT,bert,18263,295,root,2020-06-25 10:59:00 AM'
 93: + set +x
 80: + END_FMT='2020-06-25 11:03:55 AM'
 91: + END_FMT='2020-06-25 11:03:55 AM'
 85: ++ date '+%Y-%m-%d %r'
 91: + echo 'ENDING TIMING RUN AT 2020-06-25 11:03:55 AM'
 80: ENDING TIMING RUN AT 2020-06-25 11:03:55 AM
 80: + echo 'ENDING TIMING RUN AT 2020-06-25 11:03:55 AM'
 87: ++ date '+%Y-%m-%d %r'
 91: ENDING TIMING RUN AT 2020-06-25 11:03:55 AM
 80: + RESULT=295
 80: + RESULT_NAME=bert
 91: + RESULT=295
 91: + RESULT_NAME=bert
 80: + echo 'RESULT,bert,18263,295,root,2020-06-25 10:59:00 AM'
 91: + echo 'RESULT,bert,18263,295,root,2020-06-25 10:59:00 AM'
 80: RESULT,bert,18263,295,root,2020-06-25 10:59:00 AM
 80: + set +x
 88: ++ date '+%Y-%m-%d %r'
 91: RESULT,bert,18263,295,root,2020-06-25 10:59:00 AM
 91: + set +x
 82: ++ date '+%Y-%m-%d %r'
 86: ++ date '+%Y-%m-%d %r'
 81: + END_FMT='2020-06-25 11:03:55 AM'
 81: + echo 'ENDING TIMING RUN AT 2020-06-25 11:03:55 AM'
 81: ENDING TIMING RUN AT 2020-06-25 11:03:55 AM
 81: + RESULT=295
 81: + RESULT_NAME=bert
 81: RESULT,bert,18263,295,root,2020-06-25 10:59:00 AM
 81: + echo 'RESULT,bert,18263,295,root,2020-06-25 10:59:00 AM'
 81: + set +x
 83: + END_FMT='2020-06-25 11:03:55 AM'
 83: + echo 'ENDING TIMING RUN AT 2020-06-25 11:03:55 AM'
 83: ENDING TIMING RUN AT 2020-06-25 11:03:55 AM
 83: + RESULT=295
 83: + RESULT_NAME=bert
 83: RESULT,bert,18263,295,root,2020-06-25 10:59:00 AM
 83: + echo 'RESULT,bert,18263,295,root,2020-06-25 10:59:00 AM'
 83: + set +x
 94: + END_FMT='2020-06-25 11:03:55 AM'
 94: + echo 'ENDING TIMING RUN AT 2020-06-25 11:03:55 AM'
 94: ENDING TIMING RUN AT 2020-06-25 11:03:55 AM
 94: + RESULT=295
 94: + RESULT_NAME=bert
 94: RESULT,bert,18263,295,root,2020-06-25 10:59:00 AM
 94: + echo 'RESULT,bert,18263,295,root,2020-06-25 10:59:00 AM'
 94: + set +x
 84: + END_FMT='2020-06-25 11:03:55 AM'
 84: ENDING TIMING RUN AT 2020-06-25 11:03:55 AM
 84: + echo 'ENDING TIMING RUN AT 2020-06-25 11:03:55 AM'
 84: + RESULT=295
 84: + RESULT_NAME=bert
 95: + END_FMT='2020-06-25 11:03:55 AM'
 84: RESULT,bert,18263,295,root,2020-06-25 10:59:00 AM
 84: + echo 'RESULT,bert,18263,295,root,2020-06-25 10:59:00 AM'
 84: + set +x
 95: ENDING TIMING RUN AT 2020-06-25 11:03:55 AM
 95: + echo 'ENDING TIMING RUN AT 2020-06-25 11:03:55 AM'
 95: + RESULT=295
 95: + RESULT_NAME=bert
 95: RESULT,bert,18263,295,root,2020-06-25 10:59:00 AM
 95: + echo 'RESULT,bert,18263,295,root,2020-06-25 10:59:00 AM'
 95: + set +x
 92: ENDING TIMING RUN AT 2020-06-25 11:03:55 AM
 92: + END_FMT='2020-06-25 11:03:55 AM'
 92: + echo 'ENDING TIMING RUN AT 2020-06-25 11:03:55 AM'
 92: + RESULT=295
 92: + RESULT_NAME=bert
 92: RESULT,bert,18263,295,root,2020-06-25 10:59:00 AM
 92: + echo 'RESULT,bert,18263,295,root,2020-06-25 10:59:00 AM'
 92: + set +x
 87: + END_FMT='2020-06-25 11:03:55 AM'
 87: ENDING TIMING RUN AT 2020-06-25 11:03:55 AM
 87: + echo 'ENDING TIMING RUN AT 2020-06-25 11:03:55 AM'
 87: + RESULT=295
 87: + RESULT_NAME=bert
 88: + END_FMT='2020-06-25 11:03:55 AM'
 87: RESULT,bert,18263,295,root,2020-06-25 10:59:00 AM
 87: + echo 'RESULT,bert,18263,295,root,2020-06-25 10:59:00 AM'
 87: + set +x
 88: + echo 'ENDING TIMING RUN AT 2020-06-25 11:03:55 AM'
 88: ENDING TIMING RUN AT 2020-06-25 11:03:55 AM
 88: + RESULT=295
 88: + RESULT_NAME=bert
 82: + END_FMT='2020-06-25 11:03:55 AM'
 85: + END_FMT='2020-06-25 11:03:55 AM'
 88: RESULT,bert,18263,295,root,2020-06-25 10:59:00 AM
 88: + echo 'RESULT,bert,18263,295,root,2020-06-25 10:59:00 AM'
 88: + set +x
 82: ENDING TIMING RUN AT 2020-06-25 11:03:55 AM
 82: + echo 'ENDING TIMING RUN AT 2020-06-25 11:03:55 AM'
 82: + RESULT=295
 85: ENDING TIMING RUN AT 2020-06-25 11:03:55 AM
 85: + echo 'ENDING TIMING RUN AT 2020-06-25 11:03:55 AM'
 85: + RESULT=295
 82: + RESULT_NAME=bert
 85: + RESULT_NAME=bert
 86: + END_FMT='2020-06-25 11:03:55 AM'
 82: RESULT,bert,18263,295,root,2020-06-25 10:59:00 AM
 82: + echo 'RESULT,bert,18263,295,root,2020-06-25 10:59:00 AM'
 82: + set +x
 85: RESULT,bert,18263,295,root,2020-06-25 10:59:00 AM
 85: + echo 'RESULT,bert,18263,295,root,2020-06-25 10:59:00 AM'
 85: + set +x
 86: ENDING TIMING RUN AT 2020-06-25 11:03:55 AM
 86: + echo 'ENDING TIMING RUN AT 2020-06-25 11:03:55 AM'
 86: + RESULT=295
 86: RESULT,bert,18263,295,root,2020-06-25 10:59:00 AM
 86: + RESULT_NAME=bert
 86: + echo 'RESULT,bert,18263,295,root,2020-06-25 10:59:00 AM'
 86: + set +x
 28: ++ date +%s
 28: + END=1593108235
 28: ++ date '+%Y-%m-%d %r'
 28: + END_FMT='2020-06-25 11:03:55 AM'
 28: ENDING TIMING RUN AT 2020-06-25 11:03:55 AM
 28: + echo 'ENDING TIMING RUN AT 2020-06-25 11:03:55 AM'
 28: + RESULT=294
 28: RESULT,bert,18263,294,root,2020-06-25 10:59:01 AM
 28: + RESULT_NAME=bert
 28: + echo 'RESULT,bert,18263,294,root,2020-06-25 10:59:01 AM'
 28: + set +x
456: ++ date +%s
451: ++ date +%s
450: ++ date +%s
456: + END=1593108235
458: ++ date +%s
449: ++ date +%s
453: ++ date +%s
455: ++ date +%s
461: ++ date +%s
459: ++ date +%s
452: ++ date +%s
457: ++ date +%s
451: + END=1593108235
454: ++ date +%s
463: ++ date +%s
456: ++ date '+%Y-%m-%d %r'
462: ++ date +%s
450: + END=1593108235
458: + END=1593108235
451: ++ date '+%Y-%m-%d %r'
450: ++ date '+%Y-%m-%d %r'
458: ++ date '+%Y-%m-%d %r'
449: + END=1593108235
455: + END=1593108235
461: + END=1593108235
453: + END=1593108235
459: + END=1593108235
454: + END=1593108235
462: + END=1593108235
452: + END=1593108235
456: + END_FMT='2020-06-25 11:03:55 AM'
457: + END=1593108235
463: + END=1593108235
449: ++ date '+%Y-%m-%d %r'
456: ENDING TIMING RUN AT 2020-06-25 11:03:55 AM
456: + echo 'ENDING TIMING RUN AT 2020-06-25 11:03:55 AM'
456: + RESULT=295
456: + RESULT_NAME=bert
455: ++ date '+%Y-%m-%d %r'
456: RESULT,bert,18263,295,root,2020-06-25 10:59:00 AM
456: + echo 'RESULT,bert,18263,295,root,2020-06-25 10:59:00 AM'
456: + set +x
461: ++ date '+%Y-%m-%d %r'
451: + END_FMT='2020-06-25 11:03:55 AM'
451: + echo 'ENDING TIMING RUN AT 2020-06-25 11:03:55 AM'
451: ENDING TIMING RUN AT 2020-06-25 11:03:55 AM
451: + RESULT=295
451: + RESULT_NAME=bert
451: RESULT,bert,18263,295,root,2020-06-25 10:59:00 AM
451: + echo 'RESULT,bert,18263,295,root,2020-06-25 10:59:00 AM'
451: + set +x
453: ++ date '+%Y-%m-%d %r'
459: ++ date '+%Y-%m-%d %r'
450: + END_FMT='2020-06-25 11:03:55 AM'
450: ENDING TIMING RUN AT 2020-06-25 11:03:55 AM
450: + echo 'ENDING TIMING RUN AT 2020-06-25 11:03:55 AM'
454: ++ date '+%Y-%m-%d %r'
450: + RESULT=295
450: + RESULT_NAME=bert
462: ++ date '+%Y-%m-%d %r'
450: RESULT,bert,18263,295,root,2020-06-25 10:59:00 AM
450: + echo 'RESULT,bert,18263,295,root,2020-06-25 10:59:00 AM'
450: + set +x
458: + END_FMT='2020-06-25 11:03:55 AM'
458: ENDING TIMING RUN AT 2020-06-25 11:03:55 AM
458: + echo 'ENDING TIMING RUN AT 2020-06-25 11:03:55 AM'
452: ++ date '+%Y-%m-%d %r'
458: + RESULT=295
458: + RESULT_NAME=bert
458: RESULT,bert,18263,295,root,2020-06-25 10:59:00 AM
458: + echo 'RESULT,bert,18263,295,root,2020-06-25 10:59:00 AM'
458: + set +x
457: ++ date '+%Y-%m-%d %r'
463: ++ date '+%Y-%m-%d %r'
449: + END_FMT='2020-06-25 11:03:55 AM'
449: ENDING TIMING RUN AT 2020-06-25 11:03:55 AM
449: + echo 'ENDING TIMING RUN AT 2020-06-25 11:03:55 AM'
449: + RESULT=295
449: + RESULT_NAME=bert
449: RESULT,bert,18263,295,root,2020-06-25 10:59:00 AM
449: + echo 'RESULT,bert,18263,295,root,2020-06-25 10:59:00 AM'
449: + set +x
455: + END_FMT='2020-06-25 11:03:55 AM'
455: + echo 'ENDING TIMING RUN AT 2020-06-25 11:03:55 AM'
455: ENDING TIMING RUN AT 2020-06-25 11:03:55 AM
455: + RESULT=295
455: + RESULT_NAME=bert
461: + END_FMT='2020-06-25 11:03:55 AM'
455: + echo 'RESULT,bert,18263,295,root,2020-06-25 10:59:00 AM'
455: RESULT,bert,18263,295,root,2020-06-25 10:59:00 AM
455: + set +x
461: + echo 'ENDING TIMING RUN AT 2020-06-25 11:03:55 AM'
461: ENDING TIMING RUN AT 2020-06-25 11:03:55 AM
461: + RESULT=295
461: + RESULT_NAME=bert
461: RESULT,bert,18263,295,root,2020-06-25 10:59:00 AM
461: + echo 'RESULT,bert,18263,295,root,2020-06-25 10:59:00 AM'
461: + set +x
459: + END_FMT='2020-06-25 11:03:55 AM'
459: ENDING TIMING RUN AT 2020-06-25 11:03:55 AM
459: + echo 'ENDING TIMING RUN AT 2020-06-25 11:03:55 AM'
453: + END_FMT='2020-06-25 11:03:55 AM'
459: + RESULT=295
459: + RESULT_NAME=bert
459: RESULT,bert,18263,295,root,2020-06-25 10:59:00 AM
459: + echo 'RESULT,bert,18263,295,root,2020-06-25 10:59:00 AM'
459: + set +x
453: ENDING TIMING RUN AT 2020-06-25 11:03:55 AM
453: + echo 'ENDING TIMING RUN AT 2020-06-25 11:03:55 AM'
453: + RESULT=295
453: + RESULT_NAME=bert
453: + echo 'RESULT,bert,18263,295,root,2020-06-25 10:59:00 AM'
453: RESULT,bert,18263,295,root,2020-06-25 10:59:00 AM
453: + set +x
454: ENDING TIMING RUN AT 2020-06-25 11:03:55 AM
454: + END_FMT='2020-06-25 11:03:55 AM'
454: + echo 'ENDING TIMING RUN AT 2020-06-25 11:03:55 AM'
454: + RESULT=295
454: + RESULT_NAME=bert
454: + echo 'RESULT,bert,18263,295,root,2020-06-25 10:59:00 AM'
454: + set +x
462: ENDING TIMING RUN AT 2020-06-25 11:03:55 AM
462: RESULT,bert,18263,295,root,2020-06-25 10:59:00 AM
462: + END_FMT='2020-06-25 11:03:55 AM'
462: + echo 'ENDING TIMING RUN AT 2020-06-25 11:03:55 AM'
462: + RESULT=295
462: + RESULT_NAME=bert
462: + echo 'RESULT,bert,18263,295,root,2020-06-25 10:59:00 AM'
452: + END_FMT='2020-06-25 11:03:55 AM'
454: RESULT,bert,18263,295,root,2020-06-25 10:59:00 AM
462: + set +x
452: ENDING TIMING RUN AT 2020-06-25 11:03:55 AM
452: + echo 'ENDING TIMING RUN AT 2020-06-25 11:03:55 AM'
452: + RESULT=295
452: + RESULT_NAME=bert
457: + END_FMT='2020-06-25 11:03:55 AM'
452: RESULT,bert,18263,295,root,2020-06-25 10:59:00 AM
452: + echo 'RESULT,bert,18263,295,root,2020-06-25 10:59:00 AM'
452: + set +x
457: ENDING TIMING RUN AT 2020-06-25 11:03:55 AM
457: + echo 'ENDING TIMING RUN AT 2020-06-25 11:03:55 AM'
457: + RESULT=295
457: + RESULT_NAME=bert
457: RESULT,bert,18263,295,root,2020-06-25 10:59:00 AM
457: + echo 'RESULT,bert,18263,295,root,2020-06-25 10:59:00 AM'
457: + set +x
463: ENDING TIMING RUN AT 2020-06-25 11:03:55 AM
463: + END_FMT='2020-06-25 11:03:55 AM'
463: + echo 'ENDING TIMING RUN AT 2020-06-25 11:03:55 AM'
463: + RESULT=295
463: + RESULT_NAME=bert
463: + echo 'RESULT,bert,18263,295,root,2020-06-25 10:59:00 AM'
463: RESULT,bert,18263,295,root,2020-06-25 10:59:00 AM
463: + set +x
  0: ++ date +%s
  0: + END=1593108235
  0: ++ date '+%Y-%m-%d %r'
  0: + END_FMT='2020-06-25 11:03:55 AM'
  0: + echo 'ENDING TIMING RUN AT 2020-06-25 11:03:55 AM'
  0: ENDING TIMING RUN AT 2020-06-25 11:03:55 AM
  0: + RESULT=294
  0: + RESULT_NAME=bert
  0: RESULT,bert,18263,294,root,2020-06-25 10:59:01 AM
  0: + echo 'RESULT,bert,18263,294,root,2020-06-25 10:59:01 AM'
  0: + set +x
467: ++ date +%s
474: ++ date +%s
477: ++ date +%s
478: ++ date +%s
475: ++ date +%s
469: ++ date +%s
465: ++ date +%s
471: ++ date +%s
466: ++ date +%s
470: ++ date +%s
479: ++ date +%s
464: ++ date +%s
476: ++ date +%s
468: ++ date +%s
467: + END=1593108235
474: + END=1593108235
475: + END=1593108235
478: + END=1593108235
469: + END=1593108235
465: + END=1593108235
467: ++ date '+%Y-%m-%d %r'
471: + END=1593108235
477: + END=1593108235
468: + END=1593108235
476: + END=1593108235
464: + END=1593108235
470: + END=1593108235
466: + END=1593108235
474: ++ date '+%Y-%m-%d %r'
475: ++ date '+%Y-%m-%d %r'
465: ++ date '+%Y-%m-%d %r'
471: ++ date '+%Y-%m-%d %r'
478: ++ date '+%Y-%m-%d %r'
479: + END=1593108235
469: ++ date '+%Y-%m-%d %r'
476: ++ date '+%Y-%m-%d %r'
477: ++ date '+%Y-%m-%d %r'
470: ++ date '+%Y-%m-%d %r'
468: ++ date '+%Y-%m-%d %r'
464: ++ date '+%Y-%m-%d %r'
466: ++ date '+%Y-%m-%d %r'
479: ++ date '+%Y-%m-%d %r'
467: + END_FMT='2020-06-25 11:03:55 AM'
467: + echo 'ENDING TIMING RUN AT 2020-06-25 11:03:55 AM'
467: ENDING TIMING RUN AT 2020-06-25 11:03:55 AM
467: + RESULT=294
467: + RESULT_NAME=bert
467: + echo 'RESULT,bert,18263,294,root,2020-06-25 10:59:01 AM'
467: RESULT,bert,18263,294,root,2020-06-25 10:59:01 AM
467: + set +x
474: + END_FMT='2020-06-25 11:03:55 AM'
474: + echo 'ENDING TIMING RUN AT 2020-06-25 11:03:55 AM'
474: ENDING TIMING RUN AT 2020-06-25 11:03:55 AM
474: + RESULT=295
474: + RESULT_NAME=bert
474: RESULT,bert,18263,295,root,2020-06-25 10:59:01 AM
474: + echo 'RESULT,bert,18263,295,root,2020-06-25 10:59:01 AM'
474: + set +x
465: + END_FMT='2020-06-25 11:03:55 AM'
475: + END_FMT='2020-06-25 11:03:55 AM'
475: + echo 'ENDING TIMING RUN AT 2020-06-25 11:03:55 AM'
465: ENDING TIMING RUN AT 2020-06-25 11:03:55 AM
465: + echo 'ENDING TIMING RUN AT 2020-06-25 11:03:55 AM'
475: ENDING TIMING RUN AT 2020-06-25 11:03:55 AM
475: + RESULT=295
475: + RESULT_NAME=bert
465: + RESULT=295
465: + RESULT_NAME=bert
475: RESULT,bert,18263,295,root,2020-06-25 10:59:00 AM
475: + echo 'RESULT,bert,18263,295,root,2020-06-25 10:59:00 AM'
475: + set +x
465: RESULT,bert,18263,295,root,2020-06-25 10:59:01 AM
465: + echo 'RESULT,bert,18263,295,root,2020-06-25 10:59:01 AM'
465: + set +x
478: ENDING TIMING RUN AT 2020-06-25 11:03:55 AM
478: + END_FMT='2020-06-25 11:03:55 AM'
478: + echo 'ENDING TIMING RUN AT 2020-06-25 11:03:55 AM'
478: + RESULT=295
478: + RESULT_NAME=bert
478: RESULT,bert,18263,295,root,2020-06-25 10:59:01 AM
478: + echo 'RESULT,bert,18263,295,root,2020-06-25 10:59:01 AM'
478: + set +x
468: + END_FMT='2020-06-25 11:03:55 AM'
468: + echo 'ENDING TIMING RUN AT 2020-06-25 11:03:55 AM'
469: + END_FMT='2020-06-25 11:03:55 AM'
469: + echo 'ENDING TIMING RUN AT 2020-06-25 11:03:55 AM'
469: + RESULT=295
469: + RESULT_NAME=bert
470: ENDING TIMING RUN AT 2020-06-25 11:03:55 AM
470: RESULT,bert,18263,295,root,2020-06-25 10:59:01 AM
470: + END_FMT='2020-06-25 11:03:55 AM'
470: + echo 'ENDING TIMING RUN AT 2020-06-25 11:03:55 AM'
470: + RESULT=295
470: + RESULT_NAME=bert
470: + echo 'RESULT,bert,18263,295,root,2020-06-25 10:59:01 AM'
470: + set +x
471: ENDING TIMING RUN AT 2020-06-25 11:03:55 AM
471: RESULT,bert,18263,295,root,2020-06-25 10:59:01 AM
471: + END_FMT='2020-06-25 11:03:55 AM'
471: + echo 'ENDING TIMING RUN AT 2020-06-25 11:03:55 AM'
471: + RESULT=295
471: + RESULT_NAME=bert
471: + echo 'RESULT,bert,18263,295,root,2020-06-25 10:59:01 AM'
471: + set +x
476: + END_FMT='2020-06-25 11:03:55 AM'
476: + echo 'ENDING TIMING RUN AT 2020-06-25 11:03:55 AM'
476: + RESULT=295
476: + RESULT_NAME=bert
464: ENDING TIMING RUN AT 2020-06-25 11:03:55 AM
464: + END_FMT='2020-06-25 11:03:55 AM'
464: + echo 'ENDING TIMING RUN AT 2020-06-25 11:03:55 AM'
464: + RESULT=294
464: + RESULT_NAME=bert
466: ENDING TIMING RUN AT 2020-06-25 11:03:55 AM
466: + END_FMT='2020-06-25 11:03:55 AM'
466: + echo 'ENDING TIMING RUN AT 2020-06-25 11:03:55 AM'
466: + RESULT=294
466: + RESULT_NAME=bert
468: ENDING TIMING RUN AT 2020-06-25 11:03:55 AM
468: RESULT,bert,18263,294,root,2020-06-25 10:59:01 AM
468: + RESULT=294
468: + RESULT_NAME=bert
468: + echo 'RESULT,bert,18263,294,root,2020-06-25 10:59:01 AM'
468: + set +x
469: ENDING TIMING RUN AT 2020-06-25 11:03:55 AM
469: RESULT,bert,18263,295,root,2020-06-25 10:59:01 AM
469: + echo 'RESULT,bert,18263,295,root,2020-06-25 10:59:01 AM'
469: + set +x
476: ENDING TIMING RUN AT 2020-06-25 11:03:55 AM
476: RESULT,bert,18263,295,root,2020-06-25 10:59:00 AM
476: + echo 'RESULT,bert,18263,295,root,2020-06-25 10:59:00 AM'
476: + set +x
477: + END_FMT='2020-06-25 11:03:55 AM'
477: + echo 'ENDING TIMING RUN AT 2020-06-25 11:03:55 AM'
477: + RESULT=295
477: + RESULT_NAME=bert
464: RESULT,bert,18263,294,root,2020-06-25 10:59:01 AM
464: + echo 'RESULT,bert,18263,294,root,2020-06-25 10:59:01 AM'
464: + set +x
466: RESULT,bert,18263,294,root,2020-06-25 10:59:01 AM
466: + echo 'RESULT,bert,18263,294,root,2020-06-25 10:59:01 AM'
466: + set +x
477: ENDING TIMING RUN AT 2020-06-25 11:03:55 AM
477: RESULT,bert,18263,295,root,2020-06-25 10:59:00 AM
477: + echo 'RESULT,bert,18263,295,root,2020-06-25 10:59:00 AM'
477: + set +x
479: ENDING TIMING RUN AT 2020-06-25 11:03:55 AM
479: + END_FMT='2020-06-25 11:03:55 AM'
479: + echo 'ENDING TIMING RUN AT 2020-06-25 11:03:55 AM'
479: + RESULT=295
479: + RESULT_NAME=bert
479: RESULT,bert,18263,295,root,2020-06-25 10:59:00 AM
479: + echo 'RESULT,bert,18263,295,root,2020-06-25 10:59:00 AM'
479: + set +x
165: ++ date +%s
165: + END=1593108235
165: ++ date '+%Y-%m-%d %r'
165: + END_FMT='2020-06-25 11:03:55 AM'
165: + echo 'ENDING TIMING RUN AT 2020-06-25 11:03:55 AM'
165: ENDING TIMING RUN AT 2020-06-25 11:03:55 AM
165: + RESULT=295
165: + RESULT_NAME=bert
165: RESULT,bert,18263,295,root,2020-06-25 10:59:00 AM
165: + echo 'RESULT,bert,18263,295,root,2020-06-25 10:59:00 AM'
165: + set +x
228: ++ date +%s
228: + END=1593108235
228: ++ date '+%Y-%m-%d %r'
228: + END_FMT='2020-06-25 11:03:55 AM'
228: ENDING TIMING RUN AT 2020-06-25 11:03:55 AM
228: + echo 'ENDING TIMING RUN AT 2020-06-25 11:03:55 AM'
228: + RESULT=295
228: + RESULT_NAME=bert
228: RESULT,bert,18263,295,root,2020-06-25 10:59:00 AM
228: + echo 'RESULT,bert,18263,295,root,2020-06-25 10:59:00 AM'
228: + set +x
256: ++ date +%s
256: + END=1593108235
256: ++ date '+%Y-%m-%d %r'
256: + END_FMT='2020-06-25 11:03:55 AM'
256: + echo 'ENDING TIMING RUN AT 2020-06-25 11:03:55 AM'
256: ENDING TIMING RUN AT 2020-06-25 11:03:55 AM
256: + RESULT=295
256: + RESULT_NAME=bert
256: RESULT,bert,18263,295,root,2020-06-25 10:59:00 AM
256: + echo 'RESULT,bert,18263,295,root,2020-06-25 10:59:00 AM'
256: + set +x
431: ++ date +%s
431: + END=1593108235
431: ++ date '+%Y-%m-%d %r'
431: ENDING TIMING RUN AT 2020-06-25 11:03:55 AM
431: + END_FMT='2020-06-25 11:03:55 AM'
431: + echo 'ENDING TIMING RUN AT 2020-06-25 11:03:55 AM'
431: + RESULT=294
431: + RESULT_NAME=bert
431: RESULT,bert,18263,294,root,2020-06-25 10:59:01 AM
431: + echo 'RESULT,bert,18263,294,root,2020-06-25 10:59:01 AM'
431: + set +x
123: ++ date +%s
123: + END=1593108235
123: ++ date '+%Y-%m-%d %r'
123: ENDING TIMING RUN AT 2020-06-25 11:03:55 AM
123: + END_FMT='2020-06-25 11:03:55 AM'
123: + echo 'ENDING TIMING RUN AT 2020-06-25 11:03:55 AM'
123: + RESULT=295
123: + RESULT_NAME=bert
123: RESULT,bert,18263,295,root,2020-06-25 10:59:00 AM
123: + echo 'RESULT,bert,18263,295,root,2020-06-25 10:59:00 AM'
123: + set +x
437: ++ date +%s
437: + END=1593108235
437: ++ date '+%Y-%m-%d %r'
437: + END_FMT='2020-06-25 11:03:55 AM'
437: + echo 'ENDING TIMING RUN AT 2020-06-25 11:03:55 AM'
437: ENDING TIMING RUN AT 2020-06-25 11:03:55 AM
437: + RESULT=295
437: + RESULT_NAME=bert
437: RESULT,bert,18263,295,root,2020-06-25 10:59:00 AM
437: + echo 'RESULT,bert,18263,295,root,2020-06-25 10:59:00 AM'
437: + set +x
 76: ++ date +%s
 76: + END=1593108235
 76: ++ date '+%Y-%m-%d %r'
 76: + END_FMT='2020-06-25 11:03:55 AM'
 76: + echo 'ENDING TIMING RUN AT 2020-06-25 11:03:55 AM'
 76: ENDING TIMING RUN AT 2020-06-25 11:03:55 AM
 76: + RESULT=295
 76: + RESULT_NAME=bert
 76: RESULT,bert,18263,295,root,2020-06-25 10:59:00 AM
 76: + echo 'RESULT,bert,18263,295,root,2020-06-25 10:59:00 AM'
 76: + set +x
392: ++ date +%s
392: + END=1593108235
392: ++ date '+%Y-%m-%d %r'
392: + END_FMT='2020-06-25 11:03:55 AM'
392: + echo 'ENDING TIMING RUN AT 2020-06-25 11:03:55 AM'
392: ENDING TIMING RUN AT 2020-06-25 11:03:55 AM
392: + RESULT=294
392: + RESULT_NAME=bert
392: RESULT,bert,18263,294,root,2020-06-25 10:59:01 AM
392: + echo 'RESULT,bert,18263,294,root,2020-06-25 10:59:01 AM'
392: + set +x
330: ++ date +%s
330: + END=1593108236
330: ++ date '+%Y-%m-%d %r'
408: ++ date +%s
330: + END_FMT='2020-06-25 11:03:56 AM'
330: + echo 'ENDING TIMING RUN AT 2020-06-25 11:03:56 AM'
330: ENDING TIMING RUN AT 2020-06-25 11:03:56 AM
330: RESULT,bert,18263,296,root,2020-06-25 10:59:00 AM
330: + RESULT=296
330: + RESULT_NAME=bert
330: + echo 'RESULT,bert,18263,296,root,2020-06-25 10:59:00 AM'
330: + set +x
408: + END=1593108236
408: ++ date '+%Y-%m-%d %r'
408: ENDING TIMING RUN AT 2020-06-25 11:03:56 AM
408: + END_FMT='2020-06-25 11:03:56 AM'
408: + echo 'ENDING TIMING RUN AT 2020-06-25 11:03:56 AM'
408: + RESULT=295
408: + RESULT_NAME=bert
408: RESULT,bert,18263,295,root,2020-06-25 10:59:01 AM
408: + echo 'RESULT,bert,18263,295,root,2020-06-25 10:59:01 AM'
408: + set +x
240: ++ date +%s
240: + END=1593108236
240: ++ date '+%Y-%m-%d %r'
240: + END_FMT='2020-06-25 11:03:56 AM'
240: + echo 'ENDING TIMING RUN AT 2020-06-25 11:03:56 AM'
240: ENDING TIMING RUN AT 2020-06-25 11:03:56 AM
240: + RESULT=295
240: + RESULT_NAME=bert
240: + echo 'RESULT,bert,18263,295,root,2020-06-25 10:59:01 AM'
240: RESULT,bert,18263,295,root,2020-06-25 10:59:01 AM
240: + set +x
 48: ++ date +%s
 48: + END=1593108236
 48: ++ date '+%Y-%m-%d %r'
 48: + END_FMT='2020-06-25 11:03:56 AM'
 48: + echo 'ENDING TIMING RUN AT 2020-06-25 11:03:56 AM'
 48: ENDING TIMING RUN AT 2020-06-25 11:03:56 AM
 48: + RESULT=296
 48: + RESULT_NAME=bert
 48: RESULT,bert,18263,296,root,2020-06-25 10:59:00 AM
 48: + echo 'RESULT,bert,18263,296,root,2020-06-25 10:59:00 AM'
 48: + set +x
356: ++ date +%s
356: + END=1593108236
356: ++ date '+%Y-%m-%d %r'
356: + END_FMT='2020-06-25 11:03:56 AM'
356: ENDING TIMING RUN AT 2020-06-25 11:03:56 AM
356: + echo 'ENDING TIMING RUN AT 2020-06-25 11:03:56 AM'
356: + RESULT=295
356: + RESULT_NAME=bert
356: RESULT,bert,18263,295,root,2020-06-25 10:59:01 AM
356: + echo 'RESULT,bert,18263,295,root,2020-06-25 10:59:01 AM'
356: + set +x
494: ++ date +%s
494: + END=1593108236
494: ++ date '+%Y-%m-%d %r'
494: + END_FMT='2020-06-25 11:03:56 AM'
494: + echo 'ENDING TIMING RUN AT 2020-06-25 11:03:56 AM'
494: ENDING TIMING RUN AT 2020-06-25 11:03:56 AM
494: + RESULT=295
494: + RESULT_NAME=bert
494: RESULT,bert,18263,295,root,2020-06-25 10:59:01 AM
494: + echo 'RESULT,bert,18263,295,root,2020-06-25 10:59:01 AM'
494: + set +x
369: ++ date +%s
136: ++ date +%s
369: + END=1593108236
136: + END=1593108236
136: ++ date '+%Y-%m-%d %r'
369: ++ date '+%Y-%m-%d %r'
136: + END_FMT='2020-06-25 11:03:56 AM'
136: + echo 'ENDING TIMING RUN AT 2020-06-25 11:03:56 AM'
136: ENDING TIMING RUN AT 2020-06-25 11:03:56 AM
369: + END_FMT='2020-06-25 11:03:56 AM'
369: + echo 'ENDING TIMING RUN AT 2020-06-25 11:03:56 AM'
136: + RESULT=296
136: + RESULT_NAME=bert
136: RESULT,bert,18263,296,root,2020-06-25 10:59:00 AM
136: + echo 'RESULT,bert,18263,296,root,2020-06-25 10:59:00 AM'
136: + set +x
369: ENDING TIMING RUN AT 2020-06-25 11:03:56 AM
369: + RESULT=295
369: + RESULT_NAME=bert
369: RESULT,bert,18263,295,root,2020-06-25 10:59:01 AM
369: + echo 'RESULT,bert,18263,295,root,2020-06-25 10:59:01 AM'
369: + set +x
154: ++ date +%s
154: + END=1593108236
511: ++ date +%s
154: ++ date '+%Y-%m-%d %r'
511: + END=1593108236
154: + END_FMT='2020-06-25 11:03:56 AM'
154: + echo 'ENDING TIMING RUN AT 2020-06-25 11:03:56 AM'
154: ENDING TIMING RUN AT 2020-06-25 11:03:56 AM
154: + RESULT=295
154: + RESULT_NAME=bert
154: RESULT,bert,18263,295,root,2020-06-25 10:59:01 AM
154: + echo 'RESULT,bert,18263,295,root,2020-06-25 10:59:01 AM'
154: + set +x
511: ++ date '+%Y-%m-%d %r'
511: + END_FMT='2020-06-25 11:03:56 AM'
511: + echo 'ENDING TIMING RUN AT 2020-06-25 11:03:56 AM'
511: ENDING TIMING RUN AT 2020-06-25 11:03:56 AM
511: + RESULT=296
511: + RESULT_NAME=bert
511: RESULT,bert,18263,296,root,2020-06-25 10:59:00 AM
511: + echo 'RESULT,bert,18263,296,root,2020-06-25 10:59:00 AM'
511: + set +x
186: ++ date +%s
186: + END=1593108236
186: ++ date '+%Y-%m-%d %r'
186: + END_FMT='2020-06-25 11:03:56 AM'
186: + echo 'ENDING TIMING RUN AT 2020-06-25 11:03:56 AM'
186: ENDING TIMING RUN AT 2020-06-25 11:03:56 AM
186: + RESULT=295
186: + RESULT_NAME=bert
186: RESULT,bert,18263,295,root,2020-06-25 10:59:01 AM
186: + echo 'RESULT,bert,18263,295,root,2020-06-25 10:59:01 AM'
186: + set +x
206: ++ date +%s
206: + END=1593108236
206: ++ date '+%Y-%m-%d %r'
206: + END_FMT='2020-06-25 11:03:56 AM'
206: + echo 'ENDING TIMING RUN AT 2020-06-25 11:03:56 AM'
206: ENDING TIMING RUN AT 2020-06-25 11:03:56 AM
206: + RESULT=296
206: + RESULT_NAME=bert
206: RESULT,bert,18263,296,root,2020-06-25 10:59:00 AM
206: + echo 'RESULT,bert,18263,296,root,2020-06-25 10:59:00 AM'
206: + set +x
278: ++ date +%s
278: + END=1593108236
278: ++ date '+%Y-%m-%d %r'
278: + END_FMT='2020-06-25 11:03:56 AM'
278: + echo 'ENDING TIMING RUN AT 2020-06-25 11:03:56 AM'
278: ENDING TIMING RUN AT 2020-06-25 11:03:56 AM
278: + RESULT=295
278: + RESULT_NAME=bert
278: RESULT,bert,18263,295,root,2020-06-25 10:59:01 AM
278: + echo 'RESULT,bert,18263,295,root,2020-06-25 10:59:01 AM'
278: + set +x
109: ++ date +%s
109: + END=1593108236
109: ++ date '+%Y-%m-%d %r'
298: ++ date +%s
109: ENDING TIMING RUN AT 2020-06-25 11:03:56 AM
109: + END_FMT='2020-06-25 11:03:56 AM'
109: + echo 'ENDING TIMING RUN AT 2020-06-25 11:03:56 AM'
109: + RESULT=295
109: + RESULT_NAME=bert
109: + echo 'RESULT,bert,18263,295,root,2020-06-25 10:59:01 AM'
109: RESULT,bert,18263,295,root,2020-06-25 10:59:01 AM
109: + set +x
298: + END=1593108236
298: ++ date '+%Y-%m-%d %r'
298: + END_FMT='2020-06-25 11:03:56 AM'
298: + echo 'ENDING TIMING RUN AT 2020-06-25 11:03:56 AM'
298: ENDING TIMING RUN AT 2020-06-25 11:03:56 AM
298: RESULT,bert,18263,295,root,2020-06-25 10:59:01 AM
298: + RESULT=295
298: + RESULT_NAME=bert
298: + echo 'RESULT,bert,18263,295,root,2020-06-25 10:59:01 AM'
298: + set +x
309: ++ date +%s
309: + END=1593108236
309: ++ date '+%Y-%m-%d %r'
309: + END_FMT='2020-06-25 11:03:56 AM'
309: + echo 'ENDING TIMING RUN AT 2020-06-25 11:03:56 AM'
309: ENDING TIMING RUN AT 2020-06-25 11:03:56 AM
309: + RESULT=295
309: + RESULT_NAME=bert
309: RESULT,bert,18263,295,root,2020-06-25 10:59:01 AM
309: + echo 'RESULT,bert,18263,295,root,2020-06-25 10:59:01 AM'
309: + set +x
210: ++ date +%s
210: + END=1593108236
210: ++ date '+%Y-%m-%d %r'
210: + END_FMT='2020-06-25 11:03:56 AM'
210: ENDING TIMING RUN AT 2020-06-25 11:03:56 AM
210: + echo 'ENDING TIMING RUN AT 2020-06-25 11:03:56 AM'
210: + RESULT=295
210: + RESULT_NAME=bert
210: RESULT,bert,18263,295,root,2020-06-25 10:59:01 AM
210: + echo 'RESULT,bert,18263,295,root,2020-06-25 10:59:01 AM'
210: + set +x
 44: ++ date +%s
 44: + END=1593108236
 44: ++ date '+%Y-%m-%d %r'
 44: + END_FMT='2020-06-25 11:03:56 AM'
 44: + echo 'ENDING TIMING RUN AT 2020-06-25 11:03:56 AM'
 44: ENDING TIMING RUN AT 2020-06-25 11:03:56 AM
 44: + RESULT=296
 44: + RESULT_NAME=bert
 44: RESULT,bert,18263,296,root,2020-06-25 10:59:00 AM
 44: + echo 'RESULT,bert,18263,296,root,2020-06-25 10:59:00 AM'
 44: + set +x
340: ++ date +%s
340: + END=1593108236
340: ++ date '+%Y-%m-%d %r'
340: + END_FMT='2020-06-25 11:03:56 AM'
340: + echo 'ENDING TIMING RUN AT 2020-06-25 11:03:56 AM'
340: ENDING TIMING RUN AT 2020-06-25 11:03:56 AM
340: + RESULT=295
340: + RESULT_NAME=bert
340: RESULT,bert,18263,295,root,2020-06-25 10:59:01 AM
340: + echo 'RESULT,bert,18263,295,root,2020-06-25 10:59:01 AM'
340: + set +x
 90: ++ date +%s
 90: + END=1593108236
 90: ++ date '+%Y-%m-%d %r'
 90: + END_FMT='2020-06-25 11:03:56 AM'
 90: + echo 'ENDING TIMING RUN AT 2020-06-25 11:03:56 AM'
 90: ENDING TIMING RUN AT 2020-06-25 11:03:56 AM
 90: + RESULT=296
 90: + RESULT_NAME=bert
 90: RESULT,bert,18263,296,root,2020-06-25 10:59:00 AM
 90: + echo 'RESULT,bert,18263,296,root,2020-06-25 10:59:00 AM'
 90: + set +x
460: ++ date +%s
460: + END=1593108236
460: ++ date '+%Y-%m-%d %r'
460: + END_FMT='2020-06-25 11:03:56 AM'
460: + echo 'ENDING TIMING RUN AT 2020-06-25 11:03:56 AM'
460: ENDING TIMING RUN AT 2020-06-25 11:03:56 AM
460: + RESULT=296
460: + RESULT_NAME=bert
460: RESULT,bert,18263,296,root,2020-06-25 10:59:00 AM
460: + echo 'RESULT,bert,18263,296,root,2020-06-25 10:59:00 AM'
460: + set +x
473: ++ date +%s
473: + END=1593108236
473: ++ date '+%Y-%m-%d %r'
473: + END_FMT='2020-06-25 11:03:56 AM'
473: ENDING TIMING RUN AT 2020-06-25 11:03:56 AM
473: + echo 'ENDING TIMING RUN AT 2020-06-25 11:03:56 AM'
473: + RESULT=295
473: + RESULT_NAME=bert
473: + echo 'RESULT,bert,18263,295,root,2020-06-25 10:59:01 AM'
473: RESULT,bert,18263,295,root,2020-06-25 10:59:01 AM
473: + set +x
