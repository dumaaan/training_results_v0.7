+ echo 'Beginning trial 5 of 5'
Beginning trial 5 of 5
+ srun --ntasks=10 --container-name=translation python -c '
import mlperf_log_utils
from mlperf_logging.mllog import constants
mlperf_log_utils.mlperf_submission_log(constants.TRANSFORMER)'
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
:::MLLOG {"namespace": "", "time_ms": 1592948144013, "event_type": "POINT_IN_TIME", "key": "submission_benchmark", "value": "transformer", "metadata": {"file": "/workspace/translation/mlperf_log_utils.py", "lineno": 84}}
:::MLLOG {"namespace": "", "time_ms": 1592948144052, "event_type": "POINT_IN_TIME", "key": "submission_org", "value": "NVIDIA", "metadata": {"file": "/workspace/translation/mlperf_log_utils.py", "lineno": 89}}
:::MLLOG {"namespace": "", "time_ms": 1592948144052, "event_type": "POINT_IN_TIME", "key": "submission_division", "value": "closed", "metadata": {"file": "/workspace/translation/mlperf_log_utils.py", "lineno": 93}}
:::MLLOG {"namespace": "", "time_ms": 1592948144052, "event_type": "POINT_IN_TIME", "key": "submission_status", "value": "onprem", "metadata": {"file": "/workspace/translation/mlperf_log_utils.py", "lineno": 97}}
:::MLLOG {"namespace": "", "time_ms": 1592948144052, "event_type": "POINT_IN_TIME", "key": "submission_platform", "value": "10xNVIDIA DGX A100", "metadata": {"file": "/workspace/translation/mlperf_log_utils.py", "lineno": 101}}
+ '[' 1 -eq 1 ']'
+ srun --ntasks=10 bash -c 'echo -n '\''Clearing cache on '\'' && hostname && sync && sudo /sbin/sysctl vm.drop_caches=3'
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
Clearing cache on luna-0237
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
Clearing cache on luna-0231
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
Clearing cache on luna-0232
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
Clearing cache on luna-0235
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
Clearing cache on luna-0230
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
Clearing cache on luna-0228
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
Clearing cache on luna-0236
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
Clearing cache on luna-0234
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
Clearing cache on luna-0229
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
Clearing cache on luna-0233
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
+ srun --ntasks=10 --container-name=translation python -c '
from mlperf_logging.mllog import constants
from mlperf_log_utils import log_event
log_event(key=constants.CACHE_CLEAR, value=True)'
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
:::MLLOG {"namespace": "", "time_ms": 1592948149398, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
:::MLLOG {"namespace": "", "time_ms": 1592948149411, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
:::MLLOG {"namespace": "", "time_ms": 1592948149415, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
:::MLLOG {"namespace": "", "time_ms": 1592948149423, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
:::MLLOG {"namespace": "", "time_ms": 1592948149433, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
:::MLLOG {"namespace": "", "time_ms": 1592948149434, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
:::MLLOG {"namespace": "", "time_ms": 1592948149440, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
:::MLLOG {"namespace": "", "time_ms": 1592948149453, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
:::MLLOG {"namespace": "", "time_ms": 1592948149475, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
:::MLLOG {"namespace": "", "time_ms": 1592948149477, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
+ export SEED=1857
+ SEED=1857
+ srun --mpi=none --ntasks=80 --ntasks-per-node=8 --container-name=translation --container-mounts=/lustre/fsr/datasets/xformer_v0p6/utf8:/data,/lustre/fsw/mlperf-ci/14121197/results:/results ./run_and_time.sh
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
Run vars: id 377842 gpus 8 mparams
+ SEED=1857
+ MAX_TOKENS=9216
+ DATASET_DIR=/data
+ MODE=TRAIN
+ NUMEPOCHS=30
+ case "$MODE" in
+ source run_training.sh
+++ date +%s
++ START=1592948151
+++ date '+%Y-%m-%d %r'
Run vars: id 377842 gpus 8 mparams
+ SEED=1857
+ MAX_TOKENS=9216
+ DATASET_DIR=/data
+ MODE=TRAIN
+ NUMEPOCHS=30
+ case "$MODE" in
+ source run_training.sh
Run vars: id 377842 gpus 8 mparams
+ SEED=1857
+ MAX_TOKENS=9216
+ DATASET_DIR=/data
+ MODE=TRAIN
+ NUMEPOCHS=30
+ case "$MODE" in
+ source run_training.sh
+++ date +%s
Run vars: id 377842 gpus 8 mparams
+ SEED=1857
+ MAX_TOKENS=9216
+++ date +%s
+ DATASET_DIR=/data
+ MODE=TRAIN
+ NUMEPOCHS=30
+ case "$MODE" in
+ source run_training.sh
+++ date +%s
Run vars: id 377842 gpus 8 mparams
+ SEED=1857
+ MAX_TOKENS=9216
+ DATASET_DIR=/data
+ MODE=TRAIN
+ NUMEPOCHS=30
+ case "$MODE" in
+ source run_training.sh
++ START_FMT='2020-06-23 02:35:51 PM'
++ echo 'STARTING TIMING RUN AT 2020-06-23 02:35:51 PM'
STARTING TIMING RUN AT 2020-06-23 02:35:51 PM
++ [[ 80 -ne 1 ]]
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
++ export DGXSYSTEM
++ export SLURM_NTASKS_PER_NODE
++ export SLURM_NNODES
++ declare -a CMD
++ '[' -n 4 ']'
++ '[' 80 -gt 10 ']'
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
Run vars: id 377842 gpus 8 mparams
+ SEED=1857
++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 1857 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 9216 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
+ MAX_TOKENS=9216
+ DATASET_DIR=/data
+ MODE=TRAIN
+ NUMEPOCHS=30
+++ date +%s
+ case "$MODE" in
+ source run_training.sh
++ START=1592948151
Run vars: id 377842 gpus 8 mparams
+ SEED=1857
+ MAX_TOKENS=9216
+ DATASET_DIR=/data
+ MODE=TRAIN
+ NUMEPOCHS=30
+ case "$MODE" in
+ source run_training.sh
Run vars: id 377842 gpus 8 mparams
+ SEED=1857
+ MAX_TOKENS=9216
+ DATASET_DIR=/data
+ MODE=TRAIN
+ NUMEPOCHS=30
+ case "$MODE" in
+ source run_training.sh
+++ date '+%Y-%m-%d %r'
+++ date +%s
+++ date +%s
+++ date +%s
++ START=1592948151
++ START=1592948151
+++ date '+%Y-%m-%d %r'
+++ date '+%Y-%m-%d %r'
++ START=1592948151
++ START_FMT='2020-06-23 02:35:51 PM'
++ echo 'STARTING TIMING RUN AT 2020-06-23 02:35:51 PM'
STARTING TIMING RUN AT 2020-06-23 02:35:51 PM
++ [[ 80 -ne 1 ]]
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
++ export DGXSYSTEM
++ export SLURM_NTASKS_PER_NODE
++ export SLURM_NNODES
++ declare -a CMD
++ START=1592948151
++ '[' -n 2 ']'
++ '[' 80 -gt 10 ']'
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+++ date '+%Y-%m-%d %r'
+++ date '+%Y-%m-%d %r'
++ START=1592948151
++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 1857 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 9216 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
+++ date '+%Y-%m-%d %r'
++ START_FMT='2020-06-23 02:35:51 PM'
STARTING TIMING RUN AT 2020-06-23 02:35:51 PM
++ echo 'STARTING TIMING RUN AT 2020-06-23 02:35:51 PM'
++ [[ 80 -ne 1 ]]
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
++ export DGXSYSTEM
++ export SLURM_NTASKS_PER_NODE
++ export SLURM_NNODES
++ declare -a CMD
++ '[' -n 0 ']'
++ '[' 80 -gt 10 ']'
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 1857 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 9216 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
++ START_FMT='2020-06-23 02:35:51 PM'
++ echo 'STARTING TIMING RUN AT 2020-06-23 02:35:51 PM'
STARTING TIMING RUN AT 2020-06-23 02:35:51 PM
++ [[ 80 -ne 1 ]]
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
++ export DGXSYSTEM
++ export SLURM_NTASKS_PER_NODE
++ export SLURM_NNODES
++ START=1592948151
++ declare -a CMD
++ '[' -n 1 ']'
++ '[' 80 -gt 10 ']'
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 1857 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 9216 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
+++ date '+%Y-%m-%d %r'
++ START_FMT='2020-06-23 02:35:51 PM'
STARTING TIMING RUN AT 2020-06-23 02:35:51 PM
++ echo 'STARTING TIMING RUN AT 2020-06-23 02:35:51 PM'
++ [[ 80 -ne 1 ]]
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
++ START_FMT='2020-06-23 02:35:51 PM'
++ export DGXSYSTEM
++ export SLURM_NTASKS_PER_NODE
++ export SLURM_NNODES
++ echo 'STARTING TIMING RUN AT 2020-06-23 02:35:51 PM'
++ declare -a CMD
STARTING TIMING RUN AT 2020-06-23 02:35:51 PM
++ [[ 80 -ne 1 ]]
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
++ export DGXSYSTEM
++ export SLURM_NTASKS_PER_NODE
++ export SLURM_NNODES
++ '[' -n 5 ']'
++ declare -a CMD
++ '[' 80 -gt 10 ']'
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
++ '[' -n 3 ']'
++ '[' 80 -gt 10 ']'
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
++ START_FMT='2020-06-23 02:35:51 PM'
++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 1857 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 9216 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
STARTING TIMING RUN AT 2020-06-23 02:35:51 PM
++ echo 'STARTING TIMING RUN AT 2020-06-23 02:35:51 PM'
++ [[ 80 -ne 1 ]]
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 1857 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 9216 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
++ export DGXSYSTEM
++ export SLURM_NTASKS_PER_NODE
++ export SLURM_NNODES
++ declare -a CMD
++ '[' -n 6 ']'
++ '[' 80 -gt 10 ']'
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 1857 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 9216 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
++ START_FMT='2020-06-23 02:35:51 PM'
STARTING TIMING RUN AT 2020-06-23 02:35:51 PM
++ echo 'STARTING TIMING RUN AT 2020-06-23 02:35:51 PM'
++ [[ 80 -ne 1 ]]
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
++ export DGXSYSTEM
++ export SLURM_NTASKS_PER_NODE
++ export SLURM_NNODES
++ declare -a CMD
++ '[' -n 7 ']'
++ '[' 80 -gt 10 ']'
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 1857 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 9216 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
Run vars: id 377842 gpus 8 mparams
+ SEED=1857
+ MAX_TOKENS=9216
+ DATASET_DIR=/data
+ MODE=TRAIN
+ NUMEPOCHS=30
+ case "$MODE" in
+ source run_training.sh
Run vars: id 377842 gpus 8 mparams
+ SEED=1857
+ MAX_TOKENS=9216
+ DATASET_DIR=/data
+ MODE=TRAIN
+ NUMEPOCHS=30
+ case "$MODE" in
+ source run_training.sh
Run vars: id 377842 gpus 8 mparams
+ SEED=1857
+ MAX_TOKENS=9216
+ DATASET_DIR=/data
+++ date +%s
+ MODE=TRAIN
+ NUMEPOCHS=30
+ case "$MODE" in
+ source run_training.sh
+++ date +%s
+++ date +%s
Run vars: id 377842 gpus 8 mparams
+ SEED=1857
+ MAX_TOKENS=9216
+ DATASET_DIR=/data
+ MODE=TRAIN
+ NUMEPOCHS=30
+ case "$MODE" in
+ source run_training.sh
++ START=1592948151
++ START=1592948151
+++ date +%s
Run vars: id 377842 gpus 8 mparams
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
++ START=1592948151
Run vars: id 377842 gpus 8 mparams
+ SEED=1857
+ MAX_TOKENS=9216
+ DATASET_DIR=/data
+ SEED=1857
+ MODE=TRAIN
+ MAX_TOKENS=9216
+ DATASET_DIR=/data
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ NUMEPOCHS=30
+ case "$MODE" in
+ MODE=TRAIN
+ NUMEPOCHS=30
+ source run_training.sh
+ case "$MODE" in
+++ date '+%Y-%m-%d %r'
+ source run_training.sh
+++ date '+%Y-%m-%d %r'
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
Run vars: id 377842 gpus 8 mparams
+ SEED=1857
+ MAX_TOKENS=9216
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ DATASET_DIR=/data
+ MODE=TRAIN
Run vars: id 377842 gpus 8 mparams
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ NUMEPOCHS=30
+ SEED=1857
+ MAX_TOKENS=9216
+ case "$MODE" in
+ DATASET_DIR=/data
+ MODE=TRAIN
+ NUMEPOCHS=30
+ source run_training.sh
+ case "$MODE" in
+ source run_training.sh
+++ date '+%Y-%m-%d %r'
+++ date +%s
+++ date +%s
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+++ date +%s
+++ date +%s
Run vars: id 377842 gpus 8 mparams
+ SEED=1857
+ MAX_TOKENS=9216
+ DATASET_DIR=/data
+ MODE=TRAIN
+ NUMEPOCHS=30
+ case "$MODE" in
+ source run_training.sh
+++ date +%s
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
++ START=1592948151
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
++ START_FMT='2020-06-23 02:35:51 PM'
STARTING TIMING RUN AT 2020-06-23 02:35:51 PM
++ echo 'STARTING TIMING RUN AT 2020-06-23 02:35:51 PM'
++ [[ 80 -ne 1 ]]
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
++ export DGXSYSTEM
++ export SLURM_NTASKS_PER_NODE
++ export SLURM_NNODES
++ declare -a CMD
+++ date '+%Y-%m-%d %r'
++ '[' -n 7 ']'
++ '[' 80 -gt 10 ']'
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 1857 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 9216 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
++ START_FMT='2020-06-23 02:35:51 PM'
++ echo 'STARTING TIMING RUN AT 2020-06-23 02:35:51 PM'
STARTING TIMING RUN AT 2020-06-23 02:35:51 PM
++ START_FMT='2020-06-23 02:35:51 PM'
++ [[ 80 -ne 1 ]]
++ echo 'STARTING TIMING RUN AT 2020-06-23 02:35:51 PM'
STARTING TIMING RUN AT 2020-06-23 02:35:51 PM
++ START=1592948151
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
++ [[ 80 -ne 1 ]]
++ export DGXSYSTEM
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
++ export SLURM_NTASKS_PER_NODE
++ export DGXSYSTEM
++ export SLURM_NTASKS_PER_NODE
++ export SLURM_NNODES
++ export SLURM_NNODES
++ declare -a CMD
++ START=1592948151
++ declare -a CMD
++ '[' -n 0 ']'
++ '[' 80 -gt 10 ']'
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
++ '[' -n 3 ']'
++ '[' 80 -gt 10 ']'
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
++ START=1592948151
++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 1857 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 9216 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
+++ date '+%Y-%m-%d %r'
++ START=1592948151
++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 1857 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 9216 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
+++ date '+%Y-%m-%d %r'
+++ date '+%Y-%m-%d %r'
+++ date '+%Y-%m-%d %r'
++ START=1592948151
++ START_FMT='2020-06-23 02:35:51 PM'
++ echo 'STARTING TIMING RUN AT 2020-06-23 02:35:51 PM'
STARTING TIMING RUN AT 2020-06-23 02:35:51 PM
++ [[ 80 -ne 1 ]]
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
++ export DGXSYSTEM
++ export SLURM_NTASKS_PER_NODE
++ export SLURM_NNODES
+++ date '+%Y-%m-%d %r'
++ declare -a CMD
++ '[' -n 2 ']'
++ '[' 80 -gt 10 ']'
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 1857 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 9216 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
++ START_FMT='2020-06-23 02:35:51 PM'
STARTING TIMING RUN AT 2020-06-23 02:35:51 PM
++ echo 'STARTING TIMING RUN AT 2020-06-23 02:35:51 PM'
++ [[ 80 -ne 1 ]]
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
++ START_FMT='2020-06-23 02:35:51 PM'
++ export DGXSYSTEM
++ export SLURM_NTASKS_PER_NODE
++ export SLURM_NNODES
STARTING TIMING RUN AT 2020-06-23 02:35:51 PM
++ echo 'STARTING TIMING RUN AT 2020-06-23 02:35:51 PM'
++ declare -a CMD
++ [[ 80 -ne 1 ]]
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
++ export DGXSYSTEM
++ export SLURM_NTASKS_PER_NODE
++ export SLURM_NNODES
++ declare -a CMD
++ '[' -n 1 ']'
++ '[' 80 -gt 10 ']'
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
++ START_FMT='2020-06-23 02:35:51 PM'
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
STARTING TIMING RUN AT 2020-06-23 02:35:51 PM
++ '[' -n 5 ']'
++ '[' 80 -gt 10 ']'
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
++ echo 'STARTING TIMING RUN AT 2020-06-23 02:35:51 PM'
++ [[ 80 -ne 1 ]]
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
++ export DGXSYSTEM
++ export SLURM_NTASKS_PER_NODE
++ export SLURM_NNODES
++ declare -a CMD
++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 1857 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 9216 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
++ '[' -n 4 ']'
++ START_FMT='2020-06-23 02:35:51 PM'
++ '[' 80 -gt 10 ']'
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 1857 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 9216 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
STARTING TIMING RUN AT 2020-06-23 02:35:51 PM
++ echo 'STARTING TIMING RUN AT 2020-06-23 02:35:51 PM'
++ [[ 80 -ne 1 ]]
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
++ export DGXSYSTEM
++ export SLURM_NTASKS_PER_NODE
++ export SLURM_NNODES
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
++ declare -a CMD
++ '[' -n 6 ']'
++ '[' 80 -gt 10 ']'
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 1857 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 9216 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 1857 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 9216 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
Run vars: id 377842 gpus 8 mparams
+ SEED=1857
+ MAX_TOKENS=9216
+ DATASET_DIR=/data
+ MODE=TRAIN
+ NUMEPOCHS=30
+ case "$MODE" in
+ source run_training.sh
++ START_FMT='2020-06-23 02:35:51 PM'
STARTING TIMING RUN AT 2020-06-23 02:35:51 PM
++ echo 'STARTING TIMING RUN AT 2020-06-23 02:35:51 PM'
++ [[ 80 -ne 1 ]]
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
++ export DGXSYSTEM
++ export SLURM_NTASKS_PER_NODE
++ export SLURM_NNODES
++ declare -a CMD
Run vars: id 377842 gpus 8 mparams
++ '[' -n 6 ']'
++ '[' 80 -gt 10 ']'
+++ date +%s
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ SEED=1857
+ MAX_TOKENS=9216
+ DATASET_DIR=/data
+ MODE=TRAIN
+ NUMEPOCHS=30
+ case "$MODE" in
+ source run_training.sh
++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 1857 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 9216 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flatslurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
+++ date +%s
Run vars: id 377842 gpus 8 mparams
+ SEED=1857
+ MAX_TOKENS=9216
+ DATASET_DIR=/data
+ MODE=TRAIN
+ NUMEPOCHS=30
Run vars: id 377842 gpus 8 mparams
+ case "$MODE" in
+ source run_training.sh
Run vars: id 377842 gpus 8 mparams
+ SEED=1857
+ MAX_TOKENS=9216
+ DATASET_DIR=/data
+ SEED=1857
+ MODE=TRAIN
+ MAX_TOKENS=9216
+ NUMEPOCHS=30
+ DATASET_DIR=/data
+ case "$MODE" in
+ MODE=TRAIN
+ NUMEPOCHS=30
+ source run_training.sh
+ case "$MODE" in
+ source run_training.sh
Run vars: id 377842 gpus 8 mparams
+ SEED=1857
+ MAX_TOKENS=9216
+ DATASET_DIR=/data
+ MODE=TRAIN
+ NUMEPOCHS=30
+ case "$MODE" in
+ source run_training.sh
+++ date +%s
+++ date +%s
++ START=1592948151
Run vars: id 377842 gpus 8 mparams
+++ date +%s
+ SEED=1857
+ MAX_TOKENS=9216
+ DATASET_DIR=/data
+ MODE=TRAIN
+++ date +%s
+ NUMEPOCHS=30
+ case "$MODE" in
+ source run_training.sh
+++ date '+%Y-%m-%d %r'
++ START=1592948151
+++ date +%s
+++ date '+%Y-%m-%d %r'
++ START=1592948151
++ START=1592948151
++ START_FMT='2020-06-23 02:35:51 PM'
STARTING TIMING RUN AT 2020-06-23 02:35:51 PM
++ echo 'STARTING TIMING RUN AT 2020-06-23 02:35:51 PM'
++ [[ 80 -ne 1 ]]
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
++ export DGXSYSTEM
++ export SLURM_NTASKS_PER_NODE
++ export SLURM_NNODES
++ START=1592948151
++ declare -a CMD
++ START=1592948151
++ '[' -n 0 ']'
++ '[' 80 -gt 10 ']'
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
++ START_FMT='2020-06-23 02:35:51 PM'
++ echo 'STARTING TIMING RUN AT 2020-06-23 02:35:51 PM'
STARTING TIMING RUN AT 2020-06-23 02:35:51 PM
++ [[ 80 -ne 1 ]]
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
++ export DGXSYSTEM
++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 1857 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 9216 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
++ export SLURM_NTASKS_PER_NODE
++ export SLURM_NNODES
++ declare -a CMD
+++ date '+%Y-%m-%d %r'
+++ date '+%Y-%m-%d %r'
++ '[' -n 5 ']'
++ '[' 80 -gt 10 ']'
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
++ START=1592948151
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 1857 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 9216 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
+++ date '+%Y-%m-%d %r'
+++ date '+%Y-%m-%d %r'
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+++ date '+%Y-%m-%d %r'
++ START_FMT='2020-06-23 02:35:51 PM'
STARTING TIMING RUN AT 2020-06-23 02:35:51 PM
++ echo 'STARTING TIMING RUN AT 2020-06-23 02:35:51 PM'
++ [[ 80 -ne 1 ]]
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
++ export DGXSYSTEM
++ export SLURM_NTASKS_PER_NODE
++ export SLURM_NNODES
++ declare -a CMD
++ START_FMT='2020-06-23 02:35:51 PM'
++ '[' -n 1 ']'
++ '[' 80 -gt 10 ']'
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
STARTING TIMING RUN AT 2020-06-23 02:35:51 PM
++ echo 'STARTING TIMING RUN AT 2020-06-23 02:35:51 PM'
++ [[ 80 -ne 1 ]]
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
++ export DGXSYSTEM
++ export SLURM_NTASKS_PER_NODE
++ export SLURM_NNODES
++ declare -a CMD
++ '[' -n 3 ']'
++ '[' 80 -gt 10 ']'
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 1857 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 9216 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 1857 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 9216 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
++ START_FMT='2020-06-23 02:35:51 PM'
++ echo 'STARTING TIMING RUN AT 2020-06-23 02:35:51 PM'
STARTING TIMING RUN AT 2020-06-23 02:35:51 PM
++ [[ 80 -ne 1 ]]
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
++ export DGXSYSTEM
++ export SLURM_NTASKS_PER_NODE
++ export SLURM_NNODES
++ declare -a CMD
++ '[' -n 2 ']'
++ '[' 80 -gt 10 ']'
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 1857 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 9216 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
++ START_FMT='2020-06-23 02:35:51 PM'
STARTING TIMING RUN AT 2020-06-23 02:35:51 PM
++ echo 'STARTING TIMING RUN AT 2020-06-23 02:35:51 PM'
++ [[ 80 -ne 1 ]]
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
++ export DGXSYSTEM
++ export SLURM_NTASKS_PER_NODE
++ START_FMT='2020-06-23 02:35:51 PM'
++ export SLURM_NNODES
++ declare -a CMD
++ echo 'STARTING TIMING RUN AT 2020-06-23 02:35:51 PM'
STARTING TIMING RUN AT 2020-06-23 02:35:51 PM
++ [[ 80 -ne 1 ]]
++ '[' -n 4 ']'
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
++ export DGXSYSTEM
++ '[' 80 -gt 10 ']'
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
++ export SLURM_NTASKS_PER_NODE
++ export SLURM_NNODES
++ declare -a CMD
++ '[' -n 7 ']'
++ '[' 80 -gt 10 ']'
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 1857 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 9216 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 1857 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 9216 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
Run vars: id 377842 gpus 8 mparams
+ SEED=1857
+ MAX_TOKENS=9216
+ DATASET_DIR=/data
+ MODE=TRAIN
+ NUMEPOCHS=30
+ case "$MODE" in
+ source run_training.sh
+++ date +%s
Run vars: id 377842 gpus 8 mparams
+ SEED=1857
+ MAX_TOKENS=9216
+ DATASET_DIR=/data
+ MODE=TRAIN
+ NUMEPOCHS=30
+ case "$MODE" in
+ source run_training.sh
+++ date +%s
Run vars: id 377842 gpus 8 mparams
+ SEED=1857
+ MAX_TOKENS=9216
+ DATASET_DIR=/data
+ MODE=TRAIN
+ NUMEPOCHS=30
Run vars: id 377842 gpus 8 mparams
Run vars: id 377842 gpus 8 mparams
+ SEED=1857
+ MAX_TOKENS=9216
+ DATASET_DIR=/data
+ MODE=TRAIN
+ case "$MODE" in
+ source run_training.sh
+ SEED=1857
+ MAX_TOKENS=9216
+ DATASET_DIR=/data
+ MODE=TRAIN
+ NUMEPOCHS=30
+ NUMEPOCHS=30
+ case "$MODE" in
+ source run_training.sh
+ case "$MODE" in
+ source run_training.sh
+++ date +%s
+++ date +%s
+++ date +%s
Run vars: id 377842 gpus 8 mparams
+ SEED=1857
+ MAX_TOKENS=9216
+ DATASET_DIR=/data
+ MODE=TRAIN
+ NUMEPOCHS=30
+ case "$MODE" in
+ source run_training.sh
Run vars: id 377842 gpus 8 mparams
+ SEED=1857
+ MAX_TOKENS=9216
+ DATASET_DIR=/data
+ MODE=TRAIN
+ NUMEPOCHS=30
+ case "$MODE" in
+ source run_training.sh
++ START=1592948151
+++ date +%s
+++ date +%s
+++ date '+%Y-%m-%d %r'
Run vars: id 377842 gpus 8 mparams
+ SEED=1857
+ MAX_TOKENS=9216
+ DATASET_DIR=/data
+ MODE=TRAIN
+ NUMEPOCHS=30
+ case "$MODE" in
+ source run_training.sh
++ START=1592948151
+++ date +%s
Run vars: id 377842 gpus 8 mparams
+ SEED=1857
+ MAX_TOKENS=9216
+ DATASET_DIR=/data
+ MODE=TRAIN
+ NUMEPOCHS=30
+ case "$MODE" in
+ source run_training.sh
+++ date '+%Y-%m-%d %r'
++ START=1592948151
+++ date +%s
+++ date '+%Y-%m-%d %r'
++ START=1592948151
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
Run vars: id 377842 gpus 8 mparams
Run vars: id 377842 gpus 8 mparams
+ SEED=1857
+ MAX_TOKENS=9216
+ DATASET_DIR=/data
++ START=1592948151
+ MODE=TRAIN
+ NUMEPOCHS=30
++ START=1592948151
+ case "$MODE" in
+ source run_training.sh
+ SEED=1857
Run vars: id 377842 gpus 8 mparams
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ MAX_TOKENS=9216
+ DATASET_DIR=/data
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+++ date '+%Y-%m-%d %r'
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ SEED=1857
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ MODE=TRAIN
+ NUMEPOCHS=30
+ MAX_TOKENS=9216
+ case "$MODE" in
+ DATASET_DIR=/data
+ MODE=TRAIN
+ NUMEPOCHS=30
+ source run_training.sh
+ case "$MODE" in
+ source run_training.sh
+++ date '+%Y-%m-%d %r'
+++ date '+%Y-%m-%d %r'
+++ date +%s
+++ date +%s
+++ date +%s
++ START_FMT='2020-06-23 02:35:51 PM'
++ echo 'STARTING TIMING RUN AT 2020-06-23 02:35:51 PM'
++ START=1592948151
STARTING TIMING RUN AT 2020-06-23 02:35:51 PM
++ [[ 80 -ne 1 ]]
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
++ export DGXSYSTEM
++ export SLURM_NTASKS_PER_NODE
++ export SLURM_NNODES
++ declare -a CMD
Run vars: id 377842 gpus 8 mparams
++ '[' -n 7 ']'
++ '[' 80 -gt 10 ']'
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ SEED=1857
+ MAX_TOKENS=9216
+ DATASET_DIR=/data
+ MODE=TRAIN
+ NUMEPOCHS=30
+ case "$MODE" in
+ source run_training.sh
+++ date '+%Y-%m-%d %r'
++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 1857 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 9216 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat+++ date +%s
Run vars: id 377842 gpus 8 mparams
+ SEED=1857
+ MAX_TOKENS=9216
+ DATASET_DIR=/data
++ START_FMT='2020-06-23 02:35:51 PM'
+ MODE=TRAIN
+ NUMEPOCHS=30
+ case "$MODE" in
+ source run_training.sh
STARTING TIMING RUN AT 2020-06-23 02:35:51 PM
++ echo 'STARTING TIMING RUN AT 2020-06-23 02:35:51 PM'
++ [[ 80 -ne 1 ]]
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
++ export DGXSYSTEM
++ export SLURM_NTASKS_PER_NODE
++ export SLURM_NNODES
++ declare -a CMD
++ '[' -n 7 ']'
++ '[' 80 -gt 10 ']'
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 1857 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 9216 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat+++ date +%s
-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
++ START=1592948151
Run vars: id 377842 gpus 8 mparams
+ SEED=1857
+ MAX_TOKENS=9216
+ DATASET_DIR=/data
+ MODE=TRAIN
+ NUMEPOCHS=30
+++ date '+%Y-%m-%d %r'
+ case "$MODE" in
+ source run_training.sh
Run vars: id 377842 gpus 8 mparams
+ SEED=1857
+ MAX_TOKENS=9216
+ DATASET_DIR=/data
+ MODE=TRAIN
+ NUMEPOCHS=30
+ case "$MODE" in
+ source run_training.sh
Run vars: id 377842 gpus 8 mparams
++ START_FMT='2020-06-23 02:35:51 PM'
+ SEED=1857
+ MAX_TOKENS=9216
++ echo 'STARTING TIMING RUN AT 2020-06-23 02:35:51 PM'
STARTING TIMING RUN AT 2020-06-23 02:35:51 PM
+ DATASET_DIR=/data
+ MODE=TRAIN
+++ date +%s
++ [[ 80 -ne 1 ]]
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
+ NUMEPOCHS=30
++ export DGXSYSTEM
++ export SLURM_NTASKS_PER_NODE
++ export SLURM_NNODES
+ case "$MODE" in
++ declare -a CMD
+ source run_training.sh
++ '[' -n 1 ']'
++ '[' 80 -gt 10 ']'
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+++ date +%s
++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 1857 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 9216 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
++ START=1592948151
++ START_FMT='2020-06-23 02:35:51 PM'
++ echo 'STARTING TIMING RUN AT 2020-06-23 02:35:51 PM'
STARTING TIMING RUN AT 2020-06-23 02:35:51 PM
++ [[ 80 -ne 1 ]]
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
++ export DGXSYSTEM
++ export SLURM_NTASKS_PER_NODE
++ export SLURM_NNODES
++ declare -a CMD
++ '[' -n 6 ']'
++ '[' 80 -gt 10 ']'
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+++ date '+%Y-%m-%d %r'
++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 1857 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 9216 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
+++ date +%s
++ START=1592948151
++ START_FMT='2020-06-23 02:35:51 PM'
++ echo 'STARTING TIMING RUN AT 2020-06-23 02:35:51 PM'
STARTING TIMING RUN AT 2020-06-23 02:35:51 PM
++ [[ 80 -ne 1 ]]
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
++ export DGXSYSTEM
++ export SLURM_NTASKS_PER_NODE
++ export SLURM_NNODES
++ declare -a CMD
++ '[' -n 5 ']'
++ '[' 80 -gt 10 ']'
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
++ START_FMT='2020-06-23 02:35:51 PM'
++ echo 'STARTING TIMING RUN AT 2020-06-23 02:35:51 PM'
STARTING TIMING RUN AT 2020-06-23 02:35:51 PM
++ START=1592948151
+++ date '+%Y-%m-%d %r'
++ [[ 80 -ne 1 ]]
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
++ export DGXSYSTEM
++ export SLURM_NTASKS_PER_NODE
++ START_FMT='2020-06-23 02:35:51 PM'
++ export SLURM_NNODES
++ declare -a CMD
++ echo 'STARTING TIMING RUN AT 2020-06-23 02:35:51 PM'
STARTING TIMING RUN AT 2020-06-23 02:35:51 PM
++ [[ 80 -ne 1 ]]
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
++ '[' -n 0 ']'
++ export DGXSYSTEM
++ export SLURM_NTASKS_PER_NODE
++ '[' 80 -gt 10 ']'
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
++ export SLURM_NNODES
++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 1857 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 9216 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
++ declare -a CMD
++ '[' -n 4 ']'
++ '[' 80 -gt 10 ']'
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
++ START=1592948151
++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 1857 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 9216 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
+++ date '+%Y-%m-%d %r'
++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 1857 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 9216 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
++ START=1592948151
+++ date '+%Y-%m-%d %r'
++ START_FMT='2020-06-23 02:35:51 PM'
++ echo 'STARTING TIMING RUN AT 2020-06-23 02:35:51 PM'
STARTING TIMING RUN AT 2020-06-23 02:35:51 PM
++ [[ 80 -ne 1 ]]
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
++ export DGXSYSTEM
++ export SLURM_NTASKS_PER_NODE
++ export SLURM_NNODES
+++ date '+%Y-%m-%d %r'
++ declare -a CMD
++ '[' -n 5 ']'
++ '[' 80 -gt 10 ']'
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
++ START=1592948151
++ START=1592948151
++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 1857 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 9216 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
++ START=1592948151
+++ date '+%Y-%m-%d %r'
+++ date '+%Y-%m-%d %r'
++ START_FMT='2020-06-23 02:35:51 PM'
+++ date '+%Y-%m-%d %r'
STARTING TIMING RUN AT 2020-06-23 02:35:51 PM
++ echo 'STARTING TIMING RUN AT 2020-06-23 02:35:51 PM'
++ [[ 80 -ne 1 ]]
++ START=1592948151
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
++ export DGXSYSTEM
++ export SLURM_NTASKS_PER_NODE
++ export SLURM_NNODES
++ declare -a CMD
++ '[' -n 3 ']'
++ '[' 80 -gt 10 ']'
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 1857 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 9216 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
+++ date '+%Y-%m-%d %r'
Run vars: id 377842 gpus 8 mparams
+ SEED=1857
+ MAX_TOKENS=9216
+ DATASET_DIR=/data
+ MODE=TRAIN
+ NUMEPOCHS=30
+ case "$MODE" in
+ source run_training.sh
++ START_FMT='2020-06-23 02:35:51 PM'
++ echo 'STARTING TIMING RUN AT 2020-06-23 02:35:51 PM'
STARTING TIMING RUN AT 2020-06-23 02:35:51 PM
++ [[ 80 -ne 1 ]]
++ START_FMT='2020-06-23 02:35:51 PM'
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
++ export DGXSYSTEM
++ export SLURM_NTASKS_PER_NODE
STARTING TIMING RUN AT 2020-06-23 02:35:51 PM
++ echo 'STARTING TIMING RUN AT 2020-06-23 02:35:51 PM'
++ export SLURM_NNODES
++ declare -a CMD
++ [[ 80 -ne 1 ]]
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
++ export DGXSYSTEM
++ export SLURM_NTASKS_PER_NODE
++ export SLURM_NNODES
++ declare -a CMD
++ '[' -n 6 ']'
++ '[' 80 -gt 10 ']'
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
++ '[' -n 2 ']'
++ '[' 80 -gt 10 ']'
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
Run vars: id 377842 gpus 8 mparams
++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 1857 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 9216 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat++ START_FMT='2020-06-23 02:35:51 PM'
-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
+ SEED=1857
+ MAX_TOKENS=9216
+ DATASET_DIR=/data
++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 1857 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 9216 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat+ MODE=TRAIN
+ NUMEPOCHS=30
-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
+ case "$MODE" in
+ source run_training.sh
+++ date +%s
STARTING TIMING RUN AT 2020-06-23 02:35:51 PM
++ echo 'STARTING TIMING RUN AT 2020-06-23 02:35:51 PM'
++ [[ 80 -ne 1 ]]
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
++ export DGXSYSTEM
++ export SLURM_NTASKS_PER_NODE
++ export SLURM_NNODES
++ declare -a CMD
++ '[' -n 2 ']'
++ '[' 80 -gt 10 ']'
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 1857 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 9216 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat++ START_FMT='2020-06-23 02:35:51 PM'
STARTING TIMING RUN AT 2020-06-23 02:35:51 PM
-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
++ echo 'STARTING TIMING RUN AT 2020-06-23 02:35:51 PM'
++ [[ 80 -ne 1 ]]
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
++ export DGXSYSTEM
++ START_FMT='2020-06-23 02:35:51 PM'
++ export SLURM_NTASKS_PER_NODE
++ export SLURM_NNODES
++ echo 'STARTING TIMING RUN AT 2020-06-23 02:35:51 PM'
++ START_FMT='2020-06-23 02:35:51 PM'
STARTING TIMING RUN AT 2020-06-23 02:35:51 PM
+++ date +%s
++ declare -a CMD
++ [[ 80 -ne 1 ]]
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
++ export DGXSYSTEM
++ export SLURM_NTASKS_PER_NODE
++ export SLURM_NNODES
++ declare -a CMD
++ '[' -n 0 ']'
STARTING TIMING RUN AT 2020-06-23 02:35:51 PM
++ '[' 80 -gt 10 ']'
++ '[' -n 4 ']'
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
++ echo 'STARTING TIMING RUN AT 2020-06-23 02:35:51 PM'
++ '[' 80 -gt 10 ']'
++ [[ 80 -ne 1 ]]
++ START_FMT='2020-06-23 02:35:51 PM'
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
++ export DGXSYSTEM
++ export SLURM_NTASKS_PER_NODE
++ export SLURM_NNODES
++ declare -a CMD
++ echo 'STARTING TIMING RUN AT 2020-06-23 02:35:51 PM'
STARTING TIMING RUN AT 2020-06-23 02:35:51 PM
++ '[' -n 1 ']'
++ '[' 80 -gt 10 ']'
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
++ [[ 80 -ne 1 ]]
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
++ export DGXSYSTEM
++ export SLURM_NTASKS_PER_NODE
++ export SLURM_NNODES
++ declare -a CMD
++ '[' -n 2 ']'
++ '[' 80 -gt 10 ']'
++ START_FMT='2020-06-23 02:35:51 PM'
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
++ echo 'STARTING TIMING RUN AT 2020-06-23 02:35:51 PM'
++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 1857 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 9216 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flatSTARTING TIMING RUN AT 2020-06-23 02:35:51 PM
++ [[ 80 -ne 1 ]]
++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 1857 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 9216 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 1857 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 9216 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
++ export DGXSYSTEM
++ export SLURM_NTASKS_PER_NODE
++ export SLURM_NNODES
++ declare -a CMD
++ '[' -n 3 ']'
++ '[' 80 -gt 10 ']'
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 1857 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 9216 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 1857 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 9216 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
Run vars: id 377842 gpus 8 mparams
+ SEED=1857
+ MAX_TOKENS=9216
+ DATASET_DIR=/data
+ MODE=TRAIN
+ NUMEPOCHS=30
Run vars: id 377842 gpus 8 mparams
+ case "$MODE" in
+ source run_training.sh
+ SEED=1857
+ MAX_TOKENS=9216
++ START=1592948151
+ DATASET_DIR=/data
+ MODE=TRAIN
+ NUMEPOCHS=30
+ case "$MODE" in
+ source run_training.sh
+++ date '+%Y-%m-%d %r'
+++ date +%s
+++ date +%s
Run vars: id 377842 gpus 8 mparams
+ SEED=1857
+ MAX_TOKENS=9216
+ DATASET_DIR=/data
+ MODE=TRAIN
+ NUMEPOCHS=30
+ case "$MODE" in
+ source run_training.sh
++ START=1592948151
Run vars: id 377842 gpus 8 mparams
+ SEED=1857
+ MAX_TOKENS=9216
+ DATASET_DIR=/data
+ MODE=TRAIN
+ NUMEPOCHS=30
+ case "$MODE" in
+ source run_training.sh
Run vars: id 377842 gpus 8 mparams
+ SEED=1857
+ MAX_TOKENS=9216
+ DATASET_DIR=/data
+++ date '+%Y-%m-%d %r'
+ MODE=TRAIN
+ NUMEPOCHS=30
+ case "$MODE" in
+ source run_training.sh
+++ date +%s
+++ date +%s
+++ date +%s
++ START_FMT='2020-06-23 02:35:51 PM'
STARTING TIMING RUN AT 2020-06-23 02:35:51 PM
++ echo 'STARTING TIMING RUN AT 2020-06-23 02:35:51 PM'
++ [[ 80 -ne 1 ]]
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
++ export DGXSYSTEM
++ export SLURM_NTASKS_PER_NODE
++ export SLURM_NNODES
++ declare -a CMD
++ '[' -n 3 ']'
++ '[' 80 -gt 10 ']'
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 1857 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 9216 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
Run vars: id 377842 gpus 8 mparams
+ SEED=1857
+ MAX_TOKENS=9216
+ DATASET_DIR=/data
+ MODE=TRAIN
+ NUMEPOCHS=30
+ case "$MODE" in
+ source run_training.sh
++ START=1592948151
++ START_FMT='2020-06-23 02:35:51 PM'
++ echo 'STARTING TIMING RUN AT 2020-06-23 02:35:51 PM'
STARTING TIMING RUN AT 2020-06-23 02:35:51 PM
++ START=1592948151
++ [[ 80 -ne 1 ]]
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
++ export DGXSYSTEM
++ export SLURM_NTASKS_PER_NODE
++ export SLURM_NNODES
+++ date +%s
++ declare -a CMD
++ '[' -n 6 ']'
++ '[' 80 -gt 10 ']'
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+++ date '+%Y-%m-%d %r'
+++ date '+%Y-%m-%d %r'
++ START=1592948151
++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 1857 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 9216 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
++ START=1592948151
Run vars: id 377842 gpus 8 mparams
++ START=1592948151
+ SEED=1857
+++ date '+%Y-%m-%d %r'
+ MAX_TOKENS=9216
+ DATASET_DIR=/data
+ MODE=TRAIN
+ NUMEPOCHS=30
+ case "$MODE" in
+ source run_training.sh
+++ date '+%Y-%m-%d %r'
+++ date '+%Y-%m-%d %r'
+++ date +%s
++ START=1592948151
++ START_FMT='2020-06-23 02:35:51 PM'
++ echo 'STARTING TIMING RUN AT 2020-06-23 02:35:51 PM'
STARTING TIMING RUN AT 2020-06-23 02:35:51 PM
++ [[ 80 -ne 1 ]]
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
++ export DGXSYSTEM
++ export SLURM_NTASKS_PER_NODE
++ export SLURM_NNODES
++ declare -a CMD
++ '[' -n 1 ']'
++ '[' 80 -gt 10 ']'
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 1857 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 9216 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
++ START_FMT='2020-06-23 02:35:51 PM'
+++ date '+%Y-%m-%d %r'
STARTING TIMING RUN AT 2020-06-23 02:35:51 PM
++ echo 'STARTING TIMING RUN AT 2020-06-23 02:35:51 PM'
++ [[ 80 -ne 1 ]]
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
++ export DGXSYSTEM
++ export SLURM_NTASKS_PER_NODE
++ export SLURM_NNODES
++ declare -a CMD
++ START_FMT='2020-06-23 02:35:51 PM'
++ START_FMT='2020-06-23 02:35:51 PM'
STARTING TIMING RUN AT 2020-06-23 02:35:51 PM
++ echo 'STARTING TIMING RUN AT 2020-06-23 02:35:51 PM'
STARTING TIMING RUN AT 2020-06-23 02:35:51 PM
++ echo 'STARTING TIMING RUN AT 2020-06-23 02:35:51 PM'
++ '[' -n 7 ']'
++ '[' 80 -gt 10 ']'
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
++ [[ 80 -ne 1 ]]
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
++ [[ 80 -ne 1 ]]
++ export DGXSYSTEM
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
++ export SLURM_NTASKS_PER_NODE
++ export DGXSYSTEM
++ export SLURM_NNODES
++ export SLURM_NTASKS_PER_NODE
++ declare -a CMD
++ export SLURM_NNODES
++ declare -a CMD
Run vars: id 377842 gpus 8 mparams
++ '[' -n 5 ']'
++ '[' 80 -gt 10 ']'
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
++ '[' -n 4 ']'
++ '[' 80 -gt 10 ']'
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 1857 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 9216 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat+ SEED=1857
+ MAX_TOKENS=9216
-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
+ DATASET_DIR=/data
+ MODE=TRAIN
+ NUMEPOCHS=30
+ case "$MODE" in
+ source run_training.sh
++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 1857 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 9216 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 1857 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 9216 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
Run vars: id 377842 gpus 8 mparams
+ SEED=1857
+ MAX_TOKENS=9216
+ DATASET_DIR=/data
+ MODE=TRAIN
+ NUMEPOCHS=30
+ case "$MODE" in
+ source run_training.sh
Run vars: id 377842 gpus 8 mparams
Run vars: id 377842 gpus 8 mparams
+ SEED=1857
+ MAX_TOKENS=9216
+ DATASET_DIR=/data
+ MODE=TRAIN
+ NUMEPOCHS=30
+ case "$MODE" in
+ source run_training.sh
+ SEED=1857
++ START_FMT='2020-06-23 02:35:51 PM'
+ MAX_TOKENS=9216
+ DATASET_DIR=/data
+ MODE=TRAIN
+ NUMEPOCHS=30
+++ date +%s
+ case "$MODE" in
++ echo 'STARTING TIMING RUN AT 2020-06-23 02:35:51 PM'
+ source run_training.sh
STARTING TIMING RUN AT 2020-06-23 02:35:51 PM
++ [[ 80 -ne 1 ]]
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
++ export DGXSYSTEM
++ export SLURM_NTASKS_PER_NODE
++ export SLURM_NNODES
++ declare -a CMD
++ START=1592948151
++ '[' -n 0 ']'
+++ date +%s
++ '[' 80 -gt 10 ']'
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 1857 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 9216 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat+++ date +%s
+++ date '+%Y-%m-%d %r'
-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
Run vars: id 377842 gpus 8 mparams
Run vars: id 377842 gpus 8 mparams
+ SEED=1857
+ SEED=1857
+ MAX_TOKENS=9216
+ DATASET_DIR=/data
+ MAX_TOKENS=9216
+ MODE=TRAIN
+ NUMEPOCHS=30
+ DATASET_DIR=/data
+ case "$MODE" in
+ MODE=TRAIN
+ source run_training.sh
+ NUMEPOCHS=30
+ case "$MODE" in
Run vars: id 377842 gpus 8 mparams
+ SEED=1857
+ MAX_TOKENS=9216
+ DATASET_DIR=/data
+ MODE=TRAIN
+ NUMEPOCHS=30
+ source run_training.sh
+ case "$MODE" in
+ source run_training.sh
Run vars: id 377842 gpus 8 mparams
+ SEED=1857
+ MAX_TOKENS=9216
+ DATASET_DIR=/data
+ MODE=TRAIN
+ NUMEPOCHS=30
+ case "$MODE" in
+ source run_training.sh
+++ date +%s
++ START_FMT='2020-06-23 02:35:51 PM'
STARTING TIMING RUN AT 2020-06-23 02:35:51 PM
+++ date +%s
++ echo 'STARTING TIMING RUN AT 2020-06-23 02:35:51 PM'
+++ date +%s
++ [[ 80 -ne 1 ]]
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
++ export DGXSYSTEM
++ export SLURM_NTASKS_PER_NODE
++ export SLURM_NNODES
++ declare -a CMD
+++ date +%s
++ '[' -n 6 ']'
++ '[' 80 -gt 10 ']'
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
Run vars: id 377842 gpus 8 mparams
+ SEED=1857
+++ date +%s
++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 1857 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 9216 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
+ MAX_TOKENS=9216
+ DATASET_DIR=/data
+ MODE=TRAIN
+ NUMEPOCHS=30
Run vars: id 377842 gpus 8 mparams
+ case "$MODE" in
+ SEED=1857
+ MAX_TOKENS=9216
+ DATASET_DIR=/data
+ MODE=TRAIN
+ NUMEPOCHS=30
+ case "$MODE" in
+ source run_training.sh
+ source run_training.sh
Run vars: id 377842 gpus 8 mparams
+ SEED=1857
+ MAX_TOKENS=9216
+ DATASET_DIR=/data
+ MODE=TRAIN
+ NUMEPOCHS=30
+ case "$MODE" in
+ source run_training.sh
+++ date +%s
+++ date +%s
Run vars: id 377842 gpus 8 mparams
+ SEED=1857
+ MAX_TOKENS=9216
+ DATASET_DIR=/data
+ MODE=TRAIN
+ NUMEPOCHS=30
+ case "$MODE" in
+ source run_training.sh
Run vars: id 377842 gpus 8 mparams
+ SEED=1857
+ MAX_TOKENS=9216
+ DATASET_DIR=/data
+ MODE=TRAIN
+ NUMEPOCHS=30
+ case "$MODE" in
+ source run_training.sh
+++ date +%s
++ START=1592948151
Run vars: id 377842 gpus 8 mparams
+ SEED=1857
+ MAX_TOKENS=9216
+ DATASET_DIR=/data
+ MODE=TRAIN
+ NUMEPOCHS=30
+++ date +%s
+ case "$MODE" in
+ source run_training.sh
+++ date +%s
++ START=1592948151
Run vars: id 377842 gpus 8 mparams
+ SEED=1857
+ MAX_TOKENS=9216
+ DATASET_DIR=/data
+ MODE=TRAIN
+ NUMEPOCHS=30
+ case "$MODE" in
++ START_FMT='2020-06-23 02:35:51 PM'
+ source run_training.sh
STARTING TIMING RUN AT 2020-06-23 02:35:51 PM
++ echo 'STARTING TIMING RUN AT 2020-06-23 02:35:51 PM'
+++ date '+%Y-%m-%d %r'
++ [[ 80 -ne 1 ]]
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
++ export DGXSYSTEM
++ export SLURM_NTASKS_PER_NODE
++ export SLURM_NNODES
++ declare -a CMD
++ START=1592948151
++ '[' -n 2 ']'
++ '[' 80 -gt 10 ']'
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+++ date +%s
++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 1857 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 9216 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
+++ date '+%Y-%m-%d %r'
+++ date '+%Y-%m-%d %r'
++ START=1592948151
++ START=1592948151
++ START=1592948151
++ START=1592948151
+++ date +%s
+++ date '+%Y-%m-%d %r'
+++ date '+%Y-%m-%d %r'
+++ date '+%Y-%m-%d %r'
+++ date '+%Y-%m-%d %r'
++ START=1592948151
++ START=1592948151
+++ date '+%Y-%m-%d %r'
++ START=1592948151
+++ date '+%Y-%m-%d %r'
++ START=1592948151
++ START=1592948151
++ START_FMT='2020-06-23 02:35:51 PM'
++ echo 'STARTING TIMING RUN AT 2020-06-23 02:35:51 PM'
++ START_FMT='2020-06-23 02:35:51 PM'
STARTING TIMING RUN AT 2020-06-23 02:35:51 PM
++ [[ 80 -ne 1 ]]
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
++ export DGXSYSTEM
++ export SLURM_NTASKS_PER_NODE
+++ date '+%Y-%m-%d %r'
++ echo 'STARTING TIMING RUN AT 2020-06-23 02:35:51 PM'
STARTING TIMING RUN AT 2020-06-23 02:35:51 PM
++ [[ 80 -ne 1 ]]
+++ date '+%Y-%m-%d %r'
++ export SLURM_NNODES
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
++ export DGXSYSTEM
++ export SLURM_NTASKS_PER_NODE
++ export SLURM_NNODES
++ declare -a CMD
++ declare -a CMD
++ START_FMT='2020-06-23 02:35:51 PM'
STARTING TIMING RUN AT 2020-06-23 02:35:51 PM
++ '[' -n 7 ']'
++ echo 'STARTING TIMING RUN AT 2020-06-23 02:35:51 PM'
++ '[' 80 -gt 10 ']'
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
++ [[ 80 -ne 1 ]]
++ '[' -n 3 ']'
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
++ '[' 80 -gt 10 ']'
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
++ export DGXSYSTEM
++ export SLURM_NTASKS_PER_NODE
++ export SLURM_NNODES
++ declare -a CMD
+++ date '+%Y-%m-%d %r'
++ '[' -n 4 ']'
++ '[' 80 -gt 10 ']'
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 1857 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 9216 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 1857 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 9216 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 1857 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 9216 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
++ START=1592948151
++ START=1592948151
++ START_FMT='2020-06-23 02:35:51 PM'
+++ date '+%Y-%m-%d %r'
STARTING TIMING RUN AT 2020-06-23 02:35:51 PM
++ echo 'STARTING TIMING RUN AT 2020-06-23 02:35:51 PM'
++ [[ 80 -ne 1 ]]
+++ date '+%Y-%m-%d %r'
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
++ export DGXSYSTEM
Run vars: id 377842 gpus 8 mparams
++ export SLURM_NTASKS_PER_NODE
++ export SLURM_NNODES
++ declare -a CMD
++ START_FMT='2020-06-23 02:35:51 PM'
++ '[' -n 7 ']'
++ '[' 80 -gt 10 ']'
++ echo 'STARTING TIMING RUN AT 2020-06-23 02:35:51 PM'
+ SEED=1857
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
STARTING TIMING RUN AT 2020-06-23 02:35:51 PM
+ MAX_TOKENS=9216
+ DATASET_DIR=/data
++ [[ 80 -ne 1 ]]
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
++ export DGXSYSTEM
++ export SLURM_NTASKS_PER_NODE
++ export SLURM_NNODES
++ declare -a CMD
+ MODE=TRAIN
+ NUMEPOCHS=30
+ case "$MODE" in
+ source run_training.sh
++ '[' -n 0 ']'
++ '[' 80 -gt 10 ']'
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 1857 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 9216 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 1857 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 9216 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
++ START_FMT='2020-06-23 02:35:51 PM'
+++ date +%s
++ echo 'STARTING TIMING RUN AT 2020-06-23 02:35:51 PM'
STARTING TIMING RUN AT 2020-06-23 02:35:51 PM
++ [[ 80 -ne 1 ]]
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
++ export DGXSYSTEM
++ export SLURM_NTASKS_PER_NODE
++ export SLURM_NNODES
++ declare -a CMD
++ '[' -n 5 ']'
++ '[' 80 -gt 10 ']'
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 1857 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 9216 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
++ START_FMT='2020-06-23 02:35:51 PM'
STARTING TIMING RUN AT 2020-06-23 02:35:51 PM
++ echo 'STARTING TIMING RUN AT 2020-06-23 02:35:51 PM'
++ [[ 80 -ne 1 ]]
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
++ export DGXSYSTEM
++ export SLURM_NTASKS_PER_NODE
++ export SLURM_NNODES
++ declare -a CMD
Run vars: id 377842 gpus 8 mparams
+ SEED=1857
+ MAX_TOKENS=9216
+ DATASET_DIR=/data
+ MODE=TRAIN
++ '[' -n 0 ']'
+ NUMEPOCHS=30
++ '[' 80 -gt 10 ']'
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ case "$MODE" in
+ source run_training.sh
++ START_FMT='2020-06-23 02:35:51 PM'
STARTING TIMING RUN AT 2020-06-23 02:35:51 PM
++ echo 'STARTING TIMING RUN AT 2020-06-23 02:35:51 PM'
++ [[ 80 -ne 1 ]]
++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 1857 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 9216 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
++ export DGXSYSTEM
++ export SLURM_NTASKS_PER_NODE
++ export SLURM_NNODES
++ declare -a CMD
++ '[' -n 1 ']'
++ '[' 80 -gt 10 ']'
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 1857 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 9216 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
+++ date +%s
++ START=1592948151
+++ date '+%Y-%m-%d %r'
Run vars: id 377842 gpus 8 mparams
+ SEED=1857
+ MAX_TOKENS=9216
+ DATASET_DIR=/data
+ MODE=TRAIN
+ NUMEPOCHS=30
+ case "$MODE" in
+ source run_training.sh
Run vars: id 377842 gpus 8 mparams
+ SEED=1857
+ MAX_TOKENS=9216
+ DATASET_DIR=/data
+ MODE=TRAIN
Run vars: id 377842 gpus 8 mparams
+ NUMEPOCHS=30
+ case "$MODE" in
+ SEED=1857
+ MAX_TOKENS=9216
+ source run_training.sh
+ DATASET_DIR=/data
+ MODE=TRAIN
+ NUMEPOCHS=30
Run vars: id 377842 gpus 8 mparams
+ case "$MODE" in
+ source run_training.sh
+ SEED=1857
+ MAX_TOKENS=9216
+ DATASET_DIR=/data
+ MODE=TRAIN
+ NUMEPOCHS=30
Run vars: id 377842 gpus 8 mparams
+ case "$MODE" in
+ SEED=1857
+ MAX_TOKENS=9216
+ source run_training.sh
+ DATASET_DIR=/data
+ MODE=TRAIN
+ NUMEPOCHS=30
+ case "$MODE" in
+ source run_training.sh
+++ date +%s
++ START=1592948151
+++ date +%s
+++ date +%s
+++ date +%s
+++ date +%s
+++ date '+%Y-%m-%d %r'
++ START_FMT='2020-06-23 02:35:51 PM'
STARTING TIMING RUN AT 2020-06-23 02:35:51 PM
++ echo 'STARTING TIMING RUN AT 2020-06-23 02:35:51 PM'
++ [[ 80 -ne 1 ]]
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
++ export DGXSYSTEM
++ export SLURM_NTASKS_PER_NODE
++ export SLURM_NNODES
++ declare -a CMD
++ '[' -n 5 ']'
++ '[' 80 -gt 10 ']'
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 1857 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 9216 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
++ START=1592948151
++ START=1592948151
++ START=1592948151
+++ date '+%Y-%m-%d %r'
+++ date '+%Y-%m-%d %r'
++ START=1592948151
++ START=1592948151
++ START_FMT='2020-06-23 02:35:51 PM'
++ echo 'STARTING TIMING RUN AT 2020-06-23 02:35:51 PM'
+++ date '+%Y-%m-%d %r'
STARTING TIMING RUN AT 2020-06-23 02:35:51 PM
++ [[ 80 -ne 1 ]]
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
++ export DGXSYSTEM
++ export SLURM_NTASKS_PER_NODE
++ export SLURM_NNODES
++ declare -a CMD
++ '[' -n 2 ']'
++ '[' 80 -gt 10 ']'
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+++ date '+%Y-%m-%d %r'
++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 1857 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 9216 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
+++ date '+%Y-%m-%d %r'
++ START_FMT='2020-06-23 02:35:51 PM'
++ START_FMT='2020-06-23 02:35:51 PM'
++ echo 'STARTING TIMING RUN AT 2020-06-23 02:35:51 PM'
STARTING TIMING RUN AT 2020-06-23 02:35:51 PM
++ echo 'STARTING TIMING RUN AT 2020-06-23 02:35:51 PM'
STARTING TIMING RUN AT 2020-06-23 02:35:51 PM
++ [[ 80 -ne 1 ]]
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
++ [[ 80 -ne 1 ]]
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
++ export DGXSYSTEM
++ export SLURM_NTASKS_PER_NODE
++ export DGXSYSTEM
++ export SLURM_NTASKS_PER_NODE
++ export SLURM_NNODES
++ export SLURM_NNODES
++ declare -a CMD
++ declare -a CMD
++ '[' -n 3 ']'
++ '[' -n 6 ']'
++ '[' 80 -gt 10 ']'
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
++ '[' 80 -gt 10 ']'
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 1857 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 9216 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 1857 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 9216 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
++ START_FMT='2020-06-23 02:35:51 PM'
STARTING TIMING RUN AT 2020-06-23 02:35:51 PM
++ echo 'STARTING TIMING RUN AT 2020-06-23 02:35:51 PM'
++ [[ 80 -ne 1 ]]
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
++ export DGXSYSTEM
++ START_FMT='2020-06-23 02:35:51 PM'
++ export SLURM_NTASKS_PER_NODE
++ export SLURM_NNODES
++ declare -a CMD
STARTING TIMING RUN AT 2020-06-23 02:35:51 PM
++ echo 'STARTING TIMING RUN AT 2020-06-23 02:35:51 PM'
++ [[ 80 -ne 1 ]]
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
++ export DGXSYSTEM
++ '[' -n 4 ']'
++ export SLURM_NTASKS_PER_NODE
++ export SLURM_NNODES
++ '[' 80 -gt 10 ']'
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
++ declare -a CMD
++ '[' -n 7 ']'
++ '[' 80 -gt 10 ']'
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
++ START_FMT='2020-06-23 02:35:51 PM'
++ echo 'STARTING TIMING RUN AT 2020-06-23 02:35:51 PM'
STARTING TIMING RUN AT 2020-06-23 02:35:51 PM
++ [[ 80 -ne 1 ]]
++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 1857 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 9216 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
++ export DGXSYSTEM
++ export SLURM_NTASKS_PER_NODE
++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 1857 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 9216 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
++ export SLURM_NNODES
++ declare -a CMD
++ '[' -n 1 ']'
++ '[' 80 -gt 10 ']'
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 1857 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 9216 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
++ START_FMT='2020-06-23 02:35:51 PM'
++ START=1592948151
++ echo 'STARTING TIMING RUN AT 2020-06-23 02:35:51 PM'
STARTING TIMING RUN AT 2020-06-23 02:35:51 PM
++ [[ 80 -ne 1 ]]
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
++ export DGXSYSTEM
++ export SLURM_NTASKS_PER_NODE
++ export SLURM_NNODES
++ declare -a CMD
++ '[' -n 0 ']'
++ '[' 80 -gt 10 ']'
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 1857 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 9216 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
+++ date '+%Y-%m-%d %r'
++ START_FMT='2020-06-23 02:35:51 PM'
++ echo 'STARTING TIMING RUN AT 2020-06-23 02:35:51 PM'
STARTING TIMING RUN AT 2020-06-23 02:35:51 PM
++ [[ 80 -ne 1 ]]
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
++ export DGXSYSTEM
++ export SLURM_NTASKS_PER_NODE
++ export SLURM_NNODES
++ declare -a CMD
++ '[' -n 5 ']'
++ START_FMT='2020-06-23 02:35:51 PM'
++ '[' 80 -gt 10 ']'
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
++ echo 'STARTING TIMING RUN AT 2020-06-23 02:35:51 PM'
STARTING TIMING RUN AT 2020-06-23 02:35:51 PM
++ [[ 80 -ne 1 ]]
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
++ export DGXSYSTEM
++ export SLURM_NTASKS_PER_NODE
++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 1857 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 9216 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
++ export SLURM_NNODES
++ declare -a CMD
++ '[' -n 2 ']'
++ '[' 80 -gt 10 ']'
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
++ START_FMT='2020-06-23 02:35:51 PM'
++ echo 'STARTING TIMING RUN AT 2020-06-23 02:35:51 PM'
STARTING TIMING RUN AT 2020-06-23 02:35:51 PM
++ [[ 80 -ne 1 ]]
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
++ START_FMT='2020-06-23 02:35:51 PM'
++ export DGXSYSTEM
++ export SLURM_NTASKS_PER_NODE
++ echo 'STARTING TIMING RUN AT 2020-06-23 02:35:51 PM'
STARTING TIMING RUN AT 2020-06-23 02:35:51 PM
++ export SLURM_NNODES
++ declare -a CMD
++ [[ 80 -ne 1 ]]
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
++ export DGXSYSTEM
++ export SLURM_NTASKS_PER_NODE
++ export SLURM_NNODES
++ declare -a CMD
++ START_FMT='2020-06-23 02:35:51 PM'
++ '[' -n 1 ']'
++ '[' 80 -gt 10 ']'
++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 1857 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 9216 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
STARTING TIMING RUN AT 2020-06-23 02:35:51 PM
++ echo 'STARTING TIMING RUN AT 2020-06-23 02:35:51 PM'
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
++ '[' -n 6 ']'
++ '[' 80 -gt 10 ']'
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
++ [[ 80 -ne 1 ]]
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
++ export DGXSYSTEM
++ export SLURM_NTASKS_PER_NODE
++ export SLURM_NNODES
++ declare -a CMD
++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 1857 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 9216 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
++ '[' -n 4 ']'
++ '[' 80 -gt 10 ']'
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 1857 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 9216 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 1857 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 9216 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
++ START_FMT='2020-06-23 02:35:51 PM'
STARTING TIMING RUN AT 2020-06-23 02:35:51 PM
++ echo 'STARTING TIMING RUN AT 2020-06-23 02:35:51 PM'
++ [[ 80 -ne 1 ]]
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
++ export DGXSYSTEM
++ export SLURM_NTASKS_PER_NODE
++ export SLURM_NNODES
++ declare -a CMD
++ '[' -n 3 ']'
++ '[' 80 -gt 10 ']'
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 1857 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 9216 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
num_sockets = 2 num_nodes=8 cores_per_socket=64
+ exec numactl --physcpubind=64-79,192-207 --membind=4 -- python -u train.py /data --seed 1857 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 9216 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
num_sockets = 2 num_nodes=8 cores_per_socket=64
Run vars: id 377842 gpus 8 mparams
+ SEED=1857
+ MAX_TOKENS=9216
+ DATASET_DIR=/data
+ MODE=TRAIN
+ NUMEPOCHS=30
+ case "$MODE" in
+ source run_training.sh
+++ date +%s
+ exec numactl --physcpubind=16-31,144-159 --membind=1 -- python -u train.py /data --seed 1857 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 9216 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
Run vars: id 377842 gpus 8 mparams
+ SEED=1857
+ MAX_TOKENS=9216
+ DATASET_DIR=/data
+ MODE=TRAIN
+ NUMEPOCHS=30
+ case "$MODE" in
+ source run_training.sh
++ START=1592948151
+++ date '+%Y-%m-%d %r'
+++ date +%s
++ START_FMT='2020-06-23 02:35:51 PM'
++ echo 'STARTING TIMING RUN AT 2020-06-23 02:35:51 PM'
Run vars: id 377842 gpus 8 mparams
STARTING TIMING RUN AT 2020-06-23 02:35:51 PM
+ SEED=1857
++ [[ 80 -ne 1 ]]
+ MAX_TOKENS=9216
+ DATASET_DIR=/data
+ MODE=TRAIN
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
+ NUMEPOCHS=30
++ export DGXSYSTEM
++ export SLURM_NTASKS_PER_NODE
+ case "$MODE" in
+ source run_training.sh
++ export SLURM_NNODES
++ declare -a CMD
++ '[' -n 2 ']'
++ '[' 80 -gt 10 ']'
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
++ START=1592948151
++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 1857 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 9216 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
+++ date +%s
+++ date '+%Y-%m-%d %r'
Run vars: id 377842 gpus 8 mparams
+ SEED=1857
Run vars: id 377842 gpus 8 mparams
+ MAX_TOKENS=9216
+ DATASET_DIR=/data
+ SEED=1857
+ MODE=TRAIN
+ MAX_TOKENS=9216
+ NUMEPOCHS=30
+ DATASET_DIR=/data
+ case "$MODE" in
+ MODE=TRAIN
+ NUMEPOCHS=30
+ source run_training.sh
Run vars: id 377842 gpus 8 mparams
+ case "$MODE" in
+ source run_training.sh
+ SEED=1857
+ MAX_TOKENS=9216
+ DATASET_DIR=/data
+ MODE=TRAIN
+ NUMEPOCHS=30
+ case "$MODE" in
+ source run_training.sh
Run vars: id 377842 gpus 8 mparams
+ SEED=1857
+ MAX_TOKENS=9216
+ DATASET_DIR=/data
+ MODE=TRAIN
+ NUMEPOCHS=30
+ case "$MODE" in
+ source run_training.sh
+++ date +%s
+++ date +%s
+++ date +%s
+++ date +%s
Run vars: id 377842 gpus 8 mparams
+ SEED=1857
+ MAX_TOKENS=9216
+ DATASET_DIR=/data
++ START=1592948151
+ MODE=TRAIN
+ NUMEPOCHS=30
+ case "$MODE" in
++ START_FMT='2020-06-23 02:35:51 PM'
+ source run_training.sh
++ echo 'STARTING TIMING RUN AT 2020-06-23 02:35:51 PM'
STARTING TIMING RUN AT 2020-06-23 02:35:51 PM
++ [[ 80 -ne 1 ]]
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
++ export DGXSYSTEM
++ export SLURM_NTASKS_PER_NODE
++ export SLURM_NNODES
++ declare -a CMD
++ '[' -n 6 ']'
++ '[' 80 -gt 10 ']'
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+++ date '+%Y-%m-%d %r'
++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 1857 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 9216 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
+++ date +%s
++ START=1592948151
+++ date '+%Y-%m-%d %r'
++ START=1592948151
++ START=1592948151
++ START=1592948151
+++ date '+%Y-%m-%d %r'
+++ date '+%Y-%m-%d %r'
++ START_FMT='2020-06-23 02:35:51 PM'
STARTING TIMING RUN AT 2020-06-23 02:35:51 PM
++ echo 'STARTING TIMING RUN AT 2020-06-23 02:35:51 PM'
++ [[ 80 -ne 1 ]]
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
++ export DGXSYSTEM
++ export SLURM_NTASKS_PER_NODE
++ export SLURM_NNODES
++ declare -a CMD
+++ date '+%Y-%m-%d %r'
++ '[' -n 3 ']'
++ '[' 80 -gt 10 ']'
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 1857 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 9216 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
++ START=1592948151
+++ date '+%Y-%m-%d %r'
++ START_FMT='2020-06-23 02:35:52 PM'
STARTING TIMING RUN AT 2020-06-23 02:35:52 PM
++ echo 'STARTING TIMING RUN AT 2020-06-23 02:35:52 PM'
++ [[ 80 -ne 1 ]]
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
++ export DGXSYSTEM
++ export SLURM_NTASKS_PER_NODE
++ export SLURM_NNODES
++ declare -a CMD
++ '[' -n 7 ']'
++ '[' 80 -gt 10 ']'
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 1857 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 9216 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
++ START_FMT='2020-06-23 02:35:52 PM'
++ echo 'STARTING TIMING RUN AT 2020-06-23 02:35:52 PM'
STARTING TIMING RUN AT 2020-06-23 02:35:52 PM
++ [[ 80 -ne 1 ]]
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
++ export DGXSYSTEM
++ export SLURM_NTASKS_PER_NODE
++ export SLURM_NNODES
++ declare -a CMD
++ '[' -n 4 ']'
++ '[' 80 -gt 10 ']'
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
++ START_FMT='2020-06-23 02:35:52 PM'
STARTING TIMING RUN AT 2020-06-23 02:35:52 PM
++ echo 'STARTING TIMING RUN AT 2020-06-23 02:35:52 PM'
++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 1857 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 9216 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
++ [[ 80 -ne 1 ]]
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
++ export DGXSYSTEM
++ export SLURM_NTASKS_PER_NODE
++ export SLURM_NNODES
++ declare -a CMD
++ '[' -n 1 ']'
++ '[' 80 -gt 10 ']'
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
++ START_FMT='2020-06-23 02:35:52 PM'
++ echo 'STARTING TIMING RUN AT 2020-06-23 02:35:52 PM'
STARTING TIMING RUN AT 2020-06-23 02:35:52 PM
++ [[ 80 -ne 1 ]]
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
++ export DGXSYSTEM
++ export SLURM_NTASKS_PER_NODE
++ export SLURM_NNODES
++ declare -a CMD
++ '[' -n 5 ']'
++ '[' 80 -gt 10 ']'
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 1857 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 9216 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 1857 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 9216 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
++ START_FMT='2020-06-23 02:35:52 PM'
STARTING TIMING RUN AT 2020-06-23 02:35:52 PM
++ echo 'STARTING TIMING RUN AT 2020-06-23 02:35:52 PM'
++ [[ 80 -ne 1 ]]
++ DISTRIBUTED_INIT_METHOD='--distributed-init-method env://'
++ export DGXSYSTEM
++ export SLURM_NTASKS_PER_NODE
++ export SLURM_NNODES
++ declare -a CMD
++ '[' -n 0 ']'
++ '[' 80 -gt 10 ']'
++ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
++ ./bind.sh --cpu=exclusive -- python -u train.py /data --seed 1857 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 9216 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
num_sockets = 2 num_nodes=8 cores_per_socket=64
+ exec numactl --physcpubind=32-47,160-175 --membind=2 -- python -u train.py /data --seed 1857 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 9216 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
num_sockets = 2 num_nodes=8 cores_per_socket=64
+ exec numactl --physcpubind=80-95,208-223 --membind=5 -- python -u train.py /data --seed 1857 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 9216 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
num_sockets = 2 num_nodes=8 cores_per_socket=64
num_sockets = 2 num_nodes=8 cores_per_socket=64
+ exec numactl --physcpubind=96-111,224-239 --membind=6 -- python -u train.py /data --seed 1857 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 9216 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
+ exec numactl --physcpubind=112-127,240-255 --membind=7 -- python -u train.py /data --seed 1857 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 9216 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
num_sockets = 2 num_nodes=8 cores_per_socket=64
num_sockets = 2 num_nodes=8 cores_per_socket=64
+ exec numactl --physcpubind=32-47,160-175 --membind=2 -- python -u train.py /data --seed 1857 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 9216 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
+ exec numactl --physcpubind=32-47,160-175 --membind=2 -- python -u train.py /data --seed 1857 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 9216 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
num_sockets = 2 num_nodes=8 cores_per_socket=64
+ exec numactl --physcpubind=0-15,128-143 --membind=0 -- python -u train.py /data --seed 1857 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 9216 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
num_sockets = 2 num_nodes=8 cores_per_socket=64
+ exec numactl --physcpubind=0-15,128-143 --membind=0 -- python -u train.py /data --seed 1857 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 9216 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
num_sockets = 2 num_nodes=8 cores_per_socket=64
+ exec numactl --physcpubind=80-95,208-223 --membind=5 -- python -u train.py /data --seed 1857 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 9216 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
num_sockets = 2 num_nodes=8 cores_per_socket=64
num_sockets = 2 num_nodes=8 cores_per_socket=64
+ exec numactl --physcpubind=80-95,208-223 --membind=5 -- python -u train.py /data --seed 1857 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 9216 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
num_sockets = 2 num_nodes=8 cores_per_socket=64
+ exec numactl --physcpubind=64-79,192-207 --membind=4 -- python -u train.py /data --seed 1857 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 9216 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
num_sockets = 2 num_nodes=8 cores_per_socket=64
num_sockets = 2 num_nodes=8 cores_per_socket=64
+ exec numactl --physcpubind=16-31,144-159 --membind=1 -- python -u train.py /data --seed 1857 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 9216 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
+ exec numactl --physcpubind=0-15,128-143 --membind=0 -- python -u train.py /data --seed 1857 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 9216 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
+ exec numactl --physcpubind=48-63,176-191 --membind=3 -- python -u train.py /data --seed 1857 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 9216 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
num_sockets = 2 num_nodes=8 cores_per_socket=64
num_sockets = 2 num_nodes=8 cores_per_socket=64
+ exec numactl --physcpubind=0-15,128-143 --membind=0 -- python -u train.py /data --seed 1857 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 9216 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
+ exec numactl --physcpubind=80-95,208-223 --membind=5 -- python -u train.py /data --seed 1857 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 9216 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
num_sockets = 2 num_nodes=8 cores_per_socket=64
num_sockets = 2 num_nodes=8 cores_per_socket=64
num_sockets = 2 num_nodes=8 cores_per_socket=64
+ exec numactl --physcpubind=80-95,208-223 --membind=5 -- python -u train.py /data --seed 1857 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 9216 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
num_sockets = 2 num_nodes=8 cores_per_socket=64
num_sockets = 2 num_nodes=8 cores_per_socket=64
num_sockets = 2 num_nodes=8 cores_per_socket=64
num_sockets = 2 num_nodes=8 cores_per_socket=64
+ exec numactl --physcpubind=0-15,128-143 --membind=0 -- python -u train.py /data --seed 1857 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 9216 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
+ exec numactl --physcpubind=32-47,160-175 --membind=2 -- python -u train.py /data --seed 1857 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 9216 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
+ exec numactl --physcpubind=96-111,224-239 --membind=6 -- python -u train.py /data --seed 1857 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 9216 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
+ exec numactl --physcpubind=96-111,224-239 --membind=6 -- python -u train.py /data --seed 1857 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 9216 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
+ exec numactl --physcpubind=48-63,176-191 --membind=3 -- python -u train.py /data --seed 1857 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 9216 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
+ exec numactl --physcpubind=64-79,192-207 --membind=4 -- python -u train.py /data --seed 1857 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 9216 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
num_sockets = 2 num_nodes=8 cores_per_socket=64
num_sockets = 2 num_nodes=8 cores_per_socket=64
num_sockets = 2 num_nodes=8 cores_per_socket=64
+ exec numactl --physcpubind=0-15,128-143 --membind=0 -- python -u train.py /data --seed 1857 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 9216 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
+ exec numactl --physcpubind=16-31,144-159 --membind=1 -- python -u train.py /data --seed 1857 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 9216 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
num_sockets = 2 num_nodes=8 cores_per_socket=64
+ exec numactl --physcpubind=80-95,208-223 --membind=5 -- python -u train.py /data --seed 1857 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 9216 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
num_sockets = 2 num_nodes=8 cores_per_socket=64
num_sockets = 2 num_nodes=8 cores_per_socket=64
+ exec numactl --physcpubind=80-95,208-223 --membind=5 -- python -u train.py /data --seed 1857 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 9216 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
num_sockets = 2 num_nodes=8 cores_per_socket=64
num_sockets = 2 num_nodes=8 cores_per_socket=64
num_sockets = 2 num_nodes=8 cores_per_socket=64
num_sockets = 2 num_nodes=8 cores_per_socket=64
num_sockets = 2 num_nodes=8 cores_per_socket=64
+ exec numactl --physcpubind=96-111,224-239 --membind=6 -- python -u train.py /data --seed 1857 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 9216 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
num_sockets = 2 num_nodes=8 cores_per_socket=64
num_sockets = 2 num_nodes=8 cores_per_socket=64
+ exec numactl --physcpubind=0-15,128-143 --membind=0 -- python -u train.py /data --seed 1857 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 9216 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
num_sockets = 2 num_nodes=8 cores_per_socket=64
+ exec numactl --physcpubind=64-79,192-207 --membind=4 -- python -u train.py /data --seed 1857 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 9216 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
+ exec numactl --physcpubind=96-111,224-239 --membind=6 -- python -u train.py /data --seed 1857 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 9216 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
+ exec numactl --physcpubind=112-127,240-255 --membind=7 -- python -u train.py /data --seed 1857 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 9216 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
+ exec numactl --physcpubind=48-63,176-191 --membind=3 -- python -u train.py /data --seed 1857 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 9216 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
+ exec numactl --physcpubind=96-111,224-239 --membind=6 -- python -u train.py /data --seed 1857 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 9216 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
+ exec numactl --physcpubind=112-127,240-255 --membind=7 -- python -u train.py /data --seed 1857 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 9216 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
+ exec numactl --physcpubind=48-63,176-191 --membind=3 -- python -u train.py /data --seed 1857 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 9216 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
+ exec numactl --physcpubind=16-31,144-159 --membind=1 -- python -u train.py /data --seed 1857 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 9216 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
num_sockets = 2 num_nodes=8 cores_per_socket=64
+ exec numactl --physcpubind=64-79,192-207 --membind=4 -- python -u train.py /data --seed 1857 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 9216 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
num_sockets = 2 num_nodes=8 cores_per_socket=64
num_sockets = 2 num_nodes=8 cores_per_socket=64
+ exec numactl --physcpubind=16-31,144-159 --membind=1 -- python -u train.py /data --seed 1857 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 9216 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
+ exec numactl --physcpubind=64-79,192-207 --membind=4 -- python -u train.py /data --seed 1857 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 9216 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
num_sockets = 2 num_nodes=8 cores_per_socket=64
+ exec numactl --physcpubind=80-95,208-223 --membind=5 -- python -u train.py /data --seed 1857 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 9216 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
num_sockets = 2 num_nodes=8 cores_per_socket=64
num_sockets = 2 num_nodes=8 cores_per_socket=64
num_sockets = 2 num_nodes=8 cores_per_socket=64
num_sockets = 2 num_nodes=8 cores_per_socket=64
+ exec numactl --physcpubind=32-47,160-175 --membind=2 -- python -u train.py /data --seed 1857 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 9216 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
+ exec numactl --physcpubind=16-31,144-159 --membind=1 -- python -u train.py /data --seed 1857 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 9216 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
+ exec numactl --physcpubind=48-63,176-191 --membind=3 -- python -u train.py /data --seed 1857 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 9216 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
+ exec numactl --physcpubind=112-127,240-255 --membind=7 -- python -u train.py /data --seed 1857 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 9216 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
num_sockets = 2 num_nodes=8 cores_per_socket=64
num_sockets = 2 num_nodes=8 cores_per_socket=64
+ exec numactl --physcpubind=16-31,144-159 --membind=1 -- python -u train.py /data --seed 1857 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 9216 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
+ exec numactl --physcpubind=48-63,176-191 --membind=3 -- python -u train.py /data --seed 1857 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 9216 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
num_sockets = 2 num_nodes=8 cores_per_socket=64
num_sockets = 2 num_nodes=8 cores_per_socket=64
+ exec numactl --physcpubind=64-79,192-207 --membind=4 -- python -u train.py /data --seed 1857 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 9216 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
num_sockets = 2 num_nodes=8 cores_per_socket=64
num_sockets = 2 num_nodes=8 cores_per_socket=64
+ exec numactl --physcpubind=0-15,128-143 --membind=0 -- python -u train.py /data --seed 1857 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 9216 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
num_sockets = 2 num_nodes=8 cores_per_socket=64
num_sockets = 2 num_nodes=8 cores_per_socket=64
num_sockets = 2 num_nodes=8 cores_per_socket=64
num_sockets = 2 num_nodes=8 cores_per_socket=64
+ exec numactl --physcpubind=80-95,208-223 --membind=5 -- python -u train.py /data --seed 1857 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 9216 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
+ exec numactl --physcpubind=96-111,224-239 --membind=6 -- python -u train.py /data --seed 1857 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 9216 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
+ exec numactl --physcpubind=112-127,240-255 --membind=7 -- python -u train.py /data --seed 1857 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 9216 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
+ exec numactl --physcpubind=64-79,192-207 --membind=4 -- python -u train.py /data --seed 1857 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 9216 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
num_sockets = 2 num_nodes=8 cores_per_socket=64
+ exec numactl --physcpubind=48-63,176-191 --membind=3 -- python -u train.py /data --seed 1857 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 9216 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
+ exec numactl --physcpubind=32-47,160-175 --membind=2 -- python -u train.py /data --seed 1857 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 9216 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
num_sockets = 2 num_nodes=8 cores_per_socket=64
+ exec numactl --physcpubind=112-127,240-255 --membind=7 -- python -u train.py /data --seed 1857 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 9216 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
+ exec numactl --physcpubind=32-47,160-175 --membind=2 -- python -u train.py /data --seed 1857 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 9216 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
num_sockets = 2 num_nodes=8 cores_per_socket=64
+ exec numactl --physcpubind=112-127,240-255 --membind=7 -- python -u train.py /data --seed 1857 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 9216 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
num_sockets = 2 num_nodes=8 cores_per_socket=64
+ exec numactl --physcpubind=112-127,240-255 --membind=7 -- python -u train.py /data --seed 1857 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 9216 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
num_sockets = 2 num_nodes=8 cores_per_socket=64
num_sockets = 2 num_nodes=8 cores_per_socket=64
num_sockets = 2 num_nodes=8 cores_per_socket=64
+ exec numactl --physcpubind=32-47,160-175 --membind=2 -- python -u train.py /data --seed 1857 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 9216 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
+ exec numactl --physcpubind=48-63,176-191 --membind=3 -- python -u train.py /data --seed 1857 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 9216 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
+ exec numactl --physcpubind=96-111,224-239 --membind=6 -- python -u train.py /data --seed 1857 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 9216 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
num_sockets = 2 num_nodes=8 cores_per_socket=64
+ exec numactl --physcpubind=96-111,224-239 --membind=6 -- python -u train.py /data --seed 1857 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 9216 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
num_sockets = 2 num_nodes=8 cores_per_socket=64
num_sockets = 2 num_nodes=8 cores_per_socket=64
num_sockets = 2 num_nodes=8 cores_per_socket=64
num_sockets = 2 num_nodes=8 cores_per_socket=64
+ exec numactl --physcpubind=48-63,176-191 --membind=3 -- python -u train.py /data --seed 1857 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 9216 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
+ exec numactl --physcpubind=16-31,144-159 --membind=1 -- python -u train.py /data --seed 1857 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 9216 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
+ exec numactl --physcpubind=32-47,160-175 --membind=2 -- python -u train.py /data --seed 1857 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 9216 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
+ exec numactl --physcpubind=64-79,192-207 --membind=4 -- python -u train.py /data --seed 1857 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 9216 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
num_sockets = 2 num_nodes=8 cores_per_socket=64
num_sockets = 2 num_nodes=8 cores_per_socket=64
num_sockets = 2 num_nodes=8 cores_per_socket=64
num_sockets = 2 num_nodes=8 cores_per_socket=64
+ exec numactl --physcpubind=112-127,240-255 --membind=7 -- python -u train.py /data --seed 1857 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 9216 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
+ exec numactl --physcpubind=32-47,160-175 --membind=2 -- python -u train.py /data --seed 1857 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 9216 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
+ exec numactl --physcpubind=0-15,128-143 --membind=0 -- python -u train.py /data --seed 1857 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 9216 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
num_sockets = 2 num_nodes=8 cores_per_socket=64
+ exec numactl --physcpubind=16-31,144-159 --membind=1 -- python -u train.py /data --seed 1857 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 9216 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
+ exec numactl --physcpubind=96-111,224-239 --membind=6 -- python -u train.py /data --seed 1857 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 9216 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
num_sockets = 2 num_nodes=8 cores_per_socket=64
num_sockets = 2 num_nodes=8 cores_per_socket=64
+ exec numactl --physcpubind=64-79,192-207 --membind=4 -- python -u train.py /data --seed 1857 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 9216 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
+ exec numactl --physcpubind=80-95,208-223 --membind=5 -- python -u train.py /data --seed 1857 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 9216 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
num_sockets = 2 num_nodes=8 cores_per_socket=64
+ exec numactl --physcpubind=48-63,176-191 --membind=3 -- python -u train.py /data --seed 1857 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 9216 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
num_sockets = 2 num_nodes=8 cores_per_socket=64
num_sockets = 2 num_nodes=8 cores_per_socket=64
num_sockets = 2 num_nodes=8 cores_per_socket=64
+ exec numactl --physcpubind=16-31,144-159 --membind=1 -- python -u train.py /data --seed 1857 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 9216 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
+ exec numactl --physcpubind=0-15,128-143 --membind=0 -- python -u train.py /data --seed 1857 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 9216 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
+ exec numactl --physcpubind=112-127,240-255 --membind=7 -- python -u train.py /data --seed 1857 --arch transformer_wmt_en_de_big_t2t --share-all-embeddings --optimizer adam --adam-betas '(0.9, 0.997)' --adam-eps 1e-9 --clip-norm 0.0 --lr-scheduler inverse_sqrt --warmup-init-lr 0.0 --warmup-updates 400 --lr 1.732e-3 --min-lr 0.0 --dropout 0.1 --weight-decay 0.0 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --max-tokens 9216 --max-epoch 30 --target-bleu 25.0 --ignore-case --no-save --update-freq 1 --fp16 --seq-len-multiple 2 --source_lang en --target_lang de --bucket_growth_factor 1.035 --batching_scheme v0p5_better --batch_multiple_strategy dynamic --fast-xentropy --max-len-a 1 --max-len-b 50 --lenpen 0.6 --no-progress-bar --dataloader-num-workers 2 --enable-dataloader-pin-memory --multihead-attn-impl fast_with_lyrnrm_and_dropoutadd --distributed-init-method env:// --distributed-weight-update 2 --dwu-num-blocks 4 --dwu-num-rs-pg 2 --dwu-num-ar-pg 2 --dwu-num-ag-pg 0 --dwu-overlap-reductions --dwu-num-chunks 1 --dwu-flat-mt --dwu-compute-L2-grad-norm --max-source-positions 76 --max-target-positions 76 --adam-betas '(0.86,0.92)'
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0228, MASTER_PORT: 50162, WORLD_SIZE: 80, RANK: 52
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0228, MASTER_PORT: 50162, WORLD_SIZE: 80, RANK: 50
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0228, MASTER_PORT: 50162, WORLD_SIZE: 80, RANK: 65
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0228, MASTER_PORT: 50162, WORLD_SIZE: 80, RANK: 55
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0228, MASTER_PORT: 50162, WORLD_SIZE: 80, RANK: 71
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0228, MASTER_PORT: 50162, WORLD_SIZE: 80, RANK: 69
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0228, MASTER_PORT: 50162, WORLD_SIZE: 80, RANK: 9
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0228, MASTER_PORT: 50162, WORLD_SIZE: 80, RANK: 53
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0228, MASTER_PORT: 50162, WORLD_SIZE: 80, RANK: 11
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0228, MASTER_PORT: 50162, WORLD_SIZE: 80, RANK: 51
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0228, MASTER_PORT: 50162, WORLD_SIZE: 80, RANK: 10
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0228, MASTER_PORT: 50162, WORLD_SIZE: 80, RANK: 48
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0228, MASTER_PORT: 50162, WORLD_SIZE: 80, RANK: 67
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0228, MASTER_PORT: 50162, WORLD_SIZE: 80, RANK: 54
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0228, MASTER_PORT: 50162, WORLD_SIZE: 80, RANK: 66
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0228, MASTER_PORT: 50162, WORLD_SIZE: 80, RANK: 49
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0228, MASTER_PORT: 50162, WORLD_SIZE: 80, RANK: 70
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0228, MASTER_PORT: 50162, WORLD_SIZE: 80, RANK: 24
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0228, MASTER_PORT: 50162, WORLD_SIZE: 80, RANK: 68
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0228, MASTER_PORT: 50162, WORLD_SIZE: 80, RANK: 64
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0228, MASTER_PORT: 50162, WORLD_SIZE: 80, RANK: 22
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0228, MASTER_PORT: 50162, WORLD_SIZE: 80, RANK: 8
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0228, MASTER_PORT: 50162, WORLD_SIZE: 80, RANK: 14
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0228, MASTER_PORT: 50162, WORLD_SIZE: 80, RANK: 15
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0228, MASTER_PORT: 50162, WORLD_SIZE: 80, RANK: 13
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0228, MASTER_PORT: 50162, WORLD_SIZE: 80, RANK: 12
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0228, MASTER_PORT: 50162, WORLD_SIZE: 80, RANK: 37
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0228, MASTER_PORT: 50162, WORLD_SIZE: 80, RANK: 19
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0228, MASTER_PORT: 50162, WORLD_SIZE: 80, RANK: 26
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0228, MASTER_PORT: 50162, WORLD_SIZE: 80, RANK: 5
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0228, MASTER_PORT: 50162, WORLD_SIZE: 80, RANK: 20
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0228, MASTER_PORT: 50162, WORLD_SIZE: 80, RANK: 27
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0228, MASTER_PORT: 50162, WORLD_SIZE: 80, RANK: 25
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0228, MASTER_PORT: 50162, WORLD_SIZE: 80, RANK: 38
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0228, MASTER_PORT: 50162, WORLD_SIZE: 80, RANK: 0
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0228, MASTER_PORT: 50162, WORLD_SIZE: 80, RANK: 21
| distributed init done!
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0228, MASTER_PORT: 50162, WORLD_SIZE: 80, RANK: 62
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0228, MASTER_PORT: 50162, WORLD_SIZE: 80, RANK: 29
| distributed init done!
| distributed init done!
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0228, MASTER_PORT: 50162, WORLD_SIZE: 80, RANK: 32
| distributed init done!
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0228, MASTER_PORT: 50162, WORLD_SIZE: 80, RANK: 31
| distributed init done!
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0228, MASTER_PORT: 50162, WORLD_SIZE: 80, RANK: 77
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0228, MASTER_PORT: 50162, WORLD_SIZE: 80, RANK: 30
| distributed init done!
| distributed init done!
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0228, MASTER_PORT: 50162, WORLD_SIZE: 80, RANK: 42
| distributed init done!
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0228, MASTER_PORT: 50162, WORLD_SIZE: 80, RANK: 16
| distributed init done!
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0228, MASTER_PORT: 50162, WORLD_SIZE: 80, RANK: 23
| distributed init done!
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0228, MASTER_PORT: 50162, WORLD_SIZE: 80, RANK: 6
| distributed init done!
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0228, MASTER_PORT: 50162, WORLD_SIZE: 80, RANK: 46
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0228, MASTER_PORT: 50162, WORLD_SIZE: 80, RANK: 33
| distributed init done!
| distributed init done!
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0228, MASTER_PORT: 50162, WORLD_SIZE: 80, RANK: 17
| distributed init done!
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0228, MASTER_PORT: 50162, WORLD_SIZE: 80, RANK: 60
| distributed init done!
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0228, MASTER_PORT: 50162, WORLD_SIZE: 80, RANK: 18
| distributed init done!
:::MLLOG {"namespace": "", "time_ms": 1592948155816, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0228, MASTER_PORT: 50162, WORLD_SIZE: 80, RANK: 28
| distributed init done!
:::MLLOG {"namespace": "", "time_ms": 1592948155818, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0228, MASTER_PORT: 50162, WORLD_SIZE: 80, RANK: 72
| distributed init done!
:::MLLOG {"namespace": "", "time_ms": 1592948155828, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
:::MLLOG {"namespace": "", "time_ms": 1592948155828, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0228, MASTER_PORT: 50162, WORLD_SIZE: 80, RANK: 44
| distributed init done!
:::MLLOG {"namespace": "", "time_ms": 1592948155833, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
:::MLLOG {"namespace": "", "time_ms": 1592948155835, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0228, MASTER_PORT: 50162, WORLD_SIZE: 80, RANK: 36
:::MLLOG {"namespace": "", "time_ms": 1592948155847, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
| distributed init done!
:::MLLOG {"namespace": "", "time_ms": 1592948155852, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
:::MLLOG {"namespace": "", "time_ms": 1592948155854, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
:::MLLOG {"namespace": "", "time_ms": 1592948155854, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
:::MLLOG {"namespace": "", "time_ms": 1592948155856, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0228, MASTER_PORT: 50162, WORLD_SIZE: 80, RANK: 61
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0228, MASTER_PORT: 50162, WORLD_SIZE: 80, RANK: 34
| distributed init done!
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0228, MASTER_PORT: 50162, WORLD_SIZE: 80, RANK: 39
| distributed init done!
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0228, MASTER_PORT: 50162, WORLD_SIZE: 80, RANK: 35
| distributed init done!
| distributed init done!
:::MLLOG {"namespace": "", "time_ms": 1592948155854, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0228, MASTER_PORT: 50162, WORLD_SIZE: 80, RANK: 7
| distributed init done!
:::MLLOG {"namespace": "", "time_ms": 1592948155877, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0228, MASTER_PORT: 50162, WORLD_SIZE: 80, RANK: 59
| distributed init done!
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0228, MASTER_PORT: 50162, WORLD_SIZE: 80, RANK: 4
| distributed init done!
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0228, MASTER_PORT: 50162, WORLD_SIZE: 80, RANK: 3
| distributed init done!
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0228, MASTER_PORT: 50162, WORLD_SIZE: 80, RANK: 1
| distributed init done!
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0228, MASTER_PORT: 50162, WORLD_SIZE: 80, RANK: 76
| distributed init done!
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0228, MASTER_PORT: 50162, WORLD_SIZE: 80, RANK: 43
| distributed init done!
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0228, MASTER_PORT: 50162, WORLD_SIZE: 80, RANK: 45
:::MLLOG {"namespace": "", "time_ms": 1592948155901, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
| distributed init done!
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0228, MASTER_PORT: 50162, WORLD_SIZE: 80, RANK: 2
| distributed init (rank 0): env://
:::MLLOG {"namespace": "", "time_ms": 1592948155904, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
| distributed env init. MASTER_ADDR: luna-0228, MASTER_PORT: 50162, WORLD_SIZE: 80, RANK: 57
| distributed init done!
| distributed init done!
:::MLLOG {"namespace": "", "time_ms": 1592948155907, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0228, MASTER_PORT: 50162, WORLD_SIZE: 80, RANK: 79
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0228, MASTER_PORT: 50162, WORLD_SIZE: 80, RANK: 56
| distributed init done!
| distributed init done!
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0228, MASTER_PORT: 50162, WORLD_SIZE: 80, RANK: 41
| distributed init done!
:::MLLOG {"namespace": "", "time_ms": 1592948155920, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0228, MASTER_PORT: 50162, WORLD_SIZE: 80, RANK: 73
:::MLLOG {"namespace": "", "time_ms": 1592948155922, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0228, MASTER_PORT: 50162, WORLD_SIZE: 80, RANK: 63
| distributed init done!
| distributed init done!
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0228, MASTER_PORT: 50162, WORLD_SIZE: 80, RANK: 75
| distributed init done!
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0228, MASTER_PORT: 50162, WORLD_SIZE: 80, RANK: 58
| distributed init done!
:::MLLOG {"namespace": "", "time_ms": 1592948155942, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0228, MASTER_PORT: 50162, WORLD_SIZE: 80, RANK: 47
| distributed init done!
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0228, MASTER_PORT: 50162, WORLD_SIZE: 80, RANK: 40
| distributed init done!
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0228, MASTER_PORT: 50162, WORLD_SIZE: 80, RANK: 78
| distributed init (rank 0): env://
| distributed env init. MASTER_ADDR: luna-0228, MASTER_PORT: 50162, WORLD_SIZE: 80, RANK: 74
| distributed init done!
| distributed init done!
:::MLLOG {"namespace": "", "time_ms": 1592948155993, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
:::MLLOG {"namespace": "", "time_ms": 1592948155998, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
:::MLLOG {"namespace": "", "time_ms": 1592948156028, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
:::MLLOG {"namespace": "", "time_ms": 1592948156028, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
:::MLLOG {"namespace": "", "time_ms": 1592948156029, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
:::MLLOG {"namespace": "", "time_ms": 1592948156029, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
:::MLLOG {"namespace": "", "time_ms": 1592948156032, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
:::MLLOG {"namespace": "", "time_ms": 1592948156033, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
:::MLLOG {"namespace": "", "time_ms": 1592948156035, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
:::MLLOG {"namespace": "", "time_ms": 1592948156042, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
:::MLLOG {"namespace": "", "time_ms": 1592948156053, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
:::MLLOG {"namespace": "", "time_ms": 1592948156077, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
:::MLLOG {"namespace": "", "time_ms": 1592948156081, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
:::MLLOG {"namespace": "", "time_ms": 1592948156084, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
:::MLLOG {"namespace": "", "time_ms": 1592948156085, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
:::MLLOG {"namespace": "", "time_ms": 1592948156093, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
:::MLLOG {"namespace": "", "time_ms": 1592948156095, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
:::MLLOG {"namespace": "", "time_ms": 1592948156110, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
:::MLLOG {"namespace": "", "time_ms": 1592948156108, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
:::MLLOG {"namespace": "", "time_ms": 1592948156108, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
:::MLLOG {"namespace": "", "time_ms": 1592948156115, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
:::MLLOG {"namespace": "", "time_ms": 1592948156119, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
:::MLLOG {"namespace": "", "time_ms": 1592948156119, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
:::MLLOG {"namespace": "", "time_ms": 1592948156121, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
:::MLLOG {"namespace": "", "time_ms": 1592948156146, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
:::MLLOG {"namespace": "", "time_ms": 1592948156150, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
| distributed init done!
| distributed init done!
| distributed init done!
:::MLLOG {"namespace": "", "time_ms": 1592948156445, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
| distributed init done!
:::MLLOG {"namespace": "", "time_ms": 1592948156482, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
:::MLLOG {"namespace": "", "time_ms": 1592948156487, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
| distributed init done!
| distributed init done!
| distributed init done!
| distributed init done!
| distributed init done!
| distributed init done!
| distributed init done!
| distributed init done!
| distributed init done!
:::MLLOG {"namespace": "", "time_ms": 1592948156567, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
| distributed init done!
| distributed init done!
| distributed init done!
| distributed init done!
:::MLLOG {"namespace": "", "time_ms": 1592948156606, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
:::MLLOG {"namespace": "", "time_ms": 1592948156606, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
| distributed init done!
| distributed init done!
:::MLLOG {"namespace": "", "time_ms": 1592948156611, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
| distributed init done!
| distributed init done!
| distributed init done!
:::MLLOG {"namespace": "", "time_ms": 1592948156641, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
:::MLLOG {"namespace": "", "time_ms": 1592948156655, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
| distributed init done!
:::MLLOG {"namespace": "", "time_ms": 1592948156657, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
:::MLLOG {"namespace": "", "time_ms": 1592948156671, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
| distributed init done!
| distributed init done!
| distributed init done!
:::MLLOG {"namespace": "", "time_ms": 1592948156678, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
| distributed init done!
| distributed init done!
:::MLLOG {"namespace": "", "time_ms": 1592948156696, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
| distributed init done!
| distributed init done!
| distributed init done!
| distributed init done!
| distributed init done!
| distributed init done!
:::MLLOG {"namespace": "", "time_ms": 1592948156750, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
:::MLLOG {"namespace": "", "time_ms": 1592948156752, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
:::MLLOG {"namespace": "", "time_ms": 1592948156752, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
| distributed init done!
| initialized host luna-0228 as rank 0 and device id 0
:::MLLOG {"namespace": "", "time_ms": 1592948156766, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
:::MLLOG {"namespace": "", "time_ms": 1592948156775, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
:::MLLOG {"namespace": "", "time_ms": 1592948156786, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
:::MLLOG {"namespace": "", "time_ms": 1592948156784, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
:::MLLOG {"namespace": "", "time_ms": 1592948156795, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
:::MLLOG {"namespace": "", "time_ms": 1592948156807, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
:::MLLOG {"namespace": "", "time_ms": 1592948156820, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
Namespace(adam_betas='(0.86,0.92)', adam_eps=1e-09, adaptive_softmax_cutoff=None, arch='transformer_wmt_en_de_big_t2t', attention_dropout=0.1, batch_multiple_strategy='dynamic', batching_scheme='v0p5_better', beam=4, bucket_growth_factor=1.035, clip_norm=0.0, cpu=False, criterion='label_smoothed_cross_entropy', data='/data', dataloader_num_workers=2, decoder_attention_heads=16, decoder_embed_dim=1024, decoder_embed_path=None, decoder_ffn_embed_dim=4096, decoder_layers=6, decoder_learned_pos=False, decoder_normalize_before=True, device_id=0, distributed_backend='nccl', distributed_init_method='env://', distributed_port=-1, distributed_rank=0, distributed_weight_update=2, distributed_world_size=80, dropout=0.1, dwu_compute_L2_grad_norm=True, dwu_do_not_flatten_model=False, dwu_e5m2_allgather=False, dwu_flat_mt=True, dwu_full_pipeline=False, dwu_group_size=0, dwu_num_ag_pg=0, dwu_num_ar_pg=2, dwu_num_blocks=4, dwu_num_chunks=1, dwu_num_rs_pg=2, dwu_overlap_reductions=True, enable_dataloader_pin_memory=True, enable_global_stats=False, enable_parallel_backward_allred_opt=False, enable_parallel_backward_allred_opt_correctness_check=False, encoder_attention_heads=16, encoder_embed_dim=1024, encoder_embed_path=None, encoder_ffn_embed_dim=4096, encoder_layers=6, encoder_learned_pos=False, encoder_normalize_before=True, fast_xentropy=True, fp16=True, gen_subset='test', ignore_case=True, keep_interval_updates=-1, label_smoothing=0.1, left_pad_source='True', left_pad_target='False', lenpen=0.6, local_rank=0, log_format=None, log_interval=1000, log_translations=False, lr=[0.001732], lr_scheduler='inverse_sqrt', lr_shrink=0.1, max_epoch=30, max_len_a=1.0, max_len_b=50, max_sentences=None, max_sentences_valid=None, max_source_positions=76, max_target_positions=76, max_tokens=9216, max_update=0, min_len=1, min_loss_scale=0.0001, min_lr=0.0, model_overrides='{}', momentum=0.99, multihead_attn_impl='fast_with_lyrnrm_and_dropoutadd', nbest=1, no_beamable_mm=False, no_early_stop=False, no_epoch_checkpoints=False, no_progress_bar=True, no_save=True, no_token_positional_embeddings=False, num_shards=1, online_eval=False, optimizer='adam', parallel_backward_allred_cuda_nstreams=1, parallel_backward_allred_opt_threshold=0, path=None, prefix_size=0, print_alignment=False, profile=None, quiet=False, raw_text=False, relu_dropout=0.1, remove_bpe=None, replace_unk=None, restore_file='checkpoint_last.pt', sampling=False, sampling_temperature=1, sampling_topk=-1, save_dir='checkpoints', save_interval=1, save_interval_updates=0, score_reference=False, seed=1857, sentence_avg=False, seq_len_multiple=2, shard_id=0, share_all_embeddings=True, share_decoder_input_output_embed=False, skip_invalid_size_inputs_valid_test=False, source_lang='en', target_bleu=25.0, target_lang='de', task='translation', time_step=False, train_subset='train', uniform_n_seq_in_dataset=None, uniform_n_seq_per_batch=None, uniform_seq_len_per_batch=None, unkpen=0, unnormalized=False, update_freq=[1], valid_subset='valid', validate_interval=1, warmup_init_lr=0.0, warmup_updates=400, weight_decay=0.0)
:::MLLOG {"namespace": "", "time_ms": 1592948156864, "event_type": "POINT_IN_TIME", "key": "global_batch_size", "value": 737280, "metadata": {"file": "/workspace/translation/train.py", "lineno": 133}}
:::MLLOG {"namespace": "", "time_ms": 1592948156864, "event_type": "POINT_IN_TIME", "key": "opt_name", "value": "adam", "metadata": {"file": "/workspace/translation/train.py", "lineno": 134}}
:::MLLOG {"namespace": "", "time_ms": 1592948156864, "event_type": "POINT_IN_TIME", "key": "opt_base_learning_rate", "value": 0.001732, "metadata": {"file": "/workspace/translation/train.py", "lineno": 136}}
:::MLLOG {"namespace": "", "time_ms": 1592948156865, "event_type": "POINT_IN_TIME", "key": "opt_learning_rate_warmup_steps", "value": 400, "metadata": {"file": "/workspace/translation/train.py", "lineno": 137}}
:::MLLOG {"namespace": "", "time_ms": 1592948156865, "event_type": "POINT_IN_TIME", "key": "max_sequence_length", "value": 76, "metadata": {"file": "/workspace/translation/train.py", "lineno": 139, "method": "discard"}}
:::MLLOG {"namespace": "", "time_ms": 1592948156865, "event_type": "POINT_IN_TIME", "key": "opt_adam_beta_1", "value": 0.86, "metadata": {"file": "/workspace/translation/train.py", "lineno": 140}}
:::MLLOG {"namespace": "", "time_ms": 1592948156865, "event_type": "POINT_IN_TIME", "key": "opt_adam_beta_2", "value": 0.92, "metadata": {"file": "/workspace/translation/train.py", "lineno": 141}}
:::MLLOG {"namespace": "", "time_ms": 1592948156865, "event_type": "POINT_IN_TIME", "key": "opt_adam_epsilon", "value": 1e-09, "metadata": {"file": "/workspace/translation/train.py", "lineno": 142}}
:::MLLOG {"namespace": "", "time_ms": 1592948156865, "event_type": "POINT_IN_TIME", "key": "seed", "value": 1857, "metadata": {"file": "/workspace/translation/train.py", "lineno": 143}}
:::MLLOG {"namespace": "", "time_ms": 1592948156828, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
:::MLLOG {"namespace": "", "time_ms": 1592948156825, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
:::MLLOG {"namespace": "", "time_ms": 1592948156833, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
:::MLLOG {"namespace": "", "time_ms": 1592948156844, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
:::MLLOG {"namespace": "", "time_ms": 1592948156847, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
:::MLLOG {"namespace": "", "time_ms": 1592948156844, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
:::MLLOG {"namespace": "", "time_ms": 1592948156845, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
:::MLLOG {"namespace": "", "time_ms": 1592948156851, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
:::MLLOG {"namespace": "", "time_ms": 1592948156857, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
:::MLLOG {"namespace": "", "time_ms": 1592948156875, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
:::MLLOG {"namespace": "", "time_ms": 1592948156876, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
:::MLLOG {"namespace": "", "time_ms": 1592948156875, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 116}}
Using master seed from command line: 1857
Worker 0 is using worker seed: 4053508195
| [en] dictionary: 33712 types
| [de] dictionary: 33712 types
| model transformer_wmt_en_de_big_t2t, criterion LabelSmoothedCrossEntropyCriterion
| num. model params: 210808832
| training on 80 GPUs
| max tokens per GPU = 9216 and max sentences per GPU = None
DistributedFusedAdam {'distributed_weight_update': 2, 'dwu_group_size': 0, 'dwu_num_blocks': 4, 'dwu_num_chunks': 1, 'dwu_num_rs_pg': 2, 'dwu_num_ar_pg': 2, 'dwu_num_ag_pg': 0, 'overlap_reductions': True, 'full_pipeline': False, 'compute_L2_grad_norm': True, 'flat_mt': True, 'e5m2_allgather': False, 'do_not_flatten_model': False}
self._net_total_param_size=210808832, self._total_param_size=210812928, dwu_min_page_size=8192, self._block_size=52703232, self._chunk_size=52703232, self._shard_size=6587904
[0, 15, 55, 104]
model_param_fragment.size()=torch.Size([6587904]), new_param_packed_fragment.size()=torch.Size([6587904]), master_param_fragment.size()=torch.Size([6587904])
model_param_fragment.size()=torch.Size([2800640]), new_param_packed_fragment.size()=torch.Size([2800640]), master_param_fragment.size()=torch.Size([2800640])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([3783168]), new_param_packed_fragment.size()=torch.Size([3783168]), master_param_fragment.size()=torch.Size([3783168])
model_param_fragment.size()=torch.Size([465920]), new_param_packed_fragment.size()=torch.Size([465920]), master_param_fragment.size()=torch.Size([465920])
model_param_fragment.size()=torch.Size([4096]), new_param_packed_fragment.size()=torch.Size([4096]), master_param_fragment.size()=torch.Size([4096])
model_param_fragment.size()=torch.Size([4194304]), new_param_packed_fragment.size()=torch.Size([4194304]), master_param_fragment.size()=torch.Size([4194304])
model_param_fragment.size()=torch.Size([1024]), new_param_packed_fragment.size()=torch.Size([1024]), master_param_fragment.size()=torch.Size([1024])
model_param_fragment.size()=torch.Size([1024]), new_param_packed_fragment.size()=torch.Size([1024]), master_param_fragment.size()=torch.Size([1024])
model_param_fragment.size()=torch.Size([1024]), new_param_packed_fragment.size()=torch.Size([1024]), master_param_fragment.size()=torch.Size([1024])
model_param_fragment.size()=torch.Size([1024]), new_param_packed_fragment.size()=torch.Size([1024]), master_param_fragment.size()=torch.Size([1024])
model_param_fragment.size()=torch.Size([1024]), new_param_packed_fragment.size()=torch.Size([1024]), master_param_fragment.size()=torch.Size([1024])
model_param_fragment.size()=torch.Size([1918464]), new_param_packed_fragment.size()=torch.Size([1918464]), master_param_fragment.size()=torch.Size([1918464])
model_param_fragment.size()=torch.Size([2328576]), new_param_packed_fragment.size()=torch.Size([2328576]), master_param_fragment.size()=torch.Size([2328576])
model_param_fragment.size()=torch.Size([1024]), new_param_packed_fragment.size()=torch.Size([1024]), master_param_fragment.size()=torch.Size([1024])
model_param_fragment.size()=torch.Size([1024]), new_param_packed_fragment.size()=torch.Size([1024]), master_param_fragment.size()=torch.Size([1024])
model_param_fragment.size()=torch.Size([1024]), new_param_packed_fragment.size()=torch.Size([1024]), master_param_fragment.size()=torch.Size([1024])
model_param_fragment.size()=torch.Size([3145728]), new_param_packed_fragment.size()=torch.Size([3145728]), master_param_fragment.size()=torch.Size([3145728])
model_param_fragment.size()=torch.Size([1048576]), new_param_packed_fragment.size()=torch.Size([1048576]), master_param_fragment.size()=torch.Size([1048576])
model_param_fragment.size()=torch.Size([1024]), new_param_packed_fragment.size()=torch.Size([1024]), master_param_fragment.size()=torch.Size([1024])
model_param_fragment.size()=torch.Size([1024]), new_param_packed_fragment.size()=torch.Size([1024]), master_param_fragment.size()=torch.Size([1024])
model_param_fragment.size()=torch.Size([59904]), new_param_packed_fragment.size()=torch.Size([59904]), master_param_fragment.size()=torch.Size([59904])
:::MLLOG {"namespace": "", "time_ms": 1592948181970, "event_type": "INTERVAL_END", "key": "init_stop", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 212}}
:::MLLOG {"namespace": "", "time_ms": 1592948181971, "event_type": "INTERVAL_START", "key": "run_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 214}}
filename: /data/train.en-de.en
raw_text: False
| /data train 4590101 examples
filename: /data/train1.en-de.en
raw_text: False
filename: /data/train1.de-en.en
raw_text: False
srcline: tensor([  855,     3,    45,    96,   156,    10,  2688,   177,  5596,   163,     5,  9336, 14909, 12630,   527,   297, 15690,    70,     3,    68,    17,   927,    45,   482,   151,   283,  3551,  2091,     7,     5,   546,    24, 26623,  1617,  5440,    86,    15,  1524,  3522,   434,     3,   264,   199,   182,    86,    15,  4489,  8360,    69,   114,     5,   253,    41,    69,  3823,   203,     8,     5,  9336, 14909, 12630,   527,     4,     2])
| Sentences are being padded to multiples of: 2
filename: /data/test.en-de.en
raw_text: False
| /data test 3003 examples
srcline: tensor([ 7549,  4344,    64, 32364,  1259,    20, 13504,  8959,  3868,     2])
| Sentences are being padded to multiples of: 2
filename: /data/test1.en-de.en
raw_text: False
filename: /data/test1.de-en.en
raw_text: False
:::MLLOG {"namespace": "", "time_ms": 1592948182937, "event_type": "POINT_IN_TIME", "key": "train_samples", "value": 4590101, "metadata": {"file": "/workspace/translation/train.py", "lineno": 224}}
:::MLLOG {"namespace": "", "time_ms": 1592948182937, "event_type": "POINT_IN_TIME", "key": "eval_samples", "value": 3003, "metadata": {"file": "/workspace/translation/train.py", "lineno": 227}}
self.dataset.src_sizes 4590101
self.dataset.tgt_sizes 4590101
generated 15881 batches in 1.364235s
got epoch iterator 1.3645601272583008
:::MLLOG {"namespace": "", "time_ms": 1592948184302, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 257, "first_epoch_num": 1, "epoch_count": 1}}
:::MLLOG {"namespace": "", "time_ms": 1592948184302, "event_type": "INTERVAL_START", "key": "epoch_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 258, "epoch_num": 1}}
| WARNING: overflow detected, setting loss scale to: 64.0
| WARNING: overflow detected, setting loss scale to: 32.0
| WARNING: overflow detected, setting loss scale to: 16.0
| WARNING: overflow detected, setting loss scale to: 8.0
| WARNING: overflow detected, setting loss scale to: 4.0
| epoch 001 | loss 61325.405 | nll_loss 0.000 | ppl 1.00 | wps 7.01611e+06 | ups 5.1 | wpb 668506 | bsz 293 | num_updates 194 | lr 0.00084002 | gnorm 91526.704 | clip 100% | oom 0 | loss_scale 4.000 | wall 38
epoch time  18.75261926651001
:::MLLOG {"namespace": "", "time_ms": 1592948203055, "event_type": "INTERVAL_END", "key": "epoch_stop", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 273, "epoch_num": 1}}
:::MLLOG {"namespace": "", "time_ms": 1592948203056, "event_type": "INTERVAL_START", "key": "eval_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 640, "epoch_num": 1}}
self.dataset.src_sizes 3003
self.dataset.tgt_sizes 3003
generated 251 batches in 0.000904s
/workspace/translation/fairseq/sequence_generator.py:356: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead. (Triggered internally at  ../aten/src/ATen/native/BinaryOps.cpp:66.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams)
/workspace/translation/fairseq/sequence_generator.py:356: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead. (Triggered internally at  ../aten/src/ATen/native/BinaryOps.cpp:66.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams)
/workspace/translation/fairseq/sequence_generator.py:356: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead. (Triggered internally at  ../aten/src/ATen/native/BinaryOps.cpp:66.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams)
/workspace/translation/fairseq/sequence_generator.py:356: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead. (Triggered internally at  ../aten/src/ATen/native/BinaryOps.cpp:66.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams)
/workspace/translation/fairseq/sequence_generator.py:356: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead. (Triggered internally at  ../aten/src/ATen/native/BinaryOps.cpp:66.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams)
/workspace/translation/fairseq/sequence_generator.py:356: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead. (Triggered internally at  ../aten/src/ATen/native/BinaryOps.cpp:66.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams)
/workspace/translation/fairseq/sequence_generator.py:356: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead. (Triggered internally at  ../aten/src/ATen/native/BinaryOps.cpp:66.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams)
/workspace/translation/fairseq/sequence_generator.py:356: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead. (Triggered internally at  ../aten/src/ATen/native/BinaryOps.cpp:66.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams)
/workspace/translation/fairseq/sequence_generator.py:356: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead. (Triggered internally at  ../aten/src/ATen/native/BinaryOps.cpp:66.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams)
/workspace/translation/fairseq/sequence_generator.py:356: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead. (Triggered internally at  ../aten/src/ATen/native/BinaryOps.cpp:66.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams)
/workspace/translation/fairseq/sequence_generator.py:356: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead. (Triggered internally at  ../aten/src/ATen/native/BinaryOps.cpp:66.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams)
/workspace/translation/fairseq/sequence_generator.py:356: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead. (Triggered internally at  ../aten/src/ATen/native/BinaryOps.cpp:66.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams)
/workspace/translation/fairseq/sequence_generator.py:356: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead. (Triggered internally at  ../aten/src/ATen/native/BinaryOps.cpp:66.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams)
/workspace/translation/fairseq/sequence_generator.py:356: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead. (Triggered internally at  ../aten/src/ATen/native/BinaryOps.cpp:66.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams)
/workspace/translation/fairseq/sequence_generator.py:356: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead. (Triggered internally at  ../aten/src/ATen/native/BinaryOps.cpp:66.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams)
/workspace/translation/fairseq/sequence_generator.py:356: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead. (Triggered internally at  ../aten/src/ATen/native/BinaryOps.cpp:66.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams)
/workspace/translation/fairseq/sequence_generator.py:356: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead. (Triggered internally at  ../aten/src/ATen/native/BinaryOps.cpp:66.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams)
/workspace/translation/fairseq/sequence_generator.py:356: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead. (Triggered internally at  ../aten/src/ATen/native/BinaryOps.cpp:66.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams)
/workspace/translation/fairseq/sequence_generator.py:356: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead. (Triggered internally at  ../aten/src/ATen/native/BinaryOps.cpp:66.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams)
/workspace/translation/fairseq/sequence_generator.py:356: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead. (Triggered internally at  ../aten/src/ATen/native/BinaryOps.cpp:66.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams)
/workspace/translation/fairseq/sequence_generator.py:356: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead. (Triggered internally at  ../aten/src/ATen/native/BinaryOps.cpp:66.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams)
/workspace/translation/fairseq/sequence_generator.py:356: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead. (Triggered internally at  ../aten/src/ATen/native/BinaryOps.cpp:66.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams)
/workspace/translation/fairseq/sequence_generator.py:356: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead. (Triggered internally at  ../aten/src/ATen/native/BinaryOps.cpp:66.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams)
/workspace/translation/fairseq/sequence_generator.py:356: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead. (Triggered internally at  ../aten/src/ATen/native/BinaryOps.cpp:66.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams)
/workspace/translation/fairseq/sequence_generator.py:356: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead. (Triggered internally at  ../aten/src/ATen/native/BinaryOps.cpp:66.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams)
/workspace/translation/fairseq/sequence_generator.py:356: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead. (Triggered internally at  ../aten/src/ATen/native/BinaryOps.cpp:66.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams)
/workspace/translation/fairseq/sequence_generator.py:356: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead. (Triggered internally at  ../aten/src/ATen/native/BinaryOps.cpp:66.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams)
/workspace/translation/fairseq/sequence_generator.py:356: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead. (Triggered internally at  ../aten/src/ATen/native/BinaryOps.cpp:66.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams)
/workspace/translation/fairseq/sequence_generator.py:356: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead. (Triggered internally at  ../aten/src/ATen/native/BinaryOps.cpp:66.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams)
/workspace/translation/fairseq/sequence_generator.py:356: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead. (Triggered internally at  ../aten/src/ATen/native/BinaryOps.cpp:66.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams)
/workspace/translation/fairseq/sequence_generator.py:356: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead. (Triggered internally at  ../aten/src/ATen/native/BinaryOps.cpp:66.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams)
/workspace/translation/fairseq/sequence_generator.py:356: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead. (Triggered internally at  ../aten/src/ATen/native/BinaryOps.cpp:66.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams)
/workspace/translation/fairseq/sequence_generator.py:356: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead. (Triggered internally at  ../aten/src/ATen/native/BinaryOps.cpp:66.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams)
/workspace/translation/fairseq/sequence_generator.py:356: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead. (Triggered internally at  ../aten/src/ATen/native/BinaryOps.cpp:66.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams)
/workspace/translation/fairseq/sequence_generator.py:356: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead. (Triggered internally at  ../aten/src/ATen/native/BinaryOps.cpp:66.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams)
/workspace/translation/fairseq/sequence_generator.py:356: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead. (Triggered internally at  ../aten/src/ATen/native/BinaryOps.cpp:66.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams)
/workspace/translation/fairseq/sequence_generator.py:356: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead. (Triggered internally at  ../aten/src/ATen/native/BinaryOps.cpp:66.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams)
/workspace/translation/fairseq/sequence_generator.py:356: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead. (Triggered internally at  ../aten/src/ATen/native/BinaryOps.cpp:66.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams)
/workspace/translation/fairseq/sequence_generator.py:356: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead. (Triggered internally at  ../aten/src/ATen/native/BinaryOps.cpp:66.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams)
/workspace/translation/fairseq/sequence_generator.py:356: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead. (Triggered internally at  ../aten/src/ATen/native/BinaryOps.cpp:66.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams)
/workspace/translation/fairseq/sequence_generator.py:356: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead. (Triggered internally at  ../aten/src/ATen/native/BinaryOps.cpp:66.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams)
/workspace/translation/fairseq/sequence_generator.py:356: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead. (Triggered internally at  ../aten/src/ATen/native/BinaryOps.cpp:66.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams)
/workspace/translation/fairseq/sequence_generator.py:356: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead. (Triggered internally at  ../aten/src/ATen/native/BinaryOps.cpp:66.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams)
/workspace/translation/fairseq/sequence_generator.py:356: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead. (Triggered internally at  ../aten/src/ATen/native/BinaryOps.cpp:66.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams)
/workspace/translation/fairseq/sequence_generator.py:356: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead. (Triggered internally at  ../aten/src/ATen/native/BinaryOps.cpp:66.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams)
/workspace/translation/fairseq/sequence_generator.py:356: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead. (Triggered internally at  ../aten/src/ATen/native/BinaryOps.cpp:66.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams)
/workspace/translation/fairseq/sequence_generator.py:356: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead. (Triggered internally at  ../aten/src/ATen/native/BinaryOps.cpp:66.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams)
/workspace/translation/fairseq/sequence_generator.py:356: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead. (Triggered internally at  ../aten/src/ATen/native/BinaryOps.cpp:66.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams)
/workspace/translation/fairseq/sequence_generator.py:356: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead. (Triggered internally at  ../aten/src/ATen/native/BinaryOps.cpp:66.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams)
/workspace/translation/fairseq/sequence_generator.py:356: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead. (Triggered internally at  ../aten/src/ATen/native/BinaryOps.cpp:66.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams)
/workspace/translation/fairseq/sequence_generator.py:356: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead. (Triggered internally at  ../aten/src/ATen/native/BinaryOps.cpp:66.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams)
/workspace/translation/fairseq/sequence_generator.py:356: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead. (Triggered internally at  ../aten/src/ATen/native/BinaryOps.cpp:66.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams)
/workspace/translation/fairseq/sequence_generator.py:356: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead. (Triggered internally at  ../aten/src/ATen/native/BinaryOps.cpp:66.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams)
/workspace/translation/fairseq/sequence_generator.py:356: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead. (Triggered internally at  ../aten/src/ATen/native/BinaryOps.cpp:66.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams)
/workspace/translation/fairseq/sequence_generator.py:356: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead. (Triggered internally at  ../aten/src/ATen/native/BinaryOps.cpp:66.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams)
/workspace/translation/fairseq/sequence_generator.py:356: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead. (Triggered internally at  ../aten/src/ATen/native/BinaryOps.cpp:66.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams)
/workspace/translation/fairseq/sequence_generator.py:356: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead. (Triggered internally at  ../aten/src/ATen/native/BinaryOps.cpp:66.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams)
/workspace/translation/fairseq/sequence_generator.py:356: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead. (Triggered internally at  ../aten/src/ATen/native/BinaryOps.cpp:66.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams)
/workspace/translation/fairseq/sequence_generator.py:356: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead. (Triggered internally at  ../aten/src/ATen/native/BinaryOps.cpp:66.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams)
/workspace/translation/fairseq/sequence_generator.py:356: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead. (Triggered internally at  ../aten/src/ATen/native/BinaryOps.cpp:66.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams)
/workspace/translation/fairseq/sequence_generator.py:356: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead. (Triggered internally at  ../aten/src/ATen/native/BinaryOps.cpp:66.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams)
/workspace/translation/fairseq/sequence_generator.py:356: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead. (Triggered internally at  ../aten/src/ATen/native/BinaryOps.cpp:66.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams)
/workspace/translation/fairseq/sequence_generator.py:356: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead. (Triggered internally at  ../aten/src/ATen/native/BinaryOps.cpp:66.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams)
/workspace/translation/fairseq/sequence_generator.py:356: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead. (Triggered internally at  ../aten/src/ATen/native/BinaryOps.cpp:66.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams)
/workspace/translation/fairseq/sequence_generator.py:356: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead. (Triggered internally at  ../aten/src/ATen/native/BinaryOps.cpp:66.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams)
/workspace/translation/fairseq/sequence_generator.py:356: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead. (Triggered internally at  ../aten/src/ATen/native/BinaryOps.cpp:66.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams)
/workspace/translation/fairseq/sequence_generator.py:356: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead. (Triggered internally at  ../aten/src/ATen/native/BinaryOps.cpp:66.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams)
/workspace/translation/fairseq/sequence_generator.py:356: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead. (Triggered internally at  ../aten/src/ATen/native/BinaryOps.cpp:66.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams)
/workspace/translation/fairseq/sequence_generator.py:356: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead. (Triggered internally at  ../aten/src/ATen/native/BinaryOps.cpp:66.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams)
/workspace/translation/fairseq/sequence_generator.py:356: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead. (Triggered internally at  ../aten/src/ATen/native/BinaryOps.cpp:66.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams)
/workspace/translation/fairseq/sequence_generator.py:356: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead. (Triggered internally at  ../aten/src/ATen/native/BinaryOps.cpp:66.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams)
/workspace/translation/fairseq/sequence_generator.py:356: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead. (Triggered internally at  ../aten/src/ATen/native/BinaryOps.cpp:66.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams)
/workspace/translation/fairseq/sequence_generator.py:356: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead. (Triggered internally at  ../aten/src/ATen/native/BinaryOps.cpp:66.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams)
/workspace/translation/fairseq/sequence_generator.py:356: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead. (Triggered internally at  ../aten/src/ATen/native/BinaryOps.cpp:66.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams)
/workspace/translation/fairseq/sequence_generator.py:356: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead. (Triggered internally at  ../aten/src/ATen/native/BinaryOps.cpp:66.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams)
/workspace/translation/fairseq/sequence_generator.py:356: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead. (Triggered internally at  ../aten/src/ATen/native/BinaryOps.cpp:66.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams)
/workspace/translation/fairseq/sequence_generator.py:356: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead. (Triggered internally at  ../aten/src/ATen/native/BinaryOps.cpp:66.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams)
/workspace/translation/fairseq/sequence_generator.py:356: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead. (Triggered internally at  ../aten/src/ATen/native/BinaryOps.cpp:66.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams)
/workspace/translation/fairseq/sequence_generator.py:356: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead. (Triggered internally at  ../aten/src/ATen/native/BinaryOps.cpp:66.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams)
/workspace/translation/fairseq/sequence_generator.py:356: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead. (Triggered internally at  ../aten/src/ATen/native/BinaryOps.cpp:66.)
  torch.div(cand_indices, self.vocab_size, out=cand_beams)
| Translated 48 sentences (1202 tokens) in 1.1s (41.94 sentences/s, 1050.21 tokens/s)
| Generate test with beam=4: bleu_score=0.5911
| Eval completed in: 1.73s
:::MLLOG {"namespace": "", "time_ms": 1592948204781, "event_type": "INTERVAL_END", "key": "eval_stop", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 751, "epoch_num": 1}}
:::MLLOG {"namespace": "", "time_ms": 1592948204782, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.005910859908908606, "metadata": {"file": "/workspace/translation/train.py", "lineno": 280, "epoch_num": 1}}
validation and scoring  1.7275221347808838
:::MLLOG {"namespace": "", "time_ms": 1592948204783, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 295, "first_epoch_num": 1}}
:::MLLOG {"namespace": "", "time_ms": 1592948204794, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 257, "first_epoch_num": 2, "epoch_count": 1}}
:::MLLOG {"namespace": "", "time_ms": 1592948204794, "event_type": "INTERVAL_START", "key": "epoch_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 258, "epoch_num": 2}}
| epoch 002 | loss 40796.668 | nll_loss 0.000 | ppl 1.00 | wps 7.15696e+06 | ups 9.8 | wpb 668546 | bsz 280 | num_updates 393 | lr 0.00170169 | gnorm 56282.816 | clip 100% | oom 0 | loss_scale 4.000 | wall 58
epoch time  18.66331720352173
:::MLLOG {"namespace": "", "time_ms": 1592948223457, "event_type": "INTERVAL_END", "key": "epoch_stop", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 273, "epoch_num": 2}}
:::MLLOG {"namespace": "", "time_ms": 1592948223458, "event_type": "INTERVAL_START", "key": "eval_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 640, "epoch_num": 2}}
self.dataset.src_sizes 3003
self.dataset.tgt_sizes 3003
generated 251 batches in 0.000909s
| Translated 48 sentences (1261 tokens) in 0.9s (50.81 sentences/s, 1334.82 tokens/s)
| Generate test with beam=4: bleu_score=13.3828
| Eval completed in: 1.65s
:::MLLOG {"namespace": "", "time_ms": 1592948225105, "event_type": "INTERVAL_END", "key": "eval_stop", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 751, "epoch_num": 2}}
:::MLLOG {"namespace": "", "time_ms": 1592948225106, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.1338283121585846, "metadata": {"file": "/workspace/translation/train.py", "lineno": 280, "epoch_num": 2}}
validation and scoring  1.6490530967712402
:::MLLOG {"namespace": "", "time_ms": 1592948225106, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 295, "first_epoch_num": 2}}
:::MLLOG {"namespace": "", "time_ms": 1592948225107, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 257, "first_epoch_num": 3, "epoch_count": 1}}
:::MLLOG {"namespace": "", "time_ms": 1592948225107, "event_type": "INTERVAL_START", "key": "epoch_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 258, "epoch_num": 3}}
| WARNING: overflow detected, setting loss scale to: 2.0
| epoch 003 | loss 30276.721 | nll_loss 0.000 | ppl 1.00 | wps 7.13323e+06 | ups 9.7 | wpb 668536 | bsz 279 | num_updates 591 | lr 0.0014249 | gnorm 40834.989 | clip 100% | oom 0 | loss_scale 2.000 | wall 79
epoch time  18.676154375076294
:::MLLOG {"namespace": "", "time_ms": 1592948243783, "event_type": "INTERVAL_END", "key": "epoch_stop", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 273, "epoch_num": 3}}
:::MLLOG {"namespace": "", "time_ms": 1592948243783, "event_type": "INTERVAL_START", "key": "eval_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 640, "epoch_num": 3}}
self.dataset.src_sizes 3003
self.dataset.tgt_sizes 3003
generated 251 batches in 0.000901s
| Translated 48 sentences (1370 tokens) in 0.7s (64.72 sentences/s, 1847.19 tokens/s)
| Generate test with beam=4: bleu_score=21.5438
| Eval completed in: 1.41s
:::MLLOG {"namespace": "", "time_ms": 1592948245193, "event_type": "INTERVAL_END", "key": "eval_stop", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 751, "epoch_num": 3}}
:::MLLOG {"namespace": "", "time_ms": 1592948245195, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.21543775498867035, "metadata": {"file": "/workspace/translation/train.py", "lineno": 280, "epoch_num": 3}}
validation and scoring  1.41176176071167
:::MLLOG {"namespace": "", "time_ms": 1592948245195, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 295, "first_epoch_num": 3}}
:::MLLOG {"namespace": "", "time_ms": 1592948245195, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 257, "first_epoch_num": 4, "epoch_count": 1}}
:::MLLOG {"namespace": "", "time_ms": 1592948245195, "event_type": "INTERVAL_START", "key": "epoch_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 258, "epoch_num": 4}}
| epoch 004 | loss 27545.290 | nll_loss 0.000 | ppl 1.00 | wps 7.16546e+06 | ups 9.9 | wpb 668546 | bsz 285 | num_updates 790 | lr 0.00123244 | gnorm 31455.410 | clip 100% | oom 0 | loss_scale 2.000 | wall 99
epoch time  18.64069151878357
:::MLLOG {"namespace": "", "time_ms": 1592948263836, "event_type": "INTERVAL_END", "key": "epoch_stop", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 273, "epoch_num": 4}}
:::MLLOG {"namespace": "", "time_ms": 1592948263836, "event_type": "INTERVAL_START", "key": "eval_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 640, "epoch_num": 4}}
self.dataset.src_sizes 3003
self.dataset.tgt_sizes 3003
generated 251 batches in 0.000899s
| Translated 48 sentences (1381 tokens) in 0.7s (67.81 sentences/s, 1950.92 tokens/s)
| Generate test with beam=4: bleu_score=23.8709
| Eval completed in: 1.43s
:::MLLOG {"namespace": "", "time_ms": 1592948265269, "event_type": "INTERVAL_END", "key": "eval_stop", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 751, "epoch_num": 4}}
:::MLLOG {"namespace": "", "time_ms": 1592948265270, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.23870931565761566, "metadata": {"file": "/workspace/translation/train.py", "lineno": 280, "epoch_num": 4}}
validation and scoring  1.4344501495361328
:::MLLOG {"namespace": "", "time_ms": 1592948265270, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 295, "first_epoch_num": 4}}
:::MLLOG {"namespace": "", "time_ms": 1592948265270, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 257, "first_epoch_num": 5, "epoch_count": 1}}
:::MLLOG {"namespace": "", "time_ms": 1592948265271, "event_type": "INTERVAL_START", "key": "epoch_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 258, "epoch_num": 5}}
| epoch 005 | loss 26371.873 | nll_loss 0.000 | ppl 1.00 | wps 7.15095e+06 | ups 9.9 | wpb 668546 | bsz 298 | num_updates 989 | lr 0.00110149 | gnorm 25739.525 | clip 100% | oom 0 | loss_scale 2.000 | wall 119
epoch time  18.685540914535522
:::MLLOG {"namespace": "", "time_ms": 1592948283956, "event_type": "INTERVAL_END", "key": "epoch_stop", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 273, "epoch_num": 5}}
:::MLLOG {"namespace": "", "time_ms": 1592948283957, "event_type": "INTERVAL_START", "key": "eval_start", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 640, "epoch_num": 5}}
self.dataset.src_sizes 3003
self.dataset.tgt_sizes 3003
generated 251 batches in 0.000900s
| Translated 48 sentences (1370 tokens) in 0.7s (68.26 sentences/s, 1948.30 tokens/s)
| Generate test with beam=4: bleu_score=25.2576
| Eval completed in: 1.42s
:::MLLOG {"namespace": "", "time_ms": 1592948285375, "event_type": "INTERVAL_END", "key": "eval_stop", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 751, "epoch_num": 5}}
:::MLLOG {"namespace": "", "time_ms": 1592948285377, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.25257617235183716, "metadata": {"file": "/workspace/translation/train.py", "lineno": 280, "epoch_num": 5}}
validation and scoring  1.421091079711914
:::MLLOG {"namespace": "", "time_ms": 1592948285377, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 295, "first_epoch_num": 5}}
:::MLLOG {"namespace": "", "time_ms": 1592948285378, "event_type": "INTERVAL_END", "key": "run_stop", "value": null, "metadata": {"file": "/workspace/translation/train.py", "lineno": 300, "status": "success"}}
| done training in 103.4 seconds
++ ret_code=0
++ sleep 3
++ ret_code=0
++ sleep 3
++ ret_code=0
++ sleep 3
++ ret_code=0
++ sleep 3
++ ret_code=0
++ sleep 3
++ ret_code=0
++ sleep 3
++ ret_code=0
++ sleep 3
++ ret_code=0
++ sleep 3
++ ret_code=0
++ sleep 3
++ ret_code=0
++ sleep 3
++ ret_code=0
++ sleep 3
++ ret_code=0
++ sleep 3
++ ret_code=0
++ sleep 3
++ ret_code=0
++ sleep 3
++ ret_code=0
++ ret_code=0
++ sleep 3
++ sleep 3
++ ret_code=0
++ sleep 3
++ ret_code=0
++ sleep 3
++ ret_code=0
++ sleep 3
++ ret_code=0
++ sleep 3
++ ret_code=0
++ ret_code=0
++ sleep 3
++ sleep 3
++ ret_code=0
++ sleep 3
++ ret_code=0
++ ret_code=0
++ sleep 3
++ sleep 3
++ ret_code=0
++ sleep 3
++ ret_code=0
++ ret_code=0
++ sleep 3
++ sleep 3
++ ret_code=0
++ sleep 3
++ ret_code=0
++ sleep 3
++ ret_code=0
++ sleep 3
++ ret_code=0
++ sleep 3
++ ret_code=0
++ ret_code=0
++ sleep 3
++ ret_code=0
++ sleep 3
++ sleep 3
++ ret_code=0
++ ret_code=0
++ sleep 3
++ sleep 3
++ ret_code=0
++ sleep 3
++ ret_code=0
++ ret_code=0
++ sleep 3
++ sleep 3
++ ret_code=0
++ ret_code=0
++ sleep 3
++ sleep 3
++ ret_code=0
++ sleep 3
++ ret_code=0
++ sleep 3
++ ret_code=0
++ sleep 3
++ ret_code=0
++ sleep 3
++ ret_code=0
++ ret_code=0
++ sleep 3
++ ret_code=0
++ sleep 3
++ sleep 3
++ ret_code=0
++ sleep 3
++ ret_code=0
++ sleep 3
++ ret_code=0
++ sleep 3
++ ret_code=0
++ sleep 3
++ ret_code=0
++ sleep 3
++ ret_code=0
++ sleep 3
++ ret_code=0
++ sleep 3
++ ret_code=0
++ sleep 3
++ ret_code=0
++ sleep 3
++ ret_code=0
++ sleep 3
++ ret_code=0
++ sleep 3
++ ret_code=0
++ ret_code=0
++ sleep 3
++ sleep 3
++ ret_code=0
++ ret_code=0
++ ret_code=0
++ ret_code=0
++ sleep 3
++ ret_code=0
++ sleep 3
++ sleep 3
++ sleep 3
++ sleep 3
++ ret_code=0
++ sleep 3
++ ret_code=0
++ sleep 3
++ ret_code=0
++ sleep 3
++ ret_code=0
++ sleep 3
++ ret_code=0
++ sleep 3
++ ret_code=0
++ sleep 3
++ ret_code=0
++ sleep 3
++ ret_code=0
++ sleep 3
++ ret_code=0
++ sleep 3
++ ret_code=0
++ sleep 3
++ ret_code=0
++ sleep 3
++ ret_code=0
++ sleep 3
++ ret_code=0
++ sleep 3
++ [[ 0 != 0 ]]
++ [[ 0 != 0 ]]
+++ date +%s
+++ date +%s
++ END=1592948292
+++ date '+%Y-%m-%d %r'
++ END=1592948292
+++ date '+%Y-%m-%d %r'
++ END_FMT='2020-06-23 02:38:12 PM'
ENDING TIMING RUN AT 2020-06-23 02:38:12 PM
++ echo 'ENDING TIMING RUN AT 2020-06-23 02:38:12 PM'
++ RESULT=141
++ RESULT_NAME=transformer
RESULT,transformer,1857,141,root,2020-06-23 02:35:51 PM
++ echo 'RESULT,transformer,1857,141,root,2020-06-23 02:35:51 PM'
+ set +x
++ END_FMT='2020-06-23 02:38:12 PM'
ENDING TIMING RUN AT 2020-06-23 02:38:12 PM
++ echo 'ENDING TIMING RUN AT 2020-06-23 02:38:12 PM'
++ RESULT=141
++ RESULT_NAME=transformer
RESULT,transformer,1857,141,root,2020-06-23 02:35:51 PM
++ echo 'RESULT,transformer,1857,141,root,2020-06-23 02:35:51 PM'
+ set +x
++ [[ 0 != 0 ]]
++ [[ 0 != 0 ]]
+++ date +%s
+++ date +%s
++ END=1592948292
++ END=1592948292
+++ date '+%Y-%m-%d %r'
+++ date '+%Y-%m-%d %r'
++ END_FMT='2020-06-23 02:38:12 PM'
ENDING TIMING RUN AT 2020-06-23 02:38:12 PM
++ echo 'ENDING TIMING RUN AT 2020-06-23 02:38:12 PM'
++ RESULT=141
++ RESULT_NAME=transformer
RESULT,transformer,1857,141,root,2020-06-23 02:35:51 PM
++ echo 'RESULT,transformer,1857,141,root,2020-06-23 02:35:51 PM'
++ END_FMT='2020-06-23 02:38:12 PM'
+ set +x
++ echo 'ENDING TIMING RUN AT 2020-06-23 02:38:12 PM'
ENDING TIMING RUN AT 2020-06-23 02:38:12 PM
++ RESULT=141
++ RESULT_NAME=transformer
RESULT,transformer,1857,141,root,2020-06-23 02:35:51 PM
++ echo 'RESULT,transformer,1857,141,root,2020-06-23 02:35:51 PM'
+ set +x
++ [[ 0 != 0 ]]
++ [[ 0 != 0 ]]
+++ date +%s
+++ date +%s
++ END=1592948292
+++ date '+%Y-%m-%d %r'
++ END=1592948292
+++ date '+%Y-%m-%d %r'
++ END_FMT='2020-06-23 02:38:12 PM'
++ echo 'ENDING TIMING RUN AT 2020-06-23 02:38:12 PM'
ENDING TIMING RUN AT 2020-06-23 02:38:12 PM
++ RESULT=141
++ RESULT_NAME=transformer
RESULT,transformer,1857,141,root,2020-06-23 02:35:51 PM
++ echo 'RESULT,transformer,1857,141,root,2020-06-23 02:35:51 PM'
+ set +x
++ END_FMT='2020-06-23 02:38:12 PM'
++ echo 'ENDING TIMING RUN AT 2020-06-23 02:38:12 PM'
ENDING TIMING RUN AT 2020-06-23 02:38:12 PM
++ RESULT=141
++ RESULT_NAME=transformer
RESULT,transformer,1857,141,root,2020-06-23 02:35:51 PM
++ echo 'RESULT,transformer,1857,141,root,2020-06-23 02:35:51 PM'
+ set +x
++ [[ 0 != 0 ]]
++ [[ 0 != 0 ]]
+++ date +%s
+++ date +%s
++ END=1592948292
++ END=1592948292
+++ date '+%Y-%m-%d %r'
+++ date '+%Y-%m-%d %r'
++ END_FMT='2020-06-23 02:38:12 PM'
++ echo 'ENDING TIMING RUN AT 2020-06-23 02:38:12 PM'
ENDING TIMING RUN AT 2020-06-23 02:38:12 PM
++ RESULT=141
++ END_FMT='2020-06-23 02:38:12 PM'
++ RESULT_NAME=transformer
RESULT,transformer,1857,141,root,2020-06-23 02:35:51 PM
++ echo 'RESULT,transformer,1857,141,root,2020-06-23 02:35:51 PM'
++ echo 'ENDING TIMING RUN AT 2020-06-23 02:38:12 PM'
ENDING TIMING RUN AT 2020-06-23 02:38:12 PM
+ set +x
++ RESULT=141
++ RESULT_NAME=transformer
RESULT,transformer,1857,141,root,2020-06-23 02:35:51 PM
++ echo 'RESULT,transformer,1857,141,root,2020-06-23 02:35:51 PM'
+ set +x
++ [[ 0 != 0 ]]
++ [[ 0 != 0 ]]
+++ date +%s
+++ date +%s
++ END=1592948292
++ END=1592948292
+++ date '+%Y-%m-%d %r'
+++ date '+%Y-%m-%d %r'
++ END_FMT='2020-06-23 02:38:12 PM'
++ echo 'ENDING TIMING RUN AT 2020-06-23 02:38:12 PM'
ENDING TIMING RUN AT 2020-06-23 02:38:12 PM
++ RESULT=141
++ RESULT_NAME=transformer
RESULT,transformer,1857,141,root,2020-06-23 02:35:51 PM
++ echo 'RESULT,transformer,1857,141,root,2020-06-23 02:35:51 PM'
+ set +x
++ END_FMT='2020-06-23 02:38:12 PM'
++ echo 'ENDING TIMING RUN AT 2020-06-23 02:38:12 PM'
ENDING TIMING RUN AT 2020-06-23 02:38:12 PM
++ RESULT=141
++ RESULT_NAME=transformer
RESULT,transformer,1857,141,root,2020-06-23 02:35:51 PM
++ echo 'RESULT,transformer,1857,141,root,2020-06-23 02:35:51 PM'
+ set +x
++ [[ 0 != 0 ]]
++ [[ 0 != 0 ]]
+++ date +%s
+++ date +%s
++ END=1592948292
++ END=1592948292
+++ date '+%Y-%m-%d %r'
+++ date '+%Y-%m-%d %r'
++ END_FMT='2020-06-23 02:38:12 PM'
++ echo 'ENDING TIMING RUN AT 2020-06-23 02:38:12 PM'
ENDING TIMING RUN AT 2020-06-23 02:38:12 PM
++ RESULT=141
++ RESULT_NAME=transformer
RESULT,transformer,1857,141,root,2020-06-23 02:35:51 PM
++ echo 'RESULT,transformer,1857,141,root,2020-06-23 02:35:51 PM'
+ set +x
++ END_FMT='2020-06-23 02:38:12 PM'
++ echo 'ENDING TIMING RUN AT 2020-06-23 02:38:12 PM'
ENDING TIMING RUN AT 2020-06-23 02:38:12 PM
++ RESULT=141
++ RESULT_NAME=transformer
RESULT,transformer,1857,141,root,2020-06-23 02:35:51 PM
++ echo 'RESULT,transformer,1857,141,root,2020-06-23 02:35:51 PM'
+ set +x
++ [[ 0 != 0 ]]
++ [[ 0 != 0 ]]
+++ date +%s
+++ date +%s
++ END=1592948292
++ END=1592948292
+++ date '+%Y-%m-%d %r'
+++ date '+%Y-%m-%d %r'
++ END_FMT='2020-06-23 02:38:12 PM'
++ echo 'ENDING TIMING RUN AT 2020-06-23 02:38:12 PM'
ENDING TIMING RUN AT 2020-06-23 02:38:12 PM
++ RESULT=141
++ RESULT_NAME=transformer
++ echo 'RESULT,transformer,1857,141,root,2020-06-23 02:35:51 PM'
RESULT,transformer,1857,141,root,2020-06-23 02:35:51 PM
++ END_FMT='2020-06-23 02:38:12 PM'
+ set +x
++ echo 'ENDING TIMING RUN AT 2020-06-23 02:38:12 PM'
ENDING TIMING RUN AT 2020-06-23 02:38:12 PM
++ RESULT=141
++ RESULT_NAME=transformer
RESULT,transformer,1857,141,root,2020-06-23 02:35:51 PM
++ echo 'RESULT,transformer,1857,141,root,2020-06-23 02:35:51 PM'
+ set +x
++ [[ 0 != 0 ]]
+++ date +%s
++ END=1592948292
+++ date '+%Y-%m-%d %r'
++ END_FMT='2020-06-23 02:38:12 PM'
ENDING TIMING RUN AT 2020-06-23 02:38:12 PM
++ echo 'ENDING TIMING RUN AT 2020-06-23 02:38:12 PM'
++ RESULT=141
++ RESULT_NAME=transformer
RESULT,transformer,1857,141,root,2020-06-23 02:35:51 PM
++ echo 'RESULT,transformer,1857,141,root,2020-06-23 02:35:51 PM'
+ set +x
++ [[ 0 != 0 ]]
+++ date +%s
++ END=1592948292
+++ date '+%Y-%m-%d %r'
++ END_FMT='2020-06-23 02:38:12 PM'
++ echo 'ENDING TIMING RUN AT 2020-06-23 02:38:12 PM'
ENDING TIMING RUN AT 2020-06-23 02:38:12 PM
++ RESULT=141
++ RESULT_NAME=transformer
RESULT,transformer,1857,141,root,2020-06-23 02:35:51 PM
++ echo 'RESULT,transformer,1857,141,root,2020-06-23 02:35:51 PM'
+ set +x
++ [[ 0 != 0 ]]
++ [[ 0 != 0 ]]
+++ date +%s
+++ date +%s
++ END=1592948292
+++ date '+%Y-%m-%d %r'
++ END=1592948292
+++ date '+%Y-%m-%d %r'
++ END_FMT='2020-06-23 02:38:12 PM'
++ echo 'ENDING TIMING RUN AT 2020-06-23 02:38:12 PM'
ENDING TIMING RUN AT 2020-06-23 02:38:12 PM
++ RESULT=141
++ RESULT_NAME=transformer
RESULT,transformer,1857,141,root,2020-06-23 02:35:52 PM
++ echo 'RESULT,transformer,1857,141,root,2020-06-23 02:35:52 PM'
+ set +x
++ END_FMT='2020-06-23 02:38:12 PM'
ENDING TIMING RUN AT 2020-06-23 02:38:12 PM
++ [[ 0 != 0 ]]
++ echo 'ENDING TIMING RUN AT 2020-06-23 02:38:12 PM'
++ RESULT=141
++ RESULT_NAME=transformer
RESULT,transformer,1857,141,root,2020-06-23 02:35:52 PM
++ echo 'RESULT,transformer,1857,141,root,2020-06-23 02:35:52 PM'
+ set +x
++ [[ 0 != 0 ]]
+++ date +%s
+++ date +%s
++ END=1592948292
++ END=1592948292
+++ date '+%Y-%m-%d %r'
+++ date '+%Y-%m-%d %r'
++ END_FMT='2020-06-23 02:38:12 PM'
++ echo 'ENDING TIMING RUN AT 2020-06-23 02:38:12 PM'
ENDING TIMING RUN AT 2020-06-23 02:38:12 PM
++ RESULT=141
++ RESULT_NAME=transformer
RESULT,transformer,1857,141,root,2020-06-23 02:35:51 PM
++ echo 'RESULT,transformer,1857,141,root,2020-06-23 02:35:51 PM'
++ END_FMT='2020-06-23 02:38:12 PM'
+ set +x
++ echo 'ENDING TIMING RUN AT 2020-06-23 02:38:12 PM'
ENDING TIMING RUN AT 2020-06-23 02:38:12 PM
++ RESULT=141
++ RESULT_NAME=transformer
++ echo 'RESULT,transformer,1857,141,root,2020-06-23 02:35:51 PM'
RESULT,transformer,1857,141,root,2020-06-23 02:35:51 PM
+ set +x
++ [[ 0 != 0 ]]
+++ date +%s
++ [[ 0 != 0 ]]
++ [[ 0 != 0 ]]
++ [[ 0 != 0 ]]
+++ date +%s
++ [[ 0 != 0 ]]
+++ date +%s
+++ date +%s
+++ date +%s
++ END=1592948293
+++ date '+%Y-%m-%d %r'
++ END=1592948293
++ END=1592948293
++ END=1592948293
+++ date '+%Y-%m-%d %r'
+++ date '+%Y-%m-%d %r'
+++ date '+%Y-%m-%d %r'
++ END=1592948293
+++ date '+%Y-%m-%d %r'
++ END_FMT='2020-06-23 02:38:13 PM'
ENDING TIMING RUN AT 2020-06-23 02:38:13 PM
++ echo 'ENDING TIMING RUN AT 2020-06-23 02:38:13 PM'
++ RESULT=142
++ RESULT_NAME=transformer
RESULT,transformer,1857,142,root,2020-06-23 02:35:51 PM
++ echo 'RESULT,transformer,1857,142,root,2020-06-23 02:35:51 PM'
+ set +x
++ END_FMT='2020-06-23 02:38:13 PM'
++ END_FMT='2020-06-23 02:38:13 PM'
ENDING TIMING RUN AT 2020-06-23 02:38:13 PM
++ echo 'ENDING TIMING RUN AT 2020-06-23 02:38:13 PM'
ENDING TIMING RUN AT 2020-06-23 02:38:13 PM
++ echo 'ENDING TIMING RUN AT 2020-06-23 02:38:13 PM'
++ RESULT=142
++ RESULT_NAME=transformer
++ RESULT=142
++ RESULT_NAME=transformer
++ END_FMT='2020-06-23 02:38:13 PM'
RESULT,transformer,1857,142,root,2020-06-23 02:35:51 PM
++ echo 'RESULT,transformer,1857,142,root,2020-06-23 02:35:51 PM'
RESULT,transformer,1857,142,root,2020-06-23 02:35:51 PM
++ echo 'RESULT,transformer,1857,142,root,2020-06-23 02:35:51 PM'
+ set +x
ENDING TIMING RUN AT 2020-06-23 02:38:13 PM
++ echo 'ENDING TIMING RUN AT 2020-06-23 02:38:13 PM'
+ set +x
++ RESULT=142
++ RESULT_NAME=transformer
RESULT,transformer,1857,142,root,2020-06-23 02:35:51 PM
++ echo 'RESULT,transformer,1857,142,root,2020-06-23 02:35:51 PM'
+ set +x
++ END_FMT='2020-06-23 02:38:13 PM'
++ echo 'ENDING TIMING RUN AT 2020-06-23 02:38:13 PM'
ENDING TIMING RUN AT 2020-06-23 02:38:13 PM
++ RESULT=142
++ RESULT_NAME=transformer
RESULT,transformer,1857,142,root,2020-06-23 02:35:51 PM
++ echo 'RESULT,transformer,1857,142,root,2020-06-23 02:35:51 PM'
+ set +x
++ [[ 0 != 0 ]]
+++ date +%s
++ [[ 0 != 0 ]]
++ [[ 0 != 0 ]]
+++ date +%s
++ [[ 0 != 0 ]]
++ [[ 0 != 0 ]]
+++ date +%s
+++ date +%s
+++ date +%s
++ END=1592948293
+++ date '+%Y-%m-%d %r'
++ END=1592948293
++ END=1592948293
+++ date '+%Y-%m-%d %r'
+++ date '+%Y-%m-%d %r'
++ END=1592948293
+++ date '+%Y-%m-%d %r'
++ END=1592948293
+++ date '+%Y-%m-%d %r'
++ END_FMT='2020-06-23 02:38:13 PM'
ENDING TIMING RUN AT 2020-06-23 02:38:13 PM
++ echo 'ENDING TIMING RUN AT 2020-06-23 02:38:13 PM'
++ RESULT=142
++ RESULT_NAME=transformer
RESULT,transformer,1857,142,root,2020-06-23 02:35:51 PM
++ echo 'RESULT,transformer,1857,142,root,2020-06-23 02:35:51 PM'
+ set +x
++ END_FMT='2020-06-23 02:38:13 PM'
++ echo 'ENDING TIMING RUN AT 2020-06-23 02:38:13 PM'
ENDING TIMING RUN AT 2020-06-23 02:38:13 PM
++ RESULT=142
++ RESULT_NAME=transformer
RESULT,transformer,1857,142,root,2020-06-23 02:35:51 PM
++ echo 'RESULT,transformer,1857,142,root,2020-06-23 02:35:51 PM'
+ set +x
++ END_FMT='2020-06-23 02:38:13 PM'
++ echo 'ENDING TIMING RUN AT 2020-06-23 02:38:13 PM'
ENDING TIMING RUN AT 2020-06-23 02:38:13 PM
++ RESULT=142
++ RESULT_NAME=transformer
RESULT,transformer,1857,142,root,2020-06-23 02:35:51 PM
++ echo 'RESULT,transformer,1857,142,root,2020-06-23 02:35:51 PM'
+ set +x
++ END_FMT='2020-06-23 02:38:13 PM'
++ echo 'ENDING TIMING RUN AT 2020-06-23 02:38:13 PM'
ENDING TIMING RUN AT 2020-06-23 02:38:13 PM
++ RESULT=142
++ RESULT_NAME=transformer
RESULT,transformer,1857,142,root,2020-06-23 02:35:51 PM
++ echo 'RESULT,transformer,1857,142,root,2020-06-23 02:35:51 PM'
+ set +x
++ END_FMT='2020-06-23 02:38:13 PM'
++ echo 'ENDING TIMING RUN AT 2020-06-23 02:38:13 PM'
ENDING TIMING RUN AT 2020-06-23 02:38:13 PM
++ RESULT=142
++ RESULT_NAME=transformer
RESULT,transformer,1857,142,root,2020-06-23 02:35:51 PM
++ echo 'RESULT,transformer,1857,142,root,2020-06-23 02:35:51 PM'
+ set +x
++ [[ 0 != 0 ]]
++ [[ 0 != 0 ]]
++ [[ 0 != 0 ]]
++ [[ 0 != 0 ]]
++ [[ 0 != 0 ]]
+++ date +%s
+++ date +%s
+++ date +%s
+++ date +%s
+++ date +%s
++ END=1592948293
++ END=1592948293
++ END=1592948293
++ END=1592948293
+++ date '+%Y-%m-%d %r'
+++ date '+%Y-%m-%d %r'
+++ date '+%Y-%m-%d %r'
+++ date '+%Y-%m-%d %r'
++ END=1592948293
+++ date '+%Y-%m-%d %r'
++ END_FMT='2020-06-23 02:38:13 PM'
++ echo 'ENDING TIMING RUN AT 2020-06-23 02:38:13 PM'
ENDING TIMING RUN AT 2020-06-23 02:38:13 PM
++ RESULT=142
++ RESULT_NAME=transformer
RESULT,transformer,1857,142,root,2020-06-23 02:35:51 PM
++ echo 'RESULT,transformer,1857,142,root,2020-06-23 02:35:51 PM'
+ set +x
++ END_FMT='2020-06-23 02:38:13 PM'
++ echo 'ENDING TIMING RUN AT 2020-06-23 02:38:13 PM'
++ END_FMT='2020-06-23 02:38:13 PM'
ENDING TIMING RUN AT 2020-06-23 02:38:13 PM
++ RESULT=142
++ RESULT_NAME=transformer
++ echo 'ENDING TIMING RUN AT 2020-06-23 02:38:13 PM'
RESULT,transformer,1857,142,root,2020-06-23 02:35:51 PM
++ echo 'RESULT,transformer,1857,142,root,2020-06-23 02:35:51 PM'
ENDING TIMING RUN AT 2020-06-23 02:38:13 PM
+ set +x
++ RESULT=142
++ RESULT_NAME=transformer
RESULT,transformer,1857,142,root,2020-06-23 02:35:51 PM
++ echo 'RESULT,transformer,1857,142,root,2020-06-23 02:35:51 PM'
+ set +x
++ END_FMT='2020-06-23 02:38:13 PM'
++ echo 'ENDING TIMING RUN AT 2020-06-23 02:38:13 PM'
ENDING TIMING RUN AT 2020-06-23 02:38:13 PM
++ RESULT=142
++ RESULT_NAME=transformer
RESULT,transformer,1857,142,root,2020-06-23 02:35:51 PM
++ echo 'RESULT,transformer,1857,142,root,2020-06-23 02:35:51 PM'
+ set +x
++ END_FMT='2020-06-23 02:38:13 PM'
ENDING TIMING RUN AT 2020-06-23 02:38:13 PM
++ echo 'ENDING TIMING RUN AT 2020-06-23 02:38:13 PM'
++ RESULT=142
++ RESULT_NAME=transformer
RESULT,transformer,1857,142,root,2020-06-23 02:35:51 PM
++ echo 'RESULT,transformer,1857,142,root,2020-06-23 02:35:51 PM'
+ set +x
++ [[ 0 != 0 ]]
++ [[ 0 != 0 ]]
+++ date +%s
++ [[ 0 != 0 ]]
++ [[ 0 != 0 ]]
+++ date +%s
+++ date +%s
+++ date +%s
++ [[ 0 != 0 ]]
+++ date +%s
++ END=1592948293
++ END=1592948293
++ END=1592948293
++ END=1592948293
+++ date '+%Y-%m-%d %r'
+++ date '+%Y-%m-%d %r'
+++ date '+%Y-%m-%d %r'
+++ date '+%Y-%m-%d %r'
++ END=1592948293
++ END_FMT='2020-06-23 02:38:13 PM'
ENDING TIMING RUN AT 2020-06-23 02:38:13 PM
++ echo 'ENDING TIMING RUN AT 2020-06-23 02:38:13 PM'
++ RESULT=142
++ RESULT_NAME=transformer
RESULT,transformer,1857,142,root,2020-06-23 02:35:51 PM
++ echo 'RESULT,transformer,1857,142,root,2020-06-23 02:35:51 PM'
+ set +x
+++ date '+%Y-%m-%d %r'
++ END_FMT='2020-06-23 02:38:13 PM'
++ echo 'ENDING TIMING RUN AT 2020-06-23 02:38:13 PM'
ENDING TIMING RUN AT 2020-06-23 02:38:13 PM
++ RESULT=142
++ RESULT_NAME=transformer
++ END_FMT='2020-06-23 02:38:13 PM'
RESULT,transformer,1857,142,root,2020-06-23 02:35:51 PM
++ echo 'RESULT,transformer,1857,142,root,2020-06-23 02:35:51 PM'
+ set +x
++ echo 'ENDING TIMING RUN AT 2020-06-23 02:38:13 PM'
ENDING TIMING RUN AT 2020-06-23 02:38:13 PM
++ RESULT=142
++ RESULT_NAME=transformer
RESULT,transformer,1857,142,root,2020-06-23 02:35:51 PM
++ echo 'RESULT,transformer,1857,142,root,2020-06-23 02:35:51 PM'
+ set +x
++ END_FMT='2020-06-23 02:38:13 PM'
++ echo 'ENDING TIMING RUN AT 2020-06-23 02:38:13 PM'
ENDING TIMING RUN AT 2020-06-23 02:38:13 PM
++ RESULT=142
++ RESULT_NAME=transformer
RESULT,transformer,1857,142,root,2020-06-23 02:35:51 PM
++ echo 'RESULT,transformer,1857,142,root,2020-06-23 02:35:51 PM'
+ set +x
++ END_FMT='2020-06-23 02:38:13 PM'
ENDING TIMING RUN AT 2020-06-23 02:38:13 PM
++ echo 'ENDING TIMING RUN AT 2020-06-23 02:38:13 PM'
++ RESULT=142
++ RESULT_NAME=transformer
RESULT,transformer,1857,142,root,2020-06-23 02:35:51 PM
++ echo 'RESULT,transformer,1857,142,root,2020-06-23 02:35:51 PM'
+ set +x
++ [[ 0 != 0 ]]
+++ date +%s
++ [[ 0 != 0 ]]
+++ date +%s
++ [[ 0 != 0 ]]
+++ date +%s
++ [[ 0 != 0 ]]
++ [[ 0 != 0 ]]
+++ date +%s
++ END=1592948293
+++ date +%s
+++ date '+%Y-%m-%d %r'
++ END=1592948293
+++ date '+%Y-%m-%d %r'
++ END=1592948293
+++ date '+%Y-%m-%d %r'
++ END=1592948293
+++ date '+%Y-%m-%d %r'
++ END_FMT='2020-06-23 02:38:13 PM'
++ echo 'ENDING TIMING RUN AT 2020-06-23 02:38:13 PM'
ENDING TIMING RUN AT 2020-06-23 02:38:13 PM
++ RESULT=142
++ RESULT_NAME=transformer
RESULT,transformer,1857,142,root,2020-06-23 02:35:51 PM
++ echo 'RESULT,transformer,1857,142,root,2020-06-23 02:35:51 PM'
+ set +x
++ END=1592948293
slurmstepd: error: _is_a_lwp: open() /proc/255604/status failed: No such file or directory
++ END_FMT='2020-06-23 02:38:13 PM'
++ echo 'ENDING TIMING RUN AT 2020-06-23 02:38:13 PM'
ENDING TIMING RUN AT 2020-06-23 02:38:13 PM
++ RESULT=142
++ RESULT_NAME=transformer
RESULT,transformer,1857,142,root,2020-06-23 02:35:51 PM
++ echo 'RESULT,transformer,1857,142,root,2020-06-23 02:35:51 PM'
+ set +x
+++ date '+%Y-%m-%d %r'
++ END_FMT='2020-06-23 02:38:13 PM'
++ echo 'ENDING TIMING RUN AT 2020-06-23 02:38:13 PM'
ENDING TIMING RUN AT 2020-06-23 02:38:13 PM
++ RESULT=142
++ RESULT_NAME=transformer
++ END_FMT='2020-06-23 02:38:13 PM'
RESULT,transformer,1857,142,root,2020-06-23 02:35:51 PM
++ echo 'RESULT,transformer,1857,142,root,2020-06-23 02:35:51 PM'
+ set +x
ENDING TIMING RUN AT 2020-06-23 02:38:13 PM
++ echo 'ENDING TIMING RUN AT 2020-06-23 02:38:13 PM'
++ RESULT=142
++ RESULT_NAME=transformer
RESULT,transformer,1857,142,root,2020-06-23 02:35:51 PM
++ echo 'RESULT,transformer,1857,142,root,2020-06-23 02:35:51 PM'
+ set +x
++ END_FMT='2020-06-23 02:38:13 PM'
++ echo 'ENDING TIMING RUN AT 2020-06-23 02:38:13 PM'
ENDING TIMING RUN AT 2020-06-23 02:38:13 PM
++ RESULT=142
++ RESULT_NAME=transformer
RESULT,transformer,1857,142,root,2020-06-23 02:35:51 PM
++ echo 'RESULT,transformer,1857,142,root,2020-06-23 02:35:51 PM'
+ set +x
++ [[ 0 != 0 ]]
++ [[ 0 != 0 ]]
++ [[ 0 != 0 ]]
++ [[ 0 != 0 ]]
++ [[ 0 != 0 ]]
+++ date +%s
+++ date +%s
+++ date +%s
++ [[ 0 != 0 ]]
+++ date +%s
++ [[ 0 != 0 ]]
+++ date +%s
+++ date +%s
+++ date +%s
++ [[ 0 != 0 ]]
++ [[ 0 != 0 ]]
+++ date +%s
+++ date +%s
++ [[ 0 != 0 ]]
+++ date +%s
++ END=1592948293
++ END=1592948293
++ END=1592948293
++ END=1592948293
++ END=1592948293
+++ date '+%Y-%m-%d %r'
+++ date '+%Y-%m-%d %r'
+++ date '+%Y-%m-%d %r'
+++ date '+%Y-%m-%d %r'
+++ date '+%Y-%m-%d %r'
++ END=1592948293
++ END=1592948293
+++ date '+%Y-%m-%d %r'
+++ date '+%Y-%m-%d %r'
++ END=1592948293
++ END=1592948293
+++ date '+%Y-%m-%d %r'
+++ date '+%Y-%m-%d %r'
++ END_FMT='2020-06-23 02:38:13 PM'
++ echo 'ENDING TIMING RUN AT 2020-06-23 02:38:13 PM'
ENDING TIMING RUN AT 2020-06-23 02:38:13 PM
++ RESULT=142
++ RESULT_NAME=transformer
RESULT,transformer,1857,142,root,2020-06-23 02:35:51 PM
++ echo 'RESULT,transformer,1857,142,root,2020-06-23 02:35:51 PM'
+ set +x
++ END=1592948293
++ END_FMT='2020-06-23 02:38:13 PM'
++ echo 'ENDING TIMING RUN AT 2020-06-23 02:38:13 PM'
ENDING TIMING RUN AT 2020-06-23 02:38:13 PM
++ RESULT=142
++ RESULT_NAME=transformer
RESULT,transformer,1857,142,root,2020-06-23 02:35:51 PM
++ echo 'RESULT,transformer,1857,142,root,2020-06-23 02:35:51 PM'
+ set +x
++ END_FMT='2020-06-23 02:38:13 PM'
++ echo 'ENDING TIMING RUN AT 2020-06-23 02:38:13 PM'
ENDING TIMING RUN AT 2020-06-23 02:38:13 PM
++ RESULT=142
++ RESULT_NAME=transformer
RESULT,transformer,1857,142,root,2020-06-23 02:35:51 PM
++ echo 'RESULT,transformer,1857,142,root,2020-06-23 02:35:51 PM'
+ set +x
++ END_FMT='2020-06-23 02:38:13 PM'
++ echo 'ENDING TIMING RUN AT 2020-06-23 02:38:13 PM'
ENDING TIMING RUN AT 2020-06-23 02:38:13 PM
++ RESULT=142
++ RESULT_NAME=transformer
RESULT,transformer,1857,142,root,2020-06-23 02:35:51 PM
++ echo 'RESULT,transformer,1857,142,root,2020-06-23 02:35:51 PM'
+++ date '+%Y-%m-%d %r'
+ set +x
++ END_FMT='2020-06-23 02:38:13 PM'
++ END_FMT='2020-06-23 02:38:13 PM'
++ echo 'ENDING TIMING RUN AT 2020-06-23 02:38:13 PM'
ENDING TIMING RUN AT 2020-06-23 02:38:13 PM
++ echo 'ENDING TIMING RUN AT 2020-06-23 02:38:13 PM'
++ RESULT=142
++ RESULT_NAME=transformer
RESULT,transformer,1857,142,root,2020-06-23 02:35:51 PM
++ echo 'RESULT,transformer,1857,142,root,2020-06-23 02:35:51 PM'
ENDING TIMING RUN AT 2020-06-23 02:38:13 PM
++ RESULT=142
++ RESULT_NAME=transformer
+ set +x
RESULT,transformer,1857,142,root,2020-06-23 02:35:51 PM
++ echo 'RESULT,transformer,1857,142,root,2020-06-23 02:35:51 PM'
+ set +x
++ END_FMT='2020-06-23 02:38:13 PM'
ENDING TIMING RUN AT 2020-06-23 02:38:13 PM
++ echo 'ENDING TIMING RUN AT 2020-06-23 02:38:13 PM'
++ RESULT=142
++ RESULT_NAME=transformer
RESULT,transformer,1857,142,root,2020-06-23 02:35:51 PM
++ echo 'RESULT,transformer,1857,142,root,2020-06-23 02:35:51 PM'
+ set +x
++ END_FMT='2020-06-23 02:38:13 PM'
++ echo 'ENDING TIMING RUN AT 2020-06-23 02:38:13 PM'
ENDING TIMING RUN AT 2020-06-23 02:38:13 PM
++ RESULT=142
++ RESULT_NAME=transformer
RESULT,transformer,1857,142,root,2020-06-23 02:35:51 PM
++ echo 'RESULT,transformer,1857,142,root,2020-06-23 02:35:51 PM'
+ set +x
++ END_FMT='2020-06-23 02:38:13 PM'
++ echo 'ENDING TIMING RUN AT 2020-06-23 02:38:13 PM'
ENDING TIMING RUN AT 2020-06-23 02:38:13 PM
++ RESULT=142
++ RESULT_NAME=transformer
RESULT,transformer,1857,142,root,2020-06-23 02:35:51 PM
++ echo 'RESULT,transformer,1857,142,root,2020-06-23 02:35:51 PM'
+ set +x
++ END_FMT='2020-06-23 02:38:13 PM'
++ echo 'ENDING TIMING RUN AT 2020-06-23 02:38:13 PM'
ENDING TIMING RUN AT 2020-06-23 02:38:13 PM
++ RESULT=142
++ RESULT_NAME=transformer
RESULT,transformer,1857,142,root,2020-06-23 02:35:51 PM
++ echo 'RESULT,transformer,1857,142,root,2020-06-23 02:35:51 PM'
+ set +x
++ [[ 0 != 0 ]]
+++ date +%s
++ END=1592948293
+++ date '+%Y-%m-%d %r'
++ END_FMT='2020-06-23 02:38:13 PM'
++ echo 'ENDING TIMING RUN AT 2020-06-23 02:38:13 PM'
ENDING TIMING RUN AT 2020-06-23 02:38:13 PM
++ RESULT=142
++ RESULT_NAME=transformer
RESULT,transformer,1857,142,root,2020-06-23 02:35:51 PM
++ echo 'RESULT,transformer,1857,142,root,2020-06-23 02:35:51 PM'
+ set +x
++ [[ 0 != 0 ]]
+++ date +%s
++ END=1592948293
+++ date '+%Y-%m-%d %r'
++ END_FMT='2020-06-23 02:38:13 PM'
++ echo 'ENDING TIMING RUN AT 2020-06-23 02:38:13 PM'
ENDING TIMING RUN AT 2020-06-23 02:38:13 PM
++ RESULT=142
++ RESULT_NAME=transformer
++ echo 'RESULT,transformer,1857,142,root,2020-06-23 02:35:51 PM'
RESULT,transformer,1857,142,root,2020-06-23 02:35:51 PM
+ set +x
++ [[ 0 != 0 ]]
++ [[ 0 != 0 ]]
++ [[ 0 != 0 ]]
+++ date +%s
+++ date +%s
++ [[ 0 != 0 ]]
++ [[ 0 != 0 ]]
+++ date +%s
+++ date +%s
+++ date +%s
++ END=1592948293
++ END=1592948293
++ END=1592948293
+++ date '+%Y-%m-%d %r'
++ END=1592948293
++ END=1592948293
+++ date '+%Y-%m-%d %r'
+++ date '+%Y-%m-%d %r'
+++ date '+%Y-%m-%d %r'
+++ date '+%Y-%m-%d %r'
++ END_FMT='2020-06-23 02:38:13 PM'
++ echo 'ENDING TIMING RUN AT 2020-06-23 02:38:13 PM'
ENDING TIMING RUN AT 2020-06-23 02:38:13 PM
++ RESULT=142
++ RESULT_NAME=transformer
RESULT,transformer,1857,142,root,2020-06-23 02:35:51 PM
++ echo 'RESULT,transformer,1857,142,root,2020-06-23 02:35:51 PM'
+ set +x
++ END_FMT='2020-06-23 02:38:13 PM'
++ echo 'ENDING TIMING RUN AT 2020-06-23 02:38:13 PM'
ENDING TIMING RUN AT 2020-06-23 02:38:13 PM
++ RESULT=142
++ RESULT_NAME=transformer
RESULT,transformer,1857,142,root,2020-06-23 02:35:51 PM
++ echo 'RESULT,transformer,1857,142,root,2020-06-23 02:35:51 PM'
+ set +x
++ END_FMT='2020-06-23 02:38:13 PM'
++ echo 'ENDING TIMING RUN AT 2020-06-23 02:38:13 PM'
ENDING TIMING RUN AT 2020-06-23 02:38:13 PM
++ RESULT=142
++ RESULT_NAME=transformer
RESULT,transformer,1857,142,root,2020-06-23 02:35:51 PM
++ echo 'RESULT,transformer,1857,142,root,2020-06-23 02:35:51 PM'
++ END_FMT='2020-06-23 02:38:13 PM'
+ set +x
ENDING TIMING RUN AT 2020-06-23 02:38:13 PM
++ echo 'ENDING TIMING RUN AT 2020-06-23 02:38:13 PM'
++ RESULT=142
++ RESULT_NAME=transformer
RESULT,transformer,1857,142,root,2020-06-23 02:35:51 PM
++ echo 'RESULT,transformer,1857,142,root,2020-06-23 02:35:51 PM'
+ set +x
++ END_FMT='2020-06-23 02:38:13 PM'
++ echo 'ENDING TIMING RUN AT 2020-06-23 02:38:13 PM'
ENDING TIMING RUN AT 2020-06-23 02:38:13 PM
++ RESULT=142
++ RESULT_NAME=transformer
RESULT,transformer,1857,142,root,2020-06-23 02:35:51 PM
++ echo 'RESULT,transformer,1857,142,root,2020-06-23 02:35:51 PM'
+ set +x
++ [[ 0 != 0 ]]
++ [[ 0 != 0 ]]
++ [[ 0 != 0 ]]
++ [[ 0 != 0 ]]
++ [[ 0 != 0 ]]
+++ date +%s
+++ date +%s
+++ date +%s
+++ date +%s
+++ date +%s
++ END=1592948294
++ END=1592948294
+++ date '+%Y-%m-%d %r'
++ END=1592948294
++ END=1592948294
+++ date '+%Y-%m-%d %r'
++ END=1592948294
+++ date '+%Y-%m-%d %r'
+++ date '+%Y-%m-%d %r'
+++ date '+%Y-%m-%d %r'
++ END_FMT='2020-06-23 02:38:14 PM'
++ echo 'ENDING TIMING RUN AT 2020-06-23 02:38:14 PM'
ENDING TIMING RUN AT 2020-06-23 02:38:14 PM
++ RESULT=143
++ RESULT_NAME=transformer
RESULT,transformer,1857,143,root,2020-06-23 02:35:51 PM
++ echo 'RESULT,transformer,1857,143,root,2020-06-23 02:35:51 PM'
+ set +x
++ END_FMT='2020-06-23 02:38:14 PM'
++ echo 'ENDING TIMING RUN AT 2020-06-23 02:38:14 PM'
ENDING TIMING RUN AT 2020-06-23 02:38:14 PM
++ RESULT=143
++ RESULT_NAME=transformer
RESULT,transformer,1857,143,root,2020-06-23 02:35:52 PM
++ echo 'RESULT,transformer,1857,143,root,2020-06-23 02:35:52 PM'
+ set +x
++ END_FMT='2020-06-23 02:38:14 PM'
++ echo 'ENDING TIMING RUN AT 2020-06-23 02:38:14 PM'
ENDING TIMING RUN AT 2020-06-23 02:38:14 PM
++ RESULT=143
++ END_FMT='2020-06-23 02:38:14 PM'
++ RESULT_NAME=transformer
RESULT,transformer,1857,143,root,2020-06-23 02:35:51 PM
++ echo 'RESULT,transformer,1857,143,root,2020-06-23 02:35:51 PM'
++ echo 'ENDING TIMING RUN AT 2020-06-23 02:38:14 PM'
ENDING TIMING RUN AT 2020-06-23 02:38:14 PM
+ set +x
++ RESULT=143
++ RESULT_NAME=transformer
RESULT,transformer,1857,143,root,2020-06-23 02:35:51 PM
++ echo 'RESULT,transformer,1857,143,root,2020-06-23 02:35:51 PM'
++ END_FMT='2020-06-23 02:38:14 PM'
+ set +x
++ echo 'ENDING TIMING RUN AT 2020-06-23 02:38:14 PM'
ENDING TIMING RUN AT 2020-06-23 02:38:14 PM
++ RESULT=143
++ RESULT_NAME=transformer
RESULT,transformer,1857,143,root,2020-06-23 02:35:52 PM
++ echo 'RESULT,transformer,1857,143,root,2020-06-23 02:35:52 PM'
+ set +x
++ [[ 0 != 0 ]]
+++ date +%s
++ END=1592948294
+++ date '+%Y-%m-%d %r'
++ END_FMT='2020-06-23 02:38:14 PM'
++ echo 'ENDING TIMING RUN AT 2020-06-23 02:38:14 PM'
ENDING TIMING RUN AT 2020-06-23 02:38:14 PM
++ RESULT=143
++ RESULT_NAME=transformer
RESULT,transformer,1857,143,root,2020-06-23 02:35:51 PM
++ echo 'RESULT,transformer,1857,143,root,2020-06-23 02:35:51 PM'
+ set +x
++ [[ 0 != 0 ]]
+++ date +%s
++ [[ 0 != 0 ]]
++ [[ 0 != 0 ]]
++ [[ 0 != 0 ]]
+++ date +%s
+++ date +%s
+++ date +%s
++ [[ 0 != 0 ]]
++ END=1592948294
+++ date +%s
+++ date '+%Y-%m-%d %r'
++ END=1592948294
++ END=1592948294
++ END=1592948294
+++ date '+%Y-%m-%d %r'
+++ date '+%Y-%m-%d %r'
+++ date '+%Y-%m-%d %r'
++ END_FMT='2020-06-23 02:38:14 PM'
ENDING TIMING RUN AT 2020-06-23 02:38:14 PM
++ echo 'ENDING TIMING RUN AT 2020-06-23 02:38:14 PM'
++ RESULT=143
++ RESULT_NAME=transformer
RESULT,transformer,1857,143,root,2020-06-23 02:35:51 PM
++ echo 'RESULT,transformer,1857,143,root,2020-06-23 02:35:51 PM'
+ set +x
++ END=1592948294
+++ date '+%Y-%m-%d %r'
++ END_FMT='2020-06-23 02:38:14 PM'
++ echo 'ENDING TIMING RUN AT 2020-06-23 02:38:14 PM'
ENDING TIMING RUN AT 2020-06-23 02:38:14 PM
++ RESULT=143
++ RESULT_NAME=transformer
RESULT,transformer,1857,143,root,2020-06-23 02:35:51 PM
++ echo 'RESULT,transformer,1857,143,root,2020-06-23 02:35:51 PM'
+ set +x
++ END_FMT='2020-06-23 02:38:14 PM'
++ echo 'ENDING TIMING RUN AT 2020-06-23 02:38:14 PM'
ENDING TIMING RUN AT 2020-06-23 02:38:14 PM
++ END_FMT='2020-06-23 02:38:14 PM'
++ RESULT=143
++ RESULT_NAME=transformer
++ echo 'ENDING TIMING RUN AT 2020-06-23 02:38:14 PM'
ENDING TIMING RUN AT 2020-06-23 02:38:14 PM
RESULT,transformer,1857,143,root,2020-06-23 02:35:51 PM
++ echo 'RESULT,transformer,1857,143,root,2020-06-23 02:35:51 PM'
++ RESULT=143
++ RESULT_NAME=transformer
+ set +x
++ echo 'RESULT,transformer,1857,143,root,2020-06-23 02:35:51 PM'
RESULT,transformer,1857,143,root,2020-06-23 02:35:51 PM
+ set +x
++ END_FMT='2020-06-23 02:38:14 PM'
++ echo 'ENDING TIMING RUN AT 2020-06-23 02:38:14 PM'
ENDING TIMING RUN AT 2020-06-23 02:38:14 PM
++ RESULT=143
++ RESULT_NAME=transformer
RESULT,transformer,1857,143,root,2020-06-23 02:35:51 PM
++ echo 'RESULT,transformer,1857,143,root,2020-06-23 02:35:51 PM'
+ set +x
++ [[ 0 != 0 ]]
++ [[ 0 != 0 ]]
+++ date +%s
+++ date +%s
++ END=1592948294
+++ date '+%Y-%m-%d %r'
++ END=1592948294
+++ date '+%Y-%m-%d %r'
++ END_FMT='2020-06-23 02:38:14 PM'
ENDING TIMING RUN AT 2020-06-23 02:38:14 PM
++ echo 'ENDING TIMING RUN AT 2020-06-23 02:38:14 PM'
++ RESULT=143
++ RESULT_NAME=transformer
RESULT,transformer,1857,143,root,2020-06-23 02:35:51 PM
++ echo 'RESULT,transformer,1857,143,root,2020-06-23 02:35:51 PM'
+ set +x
++ END_FMT='2020-06-23 02:38:14 PM'
++ echo 'ENDING TIMING RUN AT 2020-06-23 02:38:14 PM'
ENDING TIMING RUN AT 2020-06-23 02:38:14 PM
++ RESULT=143
++ RESULT_NAME=transformer
RESULT,transformer,1857,143,root,2020-06-23 02:35:51 PM
++ echo 'RESULT,transformer,1857,143,root,2020-06-23 02:35:51 PM'
+ set +x
++ [[ 0 != 0 ]]
+++ date +%s
++ END=1592948294
+++ date '+%Y-%m-%d %r'
++ END_FMT='2020-06-23 02:38:14 PM'
++ echo 'ENDING TIMING RUN AT 2020-06-23 02:38:14 PM'
ENDING TIMING RUN AT 2020-06-23 02:38:14 PM
++ RESULT=143
++ RESULT_NAME=transformer
RESULT,transformer,1857,143,root,2020-06-23 02:35:51 PM
++ echo 'RESULT,transformer,1857,143,root,2020-06-23 02:35:51 PM'
+ set +x
++ [[ 0 != 0 ]]
+++ date +%s
++ END=1592948294
+++ date '+%Y-%m-%d %r'
++ END_FMT='2020-06-23 02:38:14 PM'
++ echo 'ENDING TIMING RUN AT 2020-06-23 02:38:14 PM'
ENDING TIMING RUN AT 2020-06-23 02:38:14 PM
++ RESULT=143
++ RESULT_NAME=transformer
RESULT,transformer,1857,143,root,2020-06-23 02:35:51 PM
++ echo 'RESULT,transformer,1857,143,root,2020-06-23 02:35:51 PM'
+ set +x
++ [[ 0 != 0 ]]
+++ date +%s
++ END=1592948294
+++ date '+%Y-%m-%d %r'
++ END_FMT='2020-06-23 02:38:14 PM'
++ echo 'ENDING TIMING RUN AT 2020-06-23 02:38:14 PM'
ENDING TIMING RUN AT 2020-06-23 02:38:14 PM
++ RESULT=143
++ RESULT_NAME=transformer
RESULT,transformer,1857,143,root,2020-06-23 02:35:51 PM
++ echo 'RESULT,transformer,1857,143,root,2020-06-23 02:35:51 PM'
+ set +x
++ [[ 0 != 0 ]]
+++ date +%s
++ END=1592948294
+++ date '+%Y-%m-%d %r'
++ END_FMT='2020-06-23 02:38:14 PM'
++ echo 'ENDING TIMING RUN AT 2020-06-23 02:38:14 PM'
ENDING TIMING RUN AT 2020-06-23 02:38:14 PM
++ RESULT=143
++ RESULT_NAME=transformer
RESULT,transformer,1857,143,root,2020-06-23 02:35:52 PM
++ echo 'RESULT,transformer,1857,143,root,2020-06-23 02:35:52 PM'
+ set +x
++ [[ 0 != 0 ]]
+++ date +%s
++ END=1592948294
+++ date '+%Y-%m-%d %r'
++ END_FMT='2020-06-23 02:38:14 PM'
++ echo 'ENDING TIMING RUN AT 2020-06-23 02:38:14 PM'
ENDING TIMING RUN AT 2020-06-23 02:38:14 PM
++ RESULT=143
++ RESULT_NAME=transformer
RESULT,transformer,1857,143,root,2020-06-23 02:35:51 PM
++ echo 'RESULT,transformer,1857,143,root,2020-06-23 02:35:51 PM'
+ set +x
