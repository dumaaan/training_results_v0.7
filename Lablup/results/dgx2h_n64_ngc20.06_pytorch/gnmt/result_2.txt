+ echo 'Beginning trial 3 of 5'
Beginning trial 3 of 5
+ srun --ntasks=64 --container-name=rnn_translator python -c '
import mlperf_log_utils
from mlperf_logging.mllog import constants
mlperf_log_utils.mlperf_submission_log(constants.GNMT)'
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
:::MLLOG {"namespace": "", "time_ms": 1592880032002, "event_type": "POINT_IN_TIME", "key": "submission_benchmark", "value": "gnmt", "metadata": {"file": "/workspace/rnn_translator/mlperf_log_utils.py", "lineno": 19}}
:::MLLOG {"namespace": "", "time_ms": 1592880032033, "event_type": "POINT_IN_TIME", "key": "submission_org", "value": "NVIDIA", "metadata": {"file": "/workspace/rnn_translator/mlperf_log_utils.py", "lineno": 24}}
:::MLLOG {"namespace": "", "time_ms": 1592880032033, "event_type": "POINT_IN_TIME", "key": "submission_division", "value": "closed", "metadata": {"file": "/workspace/rnn_translator/mlperf_log_utils.py", "lineno": 28}}
:::MLLOG {"namespace": "", "time_ms": 1592880032033, "event_type": "POINT_IN_TIME", "key": "submission_status", "value": "onprem", "metadata": {"file": "/workspace/rnn_translator/mlperf_log_utils.py", "lineno": 32}}
:::MLLOG {"namespace": "", "time_ms": 1592880032034, "event_type": "POINT_IN_TIME", "key": "submission_platform", "value": "64xNVIDIA DGX-2H", "metadata": {"file": "/workspace/rnn_translator/mlperf_log_utils.py", "lineno": 36}}
+ '[' 1 -eq 1 ']'
+ srun --ntasks=64 bash -c 'echo -n '\''Clearing cache on '\'' && hostname && sync && sudo /sbin/sysctl vm.drop_caches=3'
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
Clearing cache on circe-n005
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
Clearing cache on circe-n039
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
Clearing cache on circe-n017
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
Clearing cache on circe-n027
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
Clearing cache on circe-n013
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
Clearing cache on circe-n041
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
Clearing cache on circe-n033
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
Clearing cache on circe-n020
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
Clearing cache on circe-n032
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
Clearing cache on circe-n098
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
Clearing cache on circe-n034
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
Clearing cache on circe-n016
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
Clearing cache on circe-n014
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
Clearing cache on circe-n030
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
Clearing cache on circe-n050
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
Clearing cache on circe-n038
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
Clearing cache on circe-n048
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
Clearing cache on circe-n022
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
Clearing cache on circe-n025
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
Clearing cache on circe-n003
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
Clearing cache on circe-n049
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
Clearing cache on circe-n057
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
Clearing cache on circe-n037
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
Clearing cache on circe-n031
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
Clearing cache on circe-n029
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
Clearing cache on circe-n009
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
Clearing cache on circe-n004
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
Clearing cache on circe-n018
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
Clearing cache on circe-n044
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
Clearing cache on circe-n028
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
Clearing cache on circe-n006
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
Clearing cache on circe-n056
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
Clearing cache on circe-n010
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
Clearing cache on circe-n008
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
Clearing cache on circe-n054
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
Clearing cache on circe-n096
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
Clearing cache on circe-n040
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
Clearing cache on circe-n026
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
Clearing cache on circe-n021
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
Clearing cache on circe-n095
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
Clearing cache on circe-n007
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
Clearing cache on circe-n047
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
Clearing cache on circe-n045
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
Clearing cache on circe-n036
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
Clearing cache on circe-n043
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
Clearing cache on circe-n015
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
Clearing cache on circe-n097
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
Clearing cache on circe-n051
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
Clearing cache on circe-n059
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
Clearing cache on circe-n023
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
Clearing cache on circe-n019
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
Clearing cache on circe-n055
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
Clearing cache on circe-n011
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
Clearing cache on circe-n046
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
Clearing cache on circe-n012
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
Clearing cache on circe-n042
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
Clearing cache on circe-n058
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
Clearing cache on circe-n052
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
Clearing cache on circe-n053
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
Clearing cache on circe-n035
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
Clearing cache on circe-n060
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
Clearing cache on circe-n024
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
Clearing cache on circe-n002
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
Clearing cache on circe-n001
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
vm.drop_caches = 3
+ srun --ntasks=64 --container-name=rnn_translator python -c '
from mlperf_logging.mllog import constants
from seq2seq.utils import log_event
log_event(key=constants.CACHE_CLEAR, value=True)'
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
:::MLLOG {"namespace": "", "time_ms": 1592880038768, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
:::MLLOG {"namespace": "", "time_ms": 1592880038811, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
:::MLLOG {"namespace": "", "time_ms": 1592880038813, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
:::MLLOG {"namespace": "", "time_ms": 1592880038817, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
:::MLLOG {"namespace": "", "time_ms": 1592880038823, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
:::MLLOG {"namespace": "", "time_ms": 1592880038834, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
:::MLLOG {"namespace": "", "time_ms": 1592880038836, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
:::MLLOG {"namespace": "", "time_ms": 1592880038843, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
:::MLLOG {"namespace": "", "time_ms": 1592880038857, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
:::MLLOG {"namespace": "", "time_ms": 1592880038858, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
:::MLLOG {"namespace": "", "time_ms": 1592880038877, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
:::MLLOG {"namespace": "", "time_ms": 1592880038878, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
:::MLLOG {"namespace": "", "time_ms": 1592880038883, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
:::MLLOG {"namespace": "", "time_ms": 1592880038883, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
:::MLLOG {"namespace": "", "time_ms": 1592880038978, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
:::MLLOG {"namespace": "", "time_ms": 1592880038983, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
:::MLLOG {"namespace": "", "time_ms": 1592880038987, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
:::MLLOG {"namespace": "", "time_ms": 1592880038992, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
:::MLLOG {"namespace": "", "time_ms": 1592880038995, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
:::MLLOG {"namespace": "", "time_ms": 1592880039006, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
:::MLLOG {"namespace": "", "time_ms": 1592880039014, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
:::MLLOG {"namespace": "", "time_ms": 1592880039023, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
:::MLLOG {"namespace": "", "time_ms": 1592880039023, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
:::MLLOG {"namespace": "", "time_ms": 1592880039022, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
:::MLLOG {"namespace": "", "time_ms": 1592880039029, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
:::MLLOG {"namespace": "", "time_ms": 1592880039035, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
:::MLLOG {"namespace": "", "time_ms": 1592880039036, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
:::MLLOG {"namespace": "", "time_ms": 1592880039040, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
:::MLLOG {"namespace": "", "time_ms": 1592880039040, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
:::MLLOG {"namespace": "", "time_ms": 1592880039045, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
:::MLLOG {"namespace": "", "time_ms": 1592880039046, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
:::MLLOG {"namespace": "", "time_ms": 1592880039051, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
:::MLLOG {"namespace": "", "time_ms": 1592880039054, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
:::MLLOG {"namespace": "", "time_ms": 1592880039056, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
:::MLLOG {"namespace": "", "time_ms": 1592880039055, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
:::MLLOG {"namespace": "", "time_ms": 1592880039057, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
:::MLLOG {"namespace": "", "time_ms": 1592880039066, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
:::MLLOG {"namespace": "", "time_ms": 1592880039072, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
:::MLLOG {"namespace": "", "time_ms": 1592880039071, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
:::MLLOG {"namespace": "", "time_ms": 1592880039076, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
:::MLLOG {"namespace": "", "time_ms": 1592880039076, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
:::MLLOG {"namespace": "", "time_ms": 1592880039085, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
:::MLLOG {"namespace": "", "time_ms": 1592880039086, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
:::MLLOG {"namespace": "", "time_ms": 1592880039087, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
:::MLLOG {"namespace": "", "time_ms": 1592880039091, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
:::MLLOG {"namespace": "", "time_ms": 1592880039093, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
:::MLLOG {"namespace": "", "time_ms": 1592880039095, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
:::MLLOG {"namespace": "", "time_ms": 1592880039094, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
:::MLLOG {"namespace": "", "time_ms": 1592880039105, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
:::MLLOG {"namespace": "", "time_ms": 1592880039107, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
:::MLLOG {"namespace": "", "time_ms": 1592880039111, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
:::MLLOG {"namespace": "", "time_ms": 1592880039116, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
:::MLLOG {"namespace": "", "time_ms": 1592880039118, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
:::MLLOG {"namespace": "", "time_ms": 1592880039134, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
:::MLLOG {"namespace": "", "time_ms": 1592880039133, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
:::MLLOG {"namespace": "", "time_ms": 1592880039139, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
:::MLLOG {"namespace": "", "time_ms": 1592880039140, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
:::MLLOG {"namespace": "", "time_ms": 1592880039147, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
:::MLLOG {"namespace": "", "time_ms": 1592880039153, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
:::MLLOG {"namespace": "", "time_ms": 1592880039165, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
:::MLLOG {"namespace": "", "time_ms": 1592880039179, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
:::MLLOG {"namespace": "", "time_ms": 1592880039178, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
:::MLLOG {"namespace": "", "time_ms": 1592880039188, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
:::MLLOG {"namespace": "", "time_ms": 1592880039211, "event_type": "POINT_IN_TIME", "key": "cache_clear", "value": true, "metadata": {"file": "<string>", "lineno": 4}}
+ srun --mpi=none --ntasks=1024 --ntasks-per-node=16 --container-name=rnn_translator --container-mounts=/raid/datasets/wmt16_de_en:/data,/raid/scratch/svcnvdlfw/gnmt:/preproc_data,/gpfs/fs1/svcnvdlfw/14043860/results:/results ./run_and_time.sh
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: pyxis: reusing existing container filesystem
slurmstepd: pyxis: starting container ...
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 0 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 15 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 11 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ '[' -n 3 ']'
+ DATASET_DIR=/data
+ '[' 1024 -gt 64 ']'
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ declare -a CMD
+ echo 'running benchmark'
running benchmark
+ '[' -n 8 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ '[' -n 14 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
running benchmark
+ echo 'running benchmark'
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ '[' -n 4 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
running benchmark
+ echo 'running benchmark'
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ DECAY_INTERVAL=201
+ RESULTS_DIR=gnmt_wmt16
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ LR=5.0e-3
+ NUMEPOCHS=14
+ TRAIN_BATCH_SIZE=16
+ DIST_OPTS=
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 5 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' -n 7 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
running benchmark
running benchmark
+ echo 'running benchmark'
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 13 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 12 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ echo 'running benchmark'
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ '[' -n 1 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
running benchmark
+ echo 'running benchmark'
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ '[' -n 6 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
running benchmark
+ echo 'running benchmark'
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 2 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' -n 10 ']'
+ '[' 1024 -gt 64 ']'
+ '[' -n 9 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
running benchmark
+ echo 'running benchmark'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
running benchmark
running benchmark
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ declare -a CMD
+ DATASET_DIR=/data
+ '[' -n 6 ']'
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ DATASET_DIR=/data
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ NUMEPOCHS=14
+ DIST_OPTS=
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ MATH=fp16
+ declare -a CMD
+ echo 'running benchmark'
running benchmark
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ '[' -n 4 ']'
+ MAX_SEQ_LEN=75
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ NUMEPOCHS=14
+ DIST_OPTS=
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ DATASET_DIR=/data
running benchmark
+ echo 'running benchmark'
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ '[' -n 9 ']'
+ '[' 1024 -gt 64 ']'
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
running benchmark
+ echo 'running benchmark'
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ MATH=fp16
+ declare -a CMD
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ '[' -n 10 ']'
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ declare -a CMD
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ echo 'running benchmark'
+ '[' -n 13 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ echo 'running benchmark'
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ '[' -n 15 ']'
+ '[' 1024 -gt 64 ']'
+ DATASET_DIR=/data
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ WARMUP_STEPS=200
running benchmark
+ echo 'running benchmark'
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ DATASET_DIR=/data
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ '[' -n 2 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ DIST_OPTS=
running benchmark
+ echo 'running benchmark'
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 1 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 8 ']'
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DATASET_DIR=/data
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
running benchmark
+ echo 'running benchmark'
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ '[' -n 0 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ DATASET_DIR=/data
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
running benchmark
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ echo 'running benchmark'
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ DATASET_DIR=/data
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ TARGET=24.0
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ DECAY_INTERVAL=201
+ TARGET=24.0
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
running benchmark
+ DIST_OPTS=
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
running benchmark
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ MATH=fp16
+ declare -a CMD
+ '[' -n 7 ']'
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ '[' -n 11 ']'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ LR=5.0e-3
+ TEST_BATCH_SIZE=4
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ WARMUP_STEPS=200
+ DECAY_INTERVAL=201
+ REMAIN_STEPS=1605
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ DECAY_INTERVAL=201
+ NUMEPOCHS=14
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ DIST_OPTS=
+ NUMEPOCHS=14
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ DIST_OPTS=
+ declare -a CMD
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
running benchmark
+ declare -a CMD
+ '[' -n 14 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
running benchmark
+ '[' -n 12 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 3 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
running benchmark
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 5 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ echo 'running benchmark'
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 10 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ echo 'running benchmark'
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ '[' -n 9 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ DIST_OPTS=
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ declare -a CMD
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ '[' -n 15 ']'
+ declare -a CMD
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' -n 5 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ '[' -n 8 ']'
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
running benchmark
+ echo 'running benchmark'
running benchmark
+ echo 'running benchmark'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 4 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
running benchmark
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 3 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 7 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ echo 'running benchmark'
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ '[' -n 12 ']'
+ DATASET_DIR=/data
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ TEST_BATCH_SIZE=4
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
running benchmark
+ echo 'running benchmark'
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ DATASET_DIR=/data
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ DIST_OPTS=
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ declare -a CMD
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ declare -a CMD
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ '[' -n 14 ']'
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' -n 0 ']'
+ '[' 1024 -gt 64 ']'
+ TEST_BATCH_SIZE=4
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ WARMUP_STEPS=200
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
running benchmark
+ echo 'running benchmark'
+ MATH=fp16
+ declare -a CMD
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
running benchmark
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ '[' -n 13 ']'
+ '[' -n 1 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ TRAIN_BATCH_SIZE=16
+ echo 'running benchmark'
+ TEST_BATCH_SIZE=4
running benchmark
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
running benchmark
+ echo 'running benchmark'
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ declare -a CMD
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ '[' -n 2 ']'
+ DATASET_DIR=/data
+ NUMEPOCHS=14
+ DIST_OPTS=
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ '[' 1024 -gt 64 ']'
+ TEST_BATCH_SIZE=4
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ WARMUP_STEPS=200
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ REMAIN_STEPS=1605
running benchmark
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DECAY_INTERVAL=201
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ TARGET=24.0
running benchmark
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ MAX_SEQ_LEN=75
+ '[' -n 11 ']'
+ '[' 1024 -gt 64 ']'
+ NUMEPOCHS=14
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ echo 'running benchmark'
+ MATH=fp16
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ declare -a CMD
running benchmark
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ '[' -n 2 ']'
+ echo 'running benchmark'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
running benchmark
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ echo 'running benchmark'
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 6 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 11 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 3 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ '[' -n 14 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
running benchmark
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ echo 'running benchmark'
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ LR=5.0e-3
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ TEST_BATCH_SIZE=4
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ MATH=fp16
+ declare -a CMD
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ '[' -n 0 ']'
+ NUMEPOCHS=14
+ DIST_OPTS=
+ '[' 1024 -gt 64 ']'
+ '[' -n 8 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ echo 'running benchmark'
+ '[' -n 4 ']'
+ '[' 1024 -gt 64 ']'
running benchmark
+ echo 'running benchmark'
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
running benchmark
+ echo 'running benchmark'
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 15 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ DATASET_DIR=/data
running benchmark
+ echo 'running benchmark'
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ TEST_BATCH_SIZE=4
+ REMAIN_STEPS=1605
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ DIST_OPTS=
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 1 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' -n 5 ']'
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
running benchmark
+ echo 'running benchmark'
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
running benchmark
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ echo 'running benchmark'
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ TARGET=24.0
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ MATH=fp16
+ declare -a CMD
running benchmark
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ '[' -n 9 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ echo 'running benchmark'
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
running benchmark
+ '[' -n 10 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
running benchmark
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ echo 'running benchmark'
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 7 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 6 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
running benchmark
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DIST_OPTS=
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ declare -a CMD
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ '[' -n 13 ']'
running benchmark
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' -n 12 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
running benchmark
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ declare -a CMD
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ '[' -n 10 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
running benchmark
+ '[' -n 5 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
running benchmark
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 1 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ declare -a CMD
+ '[' -n 3 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
running benchmark
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 12 ']'
+ echo 'running benchmark'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
running benchmark
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ echo 'running benchmark'
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 2 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
running benchmark
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ TEST_BATCH_SIZE=4
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ DECAY_INTERVAL=201
+ TARGET=24.0
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ MAX_SEQ_LEN=75
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ NUMEPOCHS=14
+ DIST_OPTS=
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ '[' -n 2 ']'
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
running benchmark
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ echo 'running benchmark'
+ DATASET_DIR=/data
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ PREPROC_DATADIR=/preproc_data
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ RESULTS_DIR=gnmt_wmt16
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ LR=5.0e-3
+ MAX_SEQ_LEN=75
+ TRAIN_BATCH_SIZE=16
+ NUMEPOCHS=14
+ DIST_OPTS=
+ TEST_BATCH_SIZE=4
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
running benchmark
+ TARGET=24.0
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ MATH=fp16
+ declare -a CMD
+ MAX_SEQ_LEN=75
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ NUMEPOCHS=14
running benchmark
+ DECAY_INTERVAL=201
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ '[' -n 3 ']'
+ DIST_OPTS=
+ TARGET=24.0
+ MAX_SEQ_LEN=75
running benchmark
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ NUMEPOCHS=14
+ DIST_OPTS=
+ MATH=fp16
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ declare -a CMD
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ MATH=fp16
+ declare -a CMD
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ '[' -n 4 ']'
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ '[' -n 1 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ echo 'running benchmark'
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ echo 'running benchmark'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
running benchmark
+ TRAIN_BATCH_SIZE=16
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ DATASET_DIR=/data
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ TARGET=24.0
+ MAX_SEQ_LEN=75
running benchmark
+ NUMEPOCHS=14
+ echo 'running benchmark'
running benchmark
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ MATH=fp16
running benchmark
+ REMAIN_STEPS=1605
+ declare -a CMD
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
running benchmark
+ '[' -n 9 ']'
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ DATASET_DIR=/data
running benchmark
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ MATH=fp16
+ declare -a CMD
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ '[' -n 11 ']'
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ '[' -n 7 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ echo 'running benchmark'
+ '[' -n 11 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' 1024 -gt 64 ']'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ DATASET_DIR=/data
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ NUMEPOCHS=14
+ DIST_OPTS=
+ LR=5.0e-3
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ DATASET_DIR=/data
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
running benchmark
+ echo 'running benchmark'
+ declare -a CMD
+ PREPROC_DATADIR=/preproc_data
+ TRAIN_BATCH_SIZE=16
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ '[' -n 8 ']'
running benchmark
+ TEST_BATCH_SIZE=4
running benchmark
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ DATASET_DIR=/data
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
running benchmark
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ echo 'running benchmark'
+ PREPROC_DATADIR=/preproc_data
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ RESULTS_DIR=gnmt_wmt16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ RESULTS_DIR=gnmt_wmt16
+ echo 'running benchmark'
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ TRAIN_BATCH_SIZE=16
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ DECAY_INTERVAL=201
+ LR=5.0e-3
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ TARGET=24.0
+ TEST_BATCH_SIZE=4
+ MAX_SEQ_LEN=75
running benchmark
+ LR=5.0e-3
+ NUMEPOCHS=14
running benchmark
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DIST_OPTS=
running benchmark
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ MATH=fp16
+ MAX_SEQ_LEN=75
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ declare -a CMD
+ DECAY_INTERVAL=201
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ '[' -n 13 ']'
+ DECAY_INTERVAL=201
+ TARGET=24.0
running benchmark
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ MAX_SEQ_LEN=75
+ echo 'running benchmark'
+ NUMEPOCHS=14
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ DATASET_DIR=/data
+ NUMEPOCHS=14
+ PREPROC_DATADIR=/preproc_data
+ DIST_OPTS=
+ RESULTS_DIR=gnmt_wmt16
+ DIST_OPTS=
+ LR=5.0e-3
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ TRAIN_BATCH_SIZE=16
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ TEST_BATCH_SIZE=4
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ WARMUP_STEPS=200
running benchmark
+ declare -a CMD
+ REMAIN_STEPS=1605
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ DECAY_INTERVAL=201
running benchmark
+ TARGET=24.0
+ MAX_SEQ_LEN=75
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ NUMEPOCHS=14
+ TEST_BATCH_SIZE=4
running benchmark
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ WARMUP_STEPS=200
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ REMAIN_STEPS=1605
running benchmark
+ MATH=fp16
+ DECAY_INTERVAL=201
+ declare -a CMD
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ TARGET=24.0
+ MAX_SEQ_LEN=75
running benchmark
+ NUMEPOCHS=14
running benchmark
+ '[' -n 4 ']'
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ '[' -n 5 ']'
+ '[' -n 0 ']'
+ DIST_OPTS=
running benchmark
+ '[' 1024 -gt 64 ']'
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
running benchmark
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
running benchmark
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ echo 'running benchmark'
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ NUMEPOCHS=14
running benchmark
+ declare -a CMD
+ DATASET_DIR=/data
+ DIST_OPTS=
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ LR=5.0e-3
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ declare -a CMD
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ '[' -n 8 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ MAX_SEQ_LEN=75
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ NUMEPOCHS=14
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
running benchmark
+ DIST_OPTS=
+ echo 'running benchmark'
+ DATASET_DIR=/data
running benchmark
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ declare -a CMD
+ LR=5.0e-3
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ echo 'running benchmark'
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
running benchmark
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ '[' -n 13 ']'
running benchmark
+ '[' -n 3 ']'
+ DECAY_INTERVAL=201
running benchmark
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ TARGET=24.0
running benchmark
+ WARMUP_STEPS=200
running benchmark
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ DIST_OPTS=
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ '[' -n 13 ']'
running benchmark
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
running benchmark
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ echo 'running benchmark'
+ echo 'running benchmark'
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ DATASET_DIR=/data
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ PREPROC_DATADIR=/preproc_data
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ echo 'running benchmark'
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ '[' -n 1 ']'
+ '[' 1024 -gt 64 ']'
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
running benchmark
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
running benchmark
+ echo 'running benchmark'
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
running benchmark
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
running benchmark
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ DECAY_INTERVAL=201
+ TARGET=24.0
running benchmark
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ '[' -n 12 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ '[' -n 6 ']'
+ MATH=fp16
+ declare -a CMD
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ DIST_OPTS=
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
running benchmark
+ MATH=fp16
+ declare -a CMD
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ DATASET_DIR=/data
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ declare -a CMD
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ '[' -n 14 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 2 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ LR=5.0e-3
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ WARMUP_STEPS=200
+ '[' -n 11 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ '[' -n 8 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ DATASET_DIR=/data
+ REMAIN_STEPS=1605
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ echo 'running benchmark'
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ DATASET_DIR=/data
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ '[' -n 7 ']'
+ DIST_OPTS=
running benchmark
+ PREPROC_DATADIR=/preproc_data
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
running benchmark
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
running benchmark
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ MATH=fp16
+ declare -a CMD
running benchmark
+ TRAIN_BATCH_SIZE=16
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
running benchmark
+ declare -a CMD
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ '[' -n 12 ']'
running benchmark
+ TEST_BATCH_SIZE=4
running benchmark
+ echo 'running benchmark'
+ echo 'running benchmark'
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ '[' 1024 -gt 64 ']'
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ '[' -n 10 ']'
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
running benchmark
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
running benchmark
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ declare -a CMD
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ echo 'running benchmark'
+ LR=5.0e-3
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ echo 'running benchmark'
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ '[' -n 9 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
running benchmark
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ MATH=fp16
+ declare -a CMD
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
running benchmark
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ echo 'running benchmark'
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ DATASET_DIR=/data
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ TEST_BATCH_SIZE=4
+ DECAY_INTERVAL=201
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ MATH=fp16
+ declare -a CMD
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ LR=5.0e-3
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ WARMUP_STEPS=200
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ DECAY_INTERVAL=201
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ NUMEPOCHS=14
+ DIST_OPTS=
+ TARGET=24.0
+ LR=5.0e-3
+ DATASET_DIR=/data
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ '[' -n 6 ']'
+ TRAIN_BATCH_SIZE=16
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ '[' -n 15 ']'
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ DIST_OPTS=
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
running benchmark
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ LR=5.0e-3
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ declare -a CMD
+ TRAIN_BATCH_SIZE=16
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ NUMEPOCHS=14
+ DIST_OPTS=
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ MATH=fp16
+ declare -a CMD
+ REMAIN_STEPS=1605
running benchmark
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
running benchmark
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
running benchmark
+ DATASET_DIR=/data
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ DECAY_INTERVAL=201
+ TARGET=24.0
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ '[' -n 6 ']'
+ REMAIN_STEPS=1605
+ declare -a CMD
+ DATASET_DIR=/data
+ DECAY_INTERVAL=201
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ NUMEPOCHS=14
+ DIST_OPTS=
+ echo 'running benchmark'
+ '[' -n 14 ']'
+ '[' 1024 -gt 64 ']'
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ '[' -n 11 ']'
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ declare -a CMD
+ '[' 1024 -gt 64 ']'
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
running benchmark
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
running benchmark
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ '[' -n 3 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ '[' -n 3 ']'
running benchmark
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ WARMUP_STEPS=200
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ REMAIN_STEPS=1605
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
running benchmark
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ echo 'running benchmark'
+ NUMEPOCHS=14
running benchmark
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ DATASET_DIR=/data
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ echo 'running benchmark'
+ MATH=fp16
+ declare -a CMD
+ PREPROC_DATADIR=/preproc_data
+ '[' -n 15 ']'
+ RESULTS_DIR=gnmt_wmt16
+ MATH=fp16
+ declare -a CMD
+ echo 'running benchmark'
+ '[' -n 8 ']'
running benchmark
+ LR=5.0e-3
+ '[' -n 15 ']'
+ '[' 1024 -gt 64 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
running benchmark
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
running benchmark
+ WARMUP_STEPS=200
+ '[' -n 7 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ declare -a CMD
running benchmark
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ '[' -n 0 ']'
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ echo 'running benchmark'
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
running benchmark
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ NUMEPOCHS=14
+ DIST_OPTS=
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ echo 'running benchmark'
+ MATH=fp16
+ declare -a CMD
+ TARGET=24.0
+ MAX_SEQ_LEN=75
running benchmark
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ NUMEPOCHS=14
+ DIST_OPTS=
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ MAX_SEQ_LEN=75
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ NUMEPOCHS=14
+ DIST_OPTS=
running benchmark
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ '[' -n 14 ']'
+ '[' 1024 -gt 64 ']'
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ MATH=fp16
+ declare -a CMD
running benchmark
+ TRAIN_BATCH_SIZE=16
+ '[' -n 4 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ '[' 1024 -gt 64 ']'
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ '[' -n 6 ']'
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
running benchmark
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ echo 'running benchmark'
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
running benchmark
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ MATH=fp16
+ declare -a CMD
+ echo 'running benchmark'
+ DECAY_INTERVAL=201
+ NUMEPOCHS=14
+ DIST_OPTS=
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ TARGET=24.0
running benchmark
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ MATH=fp16
+ declare -a CMD
+ DIST_OPTS=
+ '[' -n 7 ']'
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DATASET_DIR=/data
+ declare -a CMD
+ '[' -n 10 ']'
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ PREPROC_DATADIR=/preproc_data
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ RESULTS_DIR=gnmt_wmt16
+ '[' -n 4 ']'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ LR=5.0e-3
running benchmark
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ TRAIN_BATCH_SIZE=16
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ TEST_BATCH_SIZE=4
+ echo 'running benchmark'
+ WARMUP_STEPS=200
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ REMAIN_STEPS=1605
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ echo 'running benchmark'
+ echo 'running benchmark'
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
running benchmark
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 15 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ echo 'running benchmark'
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ TARGET=24.0
+ MAX_SEQ_LEN=75
running benchmark
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ TARGET=24.0
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ DATASET_DIR=/data
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ declare -a CMD
+ MATH=fp16
+ declare -a CMD
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ LR=5.0e-3
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ '[' -n 5 ']'
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ '[' -n 0 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DIST_OPTS=
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ MAX_SEQ_LEN=75
+ '[' -n 12 ']'
+ '[' 1024 -gt 64 ']'
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ MATH=fp16
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ echo 'running benchmark'
+ DIST_OPTS=
+ '[' -n 13 ']'
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ declare -a CMD
running benchmark
+ MATH=fp16
+ declare -a CMD
+ echo 'running benchmark'
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ '[' -n 2 ']'
running benchmark
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ '[' -n 0 ']'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ DIST_OPTS=
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DATASET_DIR=/data
+ declare -a CMD
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ echo 'running benchmark'
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ '[' -n 9 ']'
+ DECAY_INTERVAL=201
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ '[' 1024 -gt 64 ']'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ echo 'running benchmark'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ NUMEPOCHS=14
+ DIST_OPTS=
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ echo 'running benchmark'
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ echo 'running benchmark'
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ declare -a CMD
+ MATH=fp16
+ declare -a CMD
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ '[' -n 9 ']'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ '[' 1024 -gt 64 ']'
+ '[' -n 14 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ LR=5.0e-3
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ TRAIN_BATCH_SIZE=16
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ WARMUP_STEPS=200
running benchmark
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ TARGET=24.0
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ REMAIN_STEPS=1605
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ DECAY_INTERVAL=201
+ DIST_OPTS=
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ DIST_OPTS=
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ MATH=fp16
+ declare -a CMD
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ '[' -n 2 ']'
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ declare -a CMD
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ '[' 1024 -gt 64 ']'
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ DECAY_INTERVAL=201
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ '[' -n 1 ']'
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
running benchmark
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DIST_OPTS=
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ MATH=fp16
+ declare -a CMD
+ echo 'running benchmark'
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ '[' -n 5 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
running benchmark
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 14 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DATASET_DIR=/data
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
running benchmark
+ DATASET_DIR=/data
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ WARMUP_STEPS=200
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
running benchmark
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ MAX_SEQ_LEN=75
+ echo 'running benchmark'
+ DIST_OPTS=
+ NUMEPOCHS=14
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
running benchmark
+ DIST_OPTS=
+ NUMEPOCHS=14
+ DIST_OPTS=
+ declare -a CMD
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ '[' -n 12 ']'
+ declare -a CMD
+ MATH=fp16
+ declare -a CMD
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' -n 7 ']'
+ '[' -n 10 ']'
+ '[' 1024 -gt 64 ']'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ DATASET_DIR=/data
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ echo 'running benchmark'
+ TARGET=24.0
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ MAX_SEQ_LEN=75
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ NUMEPOCHS=14
+ DIST_OPTS=
+ TEST_BATCH_SIZE=4
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ DECAY_INTERVAL=201
+ TARGET=24.0
running benchmark
+ RESULTS_DIR=gnmt_wmt16
+ MAX_SEQ_LEN=75
+ LR=5.0e-3
+ NUMEPOCHS=14
+ DIST_OPTS=
+ TRAIN_BATCH_SIZE=16
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ TEST_BATCH_SIZE=4
+ declare -a CMD
+ WARMUP_STEPS=200
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ '[' -n 4 ']'
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' -n 1 ']'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ '[' -n 6 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ echo 'running benchmark'
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ echo 'running benchmark'
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ TRAIN_BATCH_SIZE=16
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ TEST_BATCH_SIZE=4
+ REMAIN_STEPS=1605
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ echo 'running benchmark'
+ TARGET=24.0
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ MAX_SEQ_LEN=75
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ NUMEPOCHS=14
+ DIST_OPTS=
+ declare -a CMD
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ DATASET_DIR=/data
+ '[' -n 10 ']'
+ PREPROC_DATADIR=/preproc_data
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ RESULTS_DIR=gnmt_wmt16
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ LR=5.0e-3
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ '[' -n 8 ']'
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ REMAIN_STEPS=1605
+ LR=5.0e-3
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ TRAIN_BATCH_SIZE=16
+ DECAY_INTERVAL=201
+ TEST_BATCH_SIZE=4
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ TARGET=24.0
+ MAX_SEQ_LEN=75
running benchmark
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ echo 'running benchmark'
+ TARGET=24.0
+ NUMEPOCHS=14
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DIST_OPTS=
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
running benchmark
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ MATH=fp16
+ declare -a CMD
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ '[' -n 9 ']'
+ declare -a CMD
running benchmark
+ '[' 1024 -gt 64 ']'
+ TRAIN_BATCH_SIZE=16
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ '[' -n 1 ']'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ REMAIN_STEPS=1605
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ '[' -n 5 ']'
+ echo 'running benchmark'
+ declare -a CMD
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ '[' 1024 -gt 64 ']'
+ echo 'running benchmark'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' -n 9 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ TRAIN_BATCH_SIZE=16
+ '[' -n 0 ']'
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ MATH=fp16
+ declare -a CMD
+ '[' -n 15 ']'
+ DATASET_DIR=/data
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ echo 'running benchmark'
+ MATH=fp16
+ declare -a CMD
+ declare -a CMD
running benchmark
+ '[' -n 15 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ '[' -n 11 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ DATASET_DIR=/data
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ DATASET_DIR=/data
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ DECAY_INTERVAL=201
+ DATASET_DIR=/data
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ declare -a CMD
+ TEST_BATCH_SIZE=4
+ '[' -n 13 ']'
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ '[' -n 13 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ echo 'running benchmark'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ MATH=fp16
+ declare -a CMD
+ echo 'running benchmark'
+ '[' -n 2 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ MATH=fp16
+ declare -a CMD
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ '[' -n 6 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ echo 'running benchmark'
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ '[' -n 14 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ echo 'running benchmark'
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ '[' -n 10 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
running benchmark
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 12 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ DATASET_DIR=/data
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ NUMEPOCHS=14
+ DIST_OPTS=
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ declare -a CMD
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 7 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' -n 4 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
running benchmark
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 8 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ '[' -n 0 ']'
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ '[' -n 11 ']'
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ DATASET_DIR=/data
+ '[' -n 5 ']'
+ '[' 1024 -gt 64 ']'
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 3 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 11 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 9 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ MATH=fp16
+ declare -a CMD
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
running benchmark
+ DIST_OPTS=
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 0 ']'
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ '[' -n 1 ']'
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ REMAIN_STEPS=1605
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ echo 'running benchmark'
+ NUMEPOCHS=14
+ DIST_OPTS=
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ MATH=fp16
+ declare -a CMD
+ echo 'running benchmark'
+ '[' -n 7 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
running benchmark
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
running benchmark
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ '[' -n 2 ']'
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ echo 'running benchmark'
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 10 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 15 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 12 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ '[' -n 2 ']'
+ TARGET=24.0
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ '[' -n 9 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 8 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
running benchmark
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ echo 'running benchmark'
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ DATASET_DIR=/data
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ PREPROC_DATADIR=/preproc_data
+ TRAIN_BATCH_SIZE=16
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ LR=5.0e-3
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ TRAIN_BATCH_SIZE=16
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ REMAIN_STEPS=1605
+ WARMUP_STEPS=200
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ MAX_SEQ_LEN=75
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ NUMEPOCHS=14
+ DIST_OPTS=
+ DIST_OPTS=
running benchmark
+ MAX_SEQ_LEN=75
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
running benchmark
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ NUMEPOCHS=14
+ DIST_OPTS=
running benchmark
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ MATH=fp16
+ declare -a CMD
running benchmark
+ MATH=fp16
+ declare -a CMD
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ MATH=fp16
+ declare -a CMD
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ '[' -n 0 ']'
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ '[' 1024 -gt 64 ']'
+ NUMEPOCHS=14
+ DIST_OPTS=
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ '[' -n 15 ']'
+ MATH=fp16
+ declare -a CMD
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' -n 5 ']'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ '[' -n 13 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ echo 'running benchmark'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
running benchmark
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
running benchmark
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
running benchmark
+ DECAY_INTERVAL=201
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
running benchmark
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ NUMEPOCHS=14
+ declare -a CMD
+ DIST_OPTS=
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ declare -a CMD
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ '[' -n 5 ']'
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ '[' 1024 -gt 64 ']'
+ '[' -n 14 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
running benchmark
+ DIST_OPTS=
+ NUMEPOCHS=14
+ DIST_OPTS=
running benchmark
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ DATASET_DIR=/data
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ echo 'running benchmark'
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ '[' -n 3 ']'
running benchmark
+ '[' -n 6 ']'
+ '[' 1024 -gt 64 ']'
+ LR=5.0e-3
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ TRAIN_BATCH_SIZE=16
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ TEST_BATCH_SIZE=4
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ WARMUP_STEPS=200
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ REMAIN_STEPS=1605
+ echo 'running benchmark'
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ echo 'running benchmark'
+ DIST_OPTS=
running benchmark
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ declare -a CMD
+ DATASET_DIR=/data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ TRAIN_BATCH_SIZE=16
+ LR=5.0e-3
+ TEST_BATCH_SIZE=4
running benchmark
+ TRAIN_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ '[' -n 4 ']'
+ '[' 1024 -gt 64 ']'
+ MAX_SEQ_LEN=75
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 4 ']'
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ '[' 1024 -gt 64 ']'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
running benchmark
+ MATH=fp16
+ declare -a CMD
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ echo 'running benchmark'
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ '[' -n 12 ']'
+ '[' 1024 -gt 64 ']'
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ echo 'running benchmark'
running benchmark
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ '[' -n 3 ']'
+ TRAIN_BATCH_SIZE=16
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ DIST_OPTS=
+ echo 'running benchmark'
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 1 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ '[' -n 11 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ echo 'running benchmark'
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 7 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 8 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ declare -a CMD
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ '[' -n 10 ']'
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' -n 13 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ '[' -n 14 ']'
running benchmark
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ echo 'running benchmark'
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
running benchmark
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ MATH=fp16
+ declare -a CMD
running benchmark
+ '[' -n 15 ']'
running benchmark
+ echo 'running benchmark'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
running benchmark
+ MATH=fp16
+ declare -a CMD
+ '[' -n 0 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 7 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ DATASET_DIR=/data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ LR=5.0e-3
+ DECAY_INTERVAL=201
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ TARGET=24.0
+ WARMUP_STEPS=200
+ MAX_SEQ_LEN=75
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
running benchmark
+ TARGET=24.0
+ MATH=fp16
+ declare -a CMD
+ MAX_SEQ_LEN=75
+ '[' -n 1 ']'
running benchmark
+ NUMEPOCHS=14
+ DIST_OPTS=
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ '[' -n 10 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ echo 'running benchmark'
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ DATASET_DIR=/data
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ LR=5.0e-3
+ DATASET_DIR=/data
+ TRAIN_BATCH_SIZE=16
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ TEST_BATCH_SIZE=4
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ DECAY_INTERVAL=201
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ TARGET=24.0
+ DATASET_DIR=/data
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ TEST_BATCH_SIZE=4
+ NUMEPOCHS=14
+ DIST_OPTS=
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DIST_OPTS=
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MAX_SEQ_LEN=75
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
running benchmark
+ MATH=fp16
+ declare -a CMD
running benchmark
+ declare -a CMD
+ '[' -n 12 ']'
running benchmark
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' -n 14 ']'
+ '[' -n 8 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
running benchmark
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ '[' -n 13 ']'
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ echo 'running benchmark'
+ echo 'running benchmark'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 1 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ echo 'running benchmark'
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ DATASET_DIR=/data
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ DIST_OPTS=
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ declare -a CMD
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ declare -a CMD
+ '[' -n 0 ']'
+ '[' -n 9 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ LR=5.0e-3
running benchmark
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ echo 'running benchmark'
running benchmark
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ DIST_OPTS=
+ echo 'running benchmark'
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
running benchmark
+ declare -a CMD
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ '[' -n 6 ']'
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ echo 'running benchmark'
+ MATH=fp16
+ declare -a CMD
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
running benchmark
+ TRAIN_BATCH_SIZE=16
+ '[' -n 11 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ DATASET_DIR=/data
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
running benchmark
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
running benchmark
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
running benchmark
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ echo 'running benchmark'
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ TARGET=24.0
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ MAX_SEQ_LEN=75
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ NUMEPOCHS=14
+ declare -a CMD
+ DIST_OPTS=
+ '[' -n 2 ']'
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ '[' -n 5 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ MATH=fp16
+ declare -a CMD
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' -n 6 ']'
+ '[' 1024 -gt 64 ']'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
running benchmark
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ DATASET_DIR=/data
+ TEST_BATCH_SIZE=4
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ NUMEPOCHS=14
+ DIST_OPTS=
running benchmark
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ MATH=fp16
+ declare -a CMD
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
running benchmark
+ '[' -n 4 ']'
running benchmark
+ NUMEPOCHS=14
+ DIST_OPTS=
+ '[' 1024 -gt 64 ']'
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
running benchmark
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ MATH=fp16
+ declare -a CMD
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ echo 'running benchmark'
+ declare -a CMD
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ TEST_BATCH_SIZE=4
+ DATASET_DIR=/data
+ WARMUP_STEPS=200
running benchmark
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ '[' -n 4 ']'
+ LR=5.0e-3
+ REMAIN_STEPS=1605
+ TRAIN_BATCH_SIZE=16
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ TEST_BATCH_SIZE=4
+ DECAY_INTERVAL=201
+ TARGET=24.0
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ WARMUP_STEPS=200
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ '[' -n 14 ']'
+ '[' 1024 -gt 64 ']'
+ REMAIN_STEPS=1605
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ DIST_OPTS=
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 1 ']'
+ echo 'running benchmark'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ declare -a CMD
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ echo 'running benchmark'
+ echo 'running benchmark'
+ '[' -n 6 ']'
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
running benchmark
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
running benchmark
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
running benchmark
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ declare -a CMD
+ echo 'running benchmark'
+ '[' -n 3 ']'
+ '[' 1024 -gt 64 ']'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DATASET_DIR=/data
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ echo 'running benchmark'
+ TRAIN_BATCH_SIZE=16
running benchmark
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
running benchmark
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ '[' -n 8 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
running benchmark
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ DATASET_DIR=/data
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ DATASET_DIR=/data
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ PREPROC_DATADIR=/preproc_data
+ TEST_BATCH_SIZE=4
+ '[' -n 15 ']'
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ WARMUP_STEPS=200
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ TRAIN_BATCH_SIZE=16
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
running benchmark
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ TARGET=24.0
+ REMAIN_STEPS=1605
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ declare -a CMD
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ MAX_SEQ_LEN=75
+ '[' -n 3 ']'
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ '[' 1024 -gt 64 ']'
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ NUMEPOCHS=14
+ DIST_OPTS=
running benchmark
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ DATASET_DIR=/data
+ echo 'running benchmark'
running benchmark
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ declare -a CMD
+ echo 'running benchmark'
+ PREPROC_DATADIR=/preproc_data
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ DATASET_DIR=/data
+ DATASET_DIR=/data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
running benchmark
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ echo 'running benchmark'
+ LR=5.0e-3
running benchmark
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
running benchmark
+ TRAIN_BATCH_SIZE=16
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ '[' -n 3 ']'
running benchmark
+ TEST_BATCH_SIZE=4
running benchmark
+ TRAIN_BATCH_SIZE=16
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ TRAIN_BATCH_SIZE=16
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ WARMUP_STEPS=200
+ TEST_BATCH_SIZE=4
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ WARMUP_STEPS=200
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ DECAY_INTERVAL=201
running benchmark
+ TARGET=24.0
running benchmark
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ TARGET=24.0
running benchmark
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ DATASET_DIR=/data
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ DIST_OPTS=
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MAX_SEQ_LEN=75
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ echo 'running benchmark'
running benchmark
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
running benchmark
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ MATH=fp16
+ declare -a CMD
+ NUMEPOCHS=14
running benchmark
+ declare -a CMD
running benchmark
+ DIST_OPTS=
+ '[' -n 2 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ '[' -n 5 ']'
+ LR=5.0e-3
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ TRAIN_BATCH_SIZE=16
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ TEST_BATCH_SIZE=4
+ echo 'running benchmark'
running benchmark
+ declare -a CMD
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ WARMUP_STEPS=200
+ LR=5.0e-3
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ REMAIN_STEPS=1605
+ TRAIN_BATCH_SIZE=16
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TEST_BATCH_SIZE=4
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ DECAY_INTERVAL=201
+ DECAY_INTERVAL=201
running benchmark
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ TARGET=24.0
+ TARGET=24.0
+ DECAY_INTERVAL=201
+ MAX_SEQ_LEN=75
running benchmark
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ TARGET=24.0
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ MAX_SEQ_LEN=75
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ NUMEPOCHS=14
+ DIST_OPTS=
+ DIST_OPTS=
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
running benchmark
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
running benchmark
+ '[' -n 8 ']'
+ '[' 1024 -gt 64 ']'
+ declare -a CMD
running benchmark
+ '[' -n 2 ']'
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ MATH=fp16
+ declare -a CMD
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DATASET_DIR=/data
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ TRAIN_BATCH_SIZE=16
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ DECAY_INTERVAL=201
running benchmark
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ LR=5.0e-3
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ TEST_BATCH_SIZE=4
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ echo 'running benchmark'
running benchmark
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ '[' -n 13 ']'
+ '[' 1024 -gt 64 ']'
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
running benchmark
+ '[' -n 2 ']'
+ NUMEPOCHS=14
+ DIST_OPTS=
running benchmark
+ RESULTS_DIR=gnmt_wmt16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
running benchmark
+ LR=5.0e-3
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ TRAIN_BATCH_SIZE=16
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ TEST_BATCH_SIZE=4
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ WARMUP_STEPS=200
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
running benchmark
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ DECAY_INTERVAL=201
+ NUMEPOCHS=14
+ DIST_OPTS=
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ TARGET=24.0
+ MATH=fp16
+ declare -a CMD
+ MAX_SEQ_LEN=75
+ echo 'running benchmark'
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ NUMEPOCHS=14
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
running benchmark
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ echo 'running benchmark'
running benchmark
+ MATH=fp16
+ declare -a CMD
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ '[' -n 4 ']'
+ '[' 1024 -gt 64 ']'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ declare -a CMD
running benchmark
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
running benchmark
+ '[' -n 7 ']'
+ declare -a CMD
+ '[' -n 12 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ echo 'running benchmark'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' -n 3 ']'
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ '[' -n 10 ']'
running benchmark
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ '[' 1024 -gt 64 ']'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ echo 'running benchmark'
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ echo 'running benchmark'
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ NUMEPOCHS=14
+ DIST_OPTS=
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ declare -a CMD
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ DATASET_DIR=/data
+ '[' -n 12 ']'
+ '[' 1024 -gt 64 ']'
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ LR=5.0e-3
+ '[' -n 10 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ declare -a CMD
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ '[' -n 0 ']'
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ MATH=fp16
+ declare -a CMD
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ '[' -n 11 ']'
+ '[' 1024 -gt 64 ']'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ '[' -n 9 ']'
running benchmark
+ declare -a CMD
+ '[' -n 5 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DATASET_DIR=/data
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
running benchmark
+ LR=5.0e-3
running benchmark
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ '[' -n 4 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ WARMUP_STEPS=200
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ TARGET=24.0
+ echo 'running benchmark'
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ echo 'running benchmark'
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ declare -a CMD
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ '[' -n 11 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ DATASET_DIR=/data
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ RESULTS_DIR=gnmt_wmt16
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ echo 'running benchmark'
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ LR=5.0e-3
+ DIST_OPTS=
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ WARMUP_STEPS=200
+ declare -a CMD
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ '[' -n 13 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ TARGET=24.0
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ NUMEPOCHS=14
running benchmark
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ DIST_OPTS=
+ DIST_OPTS=
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ DATASET_DIR=/data
+ declare -a CMD
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ '[' -n 6 ']'
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ echo 'running benchmark'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ declare -a CMD
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ MATH=fp16
+ TRAIN_BATCH_SIZE=16
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ declare -a CMD
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ '[' -n 10 ']'
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ TARGET=24.0
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ echo 'running benchmark'
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ '[' -n 13 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DIST_OPTS=
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ '[' -n 6 ']'
+ LR=5.0e-3
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ echo 'running benchmark'
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
running benchmark
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ echo 'running benchmark'
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ DATASET_DIR=/data
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ NUMEPOCHS=14
+ DIST_OPTS=
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ DATASET_DIR=/data
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ declare -a CMD
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ TRAIN_BATCH_SIZE=16
+ MATH=fp16
+ declare -a CMD
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ '[' -n 8 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ TRAIN_BATCH_SIZE=16
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ '[' -n 9 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ DIST_OPTS=
+ NUMEPOCHS=14
+ DIST_OPTS=
+ MATH=fp16
+ declare -a CMD
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ declare -a CMD
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ '[' -n 15 ']'
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' -n 7 ']'
+ echo 'running benchmark'
+ '[' -n 5 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
running benchmark
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ echo 'running benchmark'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
running benchmark
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ echo 'running benchmark'
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ '[' -n 1 ']'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DATASET_DIR=/data
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ PREPROC_DATADIR=/preproc_data
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ RESULTS_DIR=gnmt_wmt16
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ LR=5.0e-3
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ TRAIN_BATCH_SIZE=16
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ declare -a CMD
+ DATASET_DIR=/data
+ DECAY_INTERVAL=201
+ echo 'running benchmark'
+ TARGET=24.0
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MAX_SEQ_LEN=75
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ NUMEPOCHS=14
+ MATH=fp16
+ declare -a CMD
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ DIST_OPTS=
+ '[' -n 0 ']'
+ '[' 1024 -gt 64 ']'
+ TEST_BATCH_SIZE=4
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ MATH=fp16
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ declare -a CMD
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ '[' -n 11 ']'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ '[' 1024 -gt 64 ']'
+ '[' -n 14 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DIST_OPTS=
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ declare -a CMD
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ '[' -n 14 ']'
+ echo 'running benchmark'
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ echo 'running benchmark'
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ declare -a CMD
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ '[' -n 9 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ TEST_BATCH_SIZE=4
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ MATH=fp16
+ declare -a CMD
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 7 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ echo 'running benchmark'
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ '[' -n 12 ']'
+ declare -a CMD
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' -n 12 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 15 ']'
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
running benchmark
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ DATASET_DIR=/data
+ echo 'running benchmark'
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 1 ']'
running benchmark
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ LR=5.0e-3
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ echo 'running benchmark'
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ '[' -n 14 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ echo 'running benchmark'
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 13 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 3 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ '[' -n 9 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 7 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 15 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
running benchmark
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 6 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' -n 11 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
running benchmark
+ echo 'running benchmark'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ echo 'running benchmark'
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ '[' -n 2 ']'
+ DIST_OPTS=
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ DATASET_DIR=/data
running benchmark
+ '[' -n 1 ']'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ '[' 1024 -gt 64 ']'
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ echo 'running benchmark'
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ echo 'running benchmark'
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
running benchmark
+ MATH=fp16
+ declare -a CMD
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ '[' -n 4 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ '[' -n 10 ']'
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
running benchmark
+ echo 'running benchmark'
+ '[' -n 0 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
running benchmark
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ LR=5.0e-3
+ DATASET_DIR=/data
+ TRAIN_BATCH_SIZE=16
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ LR=5.0e-3
+ REMAIN_STEPS=1605
+ TRAIN_BATCH_SIZE=16
+ DECAY_INTERVAL=201
+ TEST_BATCH_SIZE=4
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ DECAY_INTERVAL=201
+ MATH=fp16
+ declare -a CMD
+ TARGET=24.0
+ MAX_SEQ_LEN=75
running benchmark
+ NUMEPOCHS=14
+ DIST_OPTS=
+ '[' -n 7 ']'
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
running benchmark
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ declare -a CMD
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ '[' -n 5 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ echo 'running benchmark'
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ echo 'running benchmark'
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
running benchmark
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ DATASET_DIR=/data
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ '[' -n 8 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ WARMUP_STEPS=200
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ echo 'running benchmark'
+ MAX_SEQ_LEN=75
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ NUMEPOCHS=14
running benchmark
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 9 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ echo 'running benchmark'
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
running benchmark
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ '[' -n 13 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ DECAY_INTERVAL=201
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ DIST_OPTS=
running benchmark
+ declare -a CMD
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
running benchmark
+ '[' -n 0 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ MATH=fp16
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ declare -a CMD
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ '[' -n 4 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ echo 'running benchmark'
running benchmark
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 12 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ echo 'running benchmark'
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ DATASET_DIR=/data
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ '[' -n 8 ']'
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ TARGET=24.0
+ MAX_SEQ_LEN=75
running benchmark
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ MATH=fp16
+ declare -a CMD
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ echo 'running benchmark'
+ '[' -n 11 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DATASET_DIR=/data
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ echo 'running benchmark'
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ declare -a CMD
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ '[' -n 2 ']'
+ declare -a CMD
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
running benchmark
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ '[' -n 15 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ echo 'running benchmark'
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
running benchmark
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ '[' -n 0 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ echo 'running benchmark'
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ NUMEPOCHS=14
+ DIST_OPTS=
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ declare -a CMD
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 3 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' -n 10 ']'
+ '[' 1024 -gt 64 ']'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
running benchmark
+ echo 'running benchmark'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
running benchmark
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ declare -a CMD
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ '[' -n 14 ']'
+ '[' 1024 -gt 64 ']'
+ NUMEPOCHS=14
+ DIST_OPTS=
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ echo 'running benchmark'
+ '[' -n 5 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
running benchmark
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ '[' -n 6 ']'
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
running benchmark
+ declare -a CMD
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ '[' -n 14 ']'
+ '[' 1024 -gt 64 ']'
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ declare -a CMD
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ echo 'running benchmark'
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ '[' -n 15 ']'
+ '[' 1024 -gt 64 ']'
+ DECAY_INTERVAL=201
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
running benchmark
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ declare -a CMD
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ '[' -n 10 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
running benchmark
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ DATASET_DIR=/data
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ MATH=fp16
+ declare -a CMD
running benchmark
+ REMAIN_STEPS=1605
+ '[' -n 3 ']'
+ DECAY_INTERVAL=201
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
running benchmark
+ DIST_OPTS=
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 1 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
running benchmark
+ declare -a CMD
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
running benchmark
+ '[' -n 5 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
running benchmark
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ '[' -n 15 ']'
+ '[' 1024 -gt 64 ']'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ declare -a CMD
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' -n 3 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ '[' -n 11 ']'
+ LR=5.0e-3
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ TRAIN_BATCH_SIZE=16
+ DECAY_INTERVAL=201
+ TARGET=24.0
running benchmark
+ TEST_BATCH_SIZE=4
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ WARMUP_STEPS=200
+ DIST_OPTS=
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ echo 'running benchmark'
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ declare -a CMD
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
running benchmark
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ '[' -n 7 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
running benchmark
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ '[' -n 13 ']'
running benchmark
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ echo 'running benchmark'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ '[' -n 4 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ MATH=fp16
+ declare -a CMD
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
running benchmark
+ DECAY_INTERVAL=201
+ '[' -n 8 ']'
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ declare -a CMD
running benchmark
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ '[' -n 2 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ '[' -n 12 ']'
running benchmark
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 6 ']'
+ '[' 1024 -gt 64 ']'
running benchmark
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ DATASET_DIR=/data
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ '[' -n 5 ']'
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ declare -a CMD
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
running benchmark
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ echo 'running benchmark'
+ TEST_BATCH_SIZE=4
+ '[' -n 9 ']'
+ WARMUP_STEPS=200
running benchmark
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ TARGET=24.0
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ echo 'running benchmark'
+ DIST_OPTS=
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
running benchmark
+ MATH=fp16
+ declare -a CMD
+ '[' -n 2 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 10 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ echo 'running benchmark'
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ DECAY_INTERVAL=201
+ declare -a CMD
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ NUMEPOCHS=14
+ DIST_OPTS=
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ '[' -n 6 ']'
+ '[' 1024 -gt 64 ']'
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ MATH=fp16
+ declare -a CMD
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ declare -a CMD
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ echo 'running benchmark'
+ MATH=fp16
+ declare -a CMD
+ '[' -n 1 ']'
+ '[' -n 11 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
running benchmark
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ '[' -n 2 ']'
running benchmark
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
running benchmark
+ echo 'running benchmark'
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ declare -a CMD
+ '[' -n 8 ']'
+ '[' -n 13 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ echo 'running benchmark'
running benchmark
+ echo 'running benchmark'
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ DATASET_DIR=/data
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ DIST_OPTS=
+ LR=5.0e-3
+ DATASET_DIR=/data
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
running benchmark
+ MATH=fp16
+ declare -a CMD
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ declare -a CMD
+ TRAIN_BATCH_SIZE=16
+ LR=5.0e-3
+ '[' -n 8 ']'
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ TEST_BATCH_SIZE=4
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ TRAIN_BATCH_SIZE=16
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
running benchmark
+ WARMUP_STEPS=200
+ TEST_BATCH_SIZE=4
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ REMAIN_STEPS=1605
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ '[' -n 9 ']'
+ echo 'running benchmark'
running benchmark
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ DECAY_INTERVAL=201
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ DIST_OPTS=
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
running benchmark
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ declare -a CMD
+ '[' -n 0 ']'
+ echo 'running benchmark'
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ RESULTS_DIR=gnmt_wmt16
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ LR=5.0e-3
running benchmark
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ '[' -n 15 ']'
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ TRAIN_BATCH_SIZE=16
running benchmark
+ TRAIN_BATCH_SIZE=16
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ '[' 1024 -gt 64 ']'
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ TEST_BATCH_SIZE=4
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
running benchmark
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ WARMUP_STEPS=200
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ echo 'running benchmark'
+ REMAIN_STEPS=1605
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DECAY_INTERVAL=201
+ DATASET_DIR=/data
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ TARGET=24.0
+ MAX_SEQ_LEN=75
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ DIST_OPTS=
running benchmark
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ NUMEPOCHS=14
+ DIST_OPTS=
+ declare -a CMD
+ LR=5.0e-3
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ '[' -n 13 ']'
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ WARMUP_STEPS=200
running benchmark
+ echo 'running benchmark'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
running benchmark
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ TARGET=24.0
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ declare -a CMD
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ '[' -n 3 ']'
running benchmark
+ DIST_OPTS=
+ '[' 1024 -gt 64 ']'
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ MATH=fp16
+ declare -a CMD
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ '[' -n 7 ']'
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ echo 'running benchmark'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ DATASET_DIR=/data
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
running benchmark
+ echo 'running benchmark'
+ WARMUP_STEPS=200
running benchmark
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ MATH=fp16
+ declare -a CMD
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
running benchmark
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
running benchmark
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ '[' -n 7 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
running benchmark
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ MATH=fp16
+ declare -a CMD
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ echo 'running benchmark'
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
running benchmark
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ '[' -n 4 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
running benchmark
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ '[' -n 12 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
running benchmark
+ '[' -n 2 ']'
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ '[' 1024 -gt 64 ']'
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
running benchmark
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ '[' -n 8 ']'
+ '[' 1024 -gt 64 ']'
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ echo 'running benchmark'
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ DATASET_DIR=/data
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ DIST_OPTS=
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
running benchmark
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 9 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
running benchmark
+ '[' -n 10 ']'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ TEST_BATCH_SIZE=4
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ declare -a CMD
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
running benchmark
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ echo 'running benchmark'
+ TEST_BATCH_SIZE=4
+ declare -a CMD
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ DECAY_INTERVAL=201
+ TARGET=24.0
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ '[' -n 14 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' -n 14 ']'
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ '[' -n 7 ']'
+ DIST_OPTS=
+ echo 'running benchmark'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DIST_OPTS=
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ TEST_BATCH_SIZE=4
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MATH=fp16
+ declare -a CMD
+ echo 'running benchmark'
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ declare -a CMD
+ echo 'running benchmark'
running benchmark
+ '[' -n 11 ']'
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ '[' -n 4 ']'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ echo 'running benchmark'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ MATH=fp16
+ declare -a CMD
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
running benchmark
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
running benchmark
+ LR=5.0e-3
+ echo 'running benchmark'
+ TRAIN_BATCH_SIZE=16
running benchmark
+ echo 'running benchmark'
running benchmark
+ '[' -n 12 ']'
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
running benchmark
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ '[' -n 6 ']'
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ DATASET_DIR=/data
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
running benchmark
+ PREPROC_DATADIR=/preproc_data
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
running benchmark
+ declare -a CMD
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ '[' -n 5 ']'
+ DECAY_INTERVAL=201
running benchmark
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
running benchmark
+ TARGET=24.0
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ MAX_SEQ_LEN=75
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ NUMEPOCHS=14
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DIST_OPTS=
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ WARMUP_STEPS=200
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ '[' -n 1 ']'
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ DIST_OPTS=
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ '[' -n 4 ']'
+ RESULTS_DIR=gnmt_wmt16
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
running benchmark
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
running benchmark
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DIST_OPTS=
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ MATH=fp16
running benchmark
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ declare -a CMD
+ MATH=fp16
+ declare -a CMD
+ '[' -n 6 ']'
+ '[' -n 2 ']'
running benchmark
+ '[' 1024 -gt 64 ']'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
running benchmark
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ RESULTS_DIR=gnmt_wmt16
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ LR=5.0e-3
running benchmark
+ RESULTS_DIR=gnmt_wmt16
+ TRAIN_BATCH_SIZE=16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ TARGET=24.0
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ echo 'running benchmark'
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
running benchmark
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ declare -a CMD
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ MATH=fp16
+ declare -a CMD
+ declare -a CMD
+ RESULTS_DIR=gnmt_wmt16
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ echo 'running benchmark'
+ '[' -n 11 ']'
+ TEST_BATCH_SIZE=4
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
running benchmark
+ '[' -n 3 ']'
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ TEST_BATCH_SIZE=4
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ '[' 1024 -gt 64 ']'
+ WARMUP_STEPS=200
+ DATASET_DIR=/data
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ REMAIN_STEPS=1605
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ DECAY_INTERVAL=201
+ '[' -n 14 ']'
+ '[' 1024 -gt 64 ']'
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ NUMEPOCHS=14
+ DIST_OPTS=
+ echo 'running benchmark'
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ MATH=fp16
+ declare -a CMD
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ '[' -n 11 ']'
+ REMAIN_STEPS=1605
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ echo 'running benchmark'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ MAX_SEQ_LEN=75
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ NUMEPOCHS=14
+ '[' -n 13 ']'
+ echo 'running benchmark'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DIST_OPTS=
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ MATH=fp16
+ declare -a CMD
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
running benchmark
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ echo 'running benchmark'
+ TRAIN_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ TEST_BATCH_SIZE=4
+ '[' -n 0 ']'
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ DIST_OPTS=
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ DECAY_INTERVAL=201
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TARGET=24.0
+ MAX_SEQ_LEN=75
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ declare -a CMD
+ NUMEPOCHS=14
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ DIST_OPTS=
+ REMAIN_STEPS=1605
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ MATH=fp16
+ declare -a CMD
+ '[' -n 6 ']'
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ DIST_OPTS=
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ echo 'running benchmark'
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 1 ']'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ '[' -n 12 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ echo 'running benchmark'
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ echo 'running benchmark'
+ DATASET_DIR=/data
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ PREPROC_DATADIR=/preproc_data
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ DATASET_DIR=/data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
running benchmark
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
running benchmark
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ TEST_BATCH_SIZE=4
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ WARMUP_STEPS=200
running benchmark
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ DECAY_INTERVAL=201
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ TARGET=24.0
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ MATH=fp16
+ declare -a CMD
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ DATASET_DIR=/data
+ TEST_BATCH_SIZE=4
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
running benchmark
+ LR=5.0e-3
+ LR=5.0e-3
+ DECAY_INTERVAL=201
+ TARGET=24.0
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ '[' -n 15 ']'
+ '[' 1024 -gt 64 ']'
+ '[' -n 0 ']'
+ '[' 1024 -gt 64 ']'
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
running benchmark
+ WARMUP_STEPS=200
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ REMAIN_STEPS=1605
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DECAY_INTERVAL=201
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ TARGET=24.0
+ declare -a CMD
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ MAX_SEQ_LEN=75
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ NUMEPOCHS=14
+ MATH=fp16
+ declare -a CMD
+ echo 'running benchmark'
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ DIST_OPTS=
+ '[' -n 5 ']'
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ MATH=fp16
+ declare -a CMD
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ WARMUP_STEPS=200
running benchmark
+ '[' -n 10 ']'
+ '[' -n 5 ']'
+ '[' 1024 -gt 64 ']'
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DATASET_DIR=/data
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ PREPROC_DATADIR=/preproc_data
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ RESULTS_DIR=gnmt_wmt16
+ echo 'running benchmark'
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ MATH=fp16
+ declare -a CMD
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ LR=5.0e-3
running benchmark
+ echo 'running benchmark'
+ TRAIN_BATCH_SIZE=16
+ '[' -n 9 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ TEST_BATCH_SIZE=4
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ echo 'running benchmark'
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ DECAY_INTERVAL=201
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ TARGET=24.0
running benchmark
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ MAX_SEQ_LEN=75
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ NUMEPOCHS=14
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ DIST_OPTS=
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ MATH=fp16
+ declare -a CMD
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ '[' -n 1 ']'
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
running benchmark
+ DATASET_DIR=/data
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ LR=5.0e-3
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ echo 'running benchmark'
+ DECAY_INTERVAL=201
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ '[' -n 15 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 4 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
running benchmark
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DATASET_DIR=/data
running benchmark
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ MATH=fp16
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ declare -a CMD
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ REMAIN_STEPS=1605
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ DECAY_INTERVAL=201
+ NUMEPOCHS=14
+ DIST_OPTS=
+ TARGET=24.0
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ '[' -n 3 ']'
+ declare -a CMD
+ DIST_OPTS=
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
running benchmark
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 7 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ echo 'running benchmark'
+ '[' -n 0 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ echo 'running benchmark'
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ '[' -n 12 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ echo 'running benchmark'
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 4 ']'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ MATH=fp16
+ declare -a CMD
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 10 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ '[' -n 14 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ echo 'running benchmark'
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ echo 'running benchmark'
+ '[' -n 13 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
running benchmark
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ DATASET_DIR=/data
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ echo 'running benchmark'
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
running benchmark
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ TARGET=24.0
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ DIST_OPTS=
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ declare -a CMD
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ '[' -n 8 ']'
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ echo 'running benchmark'
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ DATASET_DIR=/data
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ PREPROC_DATADIR=/preproc_data
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ RESULTS_DIR=gnmt_wmt16
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ LR=5.0e-3
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ TRAIN_BATCH_SIZE=16
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ TEST_BATCH_SIZE=4
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ WARMUP_STEPS=200
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ REMAIN_STEPS=1605
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ DECAY_INTERVAL=201
+ TARGET=24.0
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ declare -a CMD
+ '[' -n 9 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ declare -a CMD
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ LR=5.0e-3
+ '[' -n 15 ']'
+ TRAIN_BATCH_SIZE=16
+ '[' -n 5 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ TEST_BATCH_SIZE=4
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ TARGET=24.0
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ echo 'running benchmark'
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ '[' -n 3 ']'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ '[' 1024 -gt 64 ']'
+ DATASET_DIR=/data
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ WARMUP_STEPS=200
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ echo 'running benchmark'
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
running benchmark
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 7 ']'
running benchmark
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
running benchmark
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ '[' -n 9 ']'
+ WARMUP_STEPS=200
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
running benchmark
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ '[' -n 10 ']'
running benchmark
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
running benchmark
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
running benchmark
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ LR=5.0e-3
+ DATASET_DIR=/data
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ REMAIN_STEPS=1605
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
running benchmark
+ MATH=fp16
+ declare -a CMD
+ declare -a CMD
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ declare -a CMD
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ '[' -n 8 ']'
+ DATASET_DIR=/data
+ LR=5.0e-3
+ '[' -n 2 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' -n 12 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ TRAIN_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ DECAY_INTERVAL=201
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ REMAIN_STEPS=1605
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ echo 'running benchmark'
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ NUMEPOCHS=14
+ DIST_OPTS=
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ echo 'running benchmark'
+ echo 'running benchmark'
+ declare -a CMD
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
running benchmark
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ '[' -n 1 ']'
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ MATH=fp16
+ declare -a CMD
+ '[' -n 14 ']'
+ '[' 1024 -gt 64 ']'
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ '[' -n 6 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ '[' -n 0 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ echo 'running benchmark'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 11 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 13 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ '[' -n 0 ']'
+ DATASET_DIR=/data
+ '[' 1024 -gt 64 ']'
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
running benchmark
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 6 ']'
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ echo 'running benchmark'
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
running benchmark
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ NUMEPOCHS=14
+ DIST_OPTS=
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ declare -a CMD
+ MATH=fp16
+ declare -a CMD
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ '[' -n 3 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' -n 8 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ echo 'running benchmark'
running benchmark
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 15 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
running benchmark
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ echo 'running benchmark'
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ DATASET_DIR=/data
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ NUMEPOCHS=14
+ DIST_OPTS=
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
running benchmark
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ MATH=fp16
+ declare -a CMD
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ '[' -n 11 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ DATASET_DIR=/data
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ PREPROC_DATADIR=/preproc_data
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 4 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
running benchmark
+ DATASET_DIR=/data
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ echo 'running benchmark'
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 10 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
running benchmark
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 1 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 13 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
running benchmark
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 5 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ echo 'running benchmark'
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 2 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
running benchmark
+ echo 'running benchmark'
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 9 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 12 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ echo 'running benchmark'
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 14 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' -n 7 ']'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
running benchmark
+ echo 'running benchmark'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
running benchmark
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DATASET_DIR=/data
+ DIST_OPTS=
+ PREPROC_DATADIR=/preproc_data
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ RESULTS_DIR=gnmt_wmt16
+ declare -a CMD
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ '[' -n 9 ']'
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
running benchmark
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ '[' -n 4 ']'
+ DATASET_DIR=/data
+ '[' 1024 -gt 64 ']'
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ LR=5.0e-3
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ TRAIN_BATCH_SIZE=16
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ TEST_BATCH_SIZE=4
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
running benchmark
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ NUMEPOCHS=14
+ DIST_OPTS=
running benchmark
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ echo 'running benchmark'
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ MATH=fp16
+ declare -a CMD
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ '[' -n 8 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 0 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ TRAIN_BATCH_SIZE=16
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ TARGET=24.0
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DATASET_DIR=/data
+ DIST_OPTS=
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ declare -a CMD
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ '[' -n 6 ']'
+ DIST_OPTS=
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
running benchmark
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ '[' -n 11 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
running benchmark
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 3 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 12 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
running benchmark
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ DATASET_DIR=/data
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ LR=5.0e-3
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ '[' -n 15 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
running benchmark
+ echo 'running benchmark'
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ MATH=fp16
+ declare -a CMD
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ '[' -n 5 ']'
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ '[' -n 2 ']'
running benchmark
+ echo 'running benchmark'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ echo 'running benchmark'
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ '[' -n 13 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ NUMEPOCHS=14
+ DIST_OPTS=
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ declare -a CMD
running benchmark
+ echo 'running benchmark'
+ '[' -n 14 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' -n 10 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
running benchmark
+ echo 'running benchmark'
running benchmark
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ '[' -n 7 ']'
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
running benchmark
+ echo 'running benchmark'
+ '[' -n 1 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
running benchmark
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ NUMEPOCHS=14
+ DIST_OPTS=
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ DECAY_INTERVAL=201
+ TARGET=24.0
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ declare -a CMD
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ '[' -n 13 ']'
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DECAY_INTERVAL=201
+ TARGET=24.0
running benchmark
+ '[' -n 10 ']'
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
running benchmark
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ MATH=fp16
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ declare -a CMD
running benchmark
+ echo 'running benchmark'
+ '[' -n 2 ']'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ '[' 1024 -gt 64 ']'
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ echo 'running benchmark'
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ echo 'running benchmark'
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
running benchmark
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ MATH=fp16
+ declare -a CMD
+ '[' -n 13 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ '[' -n 4 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ TRAIN_BATCH_SIZE=16
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
running benchmark
+ echo 'running benchmark'
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 0 ']'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ echo 'running benchmark'
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ DATASET_DIR=/data
+ '[' -n 14 ']'
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ DECAY_INTERVAL=201
+ TARGET=24.0
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
running benchmark
+ echo 'running benchmark'
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ DATASET_DIR=/data
+ MATH=fp16
+ PREPROC_DATADIR=/preproc_data
+ declare -a CMD
+ RESULTS_DIR=gnmt_wmt16
+ '[' -n 7 ']'
+ LR=5.0e-3
+ '[' 1024 -gt 64 ']'
+ TRAIN_BATCH_SIZE=16
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
running benchmark
+ TEST_BATCH_SIZE=4
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ WARMUP_STEPS=200
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ echo 'running benchmark'
+ MAX_SEQ_LEN=75
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ MATH=fp16
+ declare -a CMD
+ '[' -n 4 ']'
running benchmark
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ '[' 1024 -gt 64 ']'
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ echo 'running benchmark'
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ DATASET_DIR=/data
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 10 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
running benchmark
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ echo 'running benchmark'
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ DATASET_DIR=/data
+ DIST_OPTS=
+ PREPROC_DATADIR=/preproc_data
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ RESULTS_DIR=gnmt_wmt16
+ MATH=fp16
+ declare -a CMD
running benchmark
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ '[' -n 2 ']'
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ DATASET_DIR=/data
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ PREPROC_DATADIR=/preproc_data
+ TEST_BATCH_SIZE=4
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ RESULTS_DIR=gnmt_wmt16
+ WARMUP_STEPS=200
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ REMAIN_STEPS=1605
running benchmark
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ DECAY_INTERVAL=201
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ echo 'running benchmark'
running benchmark
+ TARGET=24.0
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ MAX_SEQ_LEN=75
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ NUMEPOCHS=14
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ DIST_OPTS=
+ declare -a CMD
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ MATH=fp16
running benchmark
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ declare -a CMD
+ '[' -n 5 ']'
+ '[' -n 5 ']'
running benchmark
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
running benchmark
+ echo 'running benchmark'
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ NUMEPOCHS=14
+ DIST_OPTS=
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ MATH=fp16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ declare -a CMD
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ DECAY_INTERVAL=201
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ TARGET=24.0
+ MAX_SEQ_LEN=75
running benchmark
+ '[' -n 11 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DIST_OPTS=
+ NUMEPOCHS=14
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ LR=5.0e-3
+ declare -a CMD
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ '[' -n 15 ']'
running benchmark
+ DIST_OPTS=
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ MATH=fp16
+ declare -a CMD
+ echo 'running benchmark'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' -n 1 ']'
running benchmark
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ NUMEPOCHS=14
+ DIST_OPTS=
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ '[' 1024 -gt 64 ']'
+ declare -a CMD
+ echo 'running benchmark'
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' -n 9 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
running benchmark
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ DATASET_DIR=/data
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
running benchmark
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
running benchmark
+ LR=5.0e-3
+ echo 'running benchmark'
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
running benchmark
+ DATASET_DIR=/data
+ TRAIN_BATCH_SIZE=16
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ RESULTS_DIR=gnmt_wmt16
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
running benchmark
+ LR=5.0e-3
running benchmark
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ TRAIN_BATCH_SIZE=16
+ TRAIN_BATCH_SIZE=16
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TEST_BATCH_SIZE=4
running benchmark
+ TARGET=24.0
+ TARGET=24.0
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
running benchmark
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DATASET_DIR=/data
+ WARMUP_STEPS=200
running benchmark
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
running benchmark
+ REMAIN_STEPS=1605
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ DIST_OPTS=
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ MATH=fp16
+ declare -a CMD
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ '[' -n 1 ']'
running benchmark
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ DIST_OPTS=
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
running benchmark
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ MATH=fp16
+ declare -a CMD
+ TARGET=24.0
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ echo 'running benchmark'
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ '[' -n 1 ']'
running benchmark
+ DIST_OPTS=
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
running benchmark
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ echo 'running benchmark'
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ '[' -n 6 ']'
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
running benchmark
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ MATH=fp16
+ declare -a CMD
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ DECAY_INTERVAL=201
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ TRAIN_BATCH_SIZE=16
+ TARGET=24.0
+ MAX_SEQ_LEN=75
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ NUMEPOCHS=14
+ DIST_OPTS=
+ echo 'running benchmark'
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ DIST_OPTS=
running benchmark
+ '[' -n 2 ']'
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ REMAIN_STEPS=1605
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ '[' -n 3 ']'
running benchmark
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
running benchmark
+ DATASET_DIR=/data
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
running benchmark
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ LR=5.0e-3
+ declare -a CMD
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ TRAIN_BATCH_SIZE=16
+ MAX_SEQ_LEN=75
+ '[' -n 8 ']'
+ TEST_BATCH_SIZE=4
+ NUMEPOCHS=14
+ DIST_OPTS=
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
running benchmark
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ DATASET_DIR=/data
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ DECAY_INTERVAL=201
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ '[' -n 6 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ TARGET=24.0
+ declare -a CMD
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ LR=5.0e-3
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
running benchmark
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ echo 'running benchmark'
+ echo 'running benchmark'
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
running benchmark
+ NUMEPOCHS=14
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ echo 'running benchmark'
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ '[' -n 13 ']'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ MATH=fp16
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ DECAY_INTERVAL=201
+ declare -a CMD
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ declare -a CMD
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ '[' -n 8 ']'
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ NUMEPOCHS=14
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ DIST_OPTS=
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ NUMEPOCHS=14
+ DIST_OPTS=
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ DATASET_DIR=/data
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ '[' -n 11 ']'
+ MATH=fp16
+ declare -a CMD
+ echo 'running benchmark'
+ declare -a CMD
+ echo 'running benchmark'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
running benchmark
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ '[' -n 0 ']'
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ '[' -n 15 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ NUMEPOCHS=14
+ DIST_OPTS=
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ MATH=fp16
+ declare -a CMD
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ '[' -n 10 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ declare -a CMD
+ echo 'running benchmark'
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ echo 'running benchmark'
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
running benchmark
+ NUMEPOCHS=14
+ DIST_OPTS=
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ '[' -n 3 ']'
+ DATASET_DIR=/data
+ '[' 1024 -gt 64 ']'
+ PREPROC_DATADIR=/preproc_data
+ DATASET_DIR=/data
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ PREPROC_DATADIR=/preproc_data
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ TRAIN_BATCH_SIZE=16
+ RESULTS_DIR=gnmt_wmt16
+ declare -a CMD
+ TEST_BATCH_SIZE=4
+ LR=5.0e-3
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ TRAIN_BATCH_SIZE=16
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ DECAY_INTERVAL=201
+ TEST_BATCH_SIZE=4
running benchmark
+ '[' -n 12 ']'
+ WARMUP_STEPS=200
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ echo 'running benchmark'
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ NUMEPOCHS=14
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ DIST_OPTS=
+ DIST_OPTS=
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ MATH=fp16
+ declare -a CMD
+ declare -a CMD
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ '[' -n 0 ']'
running benchmark
+ '[' 1024 -gt 64 ']'
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ TRAIN_BATCH_SIZE=16
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ '[' -n 12 ']'
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ echo 'running benchmark'
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ echo 'running benchmark'
+ '[' -n 7 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
running benchmark
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ LR=5.0e-3
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ WARMUP_STEPS=200
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
running benchmark
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ DATASET_DIR=/data
+ DIST_OPTS=
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ MATH=fp16
+ declare -a CMD
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ declare -a CMD
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ '[' -n 8 ']'
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ '[' 1024 -gt 64 ']'
+ '[' -n 7 ']'
+ NUMEPOCHS=14
+ DIST_OPTS=
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ declare -a CMD
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ echo 'running benchmark'
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ declare -a CMD
+ '[' -n 14 ']'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ '[' 1024 -gt 64 ']'
+ echo 'running benchmark'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DATASET_DIR=/data
+ '[' -n 14 ']'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ LR=5.0e-3
running benchmark
+ TRAIN_BATCH_SIZE=16
+ echo 'running benchmark'
+ TEST_BATCH_SIZE=4
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ echo 'running benchmark'
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ NUMEPOCHS=14
+ DIST_OPTS=
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ '[' -n 3 ']'
+ TEST_BATCH_SIZE=4
running benchmark
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ echo 'running benchmark'
+ DECAY_INTERVAL=201
+ TARGET=24.0
running benchmark
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
running benchmark
+ declare -a CMD
+ '[' -n 6 ']'
+ '[' 1024 -gt 64 ']'
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ echo 'running benchmark'
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 9 ']'
+ '[' 1024 -gt 64 ']'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
running benchmark
+ '[' -n 5 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ DIST_OPTS=
running benchmark
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 11 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DATASET_DIR=/data
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ declare -a CMD
+ '[' -n 12 ']'
+ '[' -n 15 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
running benchmark
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
running benchmark
+ echo 'running benchmark'
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ '[' -n 4 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ DATASET_DIR=/data
+ MATH=fp16
+ declare -a CMD
+ '[' -n 13 ']'
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ '[' 1024 -gt 64 ']'
running benchmark
+ TEST_BATCH_SIZE=4
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
running benchmark
+ DECAY_INTERVAL=201
+ TARGET=24.0
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ MAX_SEQ_LEN=75
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ echo 'running benchmark'
+ MATH=fp16
+ declare -a CMD
+ '[' -n 9 ']'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ MATH=fp16
+ declare -a CMD
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ '[' -n 11 ']'
+ DECAY_INTERVAL=201
running benchmark
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ RESULTS_DIR=gnmt_wmt16
+ NUMEPOCHS=14
+ DIST_OPTS=
+ echo 'running benchmark'
+ LR=5.0e-3
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ DATASET_DIR=/data
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ MATH=fp16
+ declare -a CMD
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ DECAY_INTERVAL=201
+ LR=5.0e-3
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ TARGET=24.0
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ TRAIN_BATCH_SIZE=16
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ declare -a CMD
+ DIST_OPTS=
+ '[' -n 0 ']'
+ '[' -n 15 ']'
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ DECAY_INTERVAL=201
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ TARGET=24.0
+ '[' 1024 -gt 64 ']'
+ declare -a CMD
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
running benchmark
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ '[' -n 2 ']'
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ MATH=fp16
+ declare -a CMD
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
running benchmark
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ echo 'running benchmark'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ '[' -n 8 ']'
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ echo 'running benchmark'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ echo 'running benchmark'
running benchmark
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
running benchmark
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ TEST_BATCH_SIZE=4
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ DECAY_INTERVAL=201
+ TARGET=24.0
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
running benchmark
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ DIST_OPTS=
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ declare -a CMD
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ '[' -n 14 ']'
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ DATASET_DIR=/data
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
running benchmark
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ TEST_BATCH_SIZE=4
running benchmark
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ WARMUP_STEPS=200
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ REMAIN_STEPS=1605
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ DECAY_INTERVAL=201
+ echo 'running benchmark'
+ TARGET=24.0
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 0 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
running benchmark
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ DATASET_DIR=/data
+ WARMUP_STEPS=200
running benchmark
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ REMAIN_STEPS=1605
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ DECAY_INTERVAL=201
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DIST_OPTS=
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ NUMEPOCHS=14
+ DIST_OPTS=
+ declare -a CMD
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ '[' -n 9 ']'
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ declare -a CMD
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' -n 6 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ echo 'running benchmark'
running benchmark
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ declare -a CMD
running benchmark
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 5 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' -n 5 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
running benchmark
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
running benchmark
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 10 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ DATASET_DIR=/data
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ DIST_OPTS=
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
running benchmark
+ DATASET_DIR=/data
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ NUMEPOCHS=14
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ DIST_OPTS=
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ declare -a CMD
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ MATH=fp16
+ declare -a CMD
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ WARMUP_STEPS=200
+ '[' -n 4 ']'
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ '[' 1024 -gt 64 ']'
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ '[' -n 4 ']'
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ NUMEPOCHS=14
+ DIST_OPTS=
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ declare -a CMD
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ '[' -n 12 ']'
+ LR=5.0e-3
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ DATASET_DIR=/data
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ LR=5.0e-3
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ '[' -n 14 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ declare -a CMD
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ '[' -n 13 ']'
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ TRAIN_BATCH_SIZE=16
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ echo 'running benchmark'
+ DECAY_INTERVAL=201
+ DATASET_DIR=/data
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ TRAIN_BATCH_SIZE=16
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ echo 'running benchmark'
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ TARGET=24.0
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ MATH=fp16
+ declare -a CMD
+ declare -a CMD
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ '[' -n 3 ']'
+ declare -a CMD
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ '[' -n 3 ']'
+ '[' -n 9 ']'
+ '[' 1024 -gt 64 ']'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ echo 'running benchmark'
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ DATASET_DIR=/data
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ '[' -n 7 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ TEST_BATCH_SIZE=4
running benchmark
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ DECAY_INTERVAL=201
+ declare -a CMD
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ '[' -n 1 ']'
+ NUMEPOCHS=14
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ echo 'running benchmark'
+ MATH=fp16
+ declare -a CMD
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ TRAIN_BATCH_SIZE=16
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ '[' -n 7 ']'
+ '[' 1024 -gt 64 ']'
+ NUMEPOCHS=14
+ DIST_OPTS=
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ echo 'running benchmark'
+ DATASET_DIR=/data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ '[' -n 6 ']'
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
running benchmark
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ MATH=fp16
+ declare -a CMD
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ '[' -n 15 ']'
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 1 ']'
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ echo 'running benchmark'
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ '[' -n 11 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
running benchmark
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ DATASET_DIR=/data
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ LR=5.0e-3
running benchmark
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 2 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 10 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ declare -a CMD
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ '[' -n 8 ']'
+ DIST_OPTS=
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ '[' -n 12 ']'
+ echo 'running benchmark'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 8 ']'
running benchmark
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 3 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
running benchmark
+ echo 'running benchmark'
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ '[' -n 5 ']'
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DATASET_DIR=/data
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
running benchmark
+ echo 'running benchmark'
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ '[' -n 2 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ DECAY_INTERVAL=201
+ TARGET=24.0
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ DIST_OPTS=
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ declare -a CMD
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ '[' -n 1 ']'
running benchmark
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MAX_SEQ_LEN=75
+ MATH=fp16
+ NUMEPOCHS=14
+ DIST_OPTS=
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ declare -a CMD
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 9 ']'
running benchmark
+ '[' -n 0 ']'
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ DATASET_DIR=/data
+ echo 'running benchmark'
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ echo 'running benchmark'
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ MATH=fp16
+ declare -a CMD
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ '[' -n 15 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ '[' -n 14 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ echo 'running benchmark'
running benchmark
+ echo 'running benchmark'
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ '[' -n 13 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
running benchmark
+ TEST_BATCH_SIZE=4
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ echo 'running benchmark'
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
running benchmark
+ TEST_BATCH_SIZE=4
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ DATASET_DIR=/data
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
running benchmark
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ NUMEPOCHS=14
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
running benchmark
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ MATH=fp16
+ declare -a CMD
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ DIST_OPTS=
running benchmark
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ '[' -n 6 ']'
+ '[' 1024 -gt 64 ']'
+ MATH=fp16
+ declare -a CMD
+ MATH=fp16
+ declare -a CMD
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ '[' -n 5 ']'
+ '[' 1024 -gt 64 ']'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ DECAY_INTERVAL=201
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ NUMEPOCHS=14
running benchmark
+ echo 'running benchmark'
+ DIST_OPTS=
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ '[' -n 6 ']'
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
running benchmark
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ DATASET_DIR=/data
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ declare -a CMD
+ echo 'running benchmark'
+ LR=5.0e-3
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ TRAIN_BATCH_SIZE=16
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ TEST_BATCH_SIZE=4
+ '[' -n 1 ']'
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ echo 'running benchmark'
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ MAX_SEQ_LEN=75
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
running benchmark
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ NUMEPOCHS=14
+ DIST_OPTS=
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ echo 'running benchmark'
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ declare -a CMD
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ DATASET_DIR=/data
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
running benchmark
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
running benchmark
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ '[' -n 0 ']'
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
running benchmark
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ DECAY_INTERVAL=201
+ TARGET=24.0
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ MAX_SEQ_LEN=75
running benchmark
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ NUMEPOCHS=14
+ DIST_OPTS=
running benchmark
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
running benchmark
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ MATH=fp16
+ declare -a CMD
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
running benchmark
+ echo 'running benchmark'
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ '[' -n 10 ']'
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ DATASET_DIR=/data
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ echo 'running benchmark'
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ TEST_BATCH_SIZE=4
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ TEST_BATCH_SIZE=4
+ DECAY_INTERVAL=201
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DATASET_DIR=/data
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ NUMEPOCHS=14
+ DIST_OPTS=
+ MATH=fp16
+ declare -a CMD
+ '[' -n 12 ']'
+ '[' 1024 -gt 64 ']'
+ LR=5.0e-3
running benchmark
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ DATASET_DIR=/data
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ declare -a CMD
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ REMAIN_STEPS=1605
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ DECAY_INTERVAL=201
+ LR=5.0e-3
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ DIST_OPTS=
+ WARMUP_STEPS=200
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ REMAIN_STEPS=1605
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ MATH=fp16
+ declare -a CMD
+ '[' -n 6 ']'
+ '[' 1024 -gt 64 ']'
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ NUMEPOCHS=14
+ DIST_OPTS=
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
running benchmark
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ NUMEPOCHS=14
+ '[' -n 3 ']'
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ DIST_OPTS=
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ declare -a CMD
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ echo 'running benchmark'
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ declare -a CMD
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ '[' -n 14 ']'
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ echo 'running benchmark'
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ declare -a CMD
running benchmark
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ '[' -n 1 ']'
+ '[' -n 7 ']'
running benchmark
+ echo 'running benchmark'
+ '[' 1024 -gt 64 ']'
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ NUMEPOCHS=14
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ '[' -n 4 ']'
+ '[' 1024 -gt 64 ']'
+ echo 'running benchmark'
+ MATH=fp16
+ declare -a CMD
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
running benchmark
+ echo 'running benchmark'
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ '[' -n 11 ']'
running benchmark
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
running benchmark
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
running benchmark
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ DECAY_INTERVAL=201
+ TARGET=24.0
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ MAX_SEQ_LEN=75
running benchmark
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 8 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
running benchmark
+ declare -a CMD
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ '[' -n 9 ']'
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ '[' 1024 -gt 64 ']'
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
running benchmark
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
running benchmark
+ DATASET_DIR=/data
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
running benchmark
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
running benchmark
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
running benchmark
+ DECAY_INTERVAL=201
+ TARGET=24.0
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DIST_OPTS=
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
running benchmark
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ TEST_BATCH_SIZE=4
+ MATH=fp16
+ declare -a CMD
+ DATASET_DIR=/data
+ '[' -n 9 ']'
+ '[' 1024 -gt 64 ']'
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
running benchmark
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
running benchmark
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 8 ']'
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ DATASET_DIR=/data
running benchmark
+ echo 'running benchmark'
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ MATH=fp16
+ declare -a CMD
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ TRAIN_BATCH_SIZE=16
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
running benchmark
+ TEST_BATCH_SIZE=4
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ WARMUP_STEPS=200
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ '[' -n 4 ']'
+ DATASET_DIR=/data
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ REMAIN_STEPS=1605
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ PREPROC_DATADIR=/preproc_data
+ '[' -n 4 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DECAY_INTERVAL=201
+ RESULTS_DIR=gnmt_wmt16
+ NUMEPOCHS=14
+ TARGET=24.0
+ LR=5.0e-3
+ DIST_OPTS=
+ MAX_SEQ_LEN=75
+ '[' -n 2 ']'
+ NUMEPOCHS=14
running benchmark
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ DIST_OPTS=
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ declare -a CMD
running benchmark
+ echo 'running benchmark'
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
running benchmark
+ echo 'running benchmark'
+ MATH=fp16
+ declare -a CMD
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ '[' -n 6 ']'
running benchmark
+ DIST_OPTS=
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
running benchmark
+ echo 'running benchmark'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
running benchmark
+ DATASET_DIR=/data
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ '[' -n 11 ']'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ MATH=fp16
+ declare -a CMD
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
running benchmark
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ LR=5.0e-3
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
running benchmark
+ echo 'running benchmark'
+ DATASET_DIR=/data
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ DECAY_INTERVAL=201
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ LR=5.0e-3
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ TARGET=24.0
running benchmark
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
running benchmark
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ '[' -n 9 ']'
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ TRAIN_BATCH_SIZE=16
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ WARMUP_STEPS=200
running benchmark
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ DATASET_DIR=/data
running benchmark
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ LR=5.0e-3
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ LR=5.0e-3
+ MAX_SEQ_LEN=75
+ MATH=fp16
+ declare -a CMD
+ TARGET=24.0
running benchmark
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ TRAIN_BATCH_SIZE=16
+ NUMEPOCHS=14
+ DIST_OPTS=
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
running benchmark
+ TARGET=24.0
+ MAX_SEQ_LEN=75
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
running benchmark
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
running benchmark
+ NUMEPOCHS=14
+ DIST_OPTS=
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ declare -a CMD
running benchmark
+ MATH=fp16
+ declare -a CMD
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ '[' -n 13 ']'
+ '[' -n 1 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
running benchmark
+ '[' 1024 -gt 64 ']'
+ MATH=fp16
+ declare -a CMD
+ MATH=fp16
+ declare -a CMD
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ '[' -n 12 ']'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ DATASET_DIR=/data
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
running benchmark
+ LR=5.0e-3
running benchmark
+ echo 'running benchmark'
+ TRAIN_BATCH_SIZE=16
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ '[' -n 13 ']'
+ '[' -n 15 ']'
+ '[' 1024 -gt 64 ']'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ echo 'running benchmark'
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ echo 'running benchmark'
+ MAX_SEQ_LEN=75
running benchmark
+ RESULTS_DIR=gnmt_wmt16
+ NUMEPOCHS=14
+ DIST_OPTS=
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ LR=5.0e-3
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
running benchmark
+ TRAIN_BATCH_SIZE=16
running benchmark
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
running benchmark
+ TEST_BATCH_SIZE=4
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ WARMUP_STEPS=200
+ MATH=fp16
+ declare -a CMD
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ DATASET_DIR=/data
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ '[' -n 0 ']'
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ NUMEPOCHS=14
+ DIST_OPTS=
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ MATH=fp16
+ DECAY_INTERVAL=201
+ TARGET=24.0
running benchmark
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ declare -a CMD
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
running benchmark
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ '[' -n 3 ']'
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ echo 'running benchmark'
+ '[' 1024 -gt 64 ']'
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
running benchmark
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ declare -a CMD
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ '[' -n 15 ']'
running benchmark
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ echo 'running benchmark'
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ echo 'running benchmark'
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ declare -a CMD
running benchmark
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ '[' -n 5 ']'
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ DIST_OPTS=
+ echo 'running benchmark'
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ declare -a CMD
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ '[' -n 14 ']'
+ '[' 1024 -gt 64 ']'
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ DATASET_DIR=/data
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ TARGET=24.0
+ echo 'running benchmark'
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ DATASET_DIR=/data
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
running benchmark
+ PREPROC_DATADIR=/preproc_data
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ declare -a CMD
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ DATASET_DIR=/data
running benchmark
+ TRAIN_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
running benchmark
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ TEST_BATCH_SIZE=4
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ LR=5.0e-3
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ '[' -n 10 ']'
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ TARGET=24.0
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ MAX_SEQ_LEN=75
running benchmark
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ NUMEPOCHS=14
+ REMAIN_STEPS=1605
+ DIST_OPTS=
+ TEST_BATCH_SIZE=4
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ DATASET_DIR=/data
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ PREPROC_DATADIR=/preproc_data
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TARGET=24.0
+ MAX_SEQ_LEN=75
running benchmark
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ declare -a CMD
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ TRAIN_BATCH_SIZE=16
+ '[' -n 11 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
running benchmark
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
running benchmark
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ echo 'running benchmark'
running benchmark
+ REMAIN_STEPS=1605
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
running benchmark
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ DECAY_INTERVAL=201
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ TARGET=24.0
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
running benchmark
+ MAX_SEQ_LEN=75
+ MATH=fp16
+ declare -a CMD
+ MATH=fp16
+ NUMEPOCHS=14
running benchmark
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ '[' -n 8 ']'
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ declare -a CMD
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ DIST_OPTS=
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ declare -a CMD
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ MATH=fp16
+ '[' -n 7 ']'
running benchmark
+ MATH=fp16
+ declare -a CMD
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ '[' -n 3 ']'
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ echo 'running benchmark'
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ declare -a CMD
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' -n 2 ']'
running benchmark
+ '[' -n 4 ']'
+ '[' 1024 -gt 64 ']'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ echo 'running benchmark'
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ echo 'running benchmark'
+ TRAIN_BATCH_SIZE=16
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ TEST_BATCH_SIZE=4
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ WARMUP_STEPS=200
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ '[' -n 2 ']'
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
running benchmark
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ TARGET=24.0
+ MATH=fp16
+ declare -a CMD
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
running benchmark
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ echo 'running benchmark'
+ RESULTS_DIR=gnmt_wmt16
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ LR=5.0e-3
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ TRAIN_BATCH_SIZE=16
+ DATASET_DIR=/data
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ '[' -n 12 ']'
+ PREPROC_DATADIR=/preproc_data
+ declare -a CMD
+ TEST_BATCH_SIZE=4
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ RESULTS_DIR=gnmt_wmt16
+ DECAY_INTERVAL=201
+ LR=5.0e-3
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ echo 'running benchmark'
+ TARGET=24.0
+ TRAIN_BATCH_SIZE=16
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ MAX_SEQ_LEN=75
+ TEST_BATCH_SIZE=4
running benchmark
+ '[' -n 14 ']'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ WARMUP_STEPS=200
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ NUMEPOCHS=14
+ REMAIN_STEPS=1605
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ DIST_OPTS=
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ echo 'running benchmark'
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MATH=fp16
+ declare -a CMD
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ echo 'running benchmark'
running benchmark
+ '[' -n 4 ']'
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ '[' -n 13 ']'
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ MATH=fp16
+ declare -a CMD
running benchmark
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ DATASET_DIR=/data
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ '[' -n 10 ']'
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ echo 'running benchmark'
+ PREPROC_DATADIR=/preproc_data
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ echo 'running benchmark'
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ NUMEPOCHS=14
+ DIST_OPTS=
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ TRAIN_BATCH_SIZE=16
+ DIST_OPTS=
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ TEST_BATCH_SIZE=4
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ DATASET_DIR=/data
running benchmark
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ WARMUP_STEPS=200
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ declare -a CMD
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ MATH=fp16
+ declare -a CMD
+ REMAIN_STEPS=1605
running benchmark
+ '[' -n 7 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ '[' -n 10 ']'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ echo 'running benchmark'
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ '[' -n 7 ']'
+ MAX_SEQ_LEN=75
+ DECAY_INTERVAL=201
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ NUMEPOCHS=14
+ DIST_OPTS=
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
running benchmark
+ TARGET=24.0
+ MAX_SEQ_LEN=75
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ MATH=fp16
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ declare -a CMD
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ '[' -n 10 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ '[' -n 5 ']'
running benchmark
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ echo 'running benchmark'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
running benchmark
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ echo 'running benchmark'
+ echo 'running benchmark'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
running benchmark
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ DATASET_DIR=/data
running benchmark
+ echo 'running benchmark'
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ DATASET_DIR=/data
+ LR=5.0e-3
+ DATASET_DIR=/data
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ DATASET_DIR=/data
running benchmark
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ '[' -n 0 ']'
+ DECAY_INTERVAL=201
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ LR=5.0e-3
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ NUMEPOCHS=14
+ DIST_OPTS=
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ declare -a CMD
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ '[' -n 12 ']'
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
running benchmark
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ '[' 1024 -gt 64 ']'
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
running benchmark
+ DECAY_INTERVAL=201
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ NUMEPOCHS=14
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DIST_OPTS=
+ echo 'running benchmark'
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MATH=fp16
+ declare -a CMD
running benchmark
+ DECAY_INTERVAL=201
+ TARGET=24.0
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ REMAIN_STEPS=1605
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ LR=5.0e-3
+ DIST_OPTS=
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ TRAIN_BATCH_SIZE=16
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ DIST_OPTS=
running benchmark
+ TEST_BATCH_SIZE=4
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ MATH=fp16
+ declare -a CMD
+ declare -a CMD
+ DECAY_INTERVAL=201
+ declare -a CMD
+ '[' -n 3 ']'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ '[' -n 11 ']'
+ '[' -n 6 ']'
+ '[' 1024 -gt 64 ']'
+ '[' -n 15 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ NUMEPOCHS=14
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ echo 'running benchmark'
+ declare -a CMD
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
running benchmark
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ DATASET_DIR=/data
+ echo 'running benchmark'
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ echo 'running benchmark'
+ DATASET_DIR=/data
running benchmark
+ '[' -n 0 ']'
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
running benchmark
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ echo 'running benchmark'
+ LR=5.0e-3
running benchmark
+ WARMUP_STEPS=200
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ TRAIN_BATCH_SIZE=16
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ DIST_OPTS=
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ MATH=fp16
+ declare -a CMD
+ '[' -n 5 ']'
+ '[' 1024 -gt 64 ']'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ '[' -n 12 ']'
+ NUMEPOCHS=14
+ DIST_OPTS=
+ echo 'running benchmark'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ declare -a CMD
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ '[' -n 14 ']'
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
running benchmark
+ echo 'running benchmark'
+ DATASET_DIR=/data
+ TRAIN_BATCH_SIZE=16
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ PREPROC_DATADIR=/preproc_data
+ TEST_BATCH_SIZE=4
+ RESULTS_DIR=gnmt_wmt16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ echo 'running benchmark'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ TEST_BATCH_SIZE=4
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ echo 'running benchmark'
+ '[' -n 6 ']'
running benchmark
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DECAY_INTERVAL=201
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 0 ']'
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ PREPROC_DATADIR=/preproc_data
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ WARMUP_STEPS=200
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ REMAIN_STEPS=1605
+ DATASET_DIR=/data
+ DECAY_INTERVAL=201
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ TARGET=24.0
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
running benchmark
+ MAX_SEQ_LEN=75
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ TRAIN_BATCH_SIZE=16
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ DIST_OPTS=
+ REMAIN_STEPS=1605
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ '[' -n 12 ']'
+ declare -a CMD
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
running benchmark
+ DIST_OPTS=
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ MATH=fp16
+ declare -a CMD
+ echo 'running benchmark'
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ '[' -n 9 ']'
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ '[' -n 15 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
running benchmark
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
running benchmark
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ WARMUP_STEPS=200
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ echo 'running benchmark'
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ echo 'running benchmark'
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ MATH=fp16
+ declare -a CMD
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ '[' -n 14 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ echo 'running benchmark'
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ TRAIN_BATCH_SIZE=16
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
running benchmark
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DECAY_INTERVAL=201
+ TARGET=24.0
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DIST_OPTS=
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ '[' -n 1 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ declare -a CMD
+ MATH=fp16
+ declare -a CMD
+ '[' -n 15 ']'
+ '[' -n 8 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
running benchmark
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' -n 13 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ echo 'running benchmark'
running benchmark
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ '[' -n 7 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
running benchmark
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
running benchmark
+ NUMEPOCHS=14
+ DIST_OPTS=
running benchmark
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ TEST_BATCH_SIZE=4
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
running benchmark
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ declare -a CMD
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ echo 'running benchmark'
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ echo 'running benchmark'
+ echo 'running benchmark'
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ '[' -n 10 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ MATH=fp16
+ declare -a CMD
running benchmark
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ RESULTS_DIR=gnmt_wmt16
running benchmark
+ '[' -n 1 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ echo 'running benchmark'
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
running benchmark
+ MATH=fp16
+ declare -a CMD
+ LR=5.0e-3
running benchmark
+ '[' -n 11 ']'
+ TRAIN_BATCH_SIZE=16
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ NUMEPOCHS=14
+ DIST_OPTS=
+ echo 'running benchmark'
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ declare -a CMD
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ '[' -n 2 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
running benchmark
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ declare -a CMD
+ '[' -n 11 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
running benchmark
+ '[' -n 8 ']'
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ echo 'running benchmark'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
running benchmark
+ '[' -n 3 ']'
+ '[' 1024 -gt 64 ']'
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ TEST_BATCH_SIZE=4
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ WARMUP_STEPS=200
running benchmark
+ REMAIN_STEPS=1605
+ echo 'running benchmark'
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 3 ']'
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DATASET_DIR=/data
running benchmark
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
running benchmark
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ NUMEPOCHS=14
+ DIST_OPTS=
+ echo 'running benchmark'
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 10 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ echo 'running benchmark'
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ DATASET_DIR=/data
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ TEST_BATCH_SIZE=4
+ DATASET_DIR=/data
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ TARGET=24.0
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
running benchmark
+ MAX_SEQ_LEN=75
+ TRAIN_BATCH_SIZE=16
+ NUMEPOCHS=14
+ TEST_BATCH_SIZE=4
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ '[' -n 9 ']'
+ '[' 1024 -gt 64 ']'
+ DIST_OPTS=
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ MATH=fp16
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ declare -a CMD
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ NUMEPOCHS=14
+ DIST_OPTS=
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ '[' -n 5 ']'
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ declare -a CMD
+ '[' 1024 -gt 64 ']'
running benchmark
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ echo 'running benchmark'
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
running benchmark
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
running benchmark
+ '[' -n 13 ']'
+ echo 'running benchmark'
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
running benchmark
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ echo 'running benchmark'
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
running benchmark
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ DATASET_DIR=/data
+ declare -a CMD
+ PREPROC_DATADIR=/preproc_data
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ '[' -n 14 ']'
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ '[' 1024 -gt 64 ']'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ DECAY_INTERVAL=201
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ TARGET=24.0
+ MAX_SEQ_LEN=75
running benchmark
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ declare -a CMD
+ '[' -n 2 ']'
+ echo 'running benchmark'
+ NUMEPOCHS=14
+ DIST_OPTS=
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
running benchmark
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DATASET_DIR=/data
+ WARMUP_STEPS=200
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ REMAIN_STEPS=1605
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
running benchmark
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ declare -a CMD
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ DIST_OPTS=
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
running benchmark
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ echo 'running benchmark'
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ declare -a CMD
running benchmark
+ RESULTS_DIR=gnmt_wmt16
+ NUMEPOCHS=14
+ DIST_OPTS=
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ '[' -n 0 ']'
+ LR=5.0e-3
+ declare -a CMD
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ TRAIN_BATCH_SIZE=16
+ '[' -n 7 ']'
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ TEST_BATCH_SIZE=4
+ DATASET_DIR=/data
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ WARMUP_STEPS=200
+ '[' 1024 -gt 64 ']'
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
running benchmark
+ REMAIN_STEPS=1605
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ echo 'running benchmark'
+ DECAY_INTERVAL=201
running benchmark
+ '[' -n 4 ']'
+ REMAIN_STEPS=1605
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ TARGET=24.0
running benchmark
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
running benchmark
+ MAX_SEQ_LEN=75
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
running benchmark
+ NUMEPOCHS=14
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ DIST_OPTS=
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ TEST_BATCH_SIZE=4
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ echo 'running benchmark'
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ echo 'running benchmark'
+ DIST_OPTS=
+ MATH=fp16
+ declare -a CMD
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ DECAY_INTERVAL=201
+ '[' -n 5 ']'
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
running benchmark
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ MATH=fp16
+ declare -a CMD
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ NUMEPOCHS=14
+ DIST_OPTS=
+ echo 'running benchmark'
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ '[' -n 6 ']'
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ '[' 1024 -gt 64 ']'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' -n 4 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DATASET_DIR=/data
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
running benchmark
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
running benchmark
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
running benchmark
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
running benchmark
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ TEST_BATCH_SIZE=4
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ echo 'running benchmark'
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ WARMUP_STEPS=200
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ echo 'running benchmark'
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
running benchmark
+ REMAIN_STEPS=1605
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ DECAY_INTERVAL=201
+ TARGET=24.0
running benchmark
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ MAX_SEQ_LEN=75
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ '[' -n 7 ']'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 7 ']'
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ echo 'running benchmark'
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ DATASET_DIR=/data
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ '[' -n 15 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ NUMEPOCHS=14
+ DIST_OPTS=
running benchmark
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 1 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ declare -a CMD
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ echo 'running benchmark'
+ '[' -n 14 ']'
+ '[' -n 8 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ echo 'running benchmark'
+ LR=5.0e-3
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ TRAIN_BATCH_SIZE=16
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ TEST_BATCH_SIZE=4
running benchmark
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
running benchmark
+ DECAY_INTERVAL=201
+ TARGET=24.0
running benchmark
+ MAX_SEQ_LEN=75
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ NUMEPOCHS=14
running benchmark
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ MATH=fp16
+ declare -a CMD
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ '[' -n 12 ']'
+ DATASET_DIR=/data
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
running benchmark
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ LR=5.0e-3
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ declare -a CMD
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
running benchmark
+ WARMUP_STEPS=200
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ echo 'running benchmark'
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ TARGET=24.0
+ '[' -n 2 ']'
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ echo 'running benchmark'
+ declare -a CMD
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
running benchmark
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ '[' -n 11 ']'
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ DIST_OPTS=
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ MATH=fp16
+ declare -a CMD
+ echo 'running benchmark'
running benchmark
+ '[' -n 5 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
running benchmark
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ LR=5.0e-3
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
running benchmark
+ TARGET=24.0
+ MAX_SEQ_LEN=75
running benchmark
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 2 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ echo 'running benchmark'
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 13 ']'
running benchmark
+ '[' 1024 -gt 64 ']'
running benchmark
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ echo 'running benchmark'
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ TRAIN_BATCH_SIZE=16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ declare -a CMD
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ '[' -n 2 ']'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
running benchmark
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ declare -a CMD
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ echo 'running benchmark'
+ '[' -n 1 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ '[' -n 15 ']'
+ echo 'running benchmark'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ DATASET_DIR=/data
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
running benchmark
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 9 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
running benchmark
+ DATASET_DIR=/data
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ DATASET_DIR=/data
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ echo 'running benchmark'
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ PREPROC_DATADIR=/preproc_data
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
running benchmark
+ RESULTS_DIR=gnmt_wmt16
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ TEST_BATCH_SIZE=4
+ LR=5.0e-3
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ WARMUP_STEPS=200
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ declare -a CMD
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
running benchmark
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ REMAIN_STEPS=1605
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ DECAY_INTERVAL=201
running benchmark
+ TARGET=24.0
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ WARMUP_STEPS=200
+ MAX_SEQ_LEN=75
+ TARGET=24.0
running benchmark
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ NUMEPOCHS=14
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
running benchmark
+ DIST_OPTS=
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ '[' -n 6 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ TARGET=24.0
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ declare -a CMD
+ MATH=fp16
+ declare -a CMD
+ declare -a CMD
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ '[' -n 0 ']'
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ '[' 1024 -gt 64 ']'
+ declare -a CMD
running benchmark
+ echo 'running benchmark'
running benchmark
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' -n 0 ']'
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ '[' -n 8 ']'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ '[' 1024 -gt 64 ']'
+ '[' -n 13 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' -n 12 ']'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ echo 'running benchmark'
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ echo 'running benchmark'
+ DATASET_DIR=/data
+ DATASET_DIR=/data
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ PREPROC_DATADIR=/preproc_data
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ echo 'running benchmark'
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
running benchmark
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ TRAIN_BATCH_SIZE=16
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
running benchmark
+ NUMEPOCHS=14
+ TEST_BATCH_SIZE=4
+ DIST_OPTS=
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ DECAY_INTERVAL=201
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ MAX_SEQ_LEN=75
+ '[' -n 4 ']'
+ DIST_OPTS=
+ NUMEPOCHS=14
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ DIST_OPTS=
running benchmark
+ MATH=fp16
+ declare -a CMD
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ '[' -n 2 ']'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ '[' 1024 -gt 64 ']'
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ MATH=fp16
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ declare -a CMD
+ echo 'running benchmark'
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ '[' -n 3 ']'
+ echo 'running benchmark'
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ RESULTS_DIR=gnmt_wmt16
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ MAX_SEQ_LEN=75
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ NUMEPOCHS=14
+ DIST_OPTS=
+ NUMEPOCHS=14
+ DIST_OPTS=
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ DATASET_DIR=/data
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ declare -a CMD
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 3 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ LR=5.0e-3
running benchmark
+ echo 'running benchmark'
+ echo 'running benchmark'
+ TRAIN_BATCH_SIZE=16
+ DATASET_DIR=/data
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ '[' -n 10 ']'
+ DIST_OPTS=
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ LR=5.0e-3
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ TRAIN_BATCH_SIZE=16
+ MATH=fp16
+ declare -a CMD
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ DECAY_INTERVAL=201
+ echo 'running benchmark'
running benchmark
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ '[' -n 6 ']'
running benchmark
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ MATH=fp16
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ declare -a CMD
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
running benchmark
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ WARMUP_STEPS=200
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ '[' -n 1 ']'
+ REMAIN_STEPS=1605
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ DATASET_DIR=/data
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ echo 'running benchmark'
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ LR=5.0e-3
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ echo 'running benchmark'
+ NUMEPOCHS=14
+ DIST_OPTS=
+ WARMUP_STEPS=200
+ MATH=fp16
+ declare -a CMD
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ DATASET_DIR=/data
+ declare -a CMD
+ TARGET=24.0
+ PREPROC_DATADIR=/preproc_data
+ '[' -n 7 ']'
+ RESULTS_DIR=gnmt_wmt16
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
running benchmark
+ '[' -n 5 ']'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ MAX_SEQ_LEN=75
+ LR=5.0e-3
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ NUMEPOCHS=14
+ DIST_OPTS=
+ '[' 1024 -gt 64 ']'
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ TRAIN_BATCH_SIZE=16
+ MATH=fp16
+ declare -a CMD
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ echo 'running benchmark'
+ '[' -n 10 ']'
+ REMAIN_STEPS=1605
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DECAY_INTERVAL=201
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ TARGET=24.0
+ LR=5.0e-3
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ MAX_SEQ_LEN=75
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ echo 'running benchmark'
+ TARGET=24.0
+ NUMEPOCHS=14
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ echo 'running benchmark'
+ DIST_OPTS=
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ MATH=fp16
+ declare -a CMD
+ MATH=fp16
+ declare -a CMD
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ '[' -n 11 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ '[' -n 4 ']'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ echo 'running benchmark'
running benchmark
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
running benchmark
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 12 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ echo 'running benchmark'
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 6 ']'
+ '[' 1024 -gt 64 ']'
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TARGET=24.0
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ NUMEPOCHS=14
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ DIST_OPTS=
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ declare -a CMD
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ echo 'running benchmark'
+ '[' -n 8 ']'
+ TARGET=24.0
+ MAX_SEQ_LEN=75
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ '[' -n 8 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
running benchmark
+ NUMEPOCHS=14
+ DIST_OPTS=
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ MATH=fp16
+ declare -a CMD
+ DATASET_DIR=/data
+ echo 'running benchmark'
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ echo 'running benchmark'
+ '[' -n 3 ']'
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ LR=5.0e-3
+ DIST_OPTS=
+ LR=5.0e-3
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ TRAIN_BATCH_SIZE=16
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ TRAIN_BATCH_SIZE=16
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
running benchmark
+ TEST_BATCH_SIZE=4
+ echo 'running benchmark'
+ '[' -n 13 ']'
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ REMAIN_STEPS=1605
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ DECAY_INTERVAL=201
+ echo 'running benchmark'
running benchmark
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ TARGET=24.0
+ DIST_OPTS=
+ MAX_SEQ_LEN=75
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ DATASET_DIR=/data
+ declare -a CMD
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ '[' -n 1 ']'
+ LR=5.0e-3
+ DATASET_DIR=/data
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TRAIN_BATCH_SIZE=16
+ '[' -n 10 ']'
+ '[' 1024 -gt 64 ']'
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
running benchmark
+ REMAIN_STEPS=1605
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ NUMEPOCHS=14
+ echo 'running benchmark'
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ MATH=fp16
+ declare -a CMD
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ MATH=fp16
+ declare -a CMD
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ echo 'running benchmark'
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ '[' -n 11 ']'
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
running benchmark
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ DIST_OPTS=
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ '[' -n 9 ']'
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ echo 'running benchmark'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
running benchmark
+ DATASET_DIR=/data
+ '[' -n 15 ']'
+ DATASET_DIR=/data
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ PREPROC_DATADIR=/preproc_data
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
running benchmark
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
running benchmark
+ TEST_BATCH_SIZE=4
+ echo 'running benchmark'
running benchmark
+ WARMUP_STEPS=200
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ REMAIN_STEPS=1605
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ TEST_BATCH_SIZE=4
+ echo 'running benchmark'
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ DECAY_INTERVAL=201
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ RESULTS_DIR=gnmt_wmt16
+ TARGET=24.0
+ DATASET_DIR=/data
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ MAX_SEQ_LEN=75
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ DECAY_INTERVAL=201
+ TEST_BATCH_SIZE=4
running benchmark
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ NUMEPOCHS=14
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DECAY_INTERVAL=201
+ DIST_OPTS=
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ NUMEPOCHS=14
+ DIST_OPTS=
+ MATH=fp16
running benchmark
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ NUMEPOCHS=14
+ DIST_OPTS=
+ declare -a CMD
+ TEST_BATCH_SIZE=4
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ declare -a CMD
+ TRAIN_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ '[' -n 9 ']'
+ TEST_BATCH_SIZE=4
+ MATH=fp16
+ declare -a CMD
+ '[' 1024 -gt 64 ']'
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
running benchmark
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ DATASET_DIR=/data
running benchmark
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ MATH=fp16
+ declare -a CMD
+ '[' -n 1 ']'
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ echo 'running benchmark'
+ DIST_OPTS=
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ MATH=fp16
+ declare -a CMD
+ '[' -n 15 ']'
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ '[' -n 5 ']'
+ '[' 1024 -gt 64 ']'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
running benchmark
+ NUMEPOCHS=14
+ DIST_OPTS=
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ '[' -n 3 ']'
+ echo 'running benchmark'
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ declare -a CMD
+ echo 'running benchmark'
+ '[' -n 11 ']'
running benchmark
+ echo 'running benchmark'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
running benchmark
+ DATASET_DIR=/data
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ echo 'running benchmark'
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ LR=5.0e-3
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ NUMEPOCHS=14
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ declare -a CMD
running benchmark
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ '[' -n 4 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ MATH=fp16
+ declare -a CMD
+ DATASET_DIR=/data
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ echo 'running benchmark'
+ LR=5.0e-3
+ DATASET_DIR=/data
+ TRAIN_BATCH_SIZE=16
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ PREPROC_DATADIR=/preproc_data
+ LR=5.0e-3
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TRAIN_BATCH_SIZE=16
+ '[' -n 14 ']'
+ TEST_BATCH_SIZE=4
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ WARMUP_STEPS=200
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ REMAIN_STEPS=1605
+ DIST_OPTS=
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
running benchmark
+ MAX_SEQ_LEN=75
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ MATH=fp16
+ declare -a CMD
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
running benchmark
+ NUMEPOCHS=14
+ DIST_OPTS=
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ DIST_OPTS=
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ '[' -n 13 ']'
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ declare -a CMD
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
running benchmark
+ '[' -n 11 ']'
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ echo 'running benchmark'
+ '[' -n 5 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
running benchmark
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
running benchmark
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ echo 'running benchmark'
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ '[' -n 0 ']'
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
running benchmark
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
running benchmark
+ '[' -n 9 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ '[' -n 14 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
running benchmark
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ echo 'running benchmark'
+ DATASET_DIR=/data
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
running benchmark
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
running benchmark
+ declare -a CMD
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ '[' -n 10 ']'
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ DATASET_DIR=/data
+ DECAY_INTERVAL=201
+ TARGET=24.0
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
running benchmark
+ MAX_SEQ_LEN=75
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ LR=5.0e-3
+ NUMEPOCHS=14
+ DIST_OPTS=
+ echo 'running benchmark'
+ TRAIN_BATCH_SIZE=16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ TEST_BATCH_SIZE=4
running benchmark
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
running benchmark
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
running benchmark
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ MATH=fp16
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ WARMUP_STEPS=200
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ declare -a CMD
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ REMAIN_STEPS=1605
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ LR=5.0e-3
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
running benchmark
+ NUMEPOCHS=14
+ DIST_OPTS=
+ NUMEPOCHS=14
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
running benchmark
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MATH=fp16
+ declare -a CMD
running benchmark
+ declare -a CMD
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
running benchmark
+ '[' -n 0 ']'
+ DIST_OPTS=
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
running benchmark
+ '[' -n 2 ']'
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ declare -a CMD
running benchmark
+ '[' -n 14 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ TRAIN_BATCH_SIZE=16
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
running benchmark
+ echo 'running benchmark'
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ '[' -n 8 ']'
+ '[' 1024 -gt 64 ']'
running benchmark
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ NUMEPOCHS=14
+ DIST_OPTS=
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ echo 'running benchmark'
running benchmark
+ echo 'running benchmark'
running benchmark
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
running benchmark
+ declare -a CMD
+ echo 'running benchmark'
+ DATASET_DIR=/data
+ '[' -n 7 ']'
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
running benchmark
+ LR=5.0e-3
running benchmark
+ TRAIN_BATCH_SIZE=16
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ REMAIN_STEPS=1605
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
running benchmark
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
running benchmark
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
running benchmark
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ MATH=fp16
+ declare -a CMD
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ '[' -n 7 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
running benchmark
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ echo 'running benchmark'
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ TARGET=24.0
+ MAX_SEQ_LEN=75
running benchmark
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ '[' -n 4 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ declare -a CMD
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ echo 'running benchmark'
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ '[' -n 15 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ TARGET=24.0
+ MAX_SEQ_LEN=75
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ NUMEPOCHS=14
running benchmark
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
running benchmark
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ MATH=fp16
+ declare -a CMD
+ DATASET_DIR=/data
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ LR=5.0e-3
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ echo 'running benchmark'
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ '[' -n 12 ']'
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ DECAY_INTERVAL=201
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ TARGET=24.0
+ declare -a CMD
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
running benchmark
+ MAX_SEQ_LEN=75
+ '[' -n 9 ']'
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ NUMEPOCHS=14
+ DIST_OPTS=
running benchmark
+ '[' 1024 -gt 64 ']'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ declare -a CMD
+ echo 'running benchmark'
+ '[' -n 3 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ echo 'running benchmark'
+ DATASET_DIR=/data
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
running benchmark
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
running benchmark
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ TARGET=24.0
running benchmark
+ MAX_SEQ_LEN=75
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ NUMEPOCHS=14
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
running benchmark
+ DIST_OPTS=
+ MATH=fp16
+ declare -a CMD
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
running benchmark
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ '[' -n 0 ']'
+ declare -a CMD
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ '[' -n 11 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ DATASET_DIR=/data
+ echo 'running benchmark'
+ echo 'running benchmark'
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ DATASET_DIR=/data
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DATASET_DIR=/data
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ PREPROC_DATADIR=/preproc_data
running benchmark
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ PREPROC_DATADIR=/preproc_data
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ RESULTS_DIR=gnmt_wmt16
+ MATH=fp16
+ declare -a CMD
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ '[' -n 13 ']'
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ WARMUP_STEPS=200
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
running benchmark
+ REMAIN_STEPS=1605
+ echo 'running benchmark'
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
running benchmark
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ DATASET_DIR=/data
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ MAX_SEQ_LEN=75
+ LR=5.0e-3
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TRAIN_BATCH_SIZE=16
running benchmark
+ MATH=fp16
+ declare -a CMD
+ NUMEPOCHS=14
+ DIST_OPTS=
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ TEST_BATCH_SIZE=4
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ declare -a CMD
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ DECAY_INTERVAL=201
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ '[' -n 7 ']'
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ '[' -n 15 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
running benchmark
+ '[' -n 2 ']'
+ '[' -n 8 ']'
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ echo 'running benchmark'
+ echo 'running benchmark'
running benchmark
+ echo 'running benchmark'
running benchmark
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DATASET_DIR=/data
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
running benchmark
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
running benchmark
+ NUMEPOCHS=14
+ DIST_OPTS=
running benchmark
+ echo 'running benchmark'
+ DECAY_INTERVAL=201
+ TARGET=24.0
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ DATASET_DIR=/data
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
running benchmark
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ MATH=fp16
+ declare -a CMD
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ '[' -n 6 ']'
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
running benchmark
+ '[' -n 12 ']'
running benchmark
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ TARGET=24.0
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
running benchmark
+ declare -a CMD
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ echo 'running benchmark'
+ MATH=fp16
+ declare -a CMD
running benchmark
+ '[' -n 12 ']'
running benchmark
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ echo 'running benchmark'
+ '[' -n 5 ']'
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ '[' 1024 -gt 64 ']'
running benchmark
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DATASET_DIR=/data
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ TEST_BATCH_SIZE=4
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
running benchmark
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ TEST_BATCH_SIZE=4
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ WARMUP_STEPS=200
+ declare -a CMD
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ '[' -n 12 ']'
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
running benchmark
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ MATH=fp16
+ declare -a CMD
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ '[' -n 6 ']'
running benchmark
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ echo 'running benchmark'
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ DATASET_DIR=/data
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ TRAIN_BATCH_SIZE=16
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ MATH=fp16
+ declare -a CMD
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 14 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' -n 10 ']'
+ '[' 1024 -gt 64 ']'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ echo 'running benchmark'
+ DATASET_DIR=/data
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ echo 'running benchmark'
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
running benchmark
+ TARGET=24.0
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
running benchmark
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
running benchmark
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ declare -a CMD
+ '[' -n 1 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ NUMEPOCHS=14
+ DIST_OPTS=
+ declare -a CMD
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ '[' -n 1 ']'
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ DATASET_DIR=/data
+ echo 'running benchmark'
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ MATH=fp16
+ declare -a CMD
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ '[' -n 9 ']'
+ DECAY_INTERVAL=201
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ '[' -n 13 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
running benchmark
+ echo 'running benchmark'
+ '[' -n 4 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
running benchmark
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
running benchmark
+ '[' -n 7 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
running benchmark
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
running benchmark
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
running benchmark
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ declare -a CMD
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ '[' -n 6 ']'
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ MAX_SEQ_LEN=75
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ NUMEPOCHS=14
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ declare -a CMD
+ '[' -n 1 ']'
+ echo 'running benchmark'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
running benchmark
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
running benchmark
+ MATH=fp16
+ declare -a CMD
+ DATASET_DIR=/data
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ '[' -n 1 ']'
+ '[' 1024 -gt 64 ']'
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
running benchmark
+ REMAIN_STEPS=1605
+ echo 'running benchmark'
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ DATASET_DIR=/data
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ declare -a CMD
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
running benchmark
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ '[' -n 5 ']'
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ DATASET_DIR=/data
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ TARGET=24.0
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ MAX_SEQ_LEN=75
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ NUMEPOCHS=14
+ DIST_OPTS=
running benchmark
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ TARGET=24.0
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
running benchmark
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ echo 'running benchmark'
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ declare -a CMD
+ '[' -n 2 ']'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ '[' -n 13 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
running benchmark
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ echo 'running benchmark'
+ DATASET_DIR=/data
running benchmark
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ declare -a CMD
+ '[' -n 10 ']'
+ '[' 1024 -gt 64 ']'
+ TRAIN_BATCH_SIZE=16
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ REMAIN_STEPS=1605
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ echo 'running benchmark'
+ '[' -n 14 ']'
+ DIST_OPTS=
+ DATASET_DIR=/data
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ MATH=fp16
+ declare -a CMD
+ LR=5.0e-3
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ WARMUP_STEPS=200
running benchmark
+ '[' -n 8 ']'
+ '[' 1024 -gt 64 ']'
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
running benchmark
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ MAX_SEQ_LEN=75
running benchmark
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ NUMEPOCHS=14
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ DIST_OPTS=
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ '[' -n 2 ']'
running benchmark
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ MATH=fp16
+ declare -a CMD
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ '[' -n 6 ']'
+ DECAY_INTERVAL=201
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ echo 'running benchmark'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ echo 'running benchmark'
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
running benchmark
+ DIST_OPTS=
+ echo 'running benchmark'
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ declare -a CMD
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ '[' -n 0 ']'
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ '[' -n 9 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
running benchmark
+ echo 'running benchmark'
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
running benchmark
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ LR=5.0e-3
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ echo 'running benchmark'
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ TEST_BATCH_SIZE=4
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MATH=fp16
+ declare -a CMD
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MAX_SEQ_LEN=75
+ MATH=fp16
+ declare -a CMD
+ NUMEPOCHS=14
+ DIST_OPTS=
+ '[' -n 12 ']'
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ declare -a CMD
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ '[' -n 11 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ '[' -n 9 ']'
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ echo 'running benchmark'
+ REMAIN_STEPS=1605
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ DECAY_INTERVAL=201
+ TARGET=24.0
running benchmark
+ LR=5.0e-3
+ MAX_SEQ_LEN=75
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ NUMEPOCHS=14
+ WARMUP_STEPS=200
+ DIST_OPTS=
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ REMAIN_STEPS=1605
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
running benchmark
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ '[' -n 2 ']'
+ MAX_SEQ_LEN=75
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
running benchmark
+ NUMEPOCHS=14
+ TEST_BATCH_SIZE=4
+ DIST_OPTS=
+ WARMUP_STEPS=200
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ REMAIN_STEPS=1605
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DECAY_INTERVAL=201
+ declare -a CMD
+ TARGET=24.0
+ echo 'running benchmark'
+ '[' -n 10 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ MAX_SEQ_LEN=75
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ '[' -n 7 ']'
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ echo 'running benchmark'
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ TRAIN_BATCH_SIZE=16
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ echo 'running benchmark'
+ TEST_BATCH_SIZE=4
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
running benchmark
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 3 ']'
+ '[' 1024 -gt 64 ']'
running benchmark
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ echo 'running benchmark'
+ DATASET_DIR=/data
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
running benchmark
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DATASET_DIR=/data
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ declare -a CMD
+ WARMUP_STEPS=200
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ '[' -n 1 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
running benchmark
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ MATH=fp16
+ declare -a CMD
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DATASET_DIR=/data
+ DIST_OPTS=
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ declare -a CMD
+ LR=5.0e-3
+ '[' -n 14 ']'
+ TRAIN_BATCH_SIZE=16
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
running benchmark
+ '[' -n 0 ']'
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ echo 'running benchmark'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
running benchmark
+ TARGET=24.0
+ MAX_SEQ_LEN=75
running benchmark
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ NUMEPOCHS=14
+ DIST_OPTS=
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ TARGET=24.0
running benchmark
+ echo 'running benchmark'
+ declare -a CMD
running benchmark
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ '[' -n 15 ']'
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ declare -a CMD
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ REMAIN_STEPS=1605
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DECAY_INTERVAL=201
+ echo 'running benchmark'
+ TARGET=24.0
+ '[' -n 4 ']'
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ MAX_SEQ_LEN=75
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ NUMEPOCHS=14
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
running benchmark
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ DIST_OPTS=
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ declare -a CMD
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DIST_OPTS=
+ '[' -n 8 ']'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ '[' 1024 -gt 64 ']'
+ echo 'running benchmark'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ echo 'running benchmark'
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ RESULTS_DIR=gnmt_wmt16
+ '[' -n 14 ']'
running benchmark
+ LR=5.0e-3
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ TRAIN_BATCH_SIZE=16
running benchmark
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ DATASET_DIR=/data
+ DATASET_DIR=/data
+ TEST_BATCH_SIZE=4
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ PREPROC_DATADIR=/preproc_data
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ LR=5.0e-3
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ DATASET_DIR=/data
+ RESULTS_DIR=gnmt_wmt16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ DECAY_INTERVAL=201
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
running benchmark
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ TEST_BATCH_SIZE=4
+ echo 'running benchmark'
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ echo 'running benchmark'
+ DECAY_INTERVAL=201
+ DECAY_INTERVAL=201
+ WARMUP_STEPS=200
+ NUMEPOCHS=14
+ DIST_OPTS=
+ TARGET=24.0
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
running benchmark
+ REMAIN_STEPS=1605
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ MATH=fp16
+ declare -a CMD
+ DIST_OPTS=
running benchmark
+ DATASET_DIR=/data
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ PREPROC_DATADIR=/preproc_data
+ MATH=fp16
+ declare -a CMD
running benchmark
+ MATH=fp16
+ declare -a CMD
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ RESULTS_DIR=gnmt_wmt16
+ '[' -n 5 ']'
+ DIST_OPTS=
+ LR=5.0e-3
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ TRAIN_BATCH_SIZE=16
+ '[' -n 9 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ MATH=fp16
+ '[' -n 2 ']'
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ TEST_BATCH_SIZE=4
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ declare -a CMD
running benchmark
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ '[' -n 8 ']'
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
running benchmark
+ '[' 1024 -gt 64 ']'
+ DECAY_INTERVAL=201
+ echo 'running benchmark'
+ echo 'running benchmark'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ TARGET=24.0
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ MAX_SEQ_LEN=75
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ NUMEPOCHS=14
+ DIST_OPTS=
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
running benchmark
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ echo 'running benchmark'
+ DATASET_DIR=/data
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ echo 'running benchmark'
+ MATH=fp16
+ declare -a CMD
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ '[' -n 1 ']'
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ TRAIN_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ DATASET_DIR=/data
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ WARMUP_STEPS=200
+ DATASET_DIR=/data
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ TRAIN_BATCH_SIZE=16
+ PREPROC_DATADIR=/preproc_data
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ MAX_SEQ_LEN=75
+ TEST_BATCH_SIZE=4
+ RESULTS_DIR=gnmt_wmt16
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ NUMEPOCHS=14
+ DIST_OPTS=
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ LR=5.0e-3
running benchmark
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ NUMEPOCHS=14
+ DIST_OPTS=
+ DIST_OPTS=
+ MATH=fp16
+ declare -a CMD
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ TRAIN_BATCH_SIZE=16
+ NUMEPOCHS=14
+ DIST_OPTS=
+ DECAY_INTERVAL=201
+ TARGET=24.0
running benchmark
+ MATH=fp16
+ declare -a CMD
+ TEST_BATCH_SIZE=4
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ MAX_SEQ_LEN=75
+ '[' -n 6 ']'
+ WARMUP_STEPS=200
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ REMAIN_STEPS=1605
+ '[' -n 7 ']'
running benchmark
+ echo 'running benchmark'
+ DECAY_INTERVAL=201
running benchmark
+ declare -a CMD
+ DATASET_DIR=/data
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ NUMEPOCHS=14
+ DIST_OPTS=
+ '[' 1024 -gt 64 ']'
+ TARGET=24.0
+ declare -a CMD
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DATASET_DIR=/data
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ MATH=fp16
+ declare -a CMD
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ MAX_SEQ_LEN=75
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
running benchmark
+ '[' -n 6 ']'
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ NUMEPOCHS=14
+ LR=5.0e-3
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ echo 'running benchmark'
+ DIST_OPTS=
+ '[' -n 4 ']'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
running benchmark
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ '[' -n 0 ']'
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ MATH=fp16
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ declare -a CMD
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ echo 'running benchmark'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ '[' -n 4 ']'
+ '[' 1024 -gt 64 ']'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
running benchmark
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
running benchmark
+ DATASET_DIR=/data
+ DATASET_DIR=/data
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ MATH=fp16
+ declare -a CMD
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ REMAIN_STEPS=1605
+ LR=5.0e-3
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
running benchmark
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ RESULTS_DIR=gnmt_wmt16
+ MAX_SEQ_LEN=75
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ LR=5.0e-3
+ echo 'running benchmark'
+ NUMEPOCHS=14
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ DATASET_DIR=/data
+ declare -a CMD
+ '[' -n 13 ']'
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TRAIN_BATCH_SIZE=16
running benchmark
+ DIST_OPTS=
+ TARGET=24.0
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ TARGET=24.0
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ MAX_SEQ_LEN=75
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ DATASET_DIR=/data
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ '[' -n 3 ']'
+ declare -a CMD
+ NUMEPOCHS=14
+ DIST_OPTS=
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ MATH=fp16
+ declare -a CMD
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ PREPROC_DATADIR=/preproc_data
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ echo 'running benchmark'
+ RESULTS_DIR=gnmt_wmt16
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ RESULTS_DIR=gnmt_wmt16
+ DECAY_INTERVAL=201
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ LR=5.0e-3
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ declare -a CMD
running benchmark
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ echo 'running benchmark'
+ TRAIN_BATCH_SIZE=16
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ '[' -n 5 ']'
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ '[' -n 15 ']'
+ TEST_BATCH_SIZE=4
+ DECAY_INTERVAL=201
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ '[' 1024 -gt 64 ']'
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ WARMUP_STEPS=200
+ WARMUP_STEPS=200
+ DIST_OPTS=
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ REMAIN_STEPS=1605
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ '[' -n 9 ']'
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
running benchmark
+ echo 'running benchmark'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ declare -a CMD
+ MATH=fp16
+ declare -a CMD
+ MAX_SEQ_LEN=75
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ NUMEPOCHS=14
+ DIST_OPTS=
running benchmark
+ '[' -n 5 ']'
running benchmark
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ NUMEPOCHS=14
running benchmark
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
running benchmark
+ DIST_OPTS=
+ echo 'running benchmark'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ MATH=fp16
+ declare -a CMD
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ declare -a CMD
running benchmark
+ '[' -n 8 ']'
+ echo 'running benchmark'
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ declare -a CMD
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ '[' -n 3 ']'
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ echo 'running benchmark'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ MATH=fp16
+ declare -a CMD
+ '[' -n 15 ']'
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' -n 12 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ '[' 1024 -gt 64 ']'
running benchmark
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
running benchmark
+ '[' -n 10 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ echo 'running benchmark'
+ echo 'running benchmark'
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ echo 'running benchmark'
running benchmark
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ NUMEPOCHS=14
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
running benchmark
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ declare -a CMD
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DATASET_DIR=/data
+ '[' -n 14 ']'
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ echo 'running benchmark'
running benchmark
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ LR=5.0e-3
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ declare -a CMD
+ TRAIN_BATCH_SIZE=16
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ NUMEPOCHS=14
+ DIST_OPTS=
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
running benchmark
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ MATH=fp16
+ declare -a CMD
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ REMAIN_STEPS=1605
+ declare -a CMD
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ echo 'running benchmark'
+ echo 'running benchmark'
+ '[' -n 7 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' -n 4 ']'
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ '[' -n 11 ']'
+ declare -a CMD
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ echo 'running benchmark'
+ echo 'running benchmark'
+ WARMUP_STEPS=200
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ '[' -n 5 ']'
+ '[' 1024 -gt 64 ']'
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ echo 'running benchmark'
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
running benchmark
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ NUMEPOCHS=14
+ DIST_OPTS=
+ TARGET=24.0
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ MAX_SEQ_LEN=75
+ declare -a CMD
running benchmark
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ echo 'running benchmark'
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ '[' -n 3 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ declare -a CMD
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ echo 'running benchmark'
+ declare -a CMD
+ '[' -n 8 ']'
+ '[' -n 13 ']'
+ '[' 1024 -gt 64 ']'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
running benchmark
+ DATASET_DIR=/data
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ PREPROC_DATADIR=/preproc_data
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ TRAIN_BATCH_SIZE=16
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ TEST_BATCH_SIZE=4
+ declare -a CMD
running benchmark
+ WARMUP_STEPS=200
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ '[' -n 15 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
running benchmark
+ TARGET=24.0
+ '[' -n 11 ']'
+ MAX_SEQ_LEN=75
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ NUMEPOCHS=14
+ DIST_OPTS=
running benchmark
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ echo 'running benchmark'
+ MATH=fp16
+ declare -a CMD
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ RESULTS_DIR=gnmt_wmt16
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ LR=5.0e-3
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ TEST_BATCH_SIZE=4
+ DECAY_INTERVAL=201
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ TARGET=24.0
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ '[' -n 9 ']'
+ '[' 1024 -gt 64 ']'
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ '[' -n 5 ']'
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ declare -a CMD
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
running benchmark
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ '[' -n 3 ']'
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' 1024 -gt 64 ']'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ echo 'running benchmark'
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ '[' -n 2 ']'
+ declare -a CMD
+ echo 'running benchmark'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ '[' -n 12 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ NUMEPOCHS=14
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ DIST_OPTS=
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
running benchmark
+ REMAIN_STEPS=1605
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
running benchmark
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ declare -a CMD
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ '[' -n 0 ']'
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ NUMEPOCHS=14
+ DIST_OPTS=
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ echo 'running benchmark'
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ declare -a CMD
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ '[' -n 4 ']'
+ declare -a CMD
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ '[' -n 11 ']'
+ '[' 1024 -gt 64 ']'
+ DATASET_DIR=/data
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ echo 'running benchmark'
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ DECAY_INTERVAL=201
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ TARGET=24.0
+ MAX_SEQ_LEN=75
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ NUMEPOCHS=14
+ DIST_OPTS=
+ echo 'running benchmark'
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ MATH=fp16
+ declare -a CMD
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ '[' -n 2 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ DATASET_DIR=/data
+ echo 'running benchmark'
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ DATASET_DIR=/data
+ NUMEPOCHS=14
+ DIST_OPTS=
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ declare -a CMD
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ LR=5.0e-3
+ '[' -n 8 ']'
+ TRAIN_BATCH_SIZE=16
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ MATH=fp16
+ declare -a CMD
+ TEST_BATCH_SIZE=4
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ DATASET_DIR=/data
+ WARMUP_STEPS=200
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ REMAIN_STEPS=1605
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ DECAY_INTERVAL=201
+ echo 'running benchmark'
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ '[' -n 7 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ NUMEPOCHS=14
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ DIST_OPTS=
+ TRAIN_BATCH_SIZE=16
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
running benchmark
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ '[' -n 12 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ declare -a CMD
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ declare -a CMD
+ TARGET=24.0
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ '[' -n 6 ']'
+ '[' 1024 -gt 64 ']'
+ NUMEPOCHS=14
+ DIST_OPTS=
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ DATASET_DIR=/data
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ DATASET_DIR=/data
+ echo 'running benchmark'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ '[' -n 9 ']'
+ '[' -n 0 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ LR=5.0e-3
+ '[' -n 12 ']'
+ '[' 1024 -gt 64 ']'
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ WARMUP_STEPS=200
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ REMAIN_STEPS=1605
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ '[' -n 11 ']'
+ REMAIN_STEPS=1605
+ echo 'running benchmark'
+ DECAY_INTERVAL=201
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
running benchmark
+ DIST_OPTS=
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ DIST_OPTS=
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ MATH=fp16
+ declare -a CMD
+ DATASET_DIR=/data
+ '[' -n 11 ']'
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ echo 'running benchmark'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ '[' -n 15 ']'
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ '[' 1024 -gt 64 ']'
+ RESULTS_DIR=gnmt_wmt16
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ LR=5.0e-3
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ echo 'running benchmark'
+ MATH=fp16
+ declare -a CMD
+ TRAIN_BATCH_SIZE=16
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ '[' -n 10 ']'
+ TEST_BATCH_SIZE=4
+ DIST_OPTS=
+ '[' -n 13 ']'
+ '[' 1024 -gt 64 ']'
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DECAY_INTERVAL=201
+ MATH=fp16
+ declare -a CMD
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ '[' -n 13 ']'
+ MATH=fp16
+ declare -a CMD
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ '[' -n 3 ']'
+ echo 'running benchmark'
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DATASET_DIR=/data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ REMAIN_STEPS=1605
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ echo 'running benchmark'
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ NUMEPOCHS=14
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ DIST_OPTS=
+ NUMEPOCHS=14
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 10 ']'
+ DATASET_DIR=/data
+ declare -a CMD
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ TEST_BATCH_SIZE=4
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ '[' -n 13 ']'
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ '[' 1024 -gt 64 ']'
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ '[' -n 14 ']'
+ '[' 1024 -gt 64 ']'
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ MATH=fp16
+ declare -a CMD
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ '[' -n 0 ']'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ echo 'running benchmark'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ DATASET_DIR=/data
+ echo 'running benchmark'
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DATASET_DIR=/data
+ DIST_OPTS=
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ declare -a CMD
+ WARMUP_STEPS=200
+ '[' -n 10 ']'
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ '[' -n 7 ']'
+ '[' 1024 -gt 64 ']'
+ NUMEPOCHS=14
+ DIST_OPTS=
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ '[' -n 14 ']'
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ echo 'running benchmark'
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ TRAIN_BATCH_SIZE=16
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ TEST_BATCH_SIZE=4
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ WARMUP_STEPS=200
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ REMAIN_STEPS=1605
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ DECAY_INTERVAL=201
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ TARGET=24.0
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ MAX_SEQ_LEN=75
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ DIST_OPTS=
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ MATH=fp16
+ TARGET=24.0
+ declare -a CMD
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ '[' -n 2 ']'
+ '[' 1024 -gt 64 ']'
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DIST_OPTS=
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ LR=5.0e-3
+ '[' -n 4 ']'
+ TRAIN_BATCH_SIZE=16
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ declare -a CMD
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ echo 'running benchmark'
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ MATH=fp16
+ declare -a CMD
+ NUMEPOCHS=14
+ DIST_OPTS=
+ '[' -n 1 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ echo 'running benchmark'
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ '[' -n 6 ']'
+ MATH=fp16
+ declare -a CMD
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ '[' -n 1 ']'
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' -n 15 ']'
+ echo 'running benchmark'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ echo 'running benchmark'
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ DECAY_INTERVAL=201
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ echo 'running benchmark'
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DIST_OPTS=
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
running benchmark
+ declare -a CMD
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ DATASET_DIR=/data
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ LR=5.0e-3
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ '[' -n 10 ']'
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ TRAIN_BATCH_SIZE=16
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ MAX_SEQ_LEN=75
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ NUMEPOCHS=14
+ DIST_OPTS=
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ echo 'running benchmark'
+ MATH=fp16
+ declare -a CMD
+ declare -a CMD
running benchmark
+ '[' -n 14 ']'
+ DATASET_DIR=/data
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ PREPROC_DATADIR=/preproc_data
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ '[' -n 0 ']'
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ echo 'running benchmark'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ TRAIN_BATCH_SIZE=16
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ REMAIN_STEPS=1605
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ DECAY_INTERVAL=201
+ TARGET=24.0
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ DATASET_DIR=/data
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ LR=5.0e-3
+ MAX_SEQ_LEN=75
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ TRAIN_BATCH_SIZE=16
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ echo 'running benchmark'
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ NUMEPOCHS=14
+ DIST_OPTS=
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
running benchmark
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ MATH=fp16
+ declare -a CMD
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ NUMEPOCHS=14
+ DIST_OPTS=
running benchmark
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ '[' -n 14 ']'
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ declare -a CMD
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ '[' -n 15 ']'
+ NUMEPOCHS=14
+ DIST_OPTS=
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ MATH=fp16
+ declare -a CMD
+ RESULTS_DIR=gnmt_wmt16
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ LR=5.0e-3
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ echo 'running benchmark'
+ WARMUP_STEPS=200
+ '[' -n 3 ']'
+ REMAIN_STEPS=1605
+ '[' 1024 -gt 64 ']'
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ MAX_SEQ_LEN=75
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ NUMEPOCHS=14
+ DIST_OPTS=
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ declare -a CMD
+ '[' -n 13 ']'
+ '[' 1024 -gt 64 ']'
+ DATASET_DIR=/data
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ PREPROC_DATADIR=/preproc_data
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ echo 'running benchmark'
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 13 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DATASET_DIR=/data
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ PREPROC_DATADIR=/preproc_data
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ RESULTS_DIR=gnmt_wmt16
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ echo 'running benchmark'
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ TEST_BATCH_SIZE=4
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ WARMUP_STEPS=200
+ RESULTS_DIR=gnmt_wmt16
+ REMAIN_STEPS=1605
+ TARGET=24.0
+ DECAY_INTERVAL=201
+ LR=5.0e-3
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ NUMEPOCHS=14
+ DIST_OPTS=
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ RESULTS_DIR=gnmt_wmt16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ LR=5.0e-3
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ MATH=fp16
+ declare -a CMD
+ WARMUP_STEPS=200
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MATH=fp16
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ declare -a CMD
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ '[' -n 3 ']'
+ DIST_OPTS=
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ RESULTS_DIR=gnmt_wmt16
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ LR=5.0e-3
+ MATH=fp16
+ declare -a CMD
+ '[' -n 8 ']'
+ TRAIN_BATCH_SIZE=16
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ TEST_BATCH_SIZE=4
running benchmark
+ '[' -n 7 ']'
+ WARMUP_STEPS=200
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
running benchmark
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ '[' -n 9 ']'
+ '[' 1024 -gt 64 ']'
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ REMAIN_STEPS=1605
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
running benchmark
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
running benchmark
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ echo 'running benchmark'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
running benchmark
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ echo 'running benchmark'
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ echo 'running benchmark'
+ REMAIN_STEPS=1605
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ DECAY_INTERVAL=201
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ DIST_OPTS=
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ '[' -n 5 ']'
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ MATH=fp16
+ declare -a CMD
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ '[' -n 0 ']'
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ MATH=fp16
+ declare -a CMD
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ echo 'running benchmark'
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ '[' -n 6 ']'
+ TEST_BATCH_SIZE=4
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ DIST_OPTS=
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
running benchmark
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 10 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ echo 'running benchmark'
running benchmark
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 8 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ echo 'running benchmark'
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 11 ']'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ '[' 1024 -gt 64 ']'
+ '[' -n 5 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ echo 'running benchmark'
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
running benchmark
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ REMAIN_STEPS=1605
+ DIST_OPTS=
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MATH=fp16
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ declare -a CMD
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ '[' -n 10 ']'
+ DATASET_DIR=/data
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ MATH=fp16
+ declare -a CMD
+ LR=5.0e-3
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ TRAIN_BATCH_SIZE=16
+ MAX_SEQ_LEN=75
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ NUMEPOCHS=14
+ DIST_OPTS=
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ REMAIN_STEPS=1605
+ echo 'running benchmark'
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ '[' -n 1 ']'
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DIST_OPTS=
+ '[' -n 2 ']'
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ MATH=fp16
+ declare -a CMD
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ echo 'running benchmark'
+ '[' -n 4 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ echo 'running benchmark'
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ echo 'running benchmark'
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ RESULTS_DIR=gnmt_wmt16
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ '[' -n 9 ']'
running benchmark
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DATASET_DIR=/data
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ DATASET_DIR=/data
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ REMAIN_STEPS=1605
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ MAX_SEQ_LEN=75
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ NUMEPOCHS=14
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ DIST_OPTS=
+ DECAY_INTERVAL=201
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ MATH=fp16
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ NUMEPOCHS=14
+ DIST_OPTS=
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ declare -a CMD
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ '[' -n 13 ']'
+ declare -a CMD
+ NUMEPOCHS=14
+ DIST_OPTS=
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' -n 13 ']'
+ '[' -n 12 ']'
+ NUMEPOCHS=14
+ DIST_OPTS=
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ declare -a CMD
+ echo 'running benchmark'
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ declare -a CMD
+ '[' -n 12 ']'
+ '[' -n 4 ']'
running benchmark
+ echo 'running benchmark'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ echo 'running benchmark'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ DATASET_DIR=/data
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ echo 'running benchmark'
+ echo 'running benchmark'
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ DATASET_DIR=/data
+ TARGET=24.0
+ PREPROC_DATADIR=/preproc_data
+ MAX_SEQ_LEN=75
+ RESULTS_DIR=gnmt_wmt16
+ NUMEPOCHS=14
+ LR=5.0e-3
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DIST_OPTS=
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ DECAY_INTERVAL=201
+ '[' -n 1 ']'
+ TARGET=24.0
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 7 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ DATASET_DIR=/data
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ DATASET_DIR=/data
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ echo 'running benchmark'
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ DECAY_INTERVAL=201
+ DATASET_DIR=/data
+ TRAIN_BATCH_SIZE=16
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ NUMEPOCHS=14
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TEST_BATCH_SIZE=4
+ MATH=fp16
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ WARMUP_STEPS=200
+ declare -a CMD
+ REMAIN_STEPS=1605
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MATH=fp16
+ declare -a CMD
+ MAX_SEQ_LEN=75
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ NUMEPOCHS=14
+ '[' -n 5 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ MATH=fp16
+ declare -a CMD
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ echo 'running benchmark'
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ '[' -n 15 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ echo 'running benchmark'
+ '[' -n 8 ']'
+ DECAY_INTERVAL=201
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ declare -a CMD
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ '[' -n 5 ']'
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ declare -a CMD
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ '[' -n 3 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ echo 'running benchmark'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ WARMUP_STEPS=200
+ DECAY_INTERVAL=201
+ REMAIN_STEPS=1605
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ DECAY_INTERVAL=201
+ NUMEPOCHS=14
+ TARGET=24.0
+ DIST_OPTS=
+ MAX_SEQ_LEN=75
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ NUMEPOCHS=14
+ DIST_OPTS=
+ MATH=fp16
+ declare -a CMD
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ '[' -n 12 ']'
+ MATH=fp16
+ '[' 1024 -gt 64 ']'
+ declare -a CMD
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' -n 6 ']'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ DATASET_DIR=/data
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ '[' 1024 -gt 64 ']'
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ echo 'running benchmark'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 11 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ REMAIN_STEPS=1605
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ MAX_SEQ_LEN=75
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ NUMEPOCHS=14
+ DIST_OPTS=
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ MATH=fp16
+ declare -a CMD
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ TEST_BATCH_SIZE=4
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ WARMUP_STEPS=200
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ '[' -n 0 ']'
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ '[' 1024 -gt 64 ']'
+ MATH=fp16
+ declare -a CMD
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ '[' -n 4 ']'
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ echo 'running benchmark'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TEST_BATCH_SIZE=4
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ MATH=fp16
+ declare -a CMD
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ echo 'running benchmark'
+ MATH=fp16
+ declare -a CMD
+ '[' -n 12 ']'
+ '[' 1024 -gt 64 ']'
+ '[' -n 11 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ echo 'running benchmark'
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ DATASET_DIR=/data
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ PREPROC_DATADIR=/preproc_data
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ RESULTS_DIR=gnmt_wmt16
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ LR=5.0e-3
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ REMAIN_STEPS=1605
+ '[' -n 9 ']'
+ DIST_OPTS=
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ MATH=fp16
+ declare -a CMD
+ DECAY_INTERVAL=201
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ echo 'running benchmark'
+ TARGET=24.0
+ '[' -n 6 ']'
+ LR=5.0e-3
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ DIST_OPTS=
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ echo 'running benchmark'
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MATH=fp16
+ declare -a CMD
+ TRAIN_BATCH_SIZE=16
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ DIST_OPTS=
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ DIST_OPTS=
+ '[' -n 6 ']'
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ TRAIN_BATCH_SIZE=16
+ MATH=fp16
+ declare -a CMD
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ '[' -n 7 ']'
+ '[' 1024 -gt 64 ']'
+ '[' -n 11 ']'
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ '[' 1024 -gt 64 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ NUMEPOCHS=14
+ DIST_OPTS=
+ echo 'running benchmark'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ echo 'running benchmark'
+ declare -a CMD
+ echo 'running benchmark'
+ '[' -n 8 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ declare -a CMD
+ '[' -n 14 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ DATASET_DIR=/data
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ declare -a CMD
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ '[' -n 4 ']'
+ NUMEPOCHS=14
+ DIST_OPTS=
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ '[' -n 10 ']'
+ DATASET_DIR=/data
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ NUMEPOCHS=14
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ DIST_OPTS=
+ MATH=fp16
+ declare -a CMD
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ DIST_OPTS=
+ '[' -n 15 ']'
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ '[' 1024 -gt 64 ']'
+ declare -a CMD
+ MATH=fp16
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' -n 2 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ declare -a CMD
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ '[' -n 4 ']'
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ '[' -n 0 ']'
+ '[' -n 6 ']'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ '[' 1024 -gt 64 ']'
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ echo 'running benchmark'
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ echo 'running benchmark'
+ echo 'running benchmark'
+ RESULTS_DIR=gnmt_wmt16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ DATASET_DIR=/data
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ echo 'running benchmark'
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ echo 'running benchmark'
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ '[' -n 15 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ LR=5.0e-3
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ TRAIN_BATCH_SIZE=16
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ '[' -n 3 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ MATH=fp16
+ declare -a CMD
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ TRAIN_BATCH_SIZE=16
+ RESULTS_DIR=gnmt_wmt16
+ NUMEPOCHS=14
+ LR=5.0e-3
+ TEST_BATCH_SIZE=4
+ DIST_OPTS=
+ WARMUP_STEPS=200
+ '[' -n 7 ']'
+ '[' 1024 -gt 64 ']'
+ DATASET_DIR=/data
+ TRAIN_BATCH_SIZE=16
+ REMAIN_STEPS=1605
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ TRAIN_BATCH_SIZE=16
+ declare -a CMD
+ MAX_SEQ_LEN=75
+ REMAIN_STEPS=1605
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ NUMEPOCHS=14
+ DIST_OPTS=
+ DECAY_INTERVAL=201
+ REMAIN_STEPS=1605
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ echo 'running benchmark'
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ '[' -n 1 ']'
+ declare -a CMD
+ MATH=fp16
+ declare -a CMD
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ '[' 1024 -gt 64 ']'
+ MATH=fp16
+ declare -a CMD
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' -n 13 ']'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ '[' -n 15 ']'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ '[' 1024 -gt 64 ']'
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ echo 'running benchmark'
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ '[' -n 14 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ echo 'running benchmark'
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ echo 'running benchmark'
+ REMAIN_STEPS=1605
+ DATASET_DIR=/data
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ PREPROC_DATADIR=/preproc_data
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ RESULTS_DIR=gnmt_wmt16
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ LR=5.0e-3
+ DIST_OPTS=
+ TRAIN_BATCH_SIZE=16
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ echo 'running benchmark'
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ REMAIN_STEPS=1605
+ declare -a CMD
+ DECAY_INTERVAL=201
+ '[' -n 2 ']'
+ TARGET=24.0
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ MAX_SEQ_LEN=75
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ NUMEPOCHS=14
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
running benchmark
+ MATH=fp16
+ declare -a CMD
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ '[' -n 9 ']'
+ '[' 1024 -gt 64 ']'
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
running benchmark
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ declare -a CMD
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ '[' -n 0 ']'
+ NUMEPOCHS=14
+ DIST_OPTS=
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ '[' -n 5 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ TARGET=24.0
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ MAX_SEQ_LEN=75
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ NUMEPOCHS=14
+ DECAY_INTERVAL=201
+ DIST_OPTS=
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ declare -a CMD
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ '[' -n 9 ']'
+ MATH=fp16
+ declare -a CMD
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ MATH=fp16
+ declare -a CMD
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ echo 'running benchmark'
+ WARMUP_STEPS=200
+ '[' -n 8 ']'
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ '[' -n 7 ']'
+ DIST_OPTS=
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ echo 'running benchmark'
+ declare -a CMD
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ '[' -n 10 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 2 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MATH=fp16
+ declare -a CMD
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 11 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' -n 3 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 1 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
running benchmark
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 12 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 14 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 14 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 12 ']'
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ TARGET=24.0
+ MAX_SEQ_LEN=75
running benchmark
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ declare -a CMD
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ '[' -n 4 ']'
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DIST_OPTS=
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ '[' -n 3 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 6 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 8 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 9 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 10 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 11 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ declare -a CMD
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ '[' -n 7 ']'
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ echo 'running benchmark'
running benchmark
+ MATH=fp16
+ declare -a CMD
+ '[' -n 5 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 1 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 2 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ NUMEPOCHS=14
+ DIST_OPTS=
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ declare -a CMD
+ '[' -n 15 ']'
+ '[' -n 13 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ DATASET_DIR=/data
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
running benchmark
+ echo 'running benchmark'
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ '[' -n 0 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 7 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 11 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 8 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
running benchmark
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
running benchmark
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 9 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 13 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ declare -a CMD
+ '[' -n 0 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ TRAIN_BATCH_SIZE=16
+ RESULTS_DIR=gnmt_wmt16
+ TEST_BATCH_SIZE=4
+ LR=5.0e-3
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ TRAIN_BATCH_SIZE=16
+ DECAY_INTERVAL=201
+ TEST_BATCH_SIZE=4
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ TARGET=24.0
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ DIST_OPTS=
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ DIST_OPTS=
+ MATH=fp16
+ declare -a CMD
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 14 ']'
+ '[' -n 15 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ echo 'running benchmark'
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
running benchmark
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 8 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ echo 'running benchmark'
running benchmark
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ MATH=fp16
+ declare -a CMD
+ DECAY_INTERVAL=201
+ TARGET=24.0
running benchmark
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ DATASET_DIR=/data
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ declare -a CMD
+ LR=5.0e-3
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ '[' -n 1 ']'
+ '[' -n 6 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ TRAIN_BATCH_SIZE=16
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ echo 'running benchmark'
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ NUMEPOCHS=14
+ DIST_OPTS=
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
running benchmark
+ echo 'running benchmark'
+ MATH=fp16
+ declare -a CMD
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ '[' -n 3 ']'
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ echo 'running benchmark'
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ NUMEPOCHS=14
+ DIST_OPTS=
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
running benchmark
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ declare -a CMD
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ '[' -n 11 ']'
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ DATASET_DIR=/data
running benchmark
+ DIST_OPTS=
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ PREPROC_DATADIR=/preproc_data
running benchmark
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ '[' 1024 -gt 64 ']'
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ MATH=fp16
+ declare -a CMD
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ TRAIN_BATCH_SIZE=16
+ '[' -n 12 ']'
+ TEST_BATCH_SIZE=4
+ '[' 1024 -gt 64 ']'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ WARMUP_STEPS=200
+ '[' -n 5 ']'
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ REMAIN_STEPS=1605
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ DECAY_INTERVAL=201
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
running benchmark
+ echo 'running benchmark'
+ TARGET=24.0
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ MAX_SEQ_LEN=75
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
running benchmark
+ NUMEPOCHS=14
+ DIST_OPTS=
running benchmark
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
running benchmark
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ declare -a CMD
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ echo 'running benchmark'
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ TEST_BATCH_SIZE=4
+ REMAIN_STEPS=1605
running benchmark
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ WARMUP_STEPS=200
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ MAX_SEQ_LEN=75
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
running benchmark
+ '[' -n 10 ']'
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ NUMEPOCHS=14
+ DIST_OPTS=
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ declare -a CMD
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ '[' -n 13 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ MATH=fp16
+ declare -a CMD
running benchmark
+ MATH=fp16
+ '[' -n 11 ']'
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ declare -a CMD
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
running benchmark
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ '[' -n 10 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ '[' -n 2 ']'
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ echo 'running benchmark'
running benchmark
+ echo 'running benchmark'
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
running benchmark
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
running benchmark
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ LR=5.0e-3
+ NUMEPOCHS=14
running benchmark
+ TEST_BATCH_SIZE=4
running benchmark
+ DIST_OPTS=
+ echo 'running benchmark'
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
running benchmark
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ declare -a CMD
+ TRAIN_BATCH_SIZE=16
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ '[' -n 0 ']'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
running benchmark
+ TEST_BATCH_SIZE=4
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ RESULTS_DIR=gnmt_wmt16
+ TRAIN_BATCH_SIZE=16
running benchmark
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ LR=5.0e-3
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ DIST_OPTS=
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ TARGET=24.0
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ DATASET_DIR=/data
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ echo 'running benchmark'
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ DIST_OPTS=
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ REMAIN_STEPS=1605
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
running benchmark
+ declare -a CMD
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ DECAY_INTERVAL=201
+ TARGET=24.0
running benchmark
+ MAX_SEQ_LEN=75
+ '[' -n 8 ']'
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ MAX_SEQ_LEN=75
+ declare -a CMD
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
running benchmark
+ NUMEPOCHS=14
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
running benchmark
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ NUMEPOCHS=14
+ DIST_OPTS=
running benchmark
+ RESULTS_DIR=gnmt_wmt16
+ MATH=fp16
+ declare -a CMD
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
running benchmark
+ echo 'running benchmark'
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ TEST_BATCH_SIZE=4
running benchmark
+ DATASET_DIR=/data
+ WARMUP_STEPS=200
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
running benchmark
+ REMAIN_STEPS=1605
+ LR=5.0e-3
+ DECAY_INTERVAL=201
+ TRAIN_BATCH_SIZE=16
+ '[' -n 8 ']'
+ TEST_BATCH_SIZE=4
+ TARGET=24.0
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DECAY_INTERVAL=201
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ DIST_OPTS=
+ NUMEPOCHS=14
running benchmark
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ DIST_OPTS=
running benchmark
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ MATH=fp16
+ declare -a CMD
running benchmark
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ declare -a CMD
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ '[' -n 1 ']'
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ echo 'running benchmark'
running benchmark
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ '[' -n 12 ']'
+ '[' 1024 -gt 64 ']'
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ echo 'running benchmark'
running benchmark
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
running benchmark
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
running benchmark
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ DECAY_INTERVAL=201
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
running benchmark
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ '[' -n 4 ']'
+ '[' 1024 -gt 64 ']'
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DATASET_DIR=/data
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DATASET_DIR=/data
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
running benchmark
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
running benchmark
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ DIST_OPTS=
+ echo 'running benchmark'
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ DATASET_DIR=/data
running benchmark
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ declare -a CMD
+ MATH=fp16
+ declare -a CMD
+ LR=5.0e-3
+ '[' -n 5 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
running benchmark
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ '[' -n 13 ']'
+ '[' 1024 -gt 64 ']'
running benchmark
+ TRAIN_BATCH_SIZE=16
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ declare -a CMD
running benchmark
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
running benchmark
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ TEST_BATCH_SIZE=4
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ TEST_BATCH_SIZE=4
running benchmark
+ '[' -n 0 ']'
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ WARMUP_STEPS=200
running benchmark
+ TRAIN_BATCH_SIZE=16
+ '[' 1024 -gt 64 ']'
+ REMAIN_STEPS=1605
+ echo 'running benchmark'
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ WARMUP_STEPS=200
running benchmark
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ '[' -n 1 ']'
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ MAX_SEQ_LEN=75
running benchmark
+ '[' 1024 -gt 64 ']'
+ TEST_BATCH_SIZE=4
running benchmark
+ NUMEPOCHS=14
+ DIST_OPTS=
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ NUMEPOCHS=14
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DECAY_INTERVAL=201
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
running benchmark
+ DIST_OPTS=
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ declare -a CMD
+ echo 'running benchmark'
running benchmark
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ '[' -n 5 ']'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ MATH=fp16
+ TARGET=24.0
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ declare -a CMD
+ echo 'running benchmark'
running benchmark
+ '[' -n 3 ']'
+ '[' 1024 -gt 64 ']'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ MAX_SEQ_LEN=75
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ echo 'running benchmark'
+ '[' -n 3 ']'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ '[' 1024 -gt 64 ']'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ echo 'running benchmark'
running benchmark
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ '[' -n 4 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ echo 'running benchmark'
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ MATH=fp16
+ declare -a CMD
running benchmark
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ MATH=fp16
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ DATASET_DIR=/data
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ LR=5.0e-3
+ RESULTS_DIR=gnmt_wmt16
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ echo 'running benchmark'
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ DECAY_INTERVAL=201
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ declare -a CMD
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ NUMEPOCHS=14
+ DIST_OPTS=
+ LR=5.0e-3
+ REMAIN_STEPS=1605
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ DECAY_INTERVAL=201
running benchmark
+ MATH=fp16
+ declare -a CMD
+ echo 'running benchmark'
+ TARGET=24.0
+ MAX_SEQ_LEN=75
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ TRAIN_BATCH_SIZE=16
+ '[' -n 15 ']'
+ TEST_BATCH_SIZE=4
+ NUMEPOCHS=14
+ DIST_OPTS=
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' -n 2 ']'
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ MATH=fp16
+ declare -a CMD
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ echo 'running benchmark'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ LR=5.0e-3
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ '[' -n 7 ']'
+ '[' -n 4 ']'
+ DATASET_DIR=/data
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ WARMUP_STEPS=200
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ '[' 1024 -gt 64 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ LR=5.0e-3
+ DIST_OPTS=
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
running benchmark
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ NUMEPOCHS=14
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ DIST_OPTS=
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ DIST_OPTS=
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ declare -a CMD
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ '[' -n 14 ']'
+ echo 'running benchmark'
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ DECAY_INTERVAL=201
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ declare -a CMD
+ TARGET=24.0
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ MAX_SEQ_LEN=75
+ DATASET_DIR=/data
+ '[' -n 0 ']'
+ '[' 1024 -gt 64 ']'
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ NUMEPOCHS=14
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ TRAIN_BATCH_SIZE=16
+ DIST_OPTS=
+ LR=5.0e-3
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
running benchmark
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ echo 'running benchmark'
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ declare -a CMD
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' -n 12 ']'
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ '[' -n 2 ']'
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ MAX_SEQ_LEN=75
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ echo 'running benchmark'
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ NUMEPOCHS=14
+ DIST_OPTS=
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ echo 'running benchmark'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ MATH=fp16
+ declare -a CMD
running benchmark
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ '[' -n 4 ']'
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ declare -a CMD
+ MATH=fp16
+ declare -a CMD
running benchmark
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ WARMUP_STEPS=200
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ echo 'running benchmark'
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ LR=5.0e-3
+ TARGET=24.0
+ NUMEPOCHS=14
+ DIST_OPTS=
+ echo 'running benchmark'
+ TRAIN_BATCH_SIZE=16
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ DATASET_DIR=/data
+ MAX_SEQ_LEN=75
+ PREPROC_DATADIR=/preproc_data
+ TEST_BATCH_SIZE=4
+ declare -a CMD
+ NUMEPOCHS=14
+ DIST_OPTS=
+ '[' -n 7 ']'
+ RESULTS_DIR=gnmt_wmt16
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ LR=5.0e-3
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ '[' -n 1 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ '[' -n 3 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ NUMEPOCHS=14
+ DIST_OPTS=
+ declare -a CMD
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ PREPROC_DATADIR=/preproc_data
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ RESULTS_DIR=gnmt_wmt16
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
running benchmark
+ echo 'running benchmark'
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ echo 'running benchmark'
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ declare -a CMD
+ '[' -n 9 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ TEST_BATCH_SIZE=4
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ echo 'running benchmark'
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ DECAY_INTERVAL=201
+ '[' -n 15 ']'
+ '[' 1024 -gt 64 ']'
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ TARGET=24.0
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ LR=5.0e-3
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
running benchmark
+ MAX_SEQ_LEN=75
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ NUMEPOCHS=14
+ declare -a CMD
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ '[' -n 6 ']'
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ DIST_OPTS=
+ echo 'running benchmark'
+ NUMEPOCHS=14
+ DIST_OPTS=
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ '[' -n 12 ']'
+ '[' 1024 -gt 64 ']'
+ declare -a CMD
+ echo 'running benchmark'
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ declare -a CMD
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ '[' -n 12 ']'
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ '[' 1024 -gt 64 ']'
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
running benchmark
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ LR=5.0e-3
+ '[' -n 0 ']'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
running benchmark
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ '[' 1024 -gt 64 ']'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
running benchmark
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ DATASET_DIR=/data
+ '[' -n 10 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ LR=5.0e-3
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ MATH=fp16
+ declare -a CMD
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ DECAY_INTERVAL=201
+ declare -a CMD
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ TARGET=24.0
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ DIST_OPTS=
+ echo 'running benchmark'
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ echo 'running benchmark'
+ declare -a CMD
+ DIST_OPTS=
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ '[' -n 10 ']'
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' -n 2 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ MATH=fp16
+ declare -a CMD
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ '[' -n 13 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ declare -a CMD
+ echo 'running benchmark'
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ '[' -n 7 ']'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ '[' -n 9 ']'
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DATASET_DIR=/data
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
running benchmark
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ echo 'running benchmark'
+ '[' -n 6 ']'
+ echo 'running benchmark'
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ echo 'running benchmark'
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ MATH=fp16
+ declare -a CMD
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ '[' -n 13 ']'
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ '[' -n 15 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TRAIN_BATCH_SIZE=16
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ TEST_BATCH_SIZE=4
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ WARMUP_STEPS=200
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ REMAIN_STEPS=1605
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ '[' -n 14 ']'
+ DATASET_DIR=/data
+ DATASET_DIR=/data
+ '[' -n 6 ']'
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ DIST_OPTS=
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ TEST_BATCH_SIZE=4
+ MATH=fp16
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ declare -a CMD
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ MAX_SEQ_LEN=75
+ '[' -n 9 ']'
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ NUMEPOCHS=14
+ DIST_OPTS=
+ '[' 1024 -gt 64 ']'
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ echo 'running benchmark'
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ MATH=fp16
+ declare -a CMD
+ REMAIN_STEPS=1605
+ echo 'running benchmark'
+ '[' -n 9 ']'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ '[' 1024 -gt 64 ']'
+ MAX_SEQ_LEN=75
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ NUMEPOCHS=14
+ DIST_OPTS=
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ echo 'running benchmark'
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ echo 'running benchmark'
+ '[' -n 2 ']'
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DIST_OPTS=
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ echo 'running benchmark'
+ declare -a CMD
+ '[' -n 5 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ DATASET_DIR=/data
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ PREPROC_DATADIR=/preproc_data
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 11 ']'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ RESULTS_DIR=gnmt_wmt16
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ TEST_BATCH_SIZE=4
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 10 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 14 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ TARGET=24.0
+ MATH=fp16
+ declare -a CMD
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 3 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ '[' -n 5 ']'
+ echo 'running benchmark'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ '[' -n 11 ']'
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ MATH=fp16
+ declare -a CMD
+ echo 'running benchmark'
+ '[' -n 14 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 6 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ echo 'running benchmark'
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ '[' -n 8 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ echo 'running benchmark'
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ DIST_OPTS=
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ declare -a CMD
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ MATH=fp16
+ declare -a CMD
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ '[' -n 7 ']'
+ '[' 1024 -gt 64 ']'
+ REMAIN_STEPS=1605
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ '[' -n 15 ']'
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DIST_OPTS=
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ echo 'running benchmark'
+ declare -a CMD
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ '[' -n 4 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 1 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 0 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
running benchmark
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 1 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
running benchmark
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ DECAY_INTERVAL=201
+ MATH=fp16
+ declare -a CMD
+ TARGET=24.0
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ '[' -n 15 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ '[' -n 12 ']'
running benchmark
+ echo 'running benchmark'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' -n 10 ']'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ echo 'running benchmark'
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
running benchmark
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ DATASET_DIR=/data
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
running benchmark
+ echo 'running benchmark'
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ LR=5.0e-3
+ NUMEPOCHS=14
+ DIST_OPTS=
+ TRAIN_BATCH_SIZE=16
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ TEST_BATCH_SIZE=4
+ MATH=fp16
+ declare -a CMD
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ '[' -n 13 ']'
+ '[' 1024 -gt 64 ']'
+ DIST_OPTS=
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ MATH=fp16
+ declare -a CMD
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ DATASET_DIR=/data
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
running benchmark
+ echo 'running benchmark'
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ '[' -n 4 ']'
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ '[' -n 9 ']'
+ DATASET_DIR=/data
running benchmark
+ echo 'running benchmark'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ DECAY_INTERVAL=201
+ TARGET=24.0
running benchmark
+ echo 'running benchmark'
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ '[' -n 2 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
running benchmark
+ echo 'running benchmark'
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 3 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ echo 'running benchmark'
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 7 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ echo 'running benchmark'
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ declare -a CMD
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ '[' -n 14 ']'
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
running benchmark
+ echo 'running benchmark'
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ '[' -n 5 ']'
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ declare -a CMD
running benchmark
+ echo 'running benchmark'
+ '[' -n 11 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ TRAIN_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ NUMEPOCHS=14
+ DIST_OPTS=
+ TARGET=24.0
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ declare -a CMD
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 6 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ echo 'running benchmark'
+ '[' -n 8 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
running benchmark
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ echo 'running benchmark'
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 15 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 8 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 13 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
running benchmark
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 3 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ echo 'running benchmark'
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ '[' -n 7 ']'
+ NUMEPOCHS=14
+ DIST_OPTS=
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ '[' -n 10 ']'
+ '[' 1024 -gt 64 ']'
+ TARGET=24.0
+ MAX_SEQ_LEN=75
running benchmark
+ echo 'running benchmark'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ MATH=fp16
+ declare -a CMD
running benchmark
+ echo 'running benchmark'
+ '[' -n 11 ']'
+ '[' 1024 -gt 64 ']'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ echo 'running benchmark'
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ MATH=fp16
+ declare -a CMD
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ '[' -n 2 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
running benchmark
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ '[' -n 0 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ '[' -n 12 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
running benchmark
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 9 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
running benchmark
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ '[' -n 5 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
running benchmark
+ echo 'running benchmark'
+ declare -a CMD
+ '[' -n 4 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
running benchmark
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 6 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ '[' -n 1 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ NUMEPOCHS=14
+ DIST_OPTS=
running benchmark
+ echo 'running benchmark'
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 14 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
slurmstepd: task_p_pre_launch: Using sched_affinity for tasks
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=3-5,51-53 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 3 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 8 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ echo 'running benchmark'
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 7 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 15 ']'
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
running benchmark
+ echo 'running benchmark'
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ DATASET_DIR=/data
+ REMAIN_STEPS=1605
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ MAX_SEQ_LEN=75
+ WARMUP_STEPS=200
+ NUMEPOCHS=14
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ MATH=fp16
+ declare -a CMD
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 3 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' -n 10 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
running benchmark
+ echo 'running benchmark'
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ declare -a CMD
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ '[' -n 6 ']'
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ declare -a CMD
running benchmark
+ echo 'running benchmark'
+ '[' -n 0 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
running benchmark
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ DATASET_DIR=/data
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ declare -a CMD
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ TRAIN_BATCH_SIZE=16
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ REMAIN_STEPS=1605
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MATH=fp16
+ declare -a CMD
+ '[' -n 2 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 1 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
running benchmark
running benchmark
+ echo 'running benchmark'
+ '[' -n 9 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
running benchmark
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 12 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ echo 'running benchmark'
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 4 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ echo 'running benchmark'
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 5 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ echo 'running benchmark'
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ declare -a CMD
+ '[' -n 14 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ '[' -n 11 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ echo 'running benchmark'
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 8 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ echo 'running benchmark'
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 13 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=6-8,54-56 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=0-2,48-50 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=33-35,81-83 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=15-17,63-65 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=30-32,78-80 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=0-2,48-50 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=3-5,51-53 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=42-44,90-92 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ LR=5.0e-3
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ declare -a CMD
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ '[' -n 15 ']'
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ '[' -n 11 ']'
+ '[' 1024 -gt 64 ']'
+ declare -a CMD
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ DATASET_DIR=/data
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ echo 'running benchmark'
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ '[' -n 6 ']'
+ TRAIN_BATCH_SIZE=16
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
running benchmark
+ echo 'running benchmark'
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ NUMEPOCHS=14
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ echo 'running benchmark'
+ MATH=fp16
running benchmark
+ declare -a CMD
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ '[' -n 1 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ '[' -n 2 ']'
+ LR=5.0e-3
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
running benchmark
+ echo 'running benchmark'
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 14 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
running benchmark
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ MATH=fp16
+ declare -a CMD
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ '[' -n 4 ']'
+ LR=5.0e-3
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ MATH=fp16
+ declare -a CMD
running benchmark
+ echo 'running benchmark'
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ '[' -n 5 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ DATASET_DIR=/data
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
running benchmark
+ echo 'running benchmark'
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 7 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
running benchmark
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ DATASET_DIR=/data
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ NUMEPOCHS=14
+ DIST_OPTS=
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ declare -a CMD
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ '[' -n 13 ']'
+ '[' -n 0 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
running benchmark
+ echo 'running benchmark'
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
running benchmark
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ '[' -n 9 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
+ echo 'running benchmark'
running benchmark
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 10 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
STARTING TIMING RUN AT 2020-06-22 07:40:42 PM
+ DATASET_DIR=/data
+ PREPROC_DATADIR=/preproc_data
+ RESULTS_DIR=gnmt_wmt16
+ LR=5.0e-3
+ TRAIN_BATCH_SIZE=16
+ TEST_BATCH_SIZE=4
+ WARMUP_STEPS=200
+ REMAIN_STEPS=1605
+ DECAY_INTERVAL=201
+ TARGET=24.0
+ MAX_SEQ_LEN=75
+ NUMEPOCHS=14
+ DIST_OPTS=
+ EXTRA_OPTS='   --fused-attention    --fused-xentropy    --prealloc-mode=once    --no-log-all-ranks    --enable-apex-allreduce-overlap    --apex-message-size 32134259    --apex-num-allreduce-streams 1    '
+ MATH=fp16
+ declare -a CMD
+ '[' -n 12 ']'
+ '[' 1024 -gt 64 ']'
+ CMD=('./bind.sh' '--cpu=exclusive' '--' 'python' '-u')
+ [[ -e /usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4 ]]
+ [[ DGX2_multi_64x16x16 == *\D\G\X\A\1\0\0* ]]
running benchmark
+ echo 'running benchmark'
+ ./bind.sh --cpu=exclusive -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=6-8,54-56 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=3-5,51-53 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=33-35,81-83 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=6-8,54-56 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=9-11,57-59 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=24-26,72-74 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=15-17,63-65 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=12-14,60-62 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=6-8,54-56 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=42-44,90-92 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=0-2,48-50 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=3-5,51-53 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=30-32,78-80 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=21-23,69-71 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=0-2,48-50 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=45-47,93-95 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=15-17,63-65 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=9-11,57-59 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=3-5,51-53 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=0-2,48-50 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=9-11,57-59 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=18-20,66-68 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=21-23,69-71 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=45-47,93-95 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=21-23,69-71 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=6-8,54-56 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=3-5,51-53 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=0-2,48-50 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=39-41,87-89 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=30-32,78-80 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=21-23,69-71 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=15-17,63-65 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=24-26,72-74 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=15-17,63-65 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=21-23,69-71 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=18-20,66-68 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=3-5,51-53 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=18-20,66-68 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=27-29,75-77 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=6-8,54-56 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=0-2,48-50 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=33-35,81-83 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=42-44,90-92 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=45-47,93-95 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=45-47,93-95 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=27-29,75-77 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=45-47,93-95 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=21-23,69-71 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=36-38,84-86 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=15-17,63-65 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=24-26,72-74 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=30-32,78-80 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=3-5,51-53 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=15-17,63-65 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=9-11,57-59 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=33-35,81-83 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=24-26,72-74 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=30-32,78-80 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=15-17,63-65 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=12-14,60-62 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=42-44,90-92 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=42-44,90-92 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=27-29,75-77 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=24-26,72-74 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=39-41,87-89 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=39-41,87-89 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=24-26,72-74 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=18-20,66-68 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=18-20,66-68 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=21-23,69-71 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=9-11,57-59 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=45-47,93-95 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=39-41,87-89 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=36-38,84-86 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=15-17,63-65 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=15-17,63-65 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=39-41,87-89 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=33-35,81-83 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=0-2,48-50 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=0-2,48-50 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=9-11,57-59 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=9-11,57-59 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=18-20,66-68 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=45-47,93-95 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=24-26,72-74 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=6-8,54-56 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=15-17,63-65 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=3-5,51-53 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=21-23,69-71 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=3-5,51-53 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=30-32,78-80 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=15-17,63-65 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=45-47,93-95 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=24-26,72-74 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=3-5,51-53 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=36-38,84-86 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=18-20,66-68 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=9-11,57-59 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=36-38,84-86 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=27-29,75-77 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=0-2,48-50 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=30-32,78-80 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=9-11,57-59 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=27-29,75-77 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=3-5,51-53 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=45-47,93-95 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=33-35,81-83 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=45-47,93-95 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=6-8,54-56 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=3-5,51-53 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=9-11,57-59 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=36-38,84-86 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=33-35,81-83 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=3-5,51-53 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=21-23,69-71 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=36-38,84-86 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=33-35,81-83 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=39-41,87-89 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=9-11,57-59 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=15-17,63-65 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=27-29,75-77 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=33-35,81-83 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=42-44,90-92 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=3-5,51-53 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=39-41,87-89 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=6-8,54-56 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=18-20,66-68 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=9-11,57-59 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=36-38,84-86 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=0-2,48-50 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=39-41,87-89 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=24-26,72-74 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=30-32,78-80 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=27-29,75-77 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=33-35,81-83 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=36-38,84-86 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=42-44,90-92 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=12-14,60-62 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=21-23,69-71 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=45-47,93-95 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=12-14,60-62 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=15-17,63-65 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=45-47,93-95 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=39-41,87-89 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=6-8,54-56 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=30-32,78-80 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=0-2,48-50 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=36-38,84-86 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=36-38,84-86 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=0-2,48-50 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=12-14,60-62 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=9-11,57-59 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=24-26,72-74 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=18-20,66-68 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=33-35,81-83 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=3-5,51-53 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=12-14,60-62 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=45-47,93-95 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=30-32,78-80 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=27-29,75-77 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=21-23,69-71 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=9-11,57-59 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=0-2,48-50 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=30-32,78-80 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=18-20,66-68 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=18-20,66-68 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=21-23,69-71 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=39-41,87-89 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=45-47,93-95 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=9-11,57-59 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=3-5,51-53 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=24-26,72-74 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=39-41,87-89 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=27-29,75-77 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=27-29,75-77 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=42-44,90-92 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=6-8,54-56 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=42-44,90-92 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=45-47,93-95 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=24-26,72-74 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=45-47,93-95 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=15-17,63-65 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=42-44,90-92 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=39-41,87-89 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=12-14,60-62 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=27-29,75-77 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=15-17,63-65 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=0-2,48-50 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=21-23,69-71 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=12-14,60-62 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=0-2,48-50 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=36-38,84-86 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=6-8,54-56 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=33-35,81-83 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=21-23,69-71 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=3-5,51-53 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=33-35,81-83 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=39-41,87-89 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=9-11,57-59 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=18-20,66-68 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=36-38,84-86 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=45-47,93-95 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=27-29,75-77 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=12-14,60-62 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=18-20,66-68 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=27-29,75-77 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=3-5,51-53 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=12-14,60-62 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=45-47,93-95 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=39-41,87-89 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=15-17,63-65 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=36-38,84-86 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=12-14,60-62 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=30-32,78-80 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=24-26,72-74 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=18-20,66-68 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=21-23,69-71 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=30-32,78-80 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=30-32,78-80 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=39-41,87-89 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=18-20,66-68 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=9-11,57-59 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=24-26,72-74 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=36-38,84-86 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=6-8,54-56 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=3-5,51-53 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=0-2,48-50 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=42-44,90-92 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=9-11,57-59 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=33-35,81-83 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=6-8,54-56 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=39-41,87-89 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=21-23,69-71 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=6-8,54-56 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=42-44,90-92 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=24-26,72-74 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=33-35,81-83 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=0-2,48-50 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=24-26,72-74 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=39-41,87-89 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=12-14,60-62 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=30-32,78-80 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=27-29,75-77 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=3-5,51-53 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=42-44,90-92 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=42-44,90-92 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=9-11,57-59 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=0-2,48-50 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=3-5,51-53 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=42-44,90-92 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=30-32,78-80 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=12-14,60-62 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=21-23,69-71 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=24-26,72-74 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=27-29,75-77 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=3-5,51-53 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=15-17,63-65 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=18-20,66-68 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=21-23,69-71 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=6-8,54-56 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=39-41,87-89 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=6-8,54-56 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=33-35,81-83 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=42-44,90-92 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=45-47,93-95 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=36-38,84-86 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=12-14,60-62 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=36-38,84-86 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=39-41,87-89 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=0-2,48-50 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=21-23,69-71 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=45-47,93-95 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=30-32,78-80 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=9-11,57-59 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=12-14,60-62 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=21-23,69-71 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=27-29,75-77 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=6-8,54-56 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=9-11,57-59 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=30-32,78-80 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=21-23,69-71 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=42-44,90-92 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=24-26,72-74 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=27-29,75-77 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=18-20,66-68 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=27-29,75-77 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=24-26,72-74 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=36-38,84-86 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=6-8,54-56 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=18-20,66-68 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=3-5,51-53 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=30-32,78-80 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=15-17,63-65 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=24-26,72-74 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=9-11,57-59 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=6-8,54-56 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=27-29,75-77 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=45-47,93-95 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=0-2,48-50 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=15-17,63-65 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=39-41,87-89 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=39-41,87-89 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=0-2,48-50 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=21-23,69-71 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=6-8,54-56 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=42-44,90-92 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=39-41,87-89 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=18-20,66-68 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=45-47,93-95 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=33-35,81-83 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=39-41,87-89 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=36-38,84-86 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=27-29,75-77 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=30-32,78-80 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=12-14,60-62 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=0-2,48-50 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=36-38,84-86 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=12-14,60-62 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=6-8,54-56 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=27-29,75-77 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=12-14,60-62 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=33-35,81-83 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=9-11,57-59 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=33-35,81-83 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=15-17,63-65 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=0-2,48-50 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=15-17,63-65 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=36-38,84-86 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=42-44,90-92 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=3-5,51-53 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=24-26,72-74 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=39-41,87-89 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=30-32,78-80 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=18-20,66-68 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=3-5,51-53 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=30-32,78-80 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=27-29,75-77 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=3-5,51-53 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=18-20,66-68 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=45-47,93-95 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=6-8,54-56 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=42-44,90-92 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=45-47,93-95 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=24-26,72-74 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=24-26,72-74 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=21-23,69-71 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=45-47,93-95 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=18-20,66-68 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=30-32,78-80 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=39-41,87-89 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=18-20,66-68 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=39-41,87-89 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=18-20,66-68 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=36-38,84-86 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=45-47,93-95 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=9-11,57-59 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=9-11,57-59 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=30-32,78-80 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=0-2,48-50 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=15-17,63-65 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=18-20,66-68 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=9-11,57-59 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=39-41,87-89 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=12-14,60-62 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=21-23,69-71 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=3-5,51-53 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=27-29,75-77 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=21-23,69-71 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=30-32,78-80 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=42-44,90-92 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=18-20,66-68 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=24-26,72-74 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=33-35,81-83 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=0-2,48-50 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=33-35,81-83 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=6-8,54-56 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=15-17,63-65 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=3-5,51-53 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=12-14,60-62 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=12-14,60-62 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=30-32,78-80 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=27-29,75-77 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=6-8,54-56 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=21-23,69-71 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=0-2,48-50 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=27-29,75-77 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=33-35,81-83 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=42-44,90-92 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=24-26,72-74 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=45-47,93-95 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=18-20,66-68 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=42-44,90-92 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=15-17,63-65 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=36-38,84-86 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=30-32,78-80 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=12-14,60-62 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=36-38,84-86 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=0-2,48-50 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=33-35,81-83 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=12-14,60-62 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=42-44,90-92 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=24-26,72-74 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=3-5,51-53 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
+ exec numactl --physcpubind=24-26,72-74 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=36-38,84-86 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=6-8,54-56 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=39-41,87-89 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=45-47,93-95 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=36-38,84-86 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=33-35,81-83 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=36-38,84-86 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=15-17,63-65 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=42-44,90-92 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=3-5,51-53 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=21-23,69-71 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=3-5,51-53 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=18-20,66-68 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=24-26,72-74 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=12-14,60-62 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=27-29,75-77 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=39-41,87-89 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=36-38,84-86 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=12-14,60-62 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=18-20,66-68 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=24-26,72-74 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=6-8,54-56 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=9-11,57-59 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=24-26,72-74 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=0-2,48-50 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=15-17,63-65 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=36-38,84-86 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=36-38,84-86 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=45-47,93-95 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=0-2,48-50 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=6-8,54-56 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=6-8,54-56 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=21-23,69-71 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=27-29,75-77 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=18-20,66-68 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=18-20,66-68 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=21-23,69-71 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=15-17,63-65 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=33-35,81-83 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=12-14,60-62 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=39-41,87-89 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=30-32,78-80 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=18-20,66-68 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=9-11,57-59 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=3-5,51-53 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=24-26,72-74 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=30-32,78-80 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=18-20,66-68 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=45-47,93-95 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=39-41,87-89 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=9-11,57-59 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=39-41,87-89 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=15-17,63-65 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=12-14,60-62 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=42-44,90-92 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=36-38,84-86 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=3-5,51-53 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=24-26,72-74 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=33-35,81-83 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=24-26,72-74 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=6-8,54-56 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=9-11,57-59 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=24-26,72-74 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=0-2,48-50 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=45-47,93-95 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=0-2,48-50 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=39-41,87-89 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=0-2,48-50 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=12-14,60-62 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=30-32,78-80 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=36-38,84-86 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=3-5,51-53 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=33-35,81-83 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=0-2,48-50 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=33-35,81-83 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=3-5,51-53 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=45-47,93-95 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=42-44,90-92 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=9-11,57-59 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=27-29,75-77 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=6-8,54-56 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=18-20,66-68 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=15-17,63-65 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=0-2,48-50 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=6-8,54-56 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=33-35,81-83 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=3-5,51-53 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=9-11,57-59 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=12-14,60-62 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=33-35,81-83 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=0-2,48-50 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=6-8,54-56 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=42-44,90-92 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=27-29,75-77 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=30-32,78-80 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=39-41,87-89 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=45-47,93-95 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=9-11,57-59 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=0-2,48-50 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=18-20,66-68 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=6-8,54-56 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=21-23,69-71 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=36-38,84-86 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=24-26,72-74 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=39-41,87-89 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=30-32,78-80 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=9-11,57-59 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=27-29,75-77 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=45-47,93-95 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=18-20,66-68 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=33-35,81-83 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=42-44,90-92 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=9-11,57-59 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=15-17,63-65 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=12-14,60-62 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=15-17,63-65 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=21-23,69-71 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=24-26,72-74 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=36-38,84-86 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=42-44,90-92 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=39-41,87-89 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=27-29,75-77 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=21-23,69-71 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=9-11,57-59 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=6-8,54-56 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=3-5,51-53 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=33-35,81-83 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=30-32,78-80 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=15-17,63-65 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=45-47,93-95 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=9-11,57-59 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=30-32,78-80 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=36-38,84-86 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=39-41,87-89 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=42-44,90-92 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=24-26,72-74 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=36-38,84-86 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=12-14,60-62 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=6-8,54-56 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=9-11,57-59 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=21-23,69-71 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=0-2,48-50 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=27-29,75-77 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=15-17,63-65 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=39-41,87-89 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=36-38,84-86 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=30-32,78-80 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=0-2,48-50 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=6-8,54-56 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=27-29,75-77 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=12-14,60-62 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=33-35,81-83 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=9-11,57-59 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=12-14,60-62 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=39-41,87-89 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=24-26,72-74 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=42-44,90-92 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=21-23,69-71 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=0-2,48-50 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=30-32,78-80 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=18-20,66-68 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=33-35,81-83 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=39-41,87-89 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=6-8,54-56 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=27-29,75-77 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=18-20,66-68 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=15-17,63-65 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=9-11,57-59 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=42-44,90-92 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=27-29,75-77 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=30-32,78-80 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=18-20,66-68 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=12-14,60-62 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=0-2,48-50 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=9-11,57-59 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=21-23,69-71 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=24-26,72-74 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=30-32,78-80 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=33-35,81-83 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=15-17,63-65 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=9-11,57-59 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=36-38,84-86 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=15-17,63-65 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=12-14,60-62 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=27-29,75-77 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=33-35,81-83 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=27-29,75-77 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=39-41,87-89 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=45-47,93-95 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=12-14,60-62 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=0-2,48-50 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=9-11,57-59 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=21-23,69-71 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=42-44,90-92 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=42-44,90-92 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=24-26,72-74 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=27-29,75-77 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=21-23,69-71 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=6-8,54-56 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=6-8,54-56 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=18-20,66-68 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=3-5,51-53 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=15-17,63-65 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=45-47,93-95 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=27-29,75-77 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=36-38,84-86 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=36-38,84-86 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=36-38,84-86 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=39-41,87-89 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=12-14,60-62 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=27-29,75-77 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=42-44,90-92 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=12-14,60-62 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=45-47,93-95 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=12-14,60-62 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=36-38,84-86 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=3-5,51-53 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=39-41,87-89 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=39-41,87-89 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=15-17,63-65 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=3-5,51-53 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=21-23,69-71 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=30-32,78-80 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=12-14,60-62 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=33-35,81-83 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=36-38,84-86 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=33-35,81-83 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=45-47,93-95 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=9-11,57-59 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=33-35,81-83 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=42-44,90-92 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=3-5,51-53 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=30-32,78-80 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=45-47,93-95 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=15-17,63-65 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=6-8,54-56 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=36-38,84-86 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=27-29,75-77 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=18-20,66-68 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=21-23,69-71 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=45-47,93-95 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=39-41,87-89 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=30-32,78-80 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=12-14,60-62 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=0-2,48-50 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=6-8,54-56 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=6-8,54-56 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=15-17,63-65 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=6-8,54-56 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=24-26,72-74 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=0-2,48-50 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=6-8,54-56 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=39-41,87-89 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=27-29,75-77 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=21-23,69-71 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=12-14,60-62 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=18-20,66-68 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=12-14,60-62 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=39-41,87-89 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=21-23,69-71 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=18-20,66-68 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=33-35,81-83 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=9-11,57-59 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=3-5,51-53 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=33-35,81-83 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=42-44,90-92 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=45-47,93-95 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=12-14,60-62 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=6-8,54-56 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=30-32,78-80 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=12-14,60-62 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=36-38,84-86 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=9-11,57-59 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=3-5,51-53 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=45-47,93-95 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=45-47,93-95 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=33-35,81-83 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=39-41,87-89 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=0-2,48-50 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=9-11,57-59 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=21-23,69-71 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=3-5,51-53 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=42-44,90-92 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=27-29,75-77 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=42-44,90-92 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=42-44,90-92 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=18-20,66-68 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=12-14,60-62 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=15-17,63-65 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=24-26,72-74 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=0-2,48-50 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=36-38,84-86 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=6-8,54-56 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=33-35,81-83 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=36-38,84-86 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=3-5,51-53 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=12-14,60-62 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=24-26,72-74 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=42-44,90-92 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=15-17,63-65 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=9-11,57-59 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=39-41,87-89 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=42-44,90-92 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=27-29,75-77 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=42-44,90-92 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=30-32,78-80 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=18-20,66-68 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=6-8,54-56 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=39-41,87-89 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=9-11,57-59 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=42-44,90-92 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=30-32,78-80 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=30-32,78-80 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=45-47,93-95 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=30-32,78-80 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=45-47,93-95 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=39-41,87-89 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=42-44,90-92 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=18-20,66-68 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=27-29,75-77 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=36-38,84-86 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=15-17,63-65 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=3-5,51-53 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=3-5,51-53 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=12-14,60-62 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=45-47,93-95 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=18-20,66-68 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=24-26,72-74 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=27-29,75-77 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=21-23,69-71 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=24-26,72-74 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=15-17,63-65 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=15-17,63-65 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=45-47,93-95 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=18-20,66-68 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=12-14,60-62 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=21-23,69-71 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=42-44,90-92 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=9-11,57-59 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=3-5,51-53 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=36-38,84-86 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=18-20,66-68 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=42-44,90-92 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=18-20,66-68 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=45-47,93-95 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=21-23,69-71 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=6-8,54-56 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=30-32,78-80 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=30-32,78-80 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=24-26,72-74 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=42-44,90-92 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=27-29,75-77 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=21-23,69-71 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=24-26,72-74 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=33-35,81-83 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=45-47,93-95 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=15-17,63-65 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=18-20,66-68 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=15-17,63-65 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=3-5,51-53 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=36-38,84-86 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=24-26,72-74 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=33-35,81-83 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=12-14,60-62 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=30-32,78-80 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=12-14,60-62 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=24-26,72-74 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=45-47,93-95 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=6-8,54-56 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=36-38,84-86 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=45-47,93-95 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=36-38,84-86 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=30-32,78-80 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=33-35,81-83 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=0-2,48-50 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=3-5,51-53 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=33-35,81-83 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=3-5,51-53 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=0-2,48-50 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=9-11,57-59 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=42-44,90-92 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=21-23,69-71 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=6-8,54-56 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=45-47,93-95 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=33-35,81-83 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=39-41,87-89 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=45-47,93-95 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=6-8,54-56 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=39-41,87-89 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=9-11,57-59 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=21-23,69-71 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=27-29,75-77 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=0-2,48-50 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=36-38,84-86 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=27-29,75-77 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=42-44,90-92 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=9-11,57-59 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=21-23,69-71 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=42-44,90-92 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=27-29,75-77 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=30-32,78-80 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=21-23,69-71 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=36-38,84-86 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=15-17,63-65 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=39-41,87-89 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=42-44,90-92 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=24-26,72-74 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=33-35,81-83 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=33-35,81-83 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=45-47,93-95 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=12-14,60-62 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=39-41,87-89 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=0-2,48-50 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=18-20,66-68 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=24-26,72-74 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=27-29,75-77 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=42-44,90-92 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=3-5,51-53 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=21-23,69-71 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=39-41,87-89 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=9-11,57-59 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=18-20,66-68 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=3-5,51-53 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=21-23,69-71 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=42-44,90-92 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=12-14,60-62 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=33-35,81-83 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=21-23,69-71 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=0-2,48-50 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=21-23,69-71 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=45-47,93-95 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=30-32,78-80 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=24-26,72-74 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=0-2,48-50 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=18-20,66-68 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=3-5,51-53 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=30-32,78-80 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=24-26,72-74 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=27-29,75-77 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=12-14,60-62 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=15-17,63-65 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=33-35,81-83 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=0-2,48-50 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=15-17,63-65 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=24-26,72-74 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=27-29,75-77 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=33-35,81-83 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=6-8,54-56 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=9-11,57-59 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=6-8,54-56 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=15-17,63-65 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=18-20,66-68 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=27-29,75-77 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=24-26,72-74 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=45-47,93-95 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=36-38,84-86 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=0-2,48-50 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=27-29,75-77 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=18-20,66-68 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=6-8,54-56 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=9-11,57-59 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=30-32,78-80 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=33-35,81-83 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=18-20,66-68 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=3-5,51-53 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=27-29,75-77 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=21-23,69-71 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=24-26,72-74 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=33-35,81-83 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=9-11,57-59 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=36-38,84-86 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=42-44,90-92 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=12-14,60-62 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=6-8,54-56 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=15-17,63-65 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=45-47,93-95 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=30-32,78-80 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=24-26,72-74 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=15-17,63-65 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=18-20,66-68 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=0-2,48-50 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=42-44,90-92 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=36-38,84-86 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=9-11,57-59 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=3-5,51-53 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=27-29,75-77 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=21-23,69-71 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=6-8,54-56 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=3-5,51-53 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=15-17,63-65 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=0-2,48-50 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=39-41,87-89 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=9-11,57-59 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=30-32,78-80 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=33-35,81-83 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=27-29,75-77 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=39-41,87-89 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=0-2,48-50 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=42-44,90-92 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=33-35,81-83 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=12-14,60-62 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=30-32,78-80 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=21-23,69-71 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=3-5,51-53 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=27-29,75-77 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=9-11,57-59 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=30-32,78-80 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=33-35,81-83 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=6-8,54-56 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=36-38,84-86 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=12-14,60-62 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=21-23,69-71 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=15-17,63-65 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=42-44,90-92 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=18-20,66-68 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=36-38,84-86 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=15-17,63-65 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=45-47,93-95 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=12-14,60-62 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=45-47,93-95 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=21-23,69-71 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=18-20,66-68 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=42-44,90-92 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=30-32,78-80 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=0-2,48-50 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=15-17,63-65 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=36-38,84-86 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=39-41,87-89 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=27-29,75-77 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=30-32,78-80 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=33-35,81-83 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=12-14,60-62 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=21-23,69-71 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=6-8,54-56 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=24-26,72-74 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=3-5,51-53 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=0-2,48-50 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=15-17,63-65 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=45-47,93-95 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=12-14,60-62 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=6-8,54-56 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=33-35,81-83 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=39-41,87-89 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=9-11,57-59 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=36-38,84-86 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=0-2,48-50 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=15-17,63-65 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=24-26,72-74 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=18-20,66-68 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=45-47,93-95 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=15-17,63-65 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=24-26,72-74 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=9-11,57-59 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=42-44,90-92 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=21-23,69-71 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=39-41,87-89 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=21-23,69-71 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=18-20,66-68 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=33-35,81-83 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=36-38,84-86 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=24-26,72-74 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=12-14,60-62 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=21-23,69-71 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=9-11,57-59 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=0-2,48-50 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=30-32,78-80 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=39-41,87-89 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=6-8,54-56 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=15-17,63-65 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=6-8,54-56 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=27-29,75-77 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=3-5,51-53 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=36-38,84-86 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=45-47,93-95 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=45-47,93-95 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=27-29,75-77 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=30-32,78-80 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=3-5,51-53 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=42-44,90-92 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=12-14,60-62 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=30-32,78-80 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=24-26,72-74 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=33-35,81-83 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=27-29,75-77 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=39-41,87-89 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=30-32,78-80 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=6-8,54-56 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=3-5,51-53 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=24-26,72-74 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=0-2,48-50 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=21-23,69-71 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=15-17,63-65 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=15-17,63-65 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
num_sockets = 2 num_nodes=2 cores_per_socket=24
+ exec numactl --physcpubind=9-11,57-59 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
:::MLLOG {"namespace": "", "time_ms": 1592880043898, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
+ exec numactl --physcpubind=42-44,90-92 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
:::MLLOG {"namespace": "", "time_ms": 1592880043899, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
+ exec numactl --physcpubind=36-38,84-86 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
:::MLLOG {"namespace": "", "time_ms": 1592880043917, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
+ exec numactl --physcpubind=12-14,60-62 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
:::MLLOG {"namespace": "", "time_ms": 1592880043938, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
+ exec numactl --physcpubind=18-20,66-68 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
:::MLLOG {"namespace": "", "time_ms": 1592880043938, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
+ exec numactl --physcpubind=33-35,81-83 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
:::MLLOG {"namespace": "", "time_ms": 1592880043943, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
+ exec numactl --physcpubind=0-2,48-50 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
:::MLLOG {"namespace": "", "time_ms": 1592880043959, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
+ exec numactl --physcpubind=33-35,81-83 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
:::MLLOG {"namespace": "", "time_ms": 1592880043971, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
+ exec numactl --physcpubind=27-29,75-77 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
:::MLLOG {"namespace": "", "time_ms": 1592880043974, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
+ exec numactl --physcpubind=42-44,90-92 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
:::MLLOG {"namespace": "", "time_ms": 1592880043976, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
+ exec numactl --physcpubind=24-26,72-74 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
:::MLLOG {"namespace": "", "time_ms": 1592880043981, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
+ exec numactl --physcpubind=39-41,87-89 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
:::MLLOG {"namespace": "", "time_ms": 1592880043988, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
+ exec numactl --physcpubind=18-20,66-68 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
:::MLLOG {"namespace": "", "time_ms": 1592880043992, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
+ exec numactl --physcpubind=3-5,51-53 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
:::MLLOG {"namespace": "", "time_ms": 1592880043990, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
+ exec numactl --physcpubind=6-8,54-56 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
:::MLLOG {"namespace": "", "time_ms": 1592880043998, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
+ exec numactl --physcpubind=0-2,48-50 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
:::MLLOG {"namespace": "", "time_ms": 1592880044001, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
+ exec numactl --physcpubind=36-38,84-86 --membind=1 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
:::MLLOG {"namespace": "", "time_ms": 1592880044003, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
+ exec numactl --physcpubind=12-14,60-62 --membind=0 -- python -u train.py --save gnmt_wmt16 --dataset-dir /data --preproc-data-dir /preproc_data/75 --target-bleu 24.0 --epochs 14 --math fp16 --max-length-train 75 --print-freq 10 --train-batch-size 16 --test-batch-size 4 --optimizer FusedAdam --lr 5.0e-3 --warmup-steps 200 --remain-steps 1605 --decay-interval 201 --fused-attention --fused-xentropy --prealloc-mode=once --no-log-all-ranks --enable-apex-allreduce-overlap --apex-message-size 32134259 --apex-num-allreduce-streams 1
:::MLLOG {"namespace": "", "time_ms": 1592880044003, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044005, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044007, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044010, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044014, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044015, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044018, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044020, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044021, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044024, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044026, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044027, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044028, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044031, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044039, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044039, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044043, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044043, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044041, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044047, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044060, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044062, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044063, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044069, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044069, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044075, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044073, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044079, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044083, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044082, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044083, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044083, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044084, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044090, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044091, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044094, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044101, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044102, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044105, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044105, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044106, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044108, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044110, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044124, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044124, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044125, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044130, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044132, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044134, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044135, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044135, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044136, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044137, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044138, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044141, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044145, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044148, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044148, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044148, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044148, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044152, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044152, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044154, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044156, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044156, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044165, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044168, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044170, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044173, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044173, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044179, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044188, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044191, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044194, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044196, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044200, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044205, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044206, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044204, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044207, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044205, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044208, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044212, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044210, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044217, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044218, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044224, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044223, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044235, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044242, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044246, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044250, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044254, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044256, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044259, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044280, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044284, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044284, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044289, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044295, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044303, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044307, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044308, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044310, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044308, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044313, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044316, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044318, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044327, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044331, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044340, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044347, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044347, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044372, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044376, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044384, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044387, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044392, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044396, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044397, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044399, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044400, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044402, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044414, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044420, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044421, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044424, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044425, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044427, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044429, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044431, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044453, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044452, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044466, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044481, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044485, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044493, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044494, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044498, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044501, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044514, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044514, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044520, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044523, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044524, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044525, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044526, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044526, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044529, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044529, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044530, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044533, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044538, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044542, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044543, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044546, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044553, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044567, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044573, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044575, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044580, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044583, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044586, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044589, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044592, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044593, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044593, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044594, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044605, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044606, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044608, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044611, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044611, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044614, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044618, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044625, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044628, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044630, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044632, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044633, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044635, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044635, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044637, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044637, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044638, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044637, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044638, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044639, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044645, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044643, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044645, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044647, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044648, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044648, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044650, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044650, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044653, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044654, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044651, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044652, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044652, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044653, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044653, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044653, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044656, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044657, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044657, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044655, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044656, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044658, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044660, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044660, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044660, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044661, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044659, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044663, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044662, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044663, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044664, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044664, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044663, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044667, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044666, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044667, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044667, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044666, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044667, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044666, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044666, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044667, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044671, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044670, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044670, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044671, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044670, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044670, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044673, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044673, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044674, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044672, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044672, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044673, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044674, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044673, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044672, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044672, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044675, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044676, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044675, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044676, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044676, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044674, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044676, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044678, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044678, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044677, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044677, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044680, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044678, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044678, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044680, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044679, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044681, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044682, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044681, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044683, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044683, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044683, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044681, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044683, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044684, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044685, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044685, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044684, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044685, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044687, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044687, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044685, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044686, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044687, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044688, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044688, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044689, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044690, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044690, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044689, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044690, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044688, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044691, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044692, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044691, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044690, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044690, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044693, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044692, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044693, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044691, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044693, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044693, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044693, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044694, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044694, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044696, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044695, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044697, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044695, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044696, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044697, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044700, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044699, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044701, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044698, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044699, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044702, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044701, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044701, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044704, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044699, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044702, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044703, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044704, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044703, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044703, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044705, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044704, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044702, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044705, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044706, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044706, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044703, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044705, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044705, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044705, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044708, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044708, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044709, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044709, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044707, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044709, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044710, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044709, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044708, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044709, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044711, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044709, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044711, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044710, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044710, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044713, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044712, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044713, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044712, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044713, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044712, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044716, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044714, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044714, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044715, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044715, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044716, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044717, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044717, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044716, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044717, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044717, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044718, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044720, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044718, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044718, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044720, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044719, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044720, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044719, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044719, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044719, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044721, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044721, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044721, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044722, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044722, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044723, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044725, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044724, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044724, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044724, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044726, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044726, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044728, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044726, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044725, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044726, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044726, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044728, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044728, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044730, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044730, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044731, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044730, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044729, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044731, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044733, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044733, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044732, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044733, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044733, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044732, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044734, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044735, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044735, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044734, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044737, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044736, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044736, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044737, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044739, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044740, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044736, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044741, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044741, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044742, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044741, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044740, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044743, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044742, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044741, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044742, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044743, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044744, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044742, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044745, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044743, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044745, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044745, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044744, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044745, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044745, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044746, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044747, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044747, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044749, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044748, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044749, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044749, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044748, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044750, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044749, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044747, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044750, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044750, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044751, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044751, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044753, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044752, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044755, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044753, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044755, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044753, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044753, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044754, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044755, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044754, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044756, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044755, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044756, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044756, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044757, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044758, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044758, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044760, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044760, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044762, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044761, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044761, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044762, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044764, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044764, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044765, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044762, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044765, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044763, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044766, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044765, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044766, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044766, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044768, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044766, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044768, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044770, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044770, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044768, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044767, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044770, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044769, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044769, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044770, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044770, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044772, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044770, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044770, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044769, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044770, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044768, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044770, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044770, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044772, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044772, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044773, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044774, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044774, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044774, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044774, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044777, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044776, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044776, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044776, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044775, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044776, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044778, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044778, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044778, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044778, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044779, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044780, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044780, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044781, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044781, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044782, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044781, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044784, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044783, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044783, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044782, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044783, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044785, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044784, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044784, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044785, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044784, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044785, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044784, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044787, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044786, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044787, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044788, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044788, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044788, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044789, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044788, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044789, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044790, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044789, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044787, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044789, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044790, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044790, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044789, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044788, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044792, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044791, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044791, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044792, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044791, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044793, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044793, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044793, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044795, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044793, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044793, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044796, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044795, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044796, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044795, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044796, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044795, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044797, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044797, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044795, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044797, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044797, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044797, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044798, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044797, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044797, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044798, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044799, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044799, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044800, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044801, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044800, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044800, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044798, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044798, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044801, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044800, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044803, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044800, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044800, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044801, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044800, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044802, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044801, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044801, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044803, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044803, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044805, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044806, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044803, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044805, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044805, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044804, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044807, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044804, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044804, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044806, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044805, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044809, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044805, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044805, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044806, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044807, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044809, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044807, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044807, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044808, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044809, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044808, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044810, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044809, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044809, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044810, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044810, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044809, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044812, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044813, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044813, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044812, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044812, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044812, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044813, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044812, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044815, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044812, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044812, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044812, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044813, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044813, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044813, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044813, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044812, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044813, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044814, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044815, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044816, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044814, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044816, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044817, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044814, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044816, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044817, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044818, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044818, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044817, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044819, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044817, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044818, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044817, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044820, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044819, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044818, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044818, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044819, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044818, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044820, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044821, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044821, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044820, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044821, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044821, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044821, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044821, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044823, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044822, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044822, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044821, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044821, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044822, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044824, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044825, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044826, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044825, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044824, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044823, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044826, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044826, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044825, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044825, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044826, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044827, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044828, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044826, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044827, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044826, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044826, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044826, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044827, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044828, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044829, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044828, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044827, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044828, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044829, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044829, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044830, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044829, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044829, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044831, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044830, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044830, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044830, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044831, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044833, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044833, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044831, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044832, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044833, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044833, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044833, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044834, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044832, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044836, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044834, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044835, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044834, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044834, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044835, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044836, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044838, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044839, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044837, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044837, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044839, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044836, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044838, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044838, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044840, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044841, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044839, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044839, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044839, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044840, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044841, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044840, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044840, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044840, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044840, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044843, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044843, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044843, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044844, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044842, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044845, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044846, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044844, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044845, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044846, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044847, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044847, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044847, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044846, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044845, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044845, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044847, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044846, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044846, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044847, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044846, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044847, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044848, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044835, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044850, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044851, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044848, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044849, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044851, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044852, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044852, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044851, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044852, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044852, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044855, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044856, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044855, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044854, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044856, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044855, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044856, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044857, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044856, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044857, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044859, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044859, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044858, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044862, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044858, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044858, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044857, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044859, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044859, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044860, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044862, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044862, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044860, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044860, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044861, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044860, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044860, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044862, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044864, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044863, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044864, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044863, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044863, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044863, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044864, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044866, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044863, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044864, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044868, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044867, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044867, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044867, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044864, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044867, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044868, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044869, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044868, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044870, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044869, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044870, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044869, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044871, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044870, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044872, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044873, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044873, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044874, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044874, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044874, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044873, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044873, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044874, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044875, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044875, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044874, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044873, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044874, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044874, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044874, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044875, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044873, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044876, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044875, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044877, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044877, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044875, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044878, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044879, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044879, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044880, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044879, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044880, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044879, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044879, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044881, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044880, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044881, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044882, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044885, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044882, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044886, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044886, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044887, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044887, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044888, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044888, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044888, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044890, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044888, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044890, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044890, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044891, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044891, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044892, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044891, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044893, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044891, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044891, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044892, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044894, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044893, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044895, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044895, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044895, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044894, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044898, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044898, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044898, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044899, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044898, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044898, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044900, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044900, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044901, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044902, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044903, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044903, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044903, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044905, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044906, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044905, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044904, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044907, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044906, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044907, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044907, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044907, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044909, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044909, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044909, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044910, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044910, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044912, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044912, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044915, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044914, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044916, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044916, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044918, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044927, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044935, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044941, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044942, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044944, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044944, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044945, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044946, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044961, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044961, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044961, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044963, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044967, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044968, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044969, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044976, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044976, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044985, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044985, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044991, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044990, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044991, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044994, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044995, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044997, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880044996, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880045002, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880045002, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880045002, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880045003, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880045003, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880045005, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880045005, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880045009, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880045008, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880045012, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880045012, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880045014, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880045016, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880045017, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880045017, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880045018, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880045018, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880045020, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880045023, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880045025, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880045023, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880045025, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880045028, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880045030, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880045031, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880045033, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880045036, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880045037, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880045037, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880045040, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880045041, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880045047, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880045048, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880045048, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880045052, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880045052, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880045053, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880045051, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880045054, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880045062, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880045065, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880045066, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
:::MLLOG {"namespace": "", "time_ms": 1592880045071, "event_type": "INTERVAL_START", "key": "init_start", "value": null, "metadata": {"file": "train.py", "lineno": 303}}
NCCL version 2.7.5+cuda11.0
0: Saving results to: results/gnmt_wmt16
0: Run arguments: Namespace(apex_message_size=32134259, apex_num_allreduce_streams=1, batching='bucketing', beam_size=5, cov_penalty_factor=0.1, cuda=True, cudnn=True, dataset_dir='/data', decay_factor=0.5, decay_interval=201, decay_steps=4, distributed_weight_update=0, dropout=0.2, dwu_e5m2_allgather=False, dwu_full_pipeline=False, dwu_grad_norm=False, dwu_group_size=0, dwu_num_ag_pg=2, dwu_num_ar_pg=4, dwu_num_blocks=8, dwu_num_chunks=4, dwu_num_rs_pg=2, dwu_overlap_reductions=False, enable_apex_allreduce_overlap=True, env=False, epochs=14, eval=True, fused_attention=True, fused_xentropy=True, grad_clip=5.0, hidden_size=1024, init_scale=1024, intra_epoch_eval=0, keep_checkpoints=0, len_norm_const=5.0, len_norm_factor=0.6, local_rank=0, log_all_ranks=False, lr=0.005, math='fp16', max_length_test=150, max_length_train=75, max_size=None, min_length_test=0, min_length_train=0, num_buckets=5, num_layers=4, optimizer='FusedAdam', optimizer_extra='{}', prealloc_mode='once', preproc_data_dir='/preproc_data/75', print_freq=10, rank=0, remain_steps=1605, results_dir='results', resume=None, save='gnmt_wmt16', save_all=False, save_freq=5000, save_path='results/gnmt_wmt16', seed=None, shard_size=80, share_embedding=True, smoothing=0.1, start_epoch=0, target_bleu=24.0, test_batch_size=4, test_loader_workers=0, train_batch_size=16, train_global_batch_size=None, train_iter_size=1, train_loader_workers=1, upscale_interval=128, use_preproc_data=True, warmup_steps=200)
0: L2 promotion: 128B
0: Using random master seed: 947987409
:::MLLOG {"namespace": "", "time_ms": 1592880068369, "event_type": "POINT_IN_TIME", "key": "seed", "value": 947987409, "metadata": {"file": "/workspace/rnn_translator/seq2seq/utils.py", "lineno": 144}}
0: Worker 0 is using worker seed: 491453415
0: Building vocabulary from /data/vocab.bpe.32000
0: Size of vocabulary: 32320
0: Fused attention flag set to True
0: GNMT(
  (embedder): Embedding(32320, 1024, padding_idx=0)
  (encoder): ResidualRecurrentEncoder(
    (rnn_layers): ModuleList(
      (0): EmuBidirLSTM(
        (bidir): LSTM(1024, 1024, bidirectional=True)
        (layer1): LSTM(1024, 1024)
        (layer2): LSTM(1024, 1024)
      )
      (1): LSTM(2048, 1024)
      (2): LSTM(1024, 1024)
      (3): LSTM(1024, 1024)
    )
    (dropout): Dropout(p=0.2, inplace=False)
    (embedder): Embedding(32320, 1024, padding_idx=0)
  )
  (decoder): ResidualRecurrentDecoder(
    (att_rnn): RecurrentAttention(
      (rnn): LSTM(1024, 1024)
      (attn): BahdanauAttention(
        (linear_q): Linear(in_features=1024, out_features=1024, bias=False)
        (linear_k): Linear(in_features=1024, out_features=1024, bias=False)
      )
      (dropout): Dropout(p=0.2, inplace=False)
    )
    (rnn_layers): ModuleList(
      (0): LSTM(2048, 1024)
      (1): LSTM(2048, 1024)
      (2): LSTM(2048, 1024)
    )
    (embedder): Embedding(32320, 1024, padding_idx=0)
    (classifier): Classifier(
      (classifier): Linear(in_features=1024, out_features=32320, bias=True)
    )
    (dropout): Dropout(p=0.2, inplace=False)
  )
)
0: Building LabelSmoothingLoss (smoothing: 0.1)
0: Fused xentropy flag set to True
0: Training optimizer config: {'optimizer': 'FusedAdam', 'lr': 0.005}
0: Number of parameters: 160671297
0: Saving state of the tokenizer
0: Initializing fp16 optimizer with multi-tenosr apply
0: Initializing fp32 clone weights
0: Using optimizer: FusedAdam (
Parameter Group 0
    betas: (0.9, 0.999)
    bias_correction: True
    eps: 1e-08
    lr: 0.005
    max_grad_norm: 0.0
    weight_decay: 0.0
)
:::MLLOG {"namespace": "", "time_ms": 1592880071831, "event_type": "POINT_IN_TIME", "key": "opt_name", "value": "adam", "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/trainer.py", "lineno": 176}}
:::MLLOG {"namespace": "", "time_ms": 1592880071832, "event_type": "POINT_IN_TIME", "key": "opt_base_learning_rate", "value": 0.005, "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/trainer.py", "lineno": 178}}
:::MLLOG {"namespace": "", "time_ms": 1592880071832, "event_type": "POINT_IN_TIME", "key": "opt_adam_beta_1", "value": 0.9, "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/trainer.py", "lineno": 180}}
:::MLLOG {"namespace": "", "time_ms": 1592880071832, "event_type": "POINT_IN_TIME", "key": "opt_adam_beta_2", "value": 0.999, "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/trainer.py", "lineno": 182}}
:::MLLOG {"namespace": "", "time_ms": 1592880071832, "event_type": "POINT_IN_TIME", "key": "opt_adam_epsilon", "value": 1e-08, "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/trainer.py", "lineno": 184}}
0: Executing preallocation
:::MLLOG {"namespace": "", "time_ms": 1592880075096, "event_type": "INTERVAL_END", "key": "init_stop", "value": null, "metadata": {"file": "train.py", "lineno": 429}}
:::MLLOG {"namespace": "", "time_ms": 1592880076273, "event_type": "INTERVAL_START", "key": "run_start", "value": null, "metadata": {"file": "train.py", "lineno": 430}}
:::MLLOG {"namespace": "", "time_ms": 1592880076273, "event_type": "POINT_IN_TIME", "key": "max_sequence_length", "value": 75, "metadata": {"file": "train.py", "lineno": 435, "method": "discard"}}
0: Reading preprocessed data file from /preproc_data/75/training.bin
0: Preprocessed data: length: 3975116 min length: 0 max length: 75
0: Opening preprocessed data /preproc_data/75/training.bin for reading
0: Processing data from /data/newstest2014.tok.bpe.32000.en
0: Filtering data, min len: 0, max len: 150
0: Pairs before: 3003, after: 3003
:::MLLOG {"namespace": "", "time_ms": 1592880076519, "event_type": "POINT_IN_TIME", "key": "global_batch_size", "value": 16384, "metadata": {"file": "train.py", "lineno": 480}}
:::MLLOG {"namespace": "", "time_ms": 1592880076520, "event_type": "POINT_IN_TIME", "key": "train_samples", "value": 3948544, "metadata": {"file": "train.py", "lineno": 488}}
:::MLLOG {"namespace": "", "time_ms": 1592880076521, "event_type": "POINT_IN_TIME", "key": "eval_samples", "value": 3003, "metadata": {"file": "train.py", "lineno": 490}}
0: Training LR schedule config: {'warmup_steps': 200, 'remain_steps': 1605, 'decay_interval': 201, 'decay_steps': 4, 'decay_factor': 0.5}
0: Scheduler warmup steps: 200
0: Scheduler remain steps: 1605
0: Scheduler decay interval: 201
0: Scheduler decay factor: 0.5
0: Scheduler max decay steps: 4
:::MLLOG {"namespace": "", "time_ms": 1592880076521, "event_type": "POINT_IN_TIME", "key": "opt_learning_rate_alt_decay_func", "value": true, "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/lr_scheduler.py", "lineno": 78}}
:::MLLOG {"namespace": "", "time_ms": 1592880076521, "event_type": "POINT_IN_TIME", "key": "opt_learning_rate_alt_warmup_func", "value": true, "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/lr_scheduler.py", "lineno": 80}}
:::MLLOG {"namespace": "", "time_ms": 1592880076521, "event_type": "POINT_IN_TIME", "key": "opt_learning_rate_decay_interval", "value": 201, "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/lr_scheduler.py", "lineno": 83}}
:::MLLOG {"namespace": "", "time_ms": 1592880076521, "event_type": "POINT_IN_TIME", "key": "opt_learning_rate_decay_factor", "value": 0.5, "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/lr_scheduler.py", "lineno": 85}}
:::MLLOG {"namespace": "", "time_ms": 1592880076521, "event_type": "POINT_IN_TIME", "key": "opt_learning_rate_decay_steps", "value": 4, "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/lr_scheduler.py", "lineno": 87}}
:::MLLOG {"namespace": "", "time_ms": 1592880076522, "event_type": "POINT_IN_TIME", "key": "opt_learning_rate_remain_steps", "value": 1605, "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/lr_scheduler.py", "lineno": 89}}
:::MLLOG {"namespace": "", "time_ms": 1592880076522, "event_type": "POINT_IN_TIME", "key": "opt_learning_rate_warmup_steps", "value": 200, "metadata": {"file": "/workspace/rnn_translator/seq2seq/train/lr_scheduler.py", "lineno": 91}}
:::MLLOG {"namespace": "", "time_ms": 1592880076522, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "train.py", "lineno": 538, "first_epoch_num": 1, "epoch_count": 1}}
:::MLLOG {"namespace": "", "time_ms": 1592880076522, "event_type": "INTERVAL_START", "key": "epoch_start", "value": null, "metadata": {"file": "train.py", "lineno": 541, "epoch_num": 1}}
0: Starting epoch 0
0: Sampler for epoch 0 uses seed 2847218072
0: Gradient norm: inf
0: Skipped batch, new scale: 512.0
0: TRAIN [0][0/241]	Time 0.380 (0.380)	Data 2.72e-01 (2.72e-01)	Tok/s 3804 (3804)	Loss/tok 10.6709 (10.6709)	LR 5.000e-05
0: Gradient norm: inf
0: Skipped batch, new scale: 256.0
0: Gradient norm: inf
0: Skipped batch, new scale: 128.0
0: Gradient norm: inf
0: Skipped batch, new scale: 64.0
0: Gradient norm: inf
0: Skipped batch, new scale: 32.0
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:123: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
0: Gradient norm: inf
0: Skipped batch, new scale: 16.0
0: TRAIN [0][10/241]	Time 0.028 (0.095)	Data 8.44e-05 (2.48e-02)	Tok/s 35220 (19892)	Loss/tok 10.0045 (10.3495)	LR 5.610e-05
0: TRAIN [0][20/241]	Time 0.025 (0.063)	Data 8.49e-05 (1.30e-02)	Tok/s 25701 (24035)	Loss/tok 9.1321 (9.9267)	LR 7.063e-05
0: TRAIN [0][30/241]	Time 0.028 (0.053)	Data 6.58e-05 (8.83e-03)	Tok/s 36436 (27638)	Loss/tok 8.8203 (9.5550)	LR 8.891e-05
0: Gradient norm: inf
0: Skipped batch, new scale: 8.0
0: TRAIN [0][40/241]	Time 0.028 (0.046)	Data 8.80e-05 (6.70e-03)	Tok/s 38761 (29062)	Loss/tok 8.4439 (9.3205)	LR 1.094e-04
0: Gradient norm: inf
0: Skipped batch, new scale: 4.0
0: Gradient norm: inf
0: Skipped batch, new scale: 2.0
0: TRAIN [0][50/241]	Time 0.022 (0.043)	Data 6.34e-05 (5.40e-03)	Tok/s 15087 (29174)	Loss/tok 7.7723 (9.1897)	LR 1.315e-04
0: Gradient norm: inf
0: Skipped batch, new scale: 1.0
0: TRAIN [0][60/241]	Time 0.025 (0.040)	Data 6.99e-05 (4.53e-03)	Tok/s 24643 (29309)	Loss/tok 8.0949 (9.0594)	LR 1.618e-04
0: TRAIN [0][70/241]	Time 0.032 (0.038)	Data 6.10e-05 (3.90e-03)	Tok/s 45751 (30333)	Loss/tok 8.3516 (8.9050)	LR 2.037e-04
0: TRAIN [0][80/241]	Time 0.025 (0.037)	Data 6.34e-05 (3.42e-03)	Tok/s 25918 (30399)	Loss/tok 7.8160 (8.7984)	LR 2.564e-04
0: TRAIN [0][90/241]	Time 0.025 (0.036)	Data 6.37e-05 (3.06e-03)	Tok/s 28336 (30415)	Loss/tok 7.9765 (8.7112)	LR 3.228e-04
0: TRAIN [0][100/241]	Time 0.025 (0.035)	Data 6.06e-05 (2.76e-03)	Tok/s 26107 (30924)	Loss/tok 7.9166 (8.6202)	LR 4.064e-04
0: TRAIN [0][110/241]	Time 0.025 (0.034)	Data 6.25e-05 (2.52e-03)	Tok/s 24661 (30916)	Loss/tok 7.8442 (8.5469)	LR 5.116e-04
0: TRAIN [0][120/241]	Time 0.025 (0.034)	Data 6.20e-05 (2.32e-03)	Tok/s 27053 (30783)	Loss/tok 7.6350 (8.4881)	LR 6.441e-04
0: TRAIN [0][130/241]	Time 0.025 (0.033)	Data 7.30e-05 (2.14e-03)	Tok/s 27541 (31214)	Loss/tok 7.4486 (8.4172)	LR 8.109e-04
0: TRAIN [0][140/241]	Time 0.028 (0.033)	Data 9.11e-05 (2.00e-03)	Tok/s 37847 (31418)	Loss/tok 7.5183 (8.3475)	LR 1.021e-03
0: TRAIN [0][150/241]	Time 0.025 (0.032)	Data 8.54e-05 (1.87e-03)	Tok/s 26970 (31517)	Loss/tok 7.3018 (8.2704)	LR 1.285e-03
0: TRAIN [0][160/241]	Time 0.025 (0.032)	Data 6.34e-05 (1.76e-03)	Tok/s 24490 (31427)	Loss/tok 6.7551 (8.1984)	LR 1.618e-03
0: TRAIN [0][170/241]	Time 0.028 (0.032)	Data 8.65e-05 (1.66e-03)	Tok/s 36492 (31556)	Loss/tok 6.7652 (8.1238)	LR 2.037e-03
0: Upscaling, new scale: 2.0
0: TRAIN [0][180/241]	Time 0.028 (0.031)	Data 7.18e-05 (1.57e-03)	Tok/s 37146 (31682)	Loss/tok 6.4355 (8.0463)	LR 2.564e-03
0: TRAIN [0][190/241]	Time 0.025 (0.031)	Data 6.20e-05 (1.49e-03)	Tok/s 25694 (31617)	Loss/tok 6.7179 (7.9820)	LR 3.228e-03
0: TRAIN [0][200/241]	Time 0.025 (0.031)	Data 6.18e-05 (1.42e-03)	Tok/s 25338 (31446)	Loss/tok 6.6044 (7.9172)	LR 4.064e-03
0: TRAIN [0][210/241]	Time 0.025 (0.031)	Data 6.56e-05 (1.36e-03)	Tok/s 26257 (31702)	Loss/tok 5.6950 (7.8296)	LR 5.000e-03
0: TRAIN [0][220/241]	Time 0.032 (0.031)	Data 7.68e-05 (1.30e-03)	Tok/s 47682 (31703)	Loss/tok 6.5168 (7.7599)	LR 5.000e-03
0: TRAIN [0][230/241]	Time 0.025 (0.030)	Data 6.65e-05 (1.25e-03)	Tok/s 26760 (31695)	Loss/tok 5.7847 (7.6868)	LR 5.000e-03
0: TRAIN [0][240/241]	Time 0.025 (0.030)	Data 4.39e-05 (1.20e-03)	Tok/s 27572 (31873)	Loss/tok 5.2854 (7.5961)	LR 5.000e-03
:::MLLOG {"namespace": "", "time_ms": 1592880083884, "event_type": "INTERVAL_END", "key": "epoch_stop", "value": null, "metadata": {"file": "train.py", "lineno": 551, "epoch_num": 1}}
:::MLLOG {"namespace": "", "time_ms": 1592880083884, "event_type": "INTERVAL_START", "key": "eval_start", "value": null, "metadata": {"file": "train.py", "lineno": 556, "epoch_num": 1}}
0: Running evaluation on test set
0: TEST [0][0/1]	Time 0.339 (0.339)	Decoder iters 149.0 (149.0)	Tok/s 1374 (1374)
0: Running moses detokenizer
0: BLEU(score=1.00903804643767, counts=[12233, 1245, 196, 45], totals=[63391, 60388, 57388, 54391], precisions=[19.297692101402408, 2.061667881035967, 0.34153481564090055, 0.082734275891232], bp=0.979933059394988, sys_len=63391, ref_len=64676)
0: Finished evaluation on test set
:::MLLOG {"namespace": "", "time_ms": 1592880084384, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.0101, "metadata": {"file": "train.py", "lineno": 562, "epoch_num": 1}}
:::MLLOG {"namespace": "", "time_ms": 1592880084384, "event_type": "INTERVAL_END", "key": "eval_stop", "value": null, "metadata": {"file": "train.py", "lineno": 565, "epoch_num": 1}}
0: Summary: Epoch: 0	Training Loss: 7.6196	Test BLEU: 1.01
0: Performance: Epoch: 0	Training: 32612866 Tok/s
0: Finished epoch 0
:::MLLOG {"namespace": "", "time_ms": 1592880084384, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "train.py", "lineno": 584, "first_epoch_num": 1}}
:::MLLOG {"namespace": "", "time_ms": 1592880084385, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "train.py", "lineno": 538, "first_epoch_num": 2, "epoch_count": 1}}
:::MLLOG {"namespace": "", "time_ms": 1592880084385, "event_type": "INTERVAL_START", "key": "epoch_start", "value": null, "metadata": {"file": "train.py", "lineno": 541, "epoch_num": 2}}
0: Starting epoch 1
0: Sampler for epoch 1 uses seed 2815393542
0: TRAIN [1][0/241]	Time 0.367 (0.367)	Data 2.84e-01 (2.84e-01)	Tok/s 1850 (1850)	Loss/tok 5.3211 (5.3211)	LR 5.000e-03
0: TRAIN [1][10/241]	Time 0.025 (0.058)	Data 7.75e-05 (2.59e-02)	Tok/s 25908 (28611)	Loss/tok 5.1226 (5.4271)	LR 5.000e-03
0: TRAIN [1][20/241]	Time 0.028 (0.043)	Data 6.65e-05 (1.36e-02)	Tok/s 38598 (30685)	Loss/tok 5.7247 (5.4107)	LR 5.000e-03
0: TRAIN [1][30/241]	Time 0.022 (0.037)	Data 6.44e-05 (9.24e-03)	Tok/s 15950 (30435)	Loss/tok 4.6270 (5.3004)	LR 5.000e-03
0: TRAIN [1][40/241]	Time 0.022 (0.035)	Data 6.10e-05 (7.00e-03)	Tok/s 15333 (30570)	Loss/tok 3.7281 (5.2222)	LR 5.000e-03
0: TRAIN [1][50/241]	Time 0.028 (0.033)	Data 6.65e-05 (5.64e-03)	Tok/s 37746 (30986)	Loss/tok 4.8105 (5.1582)	LR 5.000e-03
0: TRAIN [1][60/241]	Time 0.025 (0.032)	Data 6.29e-05 (4.73e-03)	Tok/s 29402 (31075)	Loss/tok 4.6026 (5.0985)	LR 5.000e-03
0: Upscaling, new scale: 4.0
0: TRAIN [1][70/241]	Time 0.036 (0.032)	Data 6.06e-05 (4.07e-03)	Tok/s 52630 (31748)	Loss/tok 5.1471 (5.0452)	LR 5.000e-03
0: TRAIN [1][80/241]	Time 0.028 (0.031)	Data 6.72e-05 (3.58e-03)	Tok/s 35848 (32408)	Loss/tok 4.8048 (4.9786)	LR 5.000e-03
0: TRAIN [1][90/241]	Time 0.028 (0.031)	Data 6.56e-05 (3.19e-03)	Tok/s 36688 (31901)	Loss/tok 4.1763 (4.9060)	LR 5.000e-03
0: TRAIN [1][100/241]	Time 0.028 (0.030)	Data 6.91e-05 (2.88e-03)	Tok/s 35597 (31824)	Loss/tok 4.4171 (4.8361)	LR 5.000e-03
0: TRAIN [1][110/241]	Time 0.025 (0.030)	Data 6.99e-05 (2.63e-03)	Tok/s 25397 (31535)	Loss/tok 3.8690 (4.7766)	LR 5.000e-03
0: TRAIN [1][120/241]	Time 0.025 (0.029)	Data 7.13e-05 (2.42e-03)	Tok/s 25814 (31150)	Loss/tok 3.8239 (4.7204)	LR 5.000e-03
0: TRAIN [1][130/241]	Time 0.025 (0.029)	Data 6.56e-05 (2.24e-03)	Tok/s 26145 (31278)	Loss/tok 3.6294 (4.6727)	LR 5.000e-03
0: TRAIN [1][140/241]	Time 0.025 (0.029)	Data 6.60e-05 (2.09e-03)	Tok/s 27551 (31691)	Loss/tok 3.7396 (4.6299)	LR 5.000e-03
0: TRAIN [1][150/241]	Time 0.032 (0.029)	Data 6.34e-05 (1.95e-03)	Tok/s 43012 (31569)	Loss/tok 4.0780 (4.5893)	LR 5.000e-03
0: TRAIN [1][160/241]	Time 0.023 (0.029)	Data 6.41e-05 (1.83e-03)	Tok/s 14117 (31748)	Loss/tok 2.9594 (4.5507)	LR 5.000e-03
0: TRAIN [1][170/241]	Time 0.032 (0.029)	Data 6.44e-05 (1.73e-03)	Tok/s 44860 (32078)	Loss/tok 4.1587 (4.5233)	LR 5.000e-03
0: TRAIN [1][180/241]	Time 0.033 (0.029)	Data 7.03e-05 (1.64e-03)	Tok/s 43445 (32428)	Loss/tok 4.3452 (4.4882)	LR 5.000e-03
0: TRAIN [1][190/241]	Time 0.028 (0.029)	Data 6.44e-05 (1.56e-03)	Tok/s 38716 (32392)	Loss/tok 3.9355 (4.4621)	LR 5.000e-03
0: Upscaling, new scale: 8.0
0: TRAIN [1][200/241]	Time 0.036 (0.029)	Data 6.46e-05 (1.48e-03)	Tok/s 51105 (32667)	Loss/tok 4.2267 (4.4276)	LR 5.000e-03
0: TRAIN [1][210/241]	Time 0.025 (0.029)	Data 6.25e-05 (1.42e-03)	Tok/s 27931 (32516)	Loss/tok 3.7104 (4.3998)	LR 5.000e-03
0: TRAIN [1][220/241]	Time 0.028 (0.029)	Data 7.96e-05 (1.35e-03)	Tok/s 37748 (32512)	Loss/tok 3.8213 (4.3722)	LR 5.000e-03
0: TRAIN [1][230/241]	Time 0.022 (0.029)	Data 6.37e-05 (1.30e-03)	Tok/s 16102 (32490)	Loss/tok 2.7153 (4.3483)	LR 5.000e-03
0: TRAIN [1][240/241]	Time 0.028 (0.029)	Data 4.29e-05 (1.25e-03)	Tok/s 35029 (32328)	Loss/tok 3.8023 (4.3210)	LR 5.000e-03
:::MLLOG {"namespace": "", "time_ms": 1592880091321, "event_type": "INTERVAL_END", "key": "epoch_stop", "value": null, "metadata": {"file": "train.py", "lineno": 551, "epoch_num": 2}}
:::MLLOG {"namespace": "", "time_ms": 1592880091322, "event_type": "INTERVAL_START", "key": "eval_start", "value": null, "metadata": {"file": "train.py", "lineno": 556, "epoch_num": 2}}
0: Running evaluation on test set
0: TEST [1][0/1]	Time 0.344 (0.344)	Decoder iters 149.0 (149.0)	Tok/s 1856 (1856)
0: Running moses detokenizer
0: BLEU(score=17.344173925065327, counts=[32269, 13846, 7074, 3725], totals=[64381, 61378, 58375, 55377], precisions=[50.12193038318759, 22.55857147512138, 12.118201284796573, 6.7266193546057025], bp=0.9954283845891555, sys_len=64381, ref_len=64676)
0: Finished evaluation on test set
:::MLLOG {"namespace": "", "time_ms": 1592880091769, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.1734, "metadata": {"file": "train.py", "lineno": 562, "epoch_num": 2}}
:::MLLOG {"namespace": "", "time_ms": 1592880091769, "event_type": "INTERVAL_END", "key": "eval_stop", "value": null, "metadata": {"file": "train.py", "lineno": 565, "epoch_num": 2}}
0: Summary: Epoch: 1	Training Loss: 4.3262	Test BLEU: 17.34
0: Performance: Epoch: 1	Training: 33030798 Tok/s
0: Finished epoch 1
:::MLLOG {"namespace": "", "time_ms": 1592880091769, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "train.py", "lineno": 584, "first_epoch_num": 2}}
:::MLLOG {"namespace": "", "time_ms": 1592880091769, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "train.py", "lineno": 538, "first_epoch_num": 3, "epoch_count": 1}}
:::MLLOG {"namespace": "", "time_ms": 1592880091769, "event_type": "INTERVAL_START", "key": "epoch_start", "value": null, "metadata": {"file": "train.py", "lineno": 541, "epoch_num": 3}}
0: Starting epoch 2
0: Sampler for epoch 2 uses seed 3081476479
0: TRAIN [2][0/241]	Time 0.370 (0.370)	Data 3.18e-01 (3.18e-01)	Tok/s 2781 (2781)	Loss/tok 3.8713 (3.8713)	LR 5.000e-03
0: TRAIN [2][10/241]	Time 0.028 (0.058)	Data 6.18e-05 (2.90e-02)	Tok/s 36477 (28620)	Loss/tok 3.5602 (3.7051)	LR 5.000e-03
0: TRAIN [2][20/241]	Time 0.028 (0.043)	Data 7.58e-05 (1.52e-02)	Tok/s 36766 (29733)	Loss/tok 4.1207 (3.7142)	LR 5.000e-03
0: TRAIN [2][30/241]	Time 0.025 (0.037)	Data 7.03e-05 (1.03e-02)	Tok/s 26367 (28840)	Loss/tok 3.7253 (3.7547)	LR 5.000e-03
0: TRAIN [2][40/241]	Time 0.025 (0.035)	Data 7.08e-05 (7.83e-03)	Tok/s 25363 (29944)	Loss/tok 3.3588 (3.7548)	LR 5.000e-03
0: TRAIN [2][50/241]	Time 0.032 (0.033)	Data 7.13e-05 (6.31e-03)	Tok/s 46082 (30507)	Loss/tok 3.7034 (3.7640)	LR 5.000e-03
0: TRAIN [2][60/241]	Time 0.028 (0.032)	Data 6.94e-05 (5.29e-03)	Tok/s 38709 (30984)	Loss/tok 3.4826 (3.7432)	LR 5.000e-03
0: TRAIN [2][70/241]	Time 0.025 (0.031)	Data 6.84e-05 (4.55e-03)	Tok/s 27402 (30593)	Loss/tok 3.0818 (3.7008)	LR 5.000e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 4.0
0: TRAIN [2][80/241]	Time 0.036 (0.031)	Data 6.99e-05 (4.00e-03)	Tok/s 50214 (30700)	Loss/tok 4.2602 (3.7119)	LR 5.000e-03
0: TRAIN [2][90/241]	Time 0.028 (0.030)	Data 7.06e-05 (3.57e-03)	Tok/s 36299 (30652)	Loss/tok 3.7170 (3.7003)	LR 5.000e-03
0: TRAIN [2][100/241]	Time 0.025 (0.030)	Data 6.82e-05 (3.22e-03)	Tok/s 24167 (30843)	Loss/tok 2.9080 (3.6905)	LR 5.000e-03
0: TRAIN [2][110/241]	Time 0.025 (0.030)	Data 6.82e-05 (2.94e-03)	Tok/s 26838 (30784)	Loss/tok 3.3816 (3.6710)	LR 5.000e-03
0: TRAIN [2][120/241]	Time 0.032 (0.029)	Data 7.06e-05 (2.70e-03)	Tok/s 44306 (30788)	Loss/tok 4.1254 (3.6741)	LR 5.000e-03
0: TRAIN [2][130/241]	Time 0.036 (0.029)	Data 6.82e-05 (2.50e-03)	Tok/s 48950 (31197)	Loss/tok 4.3993 (3.6922)	LR 5.000e-03
0: TRAIN [2][140/241]	Time 0.025 (0.029)	Data 7.08e-05 (2.33e-03)	Tok/s 22525 (31344)	Loss/tok 3.5635 (3.6827)	LR 5.000e-03
0: TRAIN [2][150/241]	Time 0.032 (0.029)	Data 7.01e-05 (2.18e-03)	Tok/s 44722 (31303)	Loss/tok 3.8211 (3.6730)	LR 5.000e-03
0: TRAIN [2][160/241]	Time 0.022 (0.029)	Data 6.96e-05 (2.05e-03)	Tok/s 16032 (31112)	Loss/tok 2.9005 (3.6705)	LR 5.000e-03
0: TRAIN [2][170/241]	Time 0.028 (0.029)	Data 6.94e-05 (1.93e-03)	Tok/s 36694 (31335)	Loss/tok 3.4712 (3.6637)	LR 5.000e-03
0: TRAIN [2][180/241]	Time 0.025 (0.029)	Data 6.79e-05 (1.83e-03)	Tok/s 26089 (31457)	Loss/tok 3.2118 (3.6575)	LR 5.000e-03
0: TRAIN [2][190/241]	Time 0.022 (0.029)	Data 6.77e-05 (1.74e-03)	Tok/s 15836 (31413)	Loss/tok 2.6717 (3.6417)	LR 5.000e-03
0: TRAIN [2][200/241]	Time 0.025 (0.028)	Data 6.89e-05 (1.65e-03)	Tok/s 27531 (31492)	Loss/tok 2.7711 (3.6390)	LR 5.000e-03
0: Upscaling, new scale: 8.0
0: TRAIN [2][210/241]	Time 0.032 (0.028)	Data 7.06e-05 (1.58e-03)	Tok/s 44701 (31579)	Loss/tok 3.5624 (3.6326)	LR 5.000e-03
0: TRAIN [2][220/241]	Time 0.028 (0.028)	Data 6.79e-05 (1.51e-03)	Tok/s 37136 (31699)	Loss/tok 3.4631 (3.6271)	LR 5.000e-03
0: TRAIN [2][230/241]	Time 0.025 (0.028)	Data 7.03e-05 (1.45e-03)	Tok/s 28019 (32099)	Loss/tok 3.1167 (3.6300)	LR 5.000e-03
0: TRAIN [2][240/241]	Time 0.025 (0.028)	Data 4.32e-05 (1.39e-03)	Tok/s 23456 (32295)	Loss/tok 2.9732 (3.6239)	LR 5.000e-03
:::MLLOG {"namespace": "", "time_ms": 1592880098673, "event_type": "INTERVAL_END", "key": "epoch_stop", "value": null, "metadata": {"file": "train.py", "lineno": 551, "epoch_num": 3}}
:::MLLOG {"namespace": "", "time_ms": 1592880098673, "event_type": "INTERVAL_START", "key": "eval_start", "value": null, "metadata": {"file": "train.py", "lineno": 556, "epoch_num": 3}}
0: Running evaluation on test set
0: TEST [2][0/1]	Time 0.346 (0.346)	Decoder iters 149.0 (149.0)	Tok/s 2059 (2059)
0: Running moses detokenizer
0: BLEU(score=20.011556979814042, counts=[34011, 15646, 8374, 4660], totals=[63224, 60221, 57218, 54220], precisions=[53.794445147412375, 25.98097009348898, 14.635254640148204, 8.594614533382515], bp=0.9772957481136748, sys_len=63224, ref_len=64676)
0: Finished evaluation on test set
:::MLLOG {"namespace": "", "time_ms": 1592880099124, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.20010000000000003, "metadata": {"file": "train.py", "lineno": 562, "epoch_num": 3}}
:::MLLOG {"namespace": "", "time_ms": 1592880099124, "event_type": "INTERVAL_END", "key": "eval_stop", "value": null, "metadata": {"file": "train.py", "lineno": 565, "epoch_num": 3}}
0: Summary: Epoch: 2	Training Loss: 3.6021	Test BLEU: 20.01
0: Performance: Epoch: 2	Training: 33151170 Tok/s
0: Finished epoch 2
:::MLLOG {"namespace": "", "time_ms": 1592880099124, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "train.py", "lineno": 584, "first_epoch_num": 3}}
:::MLLOG {"namespace": "", "time_ms": 1592880099124, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "train.py", "lineno": 538, "first_epoch_num": 4, "epoch_count": 1}}
:::MLLOG {"namespace": "", "time_ms": 1592880099124, "event_type": "INTERVAL_START", "key": "epoch_start", "value": null, "metadata": {"file": "train.py", "lineno": 541, "epoch_num": 4}}
0: Starting epoch 3
0: Sampler for epoch 3 uses seed 2017838408
0: TRAIN [3][0/241]	Time 0.365 (0.365)	Data 2.87e-01 (2.87e-01)	Tok/s 1603 (1603)	Loss/tok 3.0370 (3.0370)	LR 5.000e-03
0: TRAIN [3][10/241]	Time 0.032 (0.058)	Data 6.99e-05 (2.61e-02)	Tok/s 45189 (30625)	Loss/tok 3.6845 (3.3351)	LR 5.000e-03
0: TRAIN [3][20/241]	Time 0.028 (0.042)	Data 6.27e-05 (1.37e-02)	Tok/s 36905 (29010)	Loss/tok 3.3746 (3.2881)	LR 5.000e-03
0: TRAIN [3][30/241]	Time 0.032 (0.038)	Data 6.25e-05 (9.31e-03)	Tok/s 44788 (30596)	Loss/tok 3.1281 (3.3429)	LR 5.000e-03
0: TRAIN [3][40/241]	Time 0.028 (0.035)	Data 6.51e-05 (7.06e-03)	Tok/s 36195 (31935)	Loss/tok 3.6255 (3.3868)	LR 5.000e-03
0: TRAIN [3][50/241]	Time 0.025 (0.034)	Data 6.79e-05 (5.69e-03)	Tok/s 27762 (31750)	Loss/tok 3.3003 (3.3897)	LR 5.000e-03
0: TRAIN [3][60/241]	Time 0.028 (0.033)	Data 6.39e-05 (4.77e-03)	Tok/s 38601 (32178)	Loss/tok 3.1604 (3.3922)	LR 5.000e-03
0: TRAIN [3][70/241]	Time 0.032 (0.032)	Data 6.72e-05 (4.10e-03)	Tok/s 43229 (32576)	Loss/tok 3.2479 (3.3825)	LR 5.000e-03
0: TRAIN [3][80/241]	Time 0.028 (0.031)	Data 6.56e-05 (3.60e-03)	Tok/s 38012 (32596)	Loss/tok 3.2406 (3.3869)	LR 5.000e-03
0: TRAIN [3][90/241]	Time 0.028 (0.031)	Data 7.08e-05 (3.22e-03)	Tok/s 38442 (32822)	Loss/tok 3.7999 (3.3847)	LR 5.000e-03
0: Upscaling, new scale: 16.0
0: TRAIN [3][100/241]	Time 0.028 (0.030)	Data 6.72e-05 (2.90e-03)	Tok/s 39616 (32603)	Loss/tok 3.2192 (3.3495)	LR 5.000e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 8.0
0: TRAIN [3][110/241]	Time 0.025 (0.030)	Data 8.03e-05 (2.65e-03)	Tok/s 25475 (32683)	Loss/tok 3.0047 (3.3466)	LR 5.000e-03
0: TRAIN [3][120/241]	Time 0.025 (0.030)	Data 1.20e-04 (2.44e-03)	Tok/s 27269 (33046)	Loss/tok 2.8164 (3.3646)	LR 5.000e-03
0: TRAIN [3][130/241]	Time 0.025 (0.030)	Data 7.87e-05 (2.26e-03)	Tok/s 27726 (33001)	Loss/tok 3.2607 (3.3587)	LR 5.000e-03
0: TRAIN [3][140/241]	Time 0.025 (0.029)	Data 7.18e-05 (2.10e-03)	Tok/s 23594 (32737)	Loss/tok 3.0921 (3.3518)	LR 5.000e-03
0: TRAIN [3][150/241]	Time 0.028 (0.029)	Data 6.68e-05 (1.97e-03)	Tok/s 37227 (32745)	Loss/tok 3.5217 (3.3582)	LR 5.000e-03
0: TRAIN [3][160/241]	Time 0.025 (0.029)	Data 6.60e-05 (1.85e-03)	Tok/s 26326 (32535)	Loss/tok 3.1680 (3.3571)	LR 5.000e-03
0: TRAIN [3][170/241]	Time 0.028 (0.029)	Data 6.72e-05 (1.74e-03)	Tok/s 38289 (32784)	Loss/tok 3.6100 (3.3533)	LR 5.000e-03
0: TRAIN [3][180/241]	Time 0.028 (0.029)	Data 6.68e-05 (1.65e-03)	Tok/s 35789 (32643)	Loss/tok 3.2089 (3.3628)	LR 5.000e-03
0: TRAIN [3][190/241]	Time 0.028 (0.029)	Data 6.68e-05 (1.57e-03)	Tok/s 38240 (32570)	Loss/tok 3.4083 (3.3594)	LR 5.000e-03
0: TRAIN [3][200/241]	Time 0.032 (0.029)	Data 6.32e-05 (1.49e-03)	Tok/s 45187 (32498)	Loss/tok 3.5340 (3.3596)	LR 5.000e-03
0: TRAIN [3][210/241]	Time 0.036 (0.029)	Data 6.41e-05 (1.43e-03)	Tok/s 53802 (32775)	Loss/tok 3.3000 (3.3789)	LR 5.000e-03
0: TRAIN [3][220/241]	Time 0.022 (0.029)	Data 6.53e-05 (1.37e-03)	Tok/s 15575 (32587)	Loss/tok 2.6777 (3.3785)	LR 5.000e-03
0: TRAIN [3][230/241]	Time 0.028 (0.028)	Data 6.34e-05 (1.31e-03)	Tok/s 36963 (32542)	Loss/tok 3.7728 (3.3765)	LR 5.000e-03
0: Upscaling, new scale: 16.0
0: TRAIN [3][240/241]	Time 0.025 (0.028)	Data 4.43e-05 (1.26e-03)	Tok/s 24244 (32531)	Loss/tok 2.9416 (3.3735)	LR 5.000e-03
:::MLLOG {"namespace": "", "time_ms": 1592880106024, "event_type": "INTERVAL_END", "key": "epoch_stop", "value": null, "metadata": {"file": "train.py", "lineno": 551, "epoch_num": 4}}
:::MLLOG {"namespace": "", "time_ms": 1592880106024, "event_type": "INTERVAL_START", "key": "eval_start", "value": null, "metadata": {"file": "train.py", "lineno": 556, "epoch_num": 4}}
0: Running evaluation on test set
0: TEST [3][0/1]	Time 0.271 (0.271)	Decoder iters 100.0 (100.0)	Tok/s 2490 (2490)
0: Running moses detokenizer
0: BLEU(score=20.953641906883426, counts=[34336, 16263, 8884, 5017], totals=[62322, 59319, 56316, 53319], precisions=[55.094509162093644, 27.416173570019723, 15.775268129838768, 9.40940377726514], bp=0.9629328752139017, sys_len=62322, ref_len=64676)
0: Finished evaluation on test set
:::MLLOG {"namespace": "", "time_ms": 1592880106463, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.2095, "metadata": {"file": "train.py", "lineno": 562, "epoch_num": 4}}
:::MLLOG {"namespace": "", "time_ms": 1592880106463, "event_type": "INTERVAL_END", "key": "eval_stop", "value": null, "metadata": {"file": "train.py", "lineno": 565, "epoch_num": 4}}
0: Summary: Epoch: 3	Training Loss: 3.4077	Test BLEU: 20.95
0: Performance: Epoch: 3	Training: 33200970 Tok/s
0: Finished epoch 3
:::MLLOG {"namespace": "", "time_ms": 1592880106463, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "train.py", "lineno": 584, "first_epoch_num": 4}}
:::MLLOG {"namespace": "", "time_ms": 1592880106463, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "train.py", "lineno": 538, "first_epoch_num": 5, "epoch_count": 1}}
:::MLLOG {"namespace": "", "time_ms": 1592880106463, "event_type": "INTERVAL_START", "key": "epoch_start", "value": null, "metadata": {"file": "train.py", "lineno": 541, "epoch_num": 5}}
0: Starting epoch 4
0: Sampler for epoch 4 uses seed 2042307331
0: TRAIN [4][0/241]	Time 0.359 (0.359)	Data 3.20e-01 (3.20e-01)	Tok/s 2893 (2893)	Loss/tok 3.4798 (3.4798)	LR 5.000e-03
0: TRAIN [4][10/241]	Time 0.025 (0.057)	Data 6.79e-05 (2.92e-02)	Tok/s 25623 (27794)	Loss/tok 3.3492 (3.2646)	LR 5.000e-03
0: TRAIN [4][20/241]	Time 0.036 (0.043)	Data 6.89e-05 (1.53e-02)	Tok/s 49832 (30277)	Loss/tok 3.8872 (3.3121)	LR 5.000e-03
0: TRAIN [4][30/241]	Time 0.022 (0.037)	Data 6.60e-05 (1.04e-02)	Tok/s 14570 (31020)	Loss/tok 2.9887 (3.3163)	LR 5.000e-03
0: TRAIN [4][40/241]	Time 0.032 (0.035)	Data 6.29e-05 (7.88e-03)	Tok/s 48159 (31737)	Loss/tok 3.6105 (3.2963)	LR 5.000e-03
0: TRAIN [4][50/241]	Time 0.022 (0.033)	Data 6.25e-05 (6.35e-03)	Tok/s 14662 (31822)	Loss/tok 2.8448 (3.2499)	LR 5.000e-03
0: TRAIN [4][60/241]	Time 0.025 (0.032)	Data 6.39e-05 (5.32e-03)	Tok/s 24600 (31276)	Loss/tok 3.4698 (3.2409)	LR 5.000e-03
0: TRAIN [4][70/241]	Time 0.028 (0.032)	Data 6.37e-05 (4.58e-03)	Tok/s 36989 (31768)	Loss/tok 3.0427 (3.2544)	LR 5.000e-03
0: TRAIN [4][80/241]	Time 0.036 (0.031)	Data 6.34e-05 (4.02e-03)	Tok/s 52817 (32672)	Loss/tok 3.6918 (3.2999)	LR 5.000e-03
0: TRAIN [4][90/241]	Time 0.036 (0.031)	Data 6.68e-05 (3.59e-03)	Tok/s 53580 (33558)	Loss/tok 3.2333 (3.2948)	LR 5.000e-03
0: TRAIN [4][100/241]	Time 0.032 (0.031)	Data 6.44e-05 (3.24e-03)	Tok/s 46124 (33646)	Loss/tok 3.4870 (3.3036)	LR 5.000e-03
0: TRAIN [4][110/241]	Time 0.022 (0.030)	Data 6.25e-05 (2.95e-03)	Tok/s 14980 (32853)	Loss/tok 2.7511 (3.2902)	LR 5.000e-03
0: TRAIN [4][120/241]	Time 0.028 (0.030)	Data 6.08e-05 (2.71e-03)	Tok/s 38035 (32669)	Loss/tok 3.5224 (3.2898)	LR 5.000e-03
0: Upscaling, new scale: 32.0
0: Gradient norm: inf
0: Skipped batch, new scale: 16.0
0: TRAIN [4][130/241]	Time 0.036 (0.030)	Data 6.58e-05 (2.51e-03)	Tok/s 54153 (32653)	Loss/tok 4.0591 (3.2879)	LR 5.000e-03
0: TRAIN [4][140/241]	Time 0.025 (0.029)	Data 6.29e-05 (2.34e-03)	Tok/s 25398 (32490)	Loss/tok 3.4584 (3.2859)	LR 5.000e-03
0: TRAIN [4][150/241]	Time 0.025 (0.029)	Data 6.68e-05 (2.19e-03)	Tok/s 25145 (32436)	Loss/tok 2.6650 (3.2805)	LR 5.000e-03
0: TRAIN [4][160/241]	Time 0.028 (0.029)	Data 6.20e-05 (2.05e-03)	Tok/s 36559 (32520)	Loss/tok 3.8726 (3.2883)	LR 5.000e-03
0: TRAIN [4][170/241]	Time 0.025 (0.029)	Data 6.32e-05 (1.94e-03)	Tok/s 27798 (32471)	Loss/tok 3.2301 (3.2816)	LR 5.000e-03
0: TRAIN [4][180/241]	Time 0.032 (0.029)	Data 6.25e-05 (1.83e-03)	Tok/s 45554 (32677)	Loss/tok 3.0261 (3.2911)	LR 5.000e-03
0: TRAIN [4][190/241]	Time 0.025 (0.029)	Data 6.39e-05 (1.74e-03)	Tok/s 28580 (32531)	Loss/tok 3.3460 (3.2964)	LR 5.000e-03
0: TRAIN [4][200/241]	Time 0.028 (0.029)	Data 6.29e-05 (1.66e-03)	Tok/s 37158 (32530)	Loss/tok 3.2838 (3.2967)	LR 5.000e-03
0: TRAIN [4][210/241]	Time 0.028 (0.029)	Data 6.48e-05 (1.58e-03)	Tok/s 34560 (32584)	Loss/tok 3.4779 (3.2943)	LR 5.000e-03
0: TRAIN [4][220/241]	Time 0.028 (0.029)	Data 6.29e-05 (1.51e-03)	Tok/s 38153 (32449)	Loss/tok 2.9681 (3.2912)	LR 5.000e-03
0: TRAIN [4][230/241]	Time 0.025 (0.029)	Data 6.51e-05 (1.45e-03)	Tok/s 26693 (32587)	Loss/tok 3.1822 (3.2977)	LR 5.000e-03
0: TRAIN [4][240/241]	Time 0.025 (0.028)	Data 4.17e-05 (1.39e-03)	Tok/s 23986 (32488)	Loss/tok 3.1262 (3.2906)	LR 5.000e-03
:::MLLOG {"namespace": "", "time_ms": 1592880113358, "event_type": "INTERVAL_END", "key": "epoch_stop", "value": null, "metadata": {"file": "train.py", "lineno": 551, "epoch_num": 5}}
:::MLLOG {"namespace": "", "time_ms": 1592880113358, "event_type": "INTERVAL_START", "key": "eval_start", "value": null, "metadata": {"file": "train.py", "lineno": 556, "epoch_num": 5}}
0: Running evaluation on test set
0: TEST [4][0/1]	Time 0.379 (0.379)	Decoder iters 149.0 (149.0)	Tok/s 1887 (1887)
0: Running moses detokenizer
0: BLEU(score=21.8817731370902, counts=[35421, 16992, 9301, 5308], totals=[63472, 60469, 57466, 54470], precisions=[55.80570960423494, 28.100348939125833, 16.185222566387083, 9.744813658894804], bp=0.9812097849633867, sys_len=63472, ref_len=64676)
0: Finished evaluation on test set
:::MLLOG {"namespace": "", "time_ms": 1592880113841, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.2188, "metadata": {"file": "train.py", "lineno": 562, "epoch_num": 5}}
:::MLLOG {"namespace": "", "time_ms": 1592880113841, "event_type": "INTERVAL_END", "key": "eval_stop", "value": null, "metadata": {"file": "train.py", "lineno": 565, "epoch_num": 5}}
0: Summary: Epoch: 4	Training Loss: 3.3047	Test BLEU: 21.88
0: Performance: Epoch: 4	Training: 33145932 Tok/s
0: Finished epoch 4
:::MLLOG {"namespace": "", "time_ms": 1592880113842, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "train.py", "lineno": 584, "first_epoch_num": 5}}
:::MLLOG {"namespace": "", "time_ms": 1592880113842, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "train.py", "lineno": 538, "first_epoch_num": 6, "epoch_count": 1}}
:::MLLOG {"namespace": "", "time_ms": 1592880113842, "event_type": "INTERVAL_START", "key": "epoch_start", "value": null, "metadata": {"file": "train.py", "lineno": 541, "epoch_num": 6}}
0: Starting epoch 5
0: Sampler for epoch 5 uses seed 2777616140
0: TRAIN [5][0/241]	Time 0.364 (0.364)	Data 2.70e-01 (2.70e-01)	Tok/s 3059 (3059)	Loss/tok 3.5517 (3.5517)	LR 5.000e-03
0: TRAIN [5][10/241]	Time 0.028 (0.058)	Data 6.87e-05 (2.46e-02)	Tok/s 39797 (29738)	Loss/tok 3.2594 (3.2011)	LR 5.000e-03
0: Upscaling, new scale: 32.0
0: TRAIN [5][20/241]	Time 0.025 (0.042)	Data 6.56e-05 (1.29e-02)	Tok/s 25579 (29047)	Loss/tok 2.8301 (3.1790)	LR 5.000e-03
0: TRAIN [5][30/241]	Time 0.025 (0.037)	Data 6.51e-05 (8.77e-03)	Tok/s 25106 (28625)	Loss/tok 2.9574 (3.1339)	LR 5.000e-03
0: TRAIN [5][40/241]	Time 0.025 (0.034)	Data 9.56e-05 (6.65e-03)	Tok/s 24737 (29315)	Loss/tok 2.7145 (3.1612)	LR 5.000e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 16.0
0: TRAIN [5][50/241]	Time 0.025 (0.033)	Data 7.10e-05 (5.36e-03)	Tok/s 26945 (30400)	Loss/tok 2.8804 (3.1790)	LR 5.000e-03
0: TRAIN [5][60/241]	Time 0.036 (0.032)	Data 7.01e-05 (4.49e-03)	Tok/s 53705 (30993)	Loss/tok 3.6583 (3.1953)	LR 5.000e-03
0: TRAIN [5][70/241]	Time 0.032 (0.032)	Data 7.03e-05 (3.87e-03)	Tok/s 46603 (32039)	Loss/tok 3.1557 (3.1725)	LR 5.000e-03
0: TRAIN [5][80/241]	Time 0.028 (0.031)	Data 8.75e-05 (3.40e-03)	Tok/s 40350 (32516)	Loss/tok 3.1613 (3.2009)	LR 5.000e-03
0: TRAIN [5][90/241]	Time 0.028 (0.031)	Data 6.84e-05 (3.04e-03)	Tok/s 37240 (32397)	Loss/tok 3.2444 (3.1969)	LR 5.000e-03
0: TRAIN [5][100/241]	Time 0.032 (0.030)	Data 6.96e-05 (2.74e-03)	Tok/s 47240 (32551)	Loss/tok 2.9793 (3.1960)	LR 5.000e-03
0: TRAIN [5][110/241]	Time 0.025 (0.030)	Data 8.20e-05 (2.50e-03)	Tok/s 27649 (32591)	Loss/tok 2.7577 (3.2061)	LR 5.000e-03
0: TRAIN [5][120/241]	Time 0.036 (0.030)	Data 6.91e-05 (2.30e-03)	Tok/s 54796 (32989)	Loss/tok 3.5163 (3.2161)	LR 5.000e-03
0: TRAIN [5][130/241]	Time 0.025 (0.030)	Data 7.10e-05 (2.13e-03)	Tok/s 23373 (33013)	Loss/tok 3.5240 (3.2225)	LR 5.000e-03
0: TRAIN [5][140/241]	Time 0.032 (0.030)	Data 6.96e-05 (1.99e-03)	Tok/s 46729 (32976)	Loss/tok 3.2534 (3.2141)	LR 5.000e-03
0: TRAIN [5][150/241]	Time 0.025 (0.029)	Data 7.18e-05 (1.86e-03)	Tok/s 25373 (32825)	Loss/tok 2.9849 (3.2019)	LR 5.000e-03
0: TRAIN [5][160/241]	Time 0.025 (0.029)	Data 8.13e-05 (1.75e-03)	Tok/s 27216 (32788)	Loss/tok 2.9732 (3.2169)	LR 5.000e-03
0: TRAIN [5][170/241]	Time 0.036 (0.029)	Data 6.84e-05 (1.65e-03)	Tok/s 53165 (32789)	Loss/tok 3.9848 (3.2261)	LR 5.000e-03
0: Upscaling, new scale: 32.0
0: Gradient norm: inf
0: Skipped batch, new scale: 16.0
0: TRAIN [5][180/241]	Time 0.032 (0.029)	Data 6.94e-05 (1.56e-03)	Tok/s 42806 (32885)	Loss/tok 4.0684 (3.2485)	LR 5.000e-03
0: TRAIN [5][190/241]	Time 0.025 (0.029)	Data 6.87e-05 (1.48e-03)	Tok/s 26988 (33060)	Loss/tok 3.3954 (3.2524)	LR 5.000e-03
0: TRAIN [5][200/241]	Time 0.025 (0.029)	Data 7.84e-05 (1.41e-03)	Tok/s 25708 (32948)	Loss/tok 2.9912 (3.2487)	LR 5.000e-03
0: TRAIN [5][210/241]	Time 0.022 (0.029)	Data 7.37e-05 (1.35e-03)	Tok/s 15540 (32921)	Loss/tok 2.9200 (3.2425)	LR 5.000e-03
0: TRAIN [5][220/241]	Time 0.022 (0.029)	Data 6.77e-05 (1.29e-03)	Tok/s 15080 (32769)	Loss/tok 2.6854 (3.2463)	LR 5.000e-03
0: TRAIN [5][230/241]	Time 0.025 (0.029)	Data 1.08e-04 (1.24e-03)	Tok/s 24154 (32680)	Loss/tok 2.7191 (3.2509)	LR 5.000e-03
0: TRAIN [5][240/241]	Time 0.025 (0.028)	Data 4.63e-05 (1.19e-03)	Tok/s 24718 (32436)	Loss/tok 3.0212 (3.2469)	LR 5.000e-03
:::MLLOG {"namespace": "", "time_ms": 1592880120743, "event_type": "INTERVAL_END", "key": "epoch_stop", "value": null, "metadata": {"file": "train.py", "lineno": 551, "epoch_num": 6}}
:::MLLOG {"namespace": "", "time_ms": 1592880120744, "event_type": "INTERVAL_START", "key": "eval_start", "value": null, "metadata": {"file": "train.py", "lineno": 556, "epoch_num": 6}}
0: Running evaluation on test set
0: TEST [5][0/1]	Time 0.268 (0.268)	Decoder iters 113.0 (113.0)	Tok/s 2615 (2615)
0: Running moses detokenizer
0: BLEU(score=22.537389726878917, counts=[35542, 17358, 9700, 5593], totals=[63697, 60694, 57692, 54697], precisions=[55.7985462423662, 28.59920255708966, 16.813423004922694, 10.225423697826207], bp=0.9847478694845573, sys_len=63697, ref_len=64676)
0: Finished evaluation on test set
:::MLLOG {"namespace": "", "time_ms": 1592880121182, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.2254, "metadata": {"file": "train.py", "lineno": 562, "epoch_num": 6}}
:::MLLOG {"namespace": "", "time_ms": 1592880121182, "event_type": "INTERVAL_END", "key": "eval_stop", "value": null, "metadata": {"file": "train.py", "lineno": 565, "epoch_num": 6}}
0: Summary: Epoch: 5	Training Loss: 3.2510	Test BLEU: 22.54
0: Performance: Epoch: 5	Training: 33137676 Tok/s
0: Finished epoch 5
:::MLLOG {"namespace": "", "time_ms": 1592880121182, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "train.py", "lineno": 584, "first_epoch_num": 6}}
:::MLLOG {"namespace": "", "time_ms": 1592880121182, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "train.py", "lineno": 538, "first_epoch_num": 7, "epoch_count": 1}}
:::MLLOG {"namespace": "", "time_ms": 1592880121182, "event_type": "INTERVAL_START", "key": "epoch_start", "value": null, "metadata": {"file": "train.py", "lineno": 541, "epoch_num": 7}}
0: Starting epoch 6
0: Sampler for epoch 6 uses seed 986803174
0: TRAIN [6][0/241]	Time 0.372 (0.372)	Data 2.84e-01 (2.84e-01)	Tok/s 1702 (1702)	Loss/tok 3.2066 (3.2066)	LR 5.000e-03
0: TRAIN [6][10/241]	Time 0.032 (0.060)	Data 6.91e-05 (2.59e-02)	Tok/s 46449 (33214)	Loss/tok 3.4501 (3.3204)	LR 5.000e-03
0: TRAIN [6][20/241]	Time 0.028 (0.044)	Data 6.58e-05 (1.36e-02)	Tok/s 38696 (32913)	Loss/tok 3.3506 (3.2304)	LR 5.000e-03
0: TRAIN [6][30/241]	Time 0.028 (0.039)	Data 6.82e-05 (9.23e-03)	Tok/s 38132 (31490)	Loss/tok 3.1203 (3.2104)	LR 5.000e-03
0: TRAIN [6][40/241]	Time 0.025 (0.036)	Data 6.68e-05 (6.99e-03)	Tok/s 25752 (32711)	Loss/tok 2.7311 (3.2289)	LR 5.000e-03
0: TRAIN [6][50/241]	Time 0.025 (0.034)	Data 6.60e-05 (5.63e-03)	Tok/s 29314 (32841)	Loss/tok 2.8314 (3.2232)	LR 5.000e-03
0: TRAIN [6][60/241]	Time 0.028 (0.033)	Data 6.56e-05 (4.72e-03)	Tok/s 38598 (32580)	Loss/tok 3.4637 (3.2037)	LR 5.000e-03
0: Upscaling, new scale: 32.0
0: TRAIN [6][70/241]	Time 0.036 (0.032)	Data 6.68e-05 (4.07e-03)	Tok/s 50497 (32741)	Loss/tok 3.8291 (3.2155)	LR 5.000e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 16.0
0: TRAIN [6][80/241]	Time 0.025 (0.032)	Data 6.51e-05 (3.57e-03)	Tok/s 24955 (32635)	Loss/tok 3.3098 (3.2269)	LR 5.000e-03
0: TRAIN [6][90/241]	Time 0.028 (0.031)	Data 6.82e-05 (3.19e-03)	Tok/s 36039 (32665)	Loss/tok 3.1700 (3.2440)	LR 5.000e-03
0: TRAIN [6][100/241]	Time 0.032 (0.031)	Data 6.48e-05 (2.88e-03)	Tok/s 46949 (32583)	Loss/tok 3.9553 (3.2409)	LR 5.000e-03
0: TRAIN [6][110/241]	Time 0.028 (0.030)	Data 6.72e-05 (2.63e-03)	Tok/s 38265 (32308)	Loss/tok 3.4929 (3.2301)	LR 5.000e-03
0: TRAIN [6][120/241]	Time 0.032 (0.030)	Data 6.87e-05 (2.41e-03)	Tok/s 46525 (32384)	Loss/tok 2.9213 (3.2241)	LR 5.000e-03
0: TRAIN [6][130/241]	Time 0.032 (0.030)	Data 6.72e-05 (2.23e-03)	Tok/s 46647 (32516)	Loss/tok 3.1492 (3.2217)	LR 5.000e-03
0: TRAIN [6][140/241]	Time 0.032 (0.030)	Data 6.63e-05 (2.08e-03)	Tok/s 45386 (32254)	Loss/tok 3.2541 (3.2118)	LR 5.000e-03
0: TRAIN [6][150/241]	Time 0.028 (0.029)	Data 6.65e-05 (1.95e-03)	Tok/s 38383 (32442)	Loss/tok 3.0255 (3.2126)	LR 5.000e-03
0: TRAIN [6][160/241]	Time 0.028 (0.029)	Data 6.22e-05 (1.83e-03)	Tok/s 39226 (32301)	Loss/tok 3.4553 (3.2098)	LR 5.000e-03
0: TRAIN [6][170/241]	Time 0.028 (0.029)	Data 6.87e-05 (1.73e-03)	Tok/s 33648 (32265)	Loss/tok 3.6354 (3.2131)	LR 5.000e-03
0: TRAIN [6][180/241]	Time 0.025 (0.029)	Data 6.75e-05 (1.64e-03)	Tok/s 26709 (32119)	Loss/tok 2.8374 (3.2123)	LR 2.500e-03
0: TRAIN [6][190/241]	Time 0.025 (0.029)	Data 6.89e-05 (1.55e-03)	Tok/s 26665 (32222)	Loss/tok 2.9035 (3.2040)	LR 2.500e-03
0: TRAIN [6][200/241]	Time 0.025 (0.029)	Data 7.01e-05 (1.48e-03)	Tok/s 25223 (32252)	Loss/tok 3.2016 (3.2073)	LR 2.500e-03
0: Upscaling, new scale: 32.0
0: TRAIN [6][210/241]	Time 0.025 (0.029)	Data 6.25e-05 (1.41e-03)	Tok/s 26810 (32156)	Loss/tok 2.8286 (3.2026)	LR 2.500e-03
0: TRAIN [6][220/241]	Time 0.028 (0.029)	Data 6.65e-05 (1.35e-03)	Tok/s 36741 (32302)	Loss/tok 3.5446 (3.2067)	LR 2.500e-03
0: TRAIN [6][230/241]	Time 0.028 (0.029)	Data 6.70e-05 (1.30e-03)	Tok/s 35053 (32181)	Loss/tok 3.3525 (3.1994)	LR 2.500e-03
0: TRAIN [6][240/241]	Time 0.025 (0.028)	Data 4.46e-05 (1.25e-03)	Tok/s 25779 (32181)	Loss/tok 2.6314 (3.1892)	LR 2.500e-03
:::MLLOG {"namespace": "", "time_ms": 1592880128093, "event_type": "INTERVAL_END", "key": "epoch_stop", "value": null, "metadata": {"file": "train.py", "lineno": 551, "epoch_num": 7}}
:::MLLOG {"namespace": "", "time_ms": 1592880128093, "event_type": "INTERVAL_START", "key": "eval_start", "value": null, "metadata": {"file": "train.py", "lineno": 556, "epoch_num": 7}}
0: Running evaluation on test set
0: TEST [6][0/1]	Time 0.334 (0.334)	Decoder iters 129.0 (129.0)	Tok/s 2245 (2245)
0: Running moses detokenizer
0: BLEU(score=23.039157334759185, counts=[36832, 18132, 10218, 5961], totals=[66236, 63233, 60230, 57232], precisions=[55.60722265837309, 28.674900763841666, 16.964967624107587, 10.415501817165222], bp=1.0, sys_len=66236, ref_len=64676)
0: Finished evaluation on test set
:::MLLOG {"namespace": "", "time_ms": 1592880128533, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.2304, "metadata": {"file": "train.py", "lineno": 562, "epoch_num": 7}}
:::MLLOG {"namespace": "", "time_ms": 1592880128534, "event_type": "INTERVAL_END", "key": "eval_stop", "value": null, "metadata": {"file": "train.py", "lineno": 565, "epoch_num": 7}}
0: Summary: Epoch: 6	Training Loss: 3.1951	Test BLEU: 23.04
0: Performance: Epoch: 6	Training: 33189322 Tok/s
0: Finished epoch 6
:::MLLOG {"namespace": "", "time_ms": 1592880128534, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "train.py", "lineno": 584, "first_epoch_num": 7}}
:::MLLOG {"namespace": "", "time_ms": 1592880128534, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "train.py", "lineno": 538, "first_epoch_num": 8, "epoch_count": 1}}
:::MLLOG {"namespace": "", "time_ms": 1592880128534, "event_type": "INTERVAL_START", "key": "epoch_start", "value": null, "metadata": {"file": "train.py", "lineno": 541, "epoch_num": 8}}
0: Starting epoch 7
0: Sampler for epoch 7 uses seed 3138639711
0: TRAIN [7][0/241]	Time 0.361 (0.361)	Data 3.11e-01 (3.11e-01)	Tok/s 2964 (2964)	Loss/tok 2.9183 (2.9183)	LR 2.500e-03
0: TRAIN [7][10/241]	Time 0.028 (0.058)	Data 6.51e-05 (2.83e-02)	Tok/s 36705 (30980)	Loss/tok 3.0852 (3.1632)	LR 2.500e-03
0: TRAIN [7][20/241]	Time 0.028 (0.043)	Data 6.37e-05 (1.49e-02)	Tok/s 36926 (30495)	Loss/tok 3.1358 (3.0791)	LR 2.500e-03
0: TRAIN [7][30/241]	Time 0.025 (0.038)	Data 6.56e-05 (1.01e-02)	Tok/s 25006 (31542)	Loss/tok 2.6584 (3.1037)	LR 2.500e-03
0: TRAIN [7][40/241]	Time 0.025 (0.035)	Data 6.56e-05 (7.65e-03)	Tok/s 27338 (30664)	Loss/tok 2.9683 (3.1141)	LR 2.500e-03
0: TRAIN [7][50/241]	Time 0.022 (0.033)	Data 6.44e-05 (6.17e-03)	Tok/s 15352 (30646)	Loss/tok 2.8444 (3.0916)	LR 2.500e-03
0: TRAIN [7][60/241]	Time 0.025 (0.032)	Data 6.75e-05 (5.17e-03)	Tok/s 27486 (30768)	Loss/tok 3.2508 (3.0864)	LR 2.500e-03
0: TRAIN [7][70/241]	Time 0.022 (0.031)	Data 6.89e-05 (4.45e-03)	Tok/s 17033 (30899)	Loss/tok 2.5070 (3.0759)	LR 2.500e-03
0: TRAIN [7][80/241]	Time 0.022 (0.031)	Data 6.75e-05 (3.91e-03)	Tok/s 15141 (31535)	Loss/tok 2.2011 (3.1018)	LR 2.500e-03
0: TRAIN [7][90/241]	Time 0.036 (0.031)	Data 6.70e-05 (3.49e-03)	Tok/s 52462 (31460)	Loss/tok 3.2402 (3.1051)	LR 2.500e-03
0: Upscaling, new scale: 64.0
0: Gradient norm: inf
0: Skipped batch, new scale: 32.0
0: TRAIN [7][100/241]	Time 0.023 (0.030)	Data 7.10e-05 (3.15e-03)	Tok/s 25090 (31899)	Loss/tok 2.9729 (3.1095)	LR 2.500e-03
0: TRAIN [7][110/241]	Time 0.025 (0.030)	Data 6.72e-05 (2.87e-03)	Tok/s 28018 (31807)	Loss/tok 2.9010 (3.1132)	LR 2.500e-03
0: TRAIN [7][120/241]	Time 0.025 (0.030)	Data 6.77e-05 (2.64e-03)	Tok/s 26295 (31825)	Loss/tok 2.7095 (3.1114)	LR 2.500e-03
0: TRAIN [7][130/241]	Time 0.025 (0.029)	Data 6.70e-05 (2.44e-03)	Tok/s 26420 (31793)	Loss/tok 3.0064 (3.1080)	LR 2.500e-03
0: TRAIN [7][140/241]	Time 0.025 (0.029)	Data 6.63e-05 (2.27e-03)	Tok/s 24683 (31850)	Loss/tok 3.2410 (3.1078)	LR 1.250e-03
0: TRAIN [7][150/241]	Time 0.025 (0.029)	Data 7.65e-05 (2.13e-03)	Tok/s 24281 (31734)	Loss/tok 2.9097 (3.1038)	LR 1.250e-03
0: TRAIN [7][160/241]	Time 0.028 (0.029)	Data 6.53e-05 (2.00e-03)	Tok/s 35546 (31745)	Loss/tok 3.4349 (3.1020)	LR 1.250e-03
0: TRAIN [7][170/241]	Time 0.025 (0.029)	Data 6.51e-05 (1.89e-03)	Tok/s 27887 (31885)	Loss/tok 3.1133 (3.0978)	LR 1.250e-03
0: TRAIN [7][180/241]	Time 0.036 (0.029)	Data 9.85e-05 (1.79e-03)	Tok/s 50454 (32137)	Loss/tok 3.8670 (3.1114)	LR 1.250e-03
0: TRAIN [7][190/241]	Time 0.032 (0.029)	Data 6.53e-05 (1.70e-03)	Tok/s 46235 (32324)	Loss/tok 3.1476 (3.1137)	LR 1.250e-03
0: TRAIN [7][200/241]	Time 0.028 (0.029)	Data 6.70e-05 (1.62e-03)	Tok/s 38461 (32465)	Loss/tok 2.7851 (3.1077)	LR 1.250e-03
0: TRAIN [7][210/241]	Time 0.025 (0.029)	Data 6.84e-05 (1.54e-03)	Tok/s 24944 (32304)	Loss/tok 3.0846 (3.1095)	LR 1.250e-03
0: TRAIN [7][220/241]	Time 0.028 (0.028)	Data 6.56e-05 (1.48e-03)	Tok/s 37758 (32229)	Loss/tok 2.9631 (3.1042)	LR 1.250e-03
0: Upscaling, new scale: 64.0
0: TRAIN [7][230/241]	Time 0.028 (0.028)	Data 6.96e-05 (1.42e-03)	Tok/s 37956 (32290)	Loss/tok 2.8645 (3.1075)	LR 1.250e-03
0: TRAIN [7][240/241]	Time 0.022 (0.028)	Data 4.84e-05 (1.36e-03)	Tok/s 14362 (32378)	Loss/tok 2.3002 (3.1223)	LR 1.250e-03
:::MLLOG {"namespace": "", "time_ms": 1592880135434, "event_type": "INTERVAL_END", "key": "epoch_stop", "value": null, "metadata": {"file": "train.py", "lineno": 551, "epoch_num": 8}}
:::MLLOG {"namespace": "", "time_ms": 1592880135434, "event_type": "INTERVAL_START", "key": "eval_start", "value": null, "metadata": {"file": "train.py", "lineno": 556, "epoch_num": 8}}
0: Running evaluation on test set
0: TEST [7][0/1]	Time 0.230 (0.230)	Decoder iters 98.0 (98.0)	Tok/s 3123 (3123)
0: Running moses detokenizer
0: BLEU(score=23.69788089197822, counts=[36991, 18439, 10482, 6177], totals=[65767, 62764, 59761, 56765], precisions=[56.2455334742348, 29.378306035306863, 17.539867137430765, 10.881705276138465], bp=1.0, sys_len=65767, ref_len=64676)
0: Finished evaluation on test set
:::MLLOG {"namespace": "", "time_ms": 1592880135830, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.237, "metadata": {"file": "train.py", "lineno": 562, "epoch_num": 8}}
:::MLLOG {"namespace": "", "time_ms": 1592880135830, "event_type": "INTERVAL_END", "key": "eval_stop", "value": null, "metadata": {"file": "train.py", "lineno": 565, "epoch_num": 8}}
0: Summary: Epoch: 7	Training Loss: 3.0934	Test BLEU: 23.70
0: Performance: Epoch: 7	Training: 33122520 Tok/s
0: Finished epoch 7
:::MLLOG {"namespace": "", "time_ms": 1592880135831, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "train.py", "lineno": 584, "first_epoch_num": 8}}
:::MLLOG {"namespace": "", "time_ms": 1592880135831, "event_type": "INTERVAL_START", "key": "block_start", "value": null, "metadata": {"file": "train.py", "lineno": 538, "first_epoch_num": 9, "epoch_count": 1}}
:::MLLOG {"namespace": "", "time_ms": 1592880135831, "event_type": "INTERVAL_START", "key": "epoch_start", "value": null, "metadata": {"file": "train.py", "lineno": 541, "epoch_num": 9}}
0: Starting epoch 8
0: Sampler for epoch 8 uses seed 1047927207
0: TRAIN [8][0/241]	Time 0.475 (0.475)	Data 2.72e-01 (2.72e-01)	Tok/s 1321 (1321)	Loss/tok 2.6074 (2.6074)	LR 1.250e-03
0: TRAIN [8][10/241]	Time 0.025 (0.067)	Data 6.65e-05 (2.48e-02)	Tok/s 25675 (26345)	Loss/tok 3.3047 (3.0072)	LR 1.250e-03
0: TRAIN [8][20/241]	Time 0.025 (0.047)	Data 6.60e-05 (1.30e-02)	Tok/s 23717 (25197)	Loss/tok 2.5644 (2.9694)	LR 1.250e-03
0: TRAIN [8][30/241]	Time 0.032 (0.041)	Data 6.82e-05 (8.84e-03)	Tok/s 45130 (29717)	Loss/tok 3.6396 (3.0083)	LR 1.250e-03
0: TRAIN [8][40/241]	Time 0.028 (0.038)	Data 6.46e-05 (6.70e-03)	Tok/s 35845 (31500)	Loss/tok 2.8003 (2.9978)	LR 1.250e-03
0: Gradient norm: inf
0: Skipped batch, new scale: 32.0
0: TRAIN [8][50/241]	Time 0.036 (0.036)	Data 6.63e-05 (5.40e-03)	Tok/s 52869 (31094)	Loss/tok 3.0895 (3.0102)	LR 1.250e-03
0: TRAIN [8][60/241]	Time 0.028 (0.034)	Data 6.51e-05 (4.53e-03)	Tok/s 37836 (31619)	Loss/tok 3.0645 (3.0274)	LR 1.250e-03
0: TRAIN [8][70/241]	Time 0.028 (0.033)	Data 6.60e-05 (3.90e-03)	Tok/s 33818 (31729)	Loss/tok 2.7962 (3.0341)	LR 1.250e-03
0: TRAIN [8][80/241]	Time 0.025 (0.032)	Data 6.87e-05 (3.43e-03)	Tok/s 25116 (31724)	Loss/tok 3.0743 (3.0218)	LR 1.250e-03
0: TRAIN [8][90/241]	Time 0.025 (0.032)	Data 7.10e-05 (3.06e-03)	Tok/s 24762 (31672)	Loss/tok 3.1712 (3.0224)	LR 1.250e-03
0: TRAIN [8][100/241]	Time 0.025 (0.031)	Data 6.79e-05 (2.76e-03)	Tok/s 24187 (31834)	Loss/tok 2.8578 (3.0196)	LR 6.250e-04
0: TRAIN [8][110/241]	Time 0.025 (0.031)	Data 6.79e-05 (2.52e-03)	Tok/s 25481 (32129)	Loss/tok 2.8487 (3.0316)	LR 6.250e-04
0: TRAIN [8][120/241]	Time 0.036 (0.031)	Data 6.77e-05 (2.32e-03)	Tok/s 51101 (32191)	Loss/tok 3.5108 (3.0427)	LR 6.250e-04
0: TRAIN [8][130/241]	Time 0.036 (0.031)	Data 6.48e-05 (2.15e-03)	Tok/s 53075 (32521)	Loss/tok 3.3831 (3.0489)	LR 6.250e-04
0: TRAIN [8][140/241]	Time 0.028 (0.030)	Data 7.06e-05 (2.00e-03)	Tok/s 36078 (32620)	Loss/tok 3.3925 (3.0470)	LR 6.250e-04
0: TRAIN [8][150/241]	Time 0.036 (0.030)	Data 7.08e-05 (1.87e-03)	Tok/s 53489 (32699)	Loss/tok 3.2867 (3.0535)	LR 6.250e-04
0: TRAIN [8][160/241]	Time 0.028 (0.030)	Data 6.37e-05 (1.76e-03)	Tok/s 37008 (32628)	Loss/tok 3.4604 (3.0500)	LR 6.250e-04
0: TRAIN [8][170/241]	Time 0.025 (0.030)	Data 6.63e-05 (1.66e-03)	Tok/s 26328 (32516)	Loss/tok 2.7659 (3.0508)	LR 6.250e-04
0: Upscaling, new scale: 64.0
0: TRAIN [8][180/241]	Time 0.032 (0.030)	Data 6.91e-05 (1.57e-03)	Tok/s 45807 (32692)	Loss/tok 2.9335 (3.0524)	LR 6.250e-04
0: TRAIN [8][190/241]	Time 0.025 (0.029)	Data 6.89e-05 (1.49e-03)	Tok/s 26311 (32383)	Loss/tok 2.5240 (3.0485)	LR 6.250e-04
0: TRAIN [8][200/241]	Time 0.028 (0.029)	Data 7.06e-05 (1.42e-03)	Tok/s 36781 (32545)	Loss/tok 2.9917 (3.0559)	LR 6.250e-04
0: TRAIN [8][210/241]	Time 0.025 (0.029)	Data 6.60e-05 (1.36e-03)	Tok/s 26297 (32518)	Loss/tok 3.0768 (3.0526)	LR 6.250e-04
0: TRAIN [8][220/241]	Time 0.025 (0.029)	Data 7.39e-05 (1.30e-03)	Tok/s 25814 (32505)	Loss/tok 2.9423 (3.0483)	LR 6.250e-04
0: TRAIN [8][230/241]	Time 0.032 (0.029)	Data 6.79e-05 (1.25e-03)	Tok/s 43682 (32435)	Loss/tok 3.2324 (3.0447)	LR 6.250e-04
0: TRAIN [8][240/241]	Time 0.032 (0.029)	Data 4.48e-05 (1.20e-03)	Tok/s 47191 (32375)	Loss/tok 2.9208 (3.0405)	LR 6.250e-04
:::MLLOG {"namespace": "", "time_ms": 1592880142848, "event_type": "INTERVAL_END", "key": "epoch_stop", "value": null, "metadata": {"file": "train.py", "lineno": 551, "epoch_num": 9}}
:::MLLOG {"namespace": "", "time_ms": 1592880142848, "event_type": "INTERVAL_START", "key": "eval_start", "value": null, "metadata": {"file": "train.py", "lineno": 556, "epoch_num": 9}}
0: Running evaluation on test set
0: TEST [8][0/1]	Time 0.246 (0.246)	Decoder iters 106.0 (106.0)	Tok/s 2926 (2926)
0: Running moses detokenizer
0: BLEU(score=24.201162143250222, counts=[36903, 18485, 10570, 6272], totals=[64853, 61850, 58847, 55850], precisions=[56.90253342173839, 29.88682295877122, 17.961833228541813, 11.230080572963294], bp=1.0, sys_len=64853, ref_len=64676)
0: Target accuracy reached
0: Finished evaluation on test set
:::MLLOG {"namespace": "", "time_ms": 1592880143211, "event_type": "POINT_IN_TIME", "key": "eval_accuracy", "value": 0.242, "metadata": {"file": "train.py", "lineno": 562, "epoch_num": 9}}
:::MLLOG {"namespace": "", "time_ms": 1592880143211, "event_type": "INTERVAL_END", "key": "eval_stop", "value": null, "metadata": {"file": "train.py", "lineno": 565, "epoch_num": 9}}
0: Summary: Epoch: 8	Training Loss: 3.0414	Test BLEU: 24.20
0: Performance: Epoch: 8	Training: 33166510 Tok/s
0: Finished epoch 8
:::MLLOG {"namespace": "", "time_ms": 1592880143212, "event_type": "INTERVAL_END", "key": "block_stop", "value": null, "metadata": {"file": "train.py", "lineno": 584, "first_epoch_num": 9}}
0: Closing preprocessed data file
:::MLLOG {"namespace": "", "time_ms": 1592880143212, "event_type": "INTERVAL_END", "key": "run_stop", "value": null, "metadata": {"file": "train.py", "lineno": 595, "status": "success"}}
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ set +x
+ set +x
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ set +x
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ set +x
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ set +x
+ set +x
+ set +x
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ set +x
+ set +x
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ set +x
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ set +x
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ set +x
+ set +x
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ set +x
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ set +x
+ set +x
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ set +x
+ set +x
+ set +x
+ set +x
+ set +x
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ set +x
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ set +x
+ set +x
+ set +x
+ set +x
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ set +x
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ set +x
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ set +x
+ set +x
+ set +x
+ set +x
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ set +x
+ set +x
+ set +x
+ set +x
+ set +x
+ set +x
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ set +x
+ set +x
+ set +x
+ set +x
+ set +x
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ set +x
+ set +x
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ set +x
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ set +x
+ set +x
+ set +x
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ set +x
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ set +x
+ set +x
ENDING TIMING RUN AT 2020-06-22 07:42:29 PM
RESULT,RNN_TRANSLATOR,,107,nvidia,2020-06-22 07:40:42 PM
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ set +x
+ set +x
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ set +x
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ set +x
+ set +x
+ set +x
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ set +x
+ set +x
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ set +x
ENDING TIMING RUN AT 2020-06-22 07:42:29 PM
RESULT,RNN_TRANSLATOR,,107,nvidia,2020-06-22 07:40:42 PM
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
ENDING TIMING RUN AT 2020-06-22 07:42:29 PM
RESULT,RNN_TRANSLATOR,,107,nvidia,2020-06-22 07:40:42 PM
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ set +x
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ set +x
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
+ set +x
+ set +x
+ ret_code=0
+ set +x
ENDING TIMING RUN AT 2020-06-22 07:42:30 PM
RESULT,RNN_TRANSLATOR,,108,nvidia,2020-06-22 07:40:42 PM
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ ret_code=0
+ ret_code=0
+ set +x
+ set +x
ENDING TIMING RUN AT 2020-06-22 07:42:30 PM
RESULT,RNN_TRANSLATOR,,108,nvidia,2020-06-22 07:40:42 PM
+ set +x
ENDING TIMING RUN AT 2020-06-22 07:42:30 PM
RESULT,RNN_TRANSLATOR,,108,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:30 PM
RESULT,RNN_TRANSLATOR,,108,nvidia,2020-06-22 07:40:42 PM
+ ret_code=0
+ set +x
ENDING TIMING RUN AT 2020-06-22 07:42:30 PM
RESULT,RNN_TRANSLATOR,,108,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:30 PM
RESULT,RNN_TRANSLATOR,,108,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:30 PM
RESULT,RNN_TRANSLATOR,,108,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:30 PM
RESULT,RNN_TRANSLATOR,,108,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:30 PM
RESULT,RNN_TRANSLATOR,,108,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:30 PM
RESULT,RNN_TRANSLATOR,,108,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:30 PM
RESULT,RNN_TRANSLATOR,,108,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:30 PM
RESULT,RNN_TRANSLATOR,,108,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:30 PM
RESULT,RNN_TRANSLATOR,,108,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:30 PM
RESULT,RNN_TRANSLATOR,,108,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:30 PM
RESULT,RNN_TRANSLATOR,,108,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:30 PM
RESULT,RNN_TRANSLATOR,,108,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:30 PM
RESULT,RNN_TRANSLATOR,,108,nvidia,2020-06-22 07:40:42 PM
+ ret_code=0
+ set +x
ENDING TIMING RUN AT 2020-06-22 07:42:30 PM
RESULT,RNN_TRANSLATOR,,108,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:30 PM
RESULT,RNN_TRANSLATOR,,108,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:30 PM
RESULT,RNN_TRANSLATOR,,108,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:30 PM
RESULT,RNN_TRANSLATOR,,108,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:30 PM
RESULT,RNN_TRANSLATOR,,108,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:30 PM
RESULT,RNN_TRANSLATOR,,108,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:30 PM
RESULT,RNN_TRANSLATOR,,108,nvidia,2020-06-22 07:40:42 PM
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
ENDING TIMING RUN AT 2020-06-22 07:42:30 PM
RESULT,RNN_TRANSLATOR,,108,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:30 PM
RESULT,RNN_TRANSLATOR,,108,nvidia,2020-06-22 07:40:42 PM
+ ret_code=0
+ set +x
ENDING TIMING RUN AT 2020-06-22 07:42:30 PM
RESULT,RNN_TRANSLATOR,,108,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:30 PM
RESULT,RNN_TRANSLATOR,,108,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:30 PM
RESULT,RNN_TRANSLATOR,,108,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:30 PM
RESULT,RNN_TRANSLATOR,,108,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:30 PM
RESULT,RNN_TRANSLATOR,,108,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:30 PM
RESULT,RNN_TRANSLATOR,,108,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:30 PM
RESULT,RNN_TRANSLATOR,,108,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:30 PM
RESULT,RNN_TRANSLATOR,,108,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:30 PM
ENDING TIMING RUN AT 2020-06-22 07:42:30 PM
RESULT,RNN_TRANSLATOR,,108,nvidia,2020-06-22 07:40:42 PM
RESULT,RNN_TRANSLATOR,,108,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:30 PM
RESULT,RNN_TRANSLATOR,,108,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:30 PM
RESULT,RNN_TRANSLATOR,,108,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:30 PM
RESULT,RNN_TRANSLATOR,,108,nvidia,2020-06-22 07:40:42 PM
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
ENDING TIMING RUN AT 2020-06-22 07:42:30 PM
RESULT,RNN_TRANSLATOR,,108,nvidia,2020-06-22 07:40:42 PM
+ ret_code=0
+ set +x
ENDING TIMING RUN AT 2020-06-22 07:42:30 PM
RESULT,RNN_TRANSLATOR,,108,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:30 PM
RESULT,RNN_TRANSLATOR,,108,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:30 PM
RESULT,RNN_TRANSLATOR,,108,nvidia,2020-06-22 07:40:42 PM
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
ENDING TIMING RUN AT 2020-06-22 07:42:30 PM
RESULT,RNN_TRANSLATOR,,108,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:30 PM
RESULT,RNN_TRANSLATOR,,108,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:30 PM
RESULT,RNN_TRANSLATOR,,108,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:30 PM
RESULT,RNN_TRANSLATOR,,108,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:30 PM
RESULT,RNN_TRANSLATOR,,108,nvidia,2020-06-22 07:40:42 PM
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
ENDING TIMING RUN AT 2020-06-22 07:42:30 PM
RESULT,RNN_TRANSLATOR,,108,nvidia,2020-06-22 07:40:42 PM
+ ret_code=0
+ set +x
ENDING TIMING RUN AT 2020-06-22 07:42:30 PM
RESULT,RNN_TRANSLATOR,,108,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:30 PM
RESULT,RNN_TRANSLATOR,,108,nvidia,2020-06-22 07:40:42 PM
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
ENDING TIMING RUN AT 2020-06-22 07:42:30 PM
RESULT,RNN_TRANSLATOR,,108,nvidia,2020-06-22 07:40:42 PM
+ ret_code=0
+ set +x
ENDING TIMING RUN AT 2020-06-22 07:42:30 PM
RESULT,RNN_TRANSLATOR,,108,nvidia,2020-06-22 07:40:42 PM
+ ret_code=0
+ set +x
ENDING TIMING RUN AT 2020-06-22 07:42:30 PM
RESULT,RNN_TRANSLATOR,,108,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:30 PM
RESULT,RNN_TRANSLATOR,,108,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:30 PM
RESULT,RNN_TRANSLATOR,,108,nvidia,2020-06-22 07:40:42 PM
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
ENDING TIMING RUN AT 2020-06-22 07:42:30 PM
RESULT,RNN_TRANSLATOR,,108,nvidia,2020-06-22 07:40:42 PM
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
ENDING TIMING RUN AT 2020-06-22 07:42:30 PM
RESULT,RNN_TRANSLATOR,,108,nvidia,2020-06-22 07:40:42 PM
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
ENDING TIMING RUN AT 2020-06-22 07:42:30 PM
RESULT,RNN_TRANSLATOR,,108,nvidia,2020-06-22 07:40:42 PM
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
ENDING TIMING RUN AT 2020-06-22 07:42:30 PM
RESULT,RNN_TRANSLATOR,,108,nvidia,2020-06-22 07:40:42 PM
+ ret_code=0
+ set +x
+ ret_code=0
+ set +x
ENDING TIMING RUN AT 2020-06-22 07:42:30 PM
RESULT,RNN_TRANSLATOR,,108,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
slurmstepd: error: _is_a_lwp: open() /proc/47001/status failed: No such file or directory
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
slurmstepd: error: _is_a_lwp: open() /proc/75636/status failed: No such file or directory
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
slurmstepd: error: _is_a_lwp: open() /proc/96323/status failed: No such file or directory
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
slurmstepd: error: _is_a_lwp: open() /proc/4080/status failed: No such file or directory
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:32 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:33 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:33 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:33 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:33 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:33 PM
ENDING TIMING RUN AT 2020-06-22 07:42:33 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:33 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:33 PM
ENDING TIMING RUN AT 2020-06-22 07:42:33 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
RESULT,RNN_TRANSLATOR,,110,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:33 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:33 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:33 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:33 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:33 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:33 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:33 PM
ENDING TIMING RUN AT 2020-06-22 07:42:33 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 07:40:42 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:33 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:33 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:33 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:33 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:33 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:33 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:33 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:33 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:33 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:33 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:33 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:33 PM
ENDING TIMING RUN AT 2020-06-22 07:42:33 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 07:40:42 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:33 PM
ENDING TIMING RUN AT 2020-06-22 07:42:33 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 07:40:42 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:33 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:33 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:33 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:33 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:33 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:33 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:33 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:33 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:33 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:33 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:33 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:33 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:33 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:33 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:33 PM
ENDING TIMING RUN AT 2020-06-22 07:42:33 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:33 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:33 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:33 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 07:40:42 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:33 PM
ENDING TIMING RUN AT 2020-06-22 07:42:33 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 07:40:42 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:33 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:33 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:33 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:33 PM
ENDING TIMING RUN AT 2020-06-22 07:42:33 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 07:40:42 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:33 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:33 PM
ENDING TIMING RUN AT 2020-06-22 07:42:33 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 07:40:42 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:33 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:33 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:33 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:33 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:33 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:33 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:33 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:33 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:33 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:33 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:33 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:33 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:33 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:33 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:33 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:33 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:33 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:33 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:33 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:33 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:33 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:33 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:33 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:33 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:33 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:33 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:33 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:33 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:33 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:33 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:33 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:33 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:33 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:33 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:33 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:33 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:33 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:33 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:33 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:33 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:33 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:33 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:33 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:33 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:33 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:33 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:33 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:33 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:33 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:33 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:33 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:33 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:33 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:33 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:33 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:33 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:33 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:33 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:33 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:33 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:33 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:33 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:33 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:33 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:33 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:33 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:33 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 07:40:42 PM
ENDING TIMING RUN AT 2020-06-22 07:42:33 PM
RESULT,RNN_TRANSLATOR,,111,nvidia,2020-06-22 07:40:42 PM
